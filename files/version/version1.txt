.dvcignore
/tests
/dvc
/.github




.flake8
[flake8]
ignore=
    # Whitespace before ':'
    E203
    # Too many leading '#' for block comment
    E266
    # Line break occurred before a binary operator
    W503
    # unindexed parameters in the str.format, see:
    # https://pypi.org/project/flake8-string-format/
    P1
    # use `!r` instead of surrounding by quotes
    B028
max_line_length = 88
max-complexity = 10
select = B,C,E,F,W,T4,B902,T,P,TYP
show_source = true
count = true
min_python_version = 3.8.0




.git_blame-ignore-revs
# The following commits will be ignored by the GitHub blame view.
# You can configure `git blame` to use this file as the default ignore file:
#     git config blame.ignoreRevsFile .git-blame-ignore-revs
# or, you can also manually ignore commits from this file by doing:
#     git blame --ignore-revs-file .git-blame-ignore-revs <filepath>
# see the `blame.markIgnoredLines` and `blame.markUnblamableLines` options as well.
# Refer to: https://git-scm.com/docs/git-blame#Documentation/git-blame.txt---ignore-revs-fileltfilegt

# update black to 2023 stable style and change line-length to 88
a86470ccbc693d6f0a4f8066cbd1357e5191c4ff




.git_archival.txt
node: 654358010076d1c20d90edbab39733cc27c598e0
node-date: 2023-05-31T23:14:28+03:00
describe-name: 3.0.0a0-17-g6543580100
ref-names: 




.gitattributes
.git_archival.txt  export-subst




.gitignore
__pycache__/
neatlynx.conf
.idea
/cache
*.pyc

.env*/
.venv
env/
venv/
.python-version

.dvc.conf.lock
.DS_Store
build/
dist/

*.egg-info/
.eggs

hooks/*.pyc

*.rpm
*.deb

innosetup/config.ini

*.exe

.coverage
.coverage.*
coverage.xml

*.sw?

pip-wheel-metadata/
.vscode/

azurite
env.sh
tests/remotes_env
scripts/ci/gcp-creds.json
.gcp-creds.json

*~
/dvc/_dvc_version.py

.mypy_cache/
.pytest_cache/
.nox/
.tox/
htmlcov/




.mailmap
Pawe≈Ç Redzy≈Ñski <pawelredzynski@gmail.com>
Dmitry Petrov <dmitry.petrov@nevesomo.com>
Earl Hathaway <github@earlh.com>
Nabanita Dash <dashnabanita@gmail.com>
Kurian Benoy <kurian.bkk@gmail.com>
Sritanu Chakraborty <sritanu25@gmail.com>




.pre-commit-config.yaml
default_language_version:
  python: python3

ci:
  skip: [mypy, pylint, dvc-pre-commit]

repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: check-added-large-files
      - id: check-case-conflict
      - id: check-executables-have-shebangs
      - id: check-json
      - id: check-merge-conflict
        exclude: "tests/func/utils/test_strict_yaml.py"
        args: ['--assume-in-merge']
      - id: check-toml
      - id: check-yaml
      - id: debug-statements
        exclude: "dvc/_debug.py"
      - id: end-of-file-fixer
      - id: mixed-line-ending
      - id: sort-simple-yaml
      - id: trailing-whitespace
  - repo: https://github.com/lovesegfault/beautysh
    rev: v6.2.1
    hooks:
      - id: beautysh
        args: ["-i", "2"]
  - repo: https://github.com/asottile/pyupgrade
    rev: v3.3.1
    hooks:
      - id: pyupgrade
        args:
          - --py38-plus
  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black
  - repo: https://github.com/PyCQA/isort
    rev: 5.12.0
    hooks:
      - id: isort
  - repo: https://github.com/charliermarsh/ruff-pre-commit
    rev: 'v0.0.261'
    hooks:
      - id: ruff
  - repo: https://github.com/codespell-project/codespell
    rev: v2.2.4
    hooks:
      - id: codespell
        additional_dependencies: ["tomli"]
  - repo: https://github.com/PyCQA/flake8
    rev: 6.0.0
    hooks:
      - id: flake8
        additional_dependencies:
          - flake8-bugbear
          - flake8-comprehensions
          - flake8-debugger
          - flake8-string-format
          - flake8-typing-imports
  - repo: https://github.com/PyCQA/bandit
    rev: '1.7.5'
    hooks:
    - id: bandit
      args: ["-c", "pyproject.toml", "--quiet", "--format=custom"]
      files: ^dvc/
      additional_dependencies: [".[toml]"]
  - repo: local
    hooks:
    - id: mypy
      name: mypy
      entry: mypy
      files: ^dvc/
      language: system
      types: [python]
      require_serial: true
    - id: pylint
      name: pylint
      entry: pylint
      files: ^dvc/
      language: system
      types: [python]
      require_serial: true
  - hooks:
      - args:
          - git-hook
          - pre-commit
        entry: dvc
        id: dvc-pre-commit
        language: system
        name: DVC pre-commit
        stages:
          - commit
        verbose: true
        require_serial: true
      - args:
          - git-hook
          - pre-push
        entry: dvc
        id: dvc-pre-push
        language: system
        name: DVC pre-push
        stages:
          - push
        require_serial: true
      - always_run: true
        args:
          - git-hook
          - post-checkout
        entry: dvc
        id: dvc-post-checkout
        language: system
        minimum_pre_commit_version: 2.2.0
        name: DVC post-checkout
        stages:
          - post-checkout
        require_serial: true
    repo: local




.pre-commit-hooks.yaml
- args:
  - git-hook
  - pre-commit
  entry: dvc
  id: dvc-pre-commit
  language: python
  language_version: python3
  name: DVC pre-commit
  require_serial: true
  stages:
  - commit
  verbose: true
- args:
  - git-hook
  - pre-push
  entry: dvc
  id: dvc-pre-push
  language: python
  language_version: python3
  name: DVC pre-push
  require_serial: true
  stages:
  - push
- always_run: true
  args:
  - git-hook
  - post-checkout
  entry: dvc
  id: dvc-post-checkout
  language: python
  language_version: python3
  minimum_pre_commit_version: 2.2.0
  name: DVC post-checkout
  require_serial: true
  stages:
  - post-checkout




.zenodo.json
{
  "title": "DVC: Data Version Control - Git for Data & Models",
  "keywords": [
    "data-science", "data-version-control", "machine-learning", "git",
    "developer-tools", "reproducibility", "collaboration", "ai", "python"],
  "contributors": [
    {"name": "DVC team", "type": "Other", "affiliation": "Iterative"}]
}




CODE_OF_CONDUCT.md
# Contributor Covenant Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, sex characteristics, gender identity and expression,
level of experience, education, socio-economic status, nationality, personal
appearance, race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment
include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or
  advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic
  address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

## Scope

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event. Representation of a project may be
further defined and clarified by project maintainers.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the project team at info@dvc.org. All
complaints will be reviewed and investigated and will result in a response that
is deemed necessary and appropriate to the circumstances. The project team is
obligated to maintain confidentiality with regard to the reporter of an incident.
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,
available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see
https://www.contributor-covenant.org/faq




CONTRIBUTING.md
### See our contribution guide at [dvc.org](https://dvc.org/doc/user-guide/contributing/core).




LICENSE
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "{}"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright 2017-2021 Iterative, Inc.

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.




pyproject.toml
[build-system]
build-backend = "setuptools.build_meta"
requires = ["setuptools>=61", "setuptools_scm[toml]>=7"]

[project]
name = "dvc"
description = "Git for data scientists - manage your code and data together"
readme = "README.rst"
keywords = [
    "ai",
    "collaboration",
    "data-science",
    "data-version-control",
    "developer-tools",
    "git",
    "machine-learning",
    "reproducibility",
]
license = { text = "Apache License 2.0" }
maintainers = [{ name = "Iterative", email = "support@dvc.org" }]
authors = [{ name = "Dmitry Petrov", email = "dmitry@dvc.org" }]
requires-python = ">=3.8"
classifiers = [
    "Development Status :: 4 - Beta",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
]
dynamic = ["version"]
dependencies = [
    "colorama>=0.3.9",
    "configobj>=5.0.6",
    "distro>=1.3",
    "dpath<3,>=2.1.0",
    "dvc-data>=0.53.0,<0.54",
    "dvc-http>=2.29.0",
    "dvc-render>=0.3.1,<1",
    "dvc-studio-client>=0.9.2,<1",
    "dvc-task>=0.2.1,<1",
    "flatten_dict<1,>=0.4.1",
    "flufl.lock>=5",
    "funcy>=1.14",
    "grandalf<1,>=0.7",
    "hydra-core>=1.1",
    "iterative-telemetry>=0.0.7",
    "networkx>=2.5",
    "packaging>=19",
    "pathspec>=0.10.3",
    "platformdirs<4,>=3.1.1",
    "psutil>=5.8",
    "pydot>=1.2.4",
    "pygtrie>=2.3.2",
    "pyparsing>=2.4.7",
    "requests>=2.22",
    "rich>=12",
    "ruamel.yaml>=0.17.11",
    "scmrepo>=1.0.0,<2",
    "shortuuid>=0.5",
    "shtab<2,>=1.3.4",
    "tabulate>=0.8.7",
    "tomlkit>=0.11.1",
    "tqdm<5,>=4.63.1",
    "voluptuous>=0.11.7",
    "zc.lockfile>=1.2.1",
]
[project.optional-dependencies]
all = ["dvc[azure,gdrive,gs,hdfs,oss,s3,ssh,webdav,webhdfs]"]
azure = ["dvc-azure>=2.21.2"]
dev = ["dvc[azure,gdrive,gs,hdfs,lint,oss,s3,ssh,terraform,tests,webdav,webhdfs]"]
gdrive = ["dvc-gdrive==2.19.2"]
gs = ["dvc-gs==2.22.1"]
hdfs = ["dvc-hdfs==2.19"]
lint = [
    "mypy==1.3.0",
    "pylint==2.17.4",
    "types-colorama",
    "types-psutil",
    "types-requests",
    "types-tabulate",
    "types-toml",
    "types-tqdm",
]
oss = ["dvc-oss==2.19"]
s3 = ["dvc-s3==2.22.0"]
ssh = ["dvc-ssh>=2.22.1,<3"]
ssh_gssapi = ["dvc-ssh[gssapi]>=2.22.1,<3"]
terraform = ["tpi[ssh]>=2.1"]
testing = [
  "pytest-test-utils",
  "pytest-benchmark[histogram]",
  "pytest-virtualenv",
]
tests = [
    "dvc[testing]",
    "beautifulsoup4>=4.4",
    "dvc-ssh",
    "filelock",
    "flaky",
    "pytest<8,>=7",
    "pytest-cov",
    "pytest-docker>=1,<2",
    "pytest-lazy-fixture",
    "pytest-mock",
    "pytest-test-utils",
    "pytest-timeout>=2",
    "pytest-xdist>=3.2",
    'pywin32>=225; sys_platform == "win32"', # optional test dependency
]
webdav = ["dvc-webdav==2.19.1"]
webhdfs = ["dvc-webhdfs==2.19"]
webhdfs_kerberos = ["dvc-webhdfs[kerberos]==2.19"]
[project.urls]
Documentation = "https://dvc.org/doc"
Issues = "https://github.com/iterative/dvc/issues"
Source = "https://github.com/iterative/dvc"
[project.scripts]
dvc = "dvc.cli:main"
[project.entry-points."fsspec.specs"]
dvc = "dvc.api:DVCFileSystem"
[project.entry-points."pytest11"]
dvc-testing = "dvc.testing.plugin"

[tool.setuptools]
license-files = ["LICENSE"]

[tool.setuptools.packages.find]
exclude = ["tests", "tests.*"]
namespaces = false

[tool.setuptools_scm]
write_to = "dvc/_dvc_version.py"

[tool.isort]
known_first_party = ["dvc", "dvc_data", "dvc_objects", "dvc_render", "dvc_task", "tests"]
profile = "black"

[tool.pytest.ini_options]
addopts = "-ra --cov-config pyproject.toml --dist worksteal"
filterwarnings = [
    "error::ResourceWarning",
    "error::pytest.PytestUnraisableExceptionWarning",
    "error::pytest_mock.PytestMockWarning",
    # remove when aiobotocore supports botocore>=1.29.13
    "ignore:'cgi' is deprecated and slated for removal in Python 3.13:DeprecationWarning",
    # also relates to botocore, but looks like it's not going to be fixed
    # https://github.com/boto/botocore/issues/2744
    "ignore:'urllib3.contrib.pyopenssl' module is deprecated:DeprecationWarning",
    # ruamel.yaml: https://sourceforge.net/p/ruamel-yaml/tickets/452/
    # google.cloud: https://github.com/googleapis/python-storage/issues/1000
    # google.logging: https://github.com/googleapis/python-logging/issues/730
    # Also happens with `zc.lockfile`.
    "ignore:Deprecated call to `pkg_resources.declare_namespace:DeprecationWarning",
    # see https://github.com/celery/kombu/issues/1339
    # and https://github.com/celery/celery/issues/7528
    # drop when celery==5.3 && kombu==5.3 releases
    "ignore:SelectableGroups dict interface is deprecated:DeprecationWarning",
    # tpi imports pkg_resources
    "ignore:pkg_resources is deprecated as an API:DeprecationWarning",
    # see https://github.com/networkx/networkx/issues/5723.
    "ignore:nx.nx_pydot.* depends on the pydot package, which has.*known issues and is not actively maintained:DeprecationWarning",
    # remove when new version of pytest-cov gets released
    # https://github.com/pytest-dev/pytest-cov/issues/557
    "ignore:The --rsyncdir command line argument and rsyncdirs config variable are deprecated:DeprecationWarning",
]
log_level = "debug"
markers = [
    "needs_internet: Might need network access for the tests",
    "studio: Tests verifying contract between DVC and Studio",
    "vscode: Tests verifying contract between DVC and VSCode plugin",
]
testpaths = ["tests"]
xfail_strict = true

[tool.coverage.run]
branch = true
source = ["dvc", "tests"]

[tool.coverage.paths]
source = ["dvc"]

[tool.coverage.report]
exclude_lines = [
    "if __name__ == .__main__.:",
    "if TYPE_CHECKING:",
    "if typing.TYPE_CHECKING:",
    "@overload",
    "pragma: no cover",
    "raise AssertionError",
    "raise NotImplementedError",
]
show_missing = true

[tool.mypy]
check_untyped_defs = true
files = ["dvc"]
no_implicit_optional = true
pretty = true
show_column_numbers = true
show_error_codes = true
show_error_context = true
show_traceback = true
strict_equality = true
strict_concatenate = true
warn_no_return = true
warn_redundant_casts = true
warn_unreachable = true
warn_unused_configs = true

[[tool.mypy.overrides]]
ignore_missing_imports = true
module = [
    "celery.*",
    "configobj.*",
    "dpath.*",
    "distro",
    "dvc_http",
    "dvc_render.*",
    "dvc_ssh",
    "dvc_studio_client.*",
    "flatten_dict",
    "fsspec.*",
    "funcy.*",  # https://github.com/Suor/funcy/issues/106,
    "grandalf.*",
    "ipdb",
    "iterative_telemetry",
    "kombu.*",
    "networkx.*",  # https://github.com/networkx/networkx/issues/3988
    "pygtrie.*",
    "pyinstrument",
    "pyparsing",
    "pytest_benchmark.*",
    "pytest_docker.plugin",
    "pytest_virtualenv.*",
    "ruamel.*",
    "ruamel.yaml.*",
    "shortuuid",
    "shtab",
    "tpi.*",
    "viztracer",
    "voluptuous",
    "yappi",
    "zc.*",
]

[tool.codespell]
ignore-words-list = "ba,datas,fo,uptodate,cachable,falsy"

[tool.pylint.message_control]
disable = [
    "cyclic-import", "design", "duplicate-code", "fixme", "format", "import-outside-toplevel", "invalid-name",
    "missing-class-docstring", "missing-function-docstring", "missing-module-docstring", "multiple-imports",
    "raise-missing-from", "refactoring", "spelling",
    "ungrouped-imports", "unused-wildcard-import", "wrong-import-order", "wrong-import-position",
]
enable = ["c-extension-no-member", "no-else-return"]

[tool.pylint.typecheck]
generated-members = ["argparse.Namespace", "logger.trace", "logging.TRACE", "pytest.lazy_fixture", "sys.getwindowsversion"]
signature-mutators = ["funcy.decorators.decorator"]

[tool.pylint.variables]
dummy-variables-rgx = "_+$|(_[a-zA-Z0-9_]*[a-zA-Z0-9]+?$)"
ignored-argument-names = "_.*|args|kwargs"

[tool.ruff]
# external flake8 codes that should be preserved
external = ["B301", "C901", "E302", "W601"]
ignore = ["N818", "S101", "PT004", "PT007", "PT019", "SIM105", "SIM108", "SIM110", "SIM117", "TRY003", "TRY200", "TRY300", "TRY301", "PLR2004", "PLW2901", "RUF005"]
select = ["F", "E", "W", "C90", "N", "UP", "YTT", "S", "BLE", "B", "A", "C4", "T10", "EXE", "ISC", "ICN", "G", "INP", "PIE", "T20", "PT", "Q", "RET501", "RET504", "SIM", "TID", "TCH", "ARG", "PGH", "PLC", "PLE", "PLR", "PLW", "TRY", "RUF"]
show-source = true

[tool.ruff.flake8-pytest-style]
fixture-parentheses = false
mark-parentheses = false
parametrize-names-type = "csv"
raises-extend-require-match-for = ["dvc.exceptions.DvcException", "dvc.scm.SCMError", "scmrepo.exceptions.SCMError"]

[tool.ruff.flake8-tidy-imports]
[tool.ruff.flake8-tidy-imports.banned-api]
"funcy.cached_property" = {msg = "use `from dvc.utils.objects import cached_property` instead."}

[tool.ruff.flake8-type-checking]
strict = true

[tool.ruff.flake8-unused-arguments]
ignore-variadic-names = true

[tool.ruff.isort]
known-first-party = ["dvc", "dvc_data", "dvc_objects", "dvc_render", "dvc_task", "tests"]

[tool.ruff.pep8-naming]
ignore-names = ["M", "SCM"]

[tool.ruff.per-file-ignores]
"dvc/commands/**" = ["N806"]
"dvc/testing/**" = ["ARG002"]
"dvc/testing/benchmarks/**" = ["ARG001"]
"scripts/**" = ["T201", "INP001", "TRY002"]

[tool.ruff.pylint]
max-args = 10

[tool.bandit]
exclude_dirs = ["tests", "scripts", "dvc/repo/experiments/utils.py"]
skips = ["B101"]




README.rst
|Banner|

`Website <https://dvc.org>`_
‚Ä¢ `Docs <https://dvc.org/doc>`_
‚Ä¢ `Blog <http://blog.dataversioncontrol.com>`_
‚Ä¢ `Tutorial <https://dvc.org/doc/get-started>`_
‚Ä¢ `Related Technologies <https://dvc.org/doc/user-guide/related-technologies>`_
‚Ä¢ `How DVC works`_
‚Ä¢ `VS Code Extension`_
‚Ä¢ `Installation`_
‚Ä¢ `Contributing`_
‚Ä¢ `Community and Support`_

|CI| |Python Version| |Coverage| |VS Code| |DOI|

|PyPI| |PyPI Downloads| |Packages| |Brew| |Conda| |Choco| |Snap|

|

**Data Version Control** or **DVC** is a command line tool and `VS Code Extension`_ to help you develop reproducible machine learning projects:

#. **Version** your data and models.
   Store them in your cloud storage but keep their version info in your Git repo.

#. **Iterate** fast with lightweight pipelines.
   When you make changes, only run the steps impacted by those changes.

#. **Track** experiments in your local Git repo (no servers needed).

#. **Compare** any data, code, parameters, model, or performance plots.

#. **Share** experiments and automatically reproduce anyone's experiment.

Quick start
===========

    Please read our `Command Reference <https://dvc.org/doc/command-reference>`_ for a complete list.

A common CLI workflow includes:


+-----------------------------------+----------------------------------------------------------------------------------------------------+
| Task                              | Terminal                                                                                           |
+===================================+====================================================================================================+
| Track data                        | | ``$ git add train.py params.yaml``                                                               |
|                                   | | ``$ dvc add images/``                                                                            |
+-----------------------------------+----------------------------------------------------------------------------------------------------+
| Connect code and data             | | ``$ dvc stage add -n featurize -d images/ -o features/ python featurize.py``                     |
|                                   | | ``$ dvc stage add -n train -d features/ -d train.py -o model.p -M metrics.json python train.py`` |
+-----------------------------------+----------------------------------------------------------------------------------------------------+
| Make changes and experiment       | | ``$ dvc exp run -n exp-baseline``                                                                |
|                                   | | ``$ vi train.py``                                                                                |
|                                   | | ``$ dvc exp run -n exp-code-change``                                                             |
+-----------------------------------+----------------------------------------------------------------------------------------------------+
| Compare and select experiments    | | ``$ dvc exp show``                                                                               |
|                                   | | ``$ dvc exp apply exp-baseline``                                                                 |
+-----------------------------------+----------------------------------------------------------------------------------------------------+
| Share code                        | | ``$ git add .``                                                                                  |
|                                   | | ``$ git commit -m 'The baseline model'``                                                         |
|                                   | | ``$ git push``                                                                                   |
+-----------------------------------+----------------------------------------------------------------------------------------------------+
| Share data and ML models          | | ``$ dvc remote add myremote -d s3://mybucket/image_cnn``                                         |
|                                   | | ``$ dvc push``                                                                                   |
+-----------------------------------+----------------------------------------------------------------------------------------------------+

How DVC works
=============

    We encourage you to read our `Get Started
    <https://dvc.org/doc/get-started>`_ docs to better understand what DVC
    does and how it can fit your scenarios.

The closest *analogies* to describe the main DVC features are these:

#. **Git for data**: Store and share data artifacts (like Git-LFS but without a server) and models, connecting them with a Git repository. Data management meets GitOps!
#. **Makefiles** for ML: Describes how data or model artifacts are built from other data and code in a standard format. Now you can version your data pipelines with Git.
#. Local **experiment tracking**: Turn your machine into an ML experiment management platform, and collaborate with others using existing Git hosting (Github, Gitlab, etc.).

Git is employed as usual to store and version code (including DVC meta-files as placeholders for data).
DVC `stores data and model files <https://dvc.org/doc/start/data-management>`_ seamlessly in a cache outside of Git, while preserving almost the same user experience as if they were in the repo.
To share and back up the *data cache*, DVC supports multiple remote storage platforms - any cloud (S3, Azure, Google Cloud, etc.) or on-premise network storage (via SSH, for example).

|Flowchart|

`DVC pipelines <https://dvc.org/doc/start/data-management/data-pipelines>`_ (computational graphs) connect code and data together.
They specify all steps required to produce a model: input dependencies including code, data, commands to run; and output information to be saved.

Last but not least, `DVC Experiment Versioning <https://dvc.org/doc/start/experiments>`_ lets you prepare and run a large number of experiments.
Their results can be filtered and compared based on hyperparameters and metrics, and visualized with multiple plots.

.. _`VS Code Extension`:

Visual Studio Code Extension
============================

|VS Code|

To use DVC as a GUI right from your VS Code IDE, install the `DVC Extension <https://marketplace.visualstudio.com/items?itemName=Iterative.dvc>`_ from the Marketplace.
It currently features experiment tracking and data management, and more features (data pipeline support, etc.) are coming soon!

|VS Code Extension Overview|

    Note: You'll have to install core DVC on your system separately (as detailed
    below). The Extension will guide you if needed.

Installation
============

There are several ways to install DVC: in VS Code; using ``snap``, ``choco``, ``brew``, ``conda``, ``pip``; or with an OS-specific package.
Full instructions are `available here <https://dvc.org/doc/get-started/install>`_.

Snapcraft (Linux)
-----------------

|Snap|

.. code-block:: bash

   snap install dvc --classic

This corresponds to the latest tagged release.
Add ``--beta`` for the latest tagged release candidate, or ``--edge`` for the latest ``main`` version.

Chocolatey (Windows)
--------------------

|Choco|

.. code-block:: bash

   choco install dvc

Brew (mac OS)
-------------

|Brew|

.. code-block:: bash

   brew install dvc

Anaconda (Any platform)
-----------------------

|Conda|

.. code-block:: bash

   conda install -c conda-forge mamba # installs much faster than conda
   mamba install -c conda-forge dvc

Depending on the remote storage type you plan to use to keep and share your data, you might need to install optional dependencies: `dvc-s3`, `dvc-azure`, `dvc-gdrive`, `dvc-gs`, `dvc-oss`, `dvc-ssh`.

PyPI (Python)
-------------

|PyPI|

.. code-block:: bash

   pip install dvc

Depending on the remote storage type you plan to use to keep and share your data, you might need to specify one of the optional dependencies: ``s3``, ``gs``, ``azure``, ``oss``, ``ssh``. Or ``all`` to include them all.
The command should look like this: ``pip install 'dvc[s3]'`` (in this case AWS S3 dependencies such as ``boto3`` will be installed automatically).

To install the development version, run:

.. code-block:: bash

   pip install git+git://github.com/iterative/dvc

Package (Platform-specific)
---------------------------

|Packages|

Self-contained packages for Linux, Windows, and Mac are available.
The latest version of the packages can be found on the GitHub `releases page <https://github.com/iterative/dvc/releases>`_.

Ubuntu / Debian (deb)
^^^^^^^^^^^^^^^^^^^^^
.. code-block:: bash

   sudo wget https://dvc.org/deb/dvc.list -O /etc/apt/sources.list.d/dvc.list
   wget -qO - https://dvc.org/deb/iterative.asc | sudo apt-key add -
   sudo apt update
   sudo apt install dvc

Fedora / CentOS (rpm)
^^^^^^^^^^^^^^^^^^^^^
.. code-block:: bash

   sudo wget https://dvc.org/rpm/dvc.repo -O /etc/yum.repos.d/dvc.repo
   sudo rpm --import https://dvc.org/rpm/iterative.asc
   sudo yum update
   sudo yum install dvc

Contributing
============

|Maintainability|

Contributions are welcome!
Please see our `Contributing Guide <https://dvc.org/doc/user-guide/contributing/core>`_ for more details.
Thanks to all our contributors!

|Contribs|

Community and Support
=====================

* `Twitter <https://twitter.com/DVCorg>`_
* `Forum <https://discuss.dvc.org/>`_
* `Discord Chat <https://dvc.org/chat>`_
* `Email <mailto:support@dvc.org>`_
* `Mailing List <https://sweedom.us10.list-manage.com/subscribe/post?u=a08bf93caae4063c4e6a351f6&id=24c0ecc49a>`_

Copyright
=========

This project is distributed under the Apache license version 2.0 (see the LICENSE file in the project root).

By submitting a pull request to this project, you agree to license your contribution under the Apache license version 2.0 to this project.

Citation
========

|DOI|

Iterative, *DVC: Data Version Control - Git for Data & Models* (2020)
`DOI:10.5281/zenodo.012345 <https://doi.org/10.5281/zenodo.3677553>`_.

Barrak, A., Eghan, E.E. and Adams, B. `On the Co-evolution of ML Pipelines and Source Code - Empirical Study of DVC Projects <https://mcis.cs.queensu.ca/publications/2021/saner.pdf>`_ , in Proceedings of the 28th IEEE International Conference on Software Analysis, Evolution, and Reengineering, SANER 2021. Hawaii, USA.


.. |Banner| image:: https://dvc.org/img/logo-github-readme.png
   :target: https://dvc.org
   :alt: DVC logo

.. |VS Code Extension Overview| image:: https://raw.githubusercontent.com/iterative/vscode-dvc/main/extension/docs/overview.gif
   :alt: DVC Extension for VS Code

.. |CI| image:: https://github.com/iterative/dvc/workflows/Tests/badge.svg?branch=main
   :target: https://github.com/iterative/dvc/actions
   :alt: GHA Tests

.. |Maintainability| image:: https://codeclimate.com/github/iterative/dvc/badges/gpa.svg
   :target: https://codeclimate.com/github/iterative/dvc
   :alt: Code Climate

.. |Python Version| image:: https://img.shields.io/pypi/pyversions/dvc
   :target: https://pypi.org/project/dvc
   :alt: Python Version

.. |Coverage| image:: https://codecov.io/gh/iterative/dvc/branch/main/graph/badge.svg
   :target: https://codecov.io/gh/iterative/dvc
   :alt: Codecov

.. |Snap| image:: https://img.shields.io/badge/snap-install-82BEA0.svg?logo=snapcraft
   :target: https://snapcraft.io/dvc
   :alt: Snapcraft

.. |Choco| image:: https://img.shields.io/chocolatey/v/dvc?label=choco
   :target: https://chocolatey.org/packages/dvc
   :alt: Chocolatey

.. |Brew| image:: https://img.shields.io/homebrew/v/dvc?label=brew
   :target: https://formulae.brew.sh/formula/dvc
   :alt: Homebrew

.. |Conda| image:: https://img.shields.io/conda/v/conda-forge/dvc.svg?label=conda&logo=conda-forge
   :target: https://anaconda.org/conda-forge/dvc
   :alt: Conda-forge

.. |PyPI| image:: https://img.shields.io/pypi/v/dvc.svg?label=pip&logo=PyPI&logoColor=white
   :target: https://pypi.org/project/dvc
   :alt: PyPI

.. |PyPI Downloads| image:: https://img.shields.io/pypi/dm/dvc.svg?color=blue&label=Downloads&logo=pypi&logoColor=gold
   :target: https://pypi.org/project/dvc
   :alt: PyPI Downloads

.. |Packages| image:: https://img.shields.io/github/v/release/iterative/dvc?label=deb|pkg|rpm|exe&logo=GitHub
   :target: https://github.com/iterative/dvc/releases/latest
   :alt: deb|pkg|rpm|exe

.. |DOI| image:: https://img.shields.io/badge/DOI-10.5281/zenodo.3677553-blue.svg
   :target: https://doi.org/10.5281/zenodo.3677553
   :alt: DOI

.. |Flowchart| image:: https://dvc.org/img/flow.gif
   :target: https://dvc.org/img/flow.gif
   :alt: how_dvc_works

.. |Contribs| image:: https://contrib.rocks/image?repo=iterative/dvc
   :target: https://github.com/iterative/dvc/graphs/contributors
   :alt: Contributors

.. |VS Code| image:: https://img.shields.io/visual-studio-marketplace/v/Iterative.dvc?color=blue&label=VSCode&logo=visualstudiocode&logoColor=blue
   :target: https://marketplace.visualstudio.com/items?itemName=Iterative.dvc
   :alt: VS Code Extension




./dvc/.gitignore
/state
/lock
/config.local
/updater
/updater.lock
/state-journal
/state-wal
/cache
/pkg
/repos
/tmp
/experiments




./dvc/config
['remote "dvc"']
url = https://remote.dvc.org/dvc
[core]
remote = dvc




.github/codecov.yml
codecov:
  notify:
    wait_for_ci: true
coverage:
  status:
    project:
      default:
        target: auto
        threshold: 2%
    patch: off
github_checks:
  annotations: false




.github/dependabot.yml
version: 2

updates:
  - directory: "/"
    package-ecosystem: "pip"
    schedule:
      interval: "daily"
    labels:
      - "maintenance"

  - directory: "/"
    package-ecosystem: "github-actions"
    schedule:
      interval: "daily"
    labels:
      - "maintenance"




.github/mergify.yml
pull_request_rules:
  - name: backport patches to 2.x
    conditions:
      - base=main
      - label=backport
    actions:
      backport:
        branches:
          - "2.x"
        title: ({{ destination_branch }}) {{ title }}




.github/PULL_REQUEST_TEMPLATE.md
* [ ] ‚ùó I have followed the [Contributing to DVC](https://dvc.org/doc/user-guide/contributing/core) checklist.

* [ ] üìñ If this PR requires [documentation](https://dvc.org/doc) updates, I have created a separate PR (or issue, at least) in [dvc.org](https://github.com/iterative/dvc.org) and linked it here.

Thank you for the contribution - we'll try to review it as soon as possible. üôè





.github/release.yml
changelog:
  exclude:
    labels: ["skip-changelog"]
  categories:
    - title: üõ† Breaking Changes
      labels: ["breaking-change"]
    - title: üöÄ New Features and Enhancements
      labels: ["feature", "enhancement", "ui"]
    - title: ‚ö° Optimizations
      labels: ["optimize", "performance"]
    - title: üêõ Bug Fixes
      labels: ["bugfix", "bug"]
    - title: üî® Maintenance
      labels: ["maintenance", "refactoring", "chore", "build", "ci"]
    - title: Other Changes
      labels: ["*"]




.github/ISSUE_TEMPLATE/bug_report.md
---
name: "\U0001F41B Bug Report"
about: Create a bug report to help us improve DVC
---

# Bug Report

<!--
## Issue name

Issue names must follow the pattern `command: description` where the command is the dvc command that you are trying to run. The description should describe the consequence of the bug.

Example: `repro: doesn't detect input changes`
-->

## Description

<!--
A clear and concise description of what the bug is.
-->

### Reproduce

<!--
Step list of how to reproduce the bug
-->

<!--
Example:

1. dvc init
2. Copy dataset.zip to the directory
3. dvc add dataset.zip
4. dvc run -d dataset.zip -o model ./train.sh
5. modify dataset.zip
6. dvc repro
-->

### Expected

<!--
A clear and concise description of what you expect to happen.
-->

### Environment information

<!--
This is required to ensure that we can reproduce the bug.
-->

**Output of `dvc doctor`:**

```console
$ dvc doctor
```

**Additional Information (if any):**

<!--
Please check https://github.com/iterative/dvc/wiki/Debugging-DVC on ways to gather more information regarding the issue.

If applicable, please also provide a `--verbose` output of the command, eg: `dvc add --verbose`.
If the issue is regarding the performance, please attach the profiling information and the benchmark comparisons.
-->




.github/ISSUE_TEMPLATE/config.yml
blank_issues_enabled: false
contact_links:
  - name: "ü§ó Need help?"
    url: https://dvc.org/chat
    about: If you have a question, ask us on Discord. Please join with this invite üëâ




.github/ISSUE_TEMPLATE/epic_story.md
---
name: Epic/Story Issue Template
about: Template for "top" level issues - Epics (>2 weeks) / Stories (<2 weeks)
title: 'Epic: New Feature'
labels: epic
assignees:

---

## Summary / Background
What do you want to achieve, why? business context
...

## Scope

What will be impacted and what won't be?
What needs implementation and what is invariant?
e.g.
- user should be able to run workflow X from UI
- enable workflow Y, Z from CLI

## Assumptions
Product / UX assumptions as well as technical assumptions / limitations
e.g.
* Support only Python Runtime
* Focus on DVC experiments only
* Deployment environments don't change often and can be picked up from shared configuration

## Open Questions
e.g.
- How should access control work for shared artifacts (workflow X)
- Python runtime assumption - is it really valid? in light of <...>

## Blockers / Dependencies
List issues or other conditions / blockers

## General Approach
Invocation example:
```shell
$ mapper-run task.tar.gz --ray-cluster <ip>:<port>
```

## Steps

### Must have (p1)
- [ ] subissue2
- [ ] step 2
 - info
 - info

### Optional / followup (p2)
- [ ] ‚åõ step 3 wip
- [ ] step 4

## Timelines

Put your estimations here. Update once certainty changes
- end of week (Feb 3) for prototype with workflows X, Y
- Feb 15 - MVP in production
- Low priority followups can be done later




.github/ISSUE_TEMPLATE/feature_request.md
---
name: "\U0001F680 Feature Request"
about: Suggest an idea for this project
---




.github/workflows/benchmarks.yaml
name: benchmarks
on: [pull_request, workflow_dispatch]

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  bench:
    name: run benchmarks
    runs-on: ubuntu-latest
    steps:
      - uses: actions/setup-python@v4
        with:
            python-version: "3.11"
      - uses: iterative/dvc-bench@main
        with:
            pytest_options: "-k 'test_init or test_help'"




.github/workflows/codeql.yml
name: "CodeQL"

on:
  push:
    branches: ["main", "2.x"]
  pull_request:
    branches: ["main"]
  schedule:
    - cron: '32 19 * * 2'
  workflow_dispatch:

permissions:
  contents: read

jobs:
  analyze:
    name: Analyze
    runs-on: ubuntu-latest
    permissions:
      actions: read
      contents: read
      security-events: write

    strategy:
      fail-fast: false
      matrix:
        language: ['python']

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Initialize CodeQL
      uses: github/codeql-action/init@v2
      with:
        languages: ${{ matrix.language }}

    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@v2




.github/workflows/packages.yaml
name: Build packages
on:
  push:
    branches:
    - main
  release:
    types: [released, prereleased]
  workflow_dispatch:

permissions:  # added using https://github.com/step-security/secure-workflows
  contents: read

jobs:
  binary:
    permissions:
      contents: write  # for actions/upload-release-asset to upload release asset
    strategy:
      matrix:
        include:
         - {pkg: "deb",  os: "ubuntu-20.04", asset: "dvc_${{ github.event.release.tag_name }}_amd64.deb"}
         - {pkg: "rpm", os: "ubuntu-20.04", asset: "dvc-${{ github.event.release.tag_name }}-1.x86_64.rpm"}
         - {pkg: "osxpkg", os: "macos-11", asset: "dvc-${{ github.event.release.tag_name }}.pkg"}
         - {pkg: "exe",  os: "windows-2019", asset: "dvc-${{ github.event.release.tag_name }}.exe"}

    name: ${{ matrix.pkg }}
    runs-on: ${{ matrix.os }}
    steps:
    - uses: actions/checkout@v3
      with:
          fetch-depth: 0

    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"
        cache: pip
        cache-dependency-path: |
          pyproject.toml
          scripts/build-requirements.txt

    - name: Set up Ruby 2.6
      uses: ruby/setup-ruby@v1
      if: matrix.pkg != 'exe'
      with:
        ruby-version: '2.6'

    - name: Install fpm
      if: matrix.pkg != 'exe'
      run: gem install --no-document fpm

    - name: Install deps
      run: |
        pip install --upgrade pip wheel setuptools
        pip install .[all]
        pip install -r scripts/build-requirements.txt

    - name: Pull images
      run: dvc pull

    - name: Build ${{ matrix.pkg }}
      run: python scripts/build.py ${{ matrix.pkg }}

    - name: Publish ${{ matrix.pkg }}
      if: github.event_name == 'release'
      uses: actions/upload-release-asset@v1.0.2
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        upload_url: ${{ github.event.release.upload_url }}
        asset_path: scripts/${{ matrix.pkg == 'exe' && 'innosetup' || 'fpm' }}/${{ matrix.asset }}
        asset_name: ${{ matrix.asset }}
        asset_content_type: binary/octet-stream

  pip:
    runs-on: ubuntu-20.04
    permissions:
      id-token: write
    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0

    - name: Set up Python 3.8
      uses: actions/setup-python@v4
      with:
        python-version: 3.8

    - name: Install
      run: pip install --upgrade pip wheel

    - name: Force version for Test PyPI uploads
      if: ${{ !startsWith(github.ref, 'refs/tags') }}
      run: |
        pip install setuptools_scm
        echo version=$(python -m setuptools_scm | awk -F+ '{print $1}' | tail -1) >> $GITHUB_ENV

    - name: Build packages
      run: ./scripts/build_package.sh
      env:
        SETUPTOOLS_SCM_PRETEND_VERSION: ${{ env.version }}

    - name: Publish packages to PyPI
      if: github.event_name == 'release'
      uses: pypa/gh-action-pypi-publish@release/v1

    - name: Publish to Test PyPI
      if: ${{ github.event_name == 'release' || (github.event_name == 'push' && github.ref == 'refs/heads/main') }}
      uses: pypa/gh-action-pypi-publish@release/v1
      with:
        repository_url: https://test.pypi.org/legacy/
        skip_existing: true




.github/workflows/plugin_tests.yaml
name: Remote Plugin Tests

on:
  pull_request:
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

permissions:
  contents: read

jobs:
  tests:
    timeout-minutes: 45
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-20.04]
        pyv: ["3.10"]
        plugin: ["dvc-s3"]

    steps:
    - uses: actions/checkout@v3
      with:
        path: dvc

    - uses: actions/checkout@v3
      with:
        repository: iterative/${{ matrix.plugin }}
        ref: main
        path: ${{ matrix.plugin }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.pyv }}
        cache: pip
        cache-dependency-path: |
          dvc/pyproject.toml
          ${{ matrix.plugin }}/setup.cfg

    - name: Install plugin + DVC@PR
      run: |
        pip install --upgrade pip wheel
        pip install "./dvc[testing]"
        pip install -e "./${{ matrix.plugin }}[tests]"

    - name: Run plugin tests
      timeout-minutes: 15
      working-directory: ${{ matrix.plugin }}
      run: pytest -v -n=auto




.github/workflows/tests.yaml
name: Tests

on:
  push:
    branches: [main, 2.x]
  pull_request:
  schedule:
    - cron: '5 1 * * *'  # every day at 01:05
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

permissions:
  contents: read

jobs:
  tests:
    timeout-minutes: 50
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-20.04, windows-latest, macos-latest]
        pyv: ["3.8", "3.9", "3.10", "3.11"]

    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.pyv }}
        cache: pip
        cache-dependency-path: pyproject.toml
    - name: install
      run: |
        pip install --upgrade pip wheel
        pip install -e ".[dev]"
    - uses: pre-commit/action@v3.0.0
    - name: run tests
      timeout-minutes: 40
      env:
        PYTHONUTF8: 1
      run: >
        pytest -n=logical --timeout=300 --durations=100
        --cov --cov-report=xml --cov-report=term
    - name: upload coverage report
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: false
  check:
    if: always()
    needs: [tests]
    runs-on: ubuntu-latest
    steps:
      - uses: re-actors/alls-green@release/v1
        with:
          jobs: ${{ toJSON(needs) }}
  notify:
    if: github.ref == 'refs/heads/main' && failure()
    needs: [tests]
    runs-on: ubuntu-latest
    steps:
    - name: Slack Notification
      uses: rtCamp/action-slack-notify@v2.2.0
      env:
        SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
        SLACK_COLOR: ${{ job.status }}
        SLACK_MESSAGE: 'CI Failed on main :boom:'
        SLACK_TITLE: CI Status
        SLACK_USERNAME: DVC-CI




dvc/__init__.py
"""
DVC
----
Make your data science projects reproducible and shareable.
"""
import dvc.logger
from dvc.version import __version__, version_tuple  # noqa: F401

dvc.logger.setup()




dvc/__main__.py
"""Main entry point for DVC command line tool."""
import sys

from dvc.cli import main

if __name__ == "__main__":
    sys.exit(main(sys.argv[1:]))




dvc/_debug.py
from contextlib import ExitStack, contextmanager
from datetime import datetime
from typing import TYPE_CHECKING, Callable, Optional, Union

if TYPE_CHECKING:
    from argparse import Namespace
    from types import FrameType


@contextmanager
def viztracer_profile(
    path: Union[Callable[[], str], str],
    depth: int = -1,
    log_async: bool = False,
):
    try:
        import viztracer  # pylint: disable=import-error
    except ImportError:
        print("Failed to run profiler, viztracer is not installed")  # noqa: T201
        yield
        return

    tracer = viztracer.VizTracer(max_stack_depth=depth, log_async=log_async)

    tracer.start()
    yield
    tracer.stop()

    tracer.save(path() if callable(path) else path)


@contextmanager
def yappi_profile(
    path: Optional[Union[Callable[[], str], str]] = None,
    wall_clock: Optional[bool] = True,
    separate_threads: Optional[bool] = False,
):
    try:
        import yappi  # pylint: disable=import-error
    except ImportError:
        print("Failed to run profiler, yappi is not installed")  # noqa: T201
        yield
        return

    yappi.set_clock_type("wall" if wall_clock else "cpu")

    yappi.start()
    yield
    yappi.stop()

    threads = yappi.get_thread_stats()
    stats = {}
    if separate_threads:
        for thread in threads:
            ctx_id = thread.id
            stats[ctx_id] = yappi.get_func_stats(ctx_id=ctx_id)
    else:
        stats[None] = yappi.get_func_stats()

    fpath = path() if callable(path) else path
    for ctx_id, st in stats.items():
        if fpath:
            out = f"{fpath}-{ctx_id}" if ctx_id is not None else fpath
            st.save(out, type="callgrind")
        else:
            if ctx_id is not None:
                print(f"\nThread {ctx_id}")  # noqa: T201
            st.print_all()  # pylint:disable=no-member
            if ctx_id is None:
                threads.print_all()  # pylint:disable=no-member

    yappi.clear_stats()


@contextmanager
def instrument(html_output=False):
    """Run a statistical profiler"""
    try:
        from pyinstrument import Profiler  # pylint: disable=import-error
    except ImportError:
        print("Failed to run profiler, pyinstrument is not installed")  # noqa: T201
        yield
        return

    profiler = Profiler()

    profiler.start()
    yield
    profiler.stop()

    if html_output:
        profiler.open_in_browser()
        return
    print(profiler.output_text(unicode=True, color=True))  # noqa: T201


@contextmanager
def profile(dump_path: Optional[str] = None):
    """Run a cprofile"""
    import cProfile

    prof = cProfile.Profile()
    prof.enable()

    yield

    prof.disable()
    if not dump_path:
        prof.print_stats(sort="cumtime")
        return
    prof.dump_stats(dump_path)


@contextmanager
def debug():
    try:
        yield
    except Exception:  # pylint: disable=broad-except
        try:
            import ipdb as pdb  # noqa: T100, pylint: disable=import-error
        except ImportError:
            import pdb  # type: ignore[no-redef]  # noqa: T100
        pdb.post_mortem()

        raise  # prevent from jumping ahead


def _sigshow(_, frame: Optional["FrameType"]) -> None:
    import sys
    from shutil import get_terminal_size
    from traceback import format_stack

    lines = "\u2015" * get_terminal_size().columns
    stack = format_stack(frame)
    print(lines, "\n", *stack, lines, sep="", file=sys.stderr)  # noqa: T201


@contextmanager
def show_stack():
    r"""Show stack trace on SIGQUIT (Ctrl-\) or SIGINFO (Ctrl-T on macOS)."""
    import signal
    import sys

    if sys.platform != "win32":
        signal.signal(signal.SIGQUIT, _sigshow)

    try:
        signal.signal(signal.SIGINFO, _sigshow)  # only available on macOS
    except AttributeError:
        pass
    yield


def _get_path_func(tool: str, ext: str):
    fmt = f"{tool}.dvc-{{now:%Y%m%d}}_{{now:%H%M%S}}.{ext}"

    def func(now: Optional["datetime"] = None) -> str:
        return fmt.format(now=now or datetime.now())

    return func


@contextmanager
def debugtools(args: Optional["Namespace"] = None, **kwargs):
    kw = vars(args) if args else {}
    kw.update(kwargs)

    with ExitStack() as stack:
        if kw.get("pdb"):
            stack.enter_context(debug())
        if kw.get("cprofile") or kw.get("cprofile_dump"):
            stack.enter_context(profile(kw.get("cprofile_dump")))
        if kw.get("instrument") or kw.get("instrument_open"):
            stack.enter_context(instrument(kw.get("instrument_open", False)))
        if kw.get("show_stack", False):
            stack.enter_context(show_stack())
        if kw.get("yappi"):
            path_func = _get_path_func("callgrind", "out")
            stack.enter_context(
                yappi_profile(
                    path=path_func,
                    separate_threads=kw.get("yappi_separate_threads"),
                )
            )
        if (
            kw.get("viztracer")
            or kw.get("viztracer_depth")
            or kw.get("viztracer_async")
        ):
            path_func = _get_path_func("viztracer", "json")
            depth = kw.get("viztracer_depth") or -1
            log_async = kw.get("viztracer_async") or False
            prof = viztracer_profile(path=path_func, depth=depth, log_async=log_async)
            stack.enter_context(prof)
        yield


def add_debugging_flags(parser):
    from argparse import SUPPRESS

    parser.add_argument("--cprofile", action="store_true", default=False, help=SUPPRESS)
    parser.add_argument("--yappi", action="store_true", default=False, help=SUPPRESS)
    parser.add_argument(
        "--yappi-separate-threads",
        action="store_true",
        default=False,
        help=SUPPRESS,
    )
    parser.add_argument(
        "--viztracer", action="store_true", default=False, help=SUPPRESS
    )
    parser.add_argument("--viztracer-depth", type=int, help=SUPPRESS)
    parser.add_argument(
        "--viztracer-async", action="store_true", default=False, help=SUPPRESS
    )
    parser.add_argument("--cprofile-dump", help=SUPPRESS)
    parser.add_argument("--pdb", action="store_true", default=False, help=SUPPRESS)
    parser.add_argument(
        "--instrument", action="store_true", default=False, help=SUPPRESS
    )
    parser.add_argument(
        "--instrument-open", action="store_true", default=False, help=SUPPRESS
    )
    parser.add_argument(
        "--show-stack",
        "--ss",
        action="store_true",
        default=False,
        help=SUPPRESS,
    )




dvc/analytics.py
import json
import logging
import os

from .env import DVC_NO_ANALYTICS

logger = logging.getLogger(__name__)


def collect_and_send_report(args=None, return_code=None):
    """
    Collect information from the runtime/environment and the command
    being executed into a report and send it over the network.

    To prevent analytics from blocking the execution of the main thread,
    sending the report is done in a separate process.

    The inter-process communication happens through a file containing the
    report as a JSON, where the _collector_ generates it and the _sender_
    removes it after sending it.
    """
    import tempfile

    from dvc.daemon import daemon

    report = {}

    # Include command execution information on the report only when available.
    if args and hasattr(args, "func"):
        report.update({"cmd_class": args.func.__name__})

    if return_code is not None:
        report.update({"cmd_return_code": return_code})

    with tempfile.NamedTemporaryFile(delete=False, mode="w") as fobj:
        json.dump(report, fobj)
    daemon(["analytics", fobj.name])


def is_enabled():
    from dvc.config import Config, to_bool
    from dvc.utils import env2bool

    if env2bool("DVC_TEST"):
        return False

    enabled = not os.getenv(DVC_NO_ANALYTICS)
    if enabled:
        enabled = to_bool(
            Config.from_cwd(validate=False).get("core", {}).get("analytics", "true")
        )

    logger.debug("Analytics is %sabled.", "en" if enabled else "dis")

    return enabled


def send(path):
    """
    Side effect: Removes the report after sending it.

    The report is generated and stored in a temporary file, see:
    `collect_and_send_report`. Sending happens on another process,
    thus, the need of removing such file afterwards.
    """
    import requests

    url = "https://analytics.dvc.org"
    headers = {"content-type": "application/json"}

    with open(path, encoding="utf-8") as fobj:
        report = json.load(fobj)

    report.update(_runtime_info())

    try:
        requests.post(url, json=report, headers=headers, timeout=5)
    except requests.exceptions.RequestException:
        logger.debug("failed to send analytics report", exc_info=True)

    os.remove(path)


def _scm_in_use():
    from dvc.exceptions import NotDvcRepoError
    from dvc.repo import Repo
    from dvc.scm import NoSCM

    from .scm import SCM, SCMError

    try:
        scm = SCM(root_dir=Repo.find_root())
        return type(scm).__name__
    except SCMError:
        return NoSCM.__name__
    except NotDvcRepoError:
        pass


def _runtime_info():
    """
    Gather information from the environment where DVC runs to fill a report.
    """
    from iterative_telemetry import _generate_ci_id, find_or_create_user_id

    from dvc import __version__
    from dvc.utils import is_binary

    ci_id = _generate_ci_id()
    if ci_id:
        group_id, user_id = ci_id
    else:
        group_id, user_id = None, find_or_create_user_id()

    return {
        "dvc_version": __version__,
        "is_binary": is_binary(),
        "scm_class": _scm_in_use(),
        "system_info": _system_info(),
        "user_id": user_id,
        "group_id": group_id,
    }


def _system_info():
    import platform
    import sys

    import distro

    system = platform.system()

    if system == "Windows":
        version = sys.getwindowsversion()  # type: ignore[attr-defined]

        return {
            "os": "windows",
            "windows_version_build": version.build,
            "windows_version_major": version.major,
            "windows_version_minor": version.minor,
            "windows_version_service_pack": version.service_pack,
        }

    if system == "Darwin":
        return {"os": "mac", "mac_version": platform.mac_ver()[0]}

    if system == "Linux":
        return {
            "os": "linux",
            "linux_distro": distro.id(),
            "linux_distro_like": distro.like(),
            "linux_distro_version": distro.version(),
        }

    # We don't collect data for any other system.
    raise NotImplementedError




dvc/annotations.py
from dataclasses import asdict, dataclass, field, fields
from typing import Any, ClassVar, Dict, List, Optional

from funcy import compact
from voluptuous import Required


@dataclass
class Annotation:
    PARAM_DESC: ClassVar[str] = "desc"
    PARAM_TYPE: ClassVar[str] = "type"
    PARAM_LABELS: ClassVar[str] = "labels"
    PARAM_META: ClassVar[str] = "meta"

    desc: Optional[str] = None
    type: Optional[str] = None  # noqa: A003
    labels: List[str] = field(default_factory=list)
    meta: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, str]:
        return compact(asdict(self))


@dataclass
class Artifact:
    PARAM_PATH: ClassVar[str] = "path"
    PARAM_DESC: ClassVar[str] = "desc"
    PARAM_TYPE: ClassVar[str] = "type"
    PARAM_LABELS: ClassVar[str] = "labels"
    PARAM_META: ClassVar[str] = "meta"

    path: str
    desc: Optional[str] = None
    type: Optional[str] = None  # noqa: A003
    labels: List[str] = field(default_factory=list)
    meta: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, str]:
        return compact(asdict(self))


ANNOTATION_FIELDS = [field.name for field in fields(Annotation)]
ANNOTATION_SCHEMA = {
    Annotation.PARAM_DESC: str,
    Annotation.PARAM_TYPE: str,
    Annotation.PARAM_LABELS: [str],
    Annotation.PARAM_META: object,
}
ARTIFACT_SCHEMA = {
    Required(Artifact.PARAM_PATH): str,
    **ANNOTATION_SCHEMA,  # type: ignore[arg-type]
}




dvc/cachemgr.py
import os

from dvc.fs import GitFileSystem, Schemes
from dvc_data.hashfile.db import get_odb


def _get_odb(repo, settings, fs=None):
    from dvc.fs import get_cloud_fs

    if not settings:
        return None

    cls, config, fs_path = get_cloud_fs(repo, **settings)
    fs = fs or cls(**config)
    return get_odb(fs, fs_path, state=repo.state, **config)


class CacheManager:
    CACHE_DIR = "cache"
    CLOUD_SCHEMES = [
        Schemes.S3,
        Schemes.GS,
        Schemes.SSH,
        Schemes.HDFS,
        Schemes.WEBHDFS,
    ]

    def __init__(self, repo):
        self._repo = repo
        self.config = config = repo.config["cache"]
        self._odb = {}

        default = None
        if repo and repo.local_dvc_dir:
            default = os.path.join(repo.local_dvc_dir, self.CACHE_DIR)

        local = config.get("local")

        if local:
            settings = {"name": local}
        elif "dir" not in config and not default:
            settings = None
        else:
            from dvc.config_schema import LOCAL_COMMON

            url = config.get("dir") or default
            settings = {"url": url}
            for opt in LOCAL_COMMON:
                if opt in config:
                    settings[str(opt)] = config.get(opt)

        kwargs = {}
        if not isinstance(repo.fs, GitFileSystem):
            kwargs["fs"] = repo.fs

        odb = _get_odb(repo, settings, **kwargs)
        self._odb["repo"] = odb
        self._odb[Schemes.LOCAL] = odb

    def _init_odb(self, schemes):
        for scheme in schemes:
            remote = self.config.get(scheme)
            settings = {"name": remote} if remote else None
            self._odb[scheme] = _get_odb(self._repo, settings)

    def __getattr__(self, name):
        if name not in self._odb and name in self.CLOUD_SCHEMES:
            self._init_odb([name])

        try:
            return self._odb[name]
        except KeyError as exc:
            raise AttributeError from exc

    def by_scheme(self):
        self._init_odb(self.CLOUD_SCHEMES)
        yield from self._odb.items()




dvc/compare.py
from collections import abc
from itertools import chain, repeat, zip_longest
from operator import itemgetter
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    ItemsView,
    Iterable,
    Iterator,
    List,
    Mapping,
    MutableSequence,
    Optional,
    Sequence,
    Set,
    Tuple,
    Union,
    overload,
)

from funcy import reraise

if TYPE_CHECKING:
    from dvc.ui.table import CellT


class Column(List["CellT"]):
    pass


def with_value(value, default):
    return default if value is None else value


class TabularData(MutableSequence[Sequence["CellT"]]):
    def __init__(self, columns: Sequence[str], fill_value: Optional[str] = ""):
        self._columns: Dict[str, Column] = {name: Column() for name in columns}
        self._keys: List[str] = list(columns)
        self._fill_value = fill_value
        self._protected: Set[str] = set()

    @property
    def columns(self) -> List[Column]:
        return list(map(self.column, self.keys()))

    def is_protected(self, col_name) -> bool:
        return col_name in self._protected

    def protect(self, *col_names: str):
        self._protected.update(col_names)

    def unprotect(self, *col_names: str):
        self._protected = self._protected.difference(col_names)

    def column(self, name: str) -> Column:
        return self._columns[name]

    def items(self) -> ItemsView[str, Column]:
        projection = {k: self.column(k) for k in self.keys()}  # noqa: SIM118
        return projection.items()

    def keys(self) -> List[str]:
        return self._keys

    def _iter_col_row(self, row: Sequence["CellT"]) -> Iterator[Tuple["CellT", Column]]:
        for val, col in zip_longest(row, self.columns):
            if col is None:
                break
            yield with_value(val, self._fill_value), col

    def append(self, value: Sequence["CellT"]) -> None:
        for val, col in self._iter_col_row(value):
            col.append(val)

    def extend(self, values: Iterable[Sequence["CellT"]]) -> None:
        for row in values:
            self.append(row)

    def insert(self, index: int, value: Sequence["CellT"]) -> None:
        for val, col in self._iter_col_row(value):
            col.insert(index, val)

    def __iter__(self) -> Iterator[List["CellT"]]:
        return map(list, zip(*self.columns))

    def __getattr__(self, item: str) -> Column:
        with reraise(KeyError, AttributeError):
            return self.column(item)

    def __getitem__(self, item: Union[int, slice]):
        func = itemgetter(item)
        it = map(func, self.columns)
        if isinstance(item, slice):
            it = map(list, zip(*it))
        return list(it)

    @overload
    def __setitem__(self, item: int, value: Sequence["CellT"]) -> None:
        ...

    @overload
    def __setitem__(self, item: slice, value: Iterable[Sequence["CellT"]]) -> None:
        ...

    def __setitem__(self, item, value) -> None:
        it = value
        if isinstance(item, slice):
            n = len(self.columns)
            normalized_rows = (
                chain(val, repeat(self._fill_value, n - len(val))) for val in value
            )
            # we need to transpose those rows into columnar format
            # as we work in terms of column-based arrays
            it = zip(*normalized_rows)

        for i, col in self._iter_col_row(it):
            col[item] = i

    def __delitem__(self, item: Union[int, slice]) -> None:
        for col in self.columns:
            del col[item]

    def __len__(self) -> int:
        if not self._columns:
            return 0
        return len(self.columns[0])

    @property
    def shape(self) -> Tuple[int, int]:
        return len(self.columns), len(self)

    def drop(self, *col_names: str) -> None:
        for col in col_names:
            if not self.is_protected(col):
                self._keys.remove(col)
                self._columns.pop(col)

    def rename(self, from_col_name: str, to_col_name: str) -> None:
        self._columns[to_col_name] = self._columns.pop(from_col_name)
        self._keys[self._keys.index(from_col_name)] = to_col_name

    def project(self, *col_names: str) -> None:
        self.drop(*(set(self._keys) - set(col_names)))
        self._keys = list(col_names)

    def is_empty(self, col_name: str) -> bool:
        col = self.column(col_name)
        return not any(item != self._fill_value for item in col)

    def to_csv(self) -> str:
        import csv
        from io import StringIO

        buff = StringIO()
        writer = csv.writer(buff)
        writer.writerow(self.keys())

        for row in self:
            writer.writerow(row)
        return buff.getvalue()

    def add_column(self, name: str) -> None:
        self._columns[name] = Column([self._fill_value] * len(self))
        self._keys.append(name)

    def row_from_dict(self, d: Mapping[str, "CellT"]) -> None:
        keys = self.keys()
        for key in d:
            if key not in keys:
                self.add_column(key)

        row: List["CellT"] = [
            with_value(d.get(key), self._fill_value)
            for key in self.keys()  # noqa: SIM118
        ]
        self.append(row)

    def render(self, **kwargs: Any):
        from dvc.ui import ui

        if kwargs.pop("csv", False):
            ui.write(self.to_csv(), end="")
        else:
            ui.table(self, headers=self.keys(), **kwargs)

    def as_dict(
        self, cols: Optional[Iterable[str]] = None
    ) -> Iterable[Dict[str, "CellT"]]:
        keys = self.keys() if cols is None else set(cols)
        return [{k: self._columns[k][i] for k in keys} for i in range(len(self))]

    def dropna(  # noqa: C901, PLR0912
        self,
        axis: str = "rows",
        how="any",
        subset: Optional[Iterable[str]] = None,
    ):
        if axis not in ["rows", "cols"]:
            raise ValueError(
                f"Invalid 'axis' value {axis}.Choose one of ['rows', 'cols']"
            )
        if how not in ["any", "all"]:
            raise ValueError(f"Invalid 'how' value {how}. Choose one of ['any', 'all']")

        match_line: Set = set()
        match_any = True
        if how == "all":
            match_any = False

        for n_row, row in enumerate(self):
            for n_col, col in enumerate(row):
                if subset and self.keys()[n_col] not in subset:
                    continue
                if (col == self._fill_value) is match_any:
                    if axis == "rows":
                        match_line.add(n_row)
                        break
                    match_line.add(self.keys()[n_col])

        to_drop = match_line
        if how == "all":
            if axis == "rows":
                to_drop = set(range(len(self)))
            else:
                to_drop = set(self.keys())
            to_drop -= match_line

        if axis == "rows":
            for name in self.keys():  # noqa: SIM118
                self._columns[name] = Column(
                    [x for n, x in enumerate(self._columns[name]) if n not in to_drop]
                )
        else:
            self.drop(*to_drop)

    def drop_duplicates(  # noqa: C901
        self,
        axis: str = "rows",
        subset: Optional[Iterable[str]] = None,
        ignore_empty: bool = True,
    ):
        if axis not in ["rows", "cols"]:
            raise ValueError(
                f"Invalid 'axis' value {axis}.Choose one of ['rows', 'cols']"
            )

        if axis == "cols":
            cols_to_drop: List[str] = []
            for n_col, col in enumerate(self.columns):
                if subset and self.keys()[n_col] not in subset:
                    continue
                # Cast to str because Text is not hashable error
                unique_vals = {str(x) for x in col}
                if ignore_empty and self._fill_value in unique_vals:
                    unique_vals -= {self._fill_value}
                if len(unique_vals) == 1:
                    cols_to_drop.append(self.keys()[n_col])
            self.drop(*cols_to_drop)

        elif axis == "rows":
            unique_rows = []
            rows_to_drop: List[int] = []
            for n_row, row in enumerate(self):
                if subset:
                    row = [
                        col
                        for n_col, col in enumerate(row)
                        if self.keys()[n_col] in subset
                    ]

                tuple_row = tuple(row)
                if tuple_row in unique_rows:
                    rows_to_drop.append(n_row)
                else:
                    unique_rows.append(tuple_row)

            for name in self.keys():  # noqa: SIM118
                self._columns[name] = Column(
                    [
                        x
                        for n, x in enumerate(self._columns[name])
                        if n not in rows_to_drop
                    ]
                )


def _normalize_float(val: float, precision: int):
    return f"{val:.{precision}g}"


def _format_field(
    val: Any, precision: Optional[int] = None, round_digits: bool = False
) -> str:
    def _format(_val):
        if isinstance(_val, float) and precision:
            if round_digits:
                return round(_val, precision)
            return _normalize_float(_val, precision)
        if isinstance(_val, abc.Mapping):
            return {k: _format(v) for k, v in _val.items()}
        if isinstance(_val, list):
            return [_format(x) for x in _val]
        return _val

    return str(_format(val))


def diff_table(
    diff,
    title: str,
    old: bool = True,
    no_path: bool = False,
    show_changes: bool = True,
    precision: Optional[int] = None,
    round_digits: bool = False,
    on_empty_diff: Optional[str] = None,
    a_rev: Optional[str] = None,
    b_rev: Optional[str] = None,
) -> TabularData:
    a_rev = a_rev or "HEAD"
    b_rev = b_rev or "workspace"
    headers: List[str] = ["Path", title, a_rev, b_rev, "Change"]
    fill_value = "-"
    td = TabularData(headers, fill_value=fill_value)

    for fname, diff_in_file in diff.items():
        for item, change in sorted(diff_in_file.items()):
            old_value = with_value(change.get("old"), fill_value)
            new_value = with_value(change.get("new"), fill_value)
            diff_value = with_value(change.get("diff", on_empty_diff), fill_value)
            td.append(
                [
                    fname,
                    str(item),
                    _format_field(old_value, precision, round_digits),
                    _format_field(new_value, precision, round_digits),
                    _format_field(diff_value, precision, round_digits),
                ]
            )

    if no_path:
        td.drop("Path")

    if not show_changes:
        td.drop("Change")

    if not old:
        td.drop(a_rev)
        td.rename(b_rev, "Value")

    return td


def show_diff(  # noqa: PLR0913
    diff,
    title: str,
    old: bool = True,
    no_path: bool = False,
    show_changes: bool = True,
    precision: Optional[int] = None,
    round_digits: bool = False,
    on_empty_diff: Optional[str] = None,
    markdown: bool = False,
    a_rev: Optional[str] = None,
    b_rev: Optional[str] = None,
) -> None:
    td = diff_table(
        diff,
        title=title,
        old=old,
        no_path=no_path,
        show_changes=show_changes,
        precision=precision,
        round_digits=round_digits,
        on_empty_diff=on_empty_diff,
        a_rev=a_rev,
        b_rev=b_rev,
    )
    td.render(markdown=markdown)


def metrics_table(
    metrics,
    all_branches: bool = False,
    all_tags: bool = False,
    all_commits: bool = False,
    precision: Optional[int] = None,
    round_digits: bool = False,
):
    from dvc.utils.diff import format_dict
    from dvc.utils.flatten import flatten

    td = TabularData(["Revision", "Path"], fill_value="-")

    for branch, val in metrics.items():
        for fname, metric in val.get("data", {}).items():
            row_data: Dict[str, str] = {"Revision": branch, "Path": fname}
            metric = metric.get("data", {})
            flattened = (
                flatten(format_dict(metric))
                if isinstance(metric, dict)
                else {"": metric}
            )
            row_data.update(
                {
                    k: _format_field(v, precision, round_digits)
                    for k, v in flattened.items()
                }
            )
            td.row_from_dict(row_data)

    rev, path, *metrics_headers = td.keys()
    td.project(rev, path, *sorted(metrics_headers))

    if not any([all_branches, all_tags, all_commits]):
        td.drop("Revision")

    return td


def show_metrics(
    metrics,
    markdown: bool = False,
    all_branches: bool = False,
    all_tags: bool = False,
    all_commits: bool = False,
    precision: Optional[int] = None,
    round_digits: bool = False,
) -> None:
    td = metrics_table(
        metrics,
        all_branches=all_branches,
        all_tags=all_tags,
        all_commits=all_commits,
        precision=precision,
        round_digits=round_digits,
    )
    td.render(markdown=markdown)




dvc/config.py
"""DVC config objects."""
import logging
import os
import re
from contextlib import contextmanager
from functools import partial
from typing import TYPE_CHECKING, Dict, Optional

from funcy import compact, memoize, re_find

from dvc.exceptions import DvcException, NotDvcRepoError

from .utils.objects import cached_property

if TYPE_CHECKING:
    from dvc.fs import FileSystem
    from dvc.types import DictStrAny, StrPath

logger = logging.getLogger(__name__)


class ConfigError(DvcException):
    """DVC config exception."""

    def __init__(self, msg):
        super().__init__(f"config file error: {msg}")


class RemoteConfigError(ConfigError):
    pass


class NoRemoteError(RemoteConfigError):
    pass


class RemoteNotFoundError(RemoteConfigError):
    pass


class MachineConfigError(ConfigError):
    pass


class NoMachineError(MachineConfigError):
    pass


class MachineNotFoundError(MachineConfigError):
    pass


@memoize
def get_compiled_schema():
    from voluptuous import Schema

    from .config_schema import SCHEMA

    return Schema(SCHEMA)


def to_bool(value):
    from .config_schema import Bool

    return Bool(value)


class Config(dict):
    """Class that manages configuration files for a DVC repo.

    Args:
        dvc_dir (str): optional path to `.dvc` directory, that is used to
            access repo-specific configs like .dvc/config and
            .dvc/config.local.
        validate (bool): optional flag to tell dvc if it should validate the
            config or just load it as is. 'True' by default.

    Raises:
        ConfigError: thrown if config has an invalid format.
    """

    SYSTEM_LEVELS = ("system", "global")
    REPO_LEVELS = ("repo", "local")
    # In the order they shadow each other
    LEVELS = SYSTEM_LEVELS + REPO_LEVELS

    CONFIG = "config"
    CONFIG_LOCAL = "config.local"

    def __init__(
        self,
        dvc_dir: Optional["StrPath"] = None,
        validate: bool = True,
        fs: Optional["FileSystem"] = None,
        config: Optional["DictStrAny"] = None,
    ):  # pylint: disable=super-init-not-called
        from dvc.fs import LocalFileSystem

        self.dvc_dir = dvc_dir
        self.wfs = LocalFileSystem()
        self.fs = fs or self.wfs

        if dvc_dir:
            self.dvc_dir = self.fs.path.abspath(self.fs.path.realpath(dvc_dir))

        self.load(validate=validate, config=config)

    @classmethod
    def from_cwd(cls, fs: Optional["FileSystem"] = None, **kwargs):
        from dvc.repo import Repo

        try:
            dvc_dir = Repo.find_dvc_dir(fs=fs)
        except NotDvcRepoError:
            dvc_dir = None

        return cls(dvc_dir=dvc_dir, fs=fs, **kwargs)

    @classmethod
    def get_dir(cls, level):
        from dvc.dirs import global_config_dir, system_config_dir

        assert level in ("global", "system")

        if level == "global":
            return global_config_dir()
        if level == "system":
            return system_config_dir()

    @cached_property
    def files(self) -> Dict[str, str]:
        files = {
            level: os.path.join(self.get_dir(level), self.CONFIG)
            for level in ("system", "global")
        }

        if self.dvc_dir is not None:
            files["repo"] = self.fs.path.join(self.dvc_dir, self.CONFIG)
            files["local"] = self.fs.path.join(self.dvc_dir, self.CONFIG_LOCAL)

        return files

    @staticmethod
    def init(dvc_dir):
        """Initializes dvc config.

        Args:
            dvc_dir (str): path to .dvc directory.

        Returns:
            dvc.config.Config: config object.
        """
        config_file = os.path.join(dvc_dir, Config.CONFIG)
        with open(config_file, "w+", encoding="utf-8"):
            return Config(dvc_dir)

    def load(self, validate: bool = True, config: Optional["DictStrAny"] = None):
        """Loads config from all the config files.

        Raises:
            ConfigError: thrown if config has an invalid format.
        """
        conf = self.load_config_to_level()

        if config is not None:
            merge(conf, config)

        if validate:
            conf = self.validate(conf)

        self.clear()
        self.update(conf)

    def _get_fs(self, level):
        # NOTE: this might be a Gitfs, which doesn't see things outside of
        # the repo.
        return self.fs if level == "repo" else self.wfs

    def _load_config(self, level):
        from configobj import ConfigObj, ConfigObjError

        filename = self.files[level]
        fs = self._get_fs(level)

        if fs.exists(filename):
            with fs.open(filename) as fobj:
                try:
                    conf_obj = ConfigObj(fobj)
                except UnicodeDecodeError as exc:
                    raise ConfigError(str(exc)) from exc
                except ConfigObjError as exc:
                    raise ConfigError(str(exc)) from exc
        else:
            conf_obj = ConfigObj()
        return _parse_named(_lower_keys(conf_obj.dict()))

    def _save_config(self, level, conf_dict):
        from configobj import ConfigObj

        filename = self.files[level]
        fs = self._get_fs(level)

        logger.debug("Writing '%s'.", filename)

        fs.makedirs(os.path.dirname(filename))

        config = ConfigObj(_pack_named(conf_dict))
        with fs.open(filename, "wb") as fobj:
            config.write(fobj)
        config.filename = filename

    def load_one(self, level):
        conf = self._load_config(level)
        conf = self._load_paths(conf, self.files[level])

        # Auto-verify sections
        for key in get_compiled_schema().schema:
            conf.setdefault(key, {})

        return conf

    @staticmethod
    def _load_paths(conf, filename):
        abs_conf_dir = os.path.abspath(os.path.dirname(filename))

        def resolve(path):
            from .config_schema import RelPath

            if os.path.isabs(path) or re.match(r"\w+://", path):
                return path

            # on windows convert slashes to backslashes
            # to have path compatible with abs_conf_dir
            if os.path.sep == "\\" and "/" in path:
                path = path.replace("/", "\\")

            return RelPath(os.path.join(abs_conf_dir, path))

        return Config._map_dirs(conf, resolve)

    @staticmethod
    def _to_relpath(conf_dir, path):
        from dvc.fs import localfs
        from dvc.utils import relpath

        from .config_schema import RelPath

        if re.match(r"\w+://", path):
            return path

        if isinstance(path, RelPath) or not os.path.isabs(path):
            path = relpath(path, conf_dir)
            return localfs.path.as_posix(path)

        return path

    @staticmethod
    def _save_paths(conf, filename):
        conf_dir = os.path.dirname(filename)
        rel = partial(Config._to_relpath, conf_dir)

        return Config._map_dirs(conf, rel)

    @staticmethod
    def _map_dirs(conf, func):
        from voluptuous import ALLOW_EXTRA, Schema

        dirs_schema = {
            "cache": {"dir": func},
            "remote": {
                str: {
                    "url": func,
                    "gdrive_user_credentials_file": func,
                    "gdrive_service_account_json_file_path": func,
                    "credentialpath": func,
                    "keyfile": func,
                    "cert_path": func,
                    "key_path": func,
                }
            },
            "machine": {
                str: {
                    "startup_script": func,
                    "setup_script": func,
                }
            },
        }
        return Schema(dirs_schema, extra=ALLOW_EXTRA)(conf)

    def load_config_to_level(self, level=None):
        merged_conf: Dict = {}
        for merge_level in self.LEVELS:
            if merge_level == level:
                break
            if merge_level in self.files:
                merge(merged_conf, self.load_one(merge_level))
        return merged_conf

    def read(self, level=None):
        # NOTE: we read from a merged config by default, same as git config
        if level is None:
            return self.load_config_to_level()
        return self.load_one(level)

    @contextmanager
    def edit(self, level=None, validate=True):
        # NOTE: we write to repo config by default, same as git config
        level = level or "repo"
        if self.dvc_dir is None and level in self.REPO_LEVELS:
            raise ConfigError("Not inside a DVC repo")

        conf = self.load_one(level)
        yield conf

        conf = self._save_paths(conf, self.files[level])

        merged_conf = self.load_config_to_level(level)
        merge(merged_conf, conf)

        if validate:
            self.validate(merged_conf)

        self._save_config(level, conf)
        self.load(validate=validate)

    @staticmethod
    def validate(data):
        from voluptuous import Invalid

        try:
            return get_compiled_schema()(data)
        except Invalid as exc:
            raise ConfigError(str(exc)) from None


def _parse_named(conf):
    result: Dict[str, Dict] = {"remote": {}, "machine": {}}

    for section, val in conf.items():
        match = re_find(r'^\s*(remote|machine)\s*"(.*)"\s*$', section)
        if match:
            key, name = match
            result[key][name] = val
        else:
            result[section] = val

    return result


def _pack_named(conf):
    # Drop empty sections
    result = compact(conf)

    # Transform remote.name -> 'remote "name"'
    for key in ("remote", "machine"):
        for name, val in conf[key].items():
            result[f'{key} "{name}"'] = val
        result.pop(key, None)

    return result


def merge(into, update):
    """Merges second dict into first recursively"""
    for key, val in update.items():
        if isinstance(into.get(key), dict) and isinstance(val, dict):
            merge(into[key], val)
        else:
            into[key] = val


def _lower_keys(data):
    return {
        k.lower(): _lower_keys(v) if isinstance(v, dict) else v for k, v in data.items()
    }




dvc/config_schema.py
import logging
import os
from urllib.parse import urlparse

from funcy import once, walk_values
from voluptuous import (
    REMOVE_EXTRA,
    All,
    Any,
    Coerce,
    Invalid,
    Lower,
    Optional,
    Range,
    Schema,
)

logger = logging.getLogger(__name__)

Bool = All(
    Lower,
    Any("true", "false"),
    lambda v: v == "true",
    msg="expected true or false",
)


def supported_cache_type(types):
    """Checks if link type config option consists only of valid values.

    Args:
        types (list/string): type(s) of links that dvc should try out.
    """
    if types is None:
        return None
    if isinstance(types, str):
        types = [typ.strip() for typ in types.split(",")]

    unsupported = set(types) - {"reflink", "hardlink", "symlink", "copy"}
    if unsupported:
        raise Invalid("Unsupported cache type(s): {}".format(", ".join(unsupported)))

    return types


def Choices(*choices):  # noqa: N802
    """Checks that value belongs to the specified set of values

    Args:
        *choices: pass allowed values as arguments, or pass a list or
            tuple as a single argument
    """
    return Any(*choices, msg="expected one of {}".format(", ".join(choices)))


def ByUrl(mapping):  # noqa: N802
    schemas = walk_values(Schema, mapping)

    def validate(data):
        if "url" not in data:
            raise Invalid("expected 'url'")

        parsed = urlparse(data["url"])
        # Windows absolute paths should really have scheme == "" (local)
        if os.name == "nt" and len(parsed.scheme) == 1 and not parsed.netloc:
            return schemas[""](data)
        if not parsed.netloc:
            return schemas[""](data)
        if parsed.scheme not in schemas:
            raise Invalid(f"Unsupported URL type {parsed.scheme}://")

        return schemas[parsed.scheme](data)

    return validate


class RelPath(str):
    pass


class FeatureSchema(Schema):
    def __init__(self, schema, required=False):
        super().__init__(schema, required=required, extra=REMOVE_EXTRA)

    @staticmethod
    @once
    def _log_deprecated(keys):
        # only run this once per session
        message = "%s config option%s unsupported"
        paths = ", ".join(f"'feature.{key}'" for key in keys)
        pluralize = " is" if len(keys) == 1 else "s are"
        logger.warning(message, paths, pluralize)

    def __call__(self, data):
        ret = super().__call__(data)
        extra_keys = data.keys() - ret.keys()
        if extra_keys:
            self._log_deprecated(sorted(extra_keys))
        return ret


REMOTE_COMMON = {
    "url": str,
    "checksum_jobs": All(Coerce(int), Range(1)),
    "jobs": All(Coerce(int), Range(1)),
    Optional("worktree"): Bool,
    Optional("no_traverse"): Bool,  # obsoleted
    Optional("version_aware"): Bool,
}
LOCAL_COMMON = {
    "type": supported_cache_type,
    Optional("protected", default=False): Bool,  # obsoleted
    "shared": All(Lower, Choices("group")),
    Optional("slow_link_warning", default=True): Bool,
    Optional("verify", default=False): Bool,
}
HTTP_COMMON = {
    "auth": All(Lower, Choices("basic", "digest", "custom")),
    "custom_auth_header": str,
    "user": str,
    "password": str,
    "ask_password": Bool,
    "ssl_verify": Any(Bool, str),
    "method": str,
    "connect_timeout": All(Coerce(float), Range(0, min_included=True)),
    "read_timeout": All(Coerce(float), Range(0, min_included=True)),
    Optional("verify", default=False): Bool,
}
WEBDAV_COMMON = {
    "user": str,
    "password": str,
    "ask_password": Bool,
    "token": str,
    "custom_auth_header": str,
    "cert_path": str,
    "key_path": str,
    "timeout": Coerce(int),
    "ssl_verify": Any(Bool, str),
    Optional("verify", default=False): Bool,
}

SCHEMA = {
    "core": {
        "remote": Lower,
        "checksum_jobs": All(Coerce(int), Range(1)),
        Optional("interactive", default=False): Bool,
        Optional("analytics", default=True): Bool,
        Optional("hardlink_lock", default=False): Bool,
        Optional("no_scm", default=False): Bool,
        Optional("autostage", default=False): Bool,
        Optional("experiments"): Bool,  # obsoleted
        Optional("check_update", default=True): Bool,
        "site_cache_dir": str,
        "machine": Lower,
    },
    "cache": {
        "local": str,
        "s3": str,
        "gs": str,
        "hdfs": str,
        "webhdfs": str,
        "ssh": str,
        "azure": str,
        # This is for default local cache
        "dir": str,
        **LOCAL_COMMON,
    },
    "remote": {
        str: ByUrl(
            {
                "": {**LOCAL_COMMON, **REMOTE_COMMON},
                "s3": {
                    "region": str,
                    "profile": str,
                    "credentialpath": str,
                    "configpath": str,
                    "endpointurl": str,
                    "access_key_id": str,
                    "secret_access_key": str,
                    "session_token": str,
                    Optional("listobjects", default=False): Bool,  # obsoleted
                    Optional("use_ssl", default=True): Bool,
                    "ssl_verify": Any(Bool, str),
                    "sse": str,
                    "sse_kms_key_id": str,
                    "sse_customer_algorithm": str,
                    "sse_customer_key": str,
                    "acl": str,
                    "grant_read": str,
                    "grant_read_acp": str,
                    "grant_write_acp": str,
                    "grant_full_control": str,
                    "cache_regions": bool,
                    "read_timeout": Coerce(int),
                    "connect_timeout": Coerce(int),
                    Optional("verify", default=False): Bool,
                    **REMOTE_COMMON,
                },
                "gs": {
                    "projectname": str,
                    "credentialpath": str,
                    "endpointurl": str,
                    Optional("verify", default=False): Bool,
                    **REMOTE_COMMON,
                },
                "ssh": {
                    "type": supported_cache_type,
                    "port": Coerce(int),
                    "user": str,
                    "password": str,
                    "ask_password": Bool,
                    "passphrase": str,
                    "ask_passphrase": Bool,
                    "keyfile": str,
                    "timeout": Coerce(int),
                    "gss_auth": Bool,
                    "allow_agent": Bool,
                    "max_sessions": Coerce(int),
                    Optional("verify", default=False): Bool,
                    **REMOTE_COMMON,
                },
                "hdfs": {"user": str, "kerb_ticket": str, **REMOTE_COMMON},
                "webhdfs": {
                    "kerberos": Bool,
                    "kerberos_principal": str,
                    "proxy_to": str,
                    "ssl_verify": Any(Bool, str),
                    "token": str,
                    "use_https": Bool,
                    Optional("verify", default=False): Bool,
                    **REMOTE_COMMON,
                },
                "azure": {
                    "connection_string": str,
                    "sas_token": str,
                    "account_name": str,
                    "account_key": str,
                    "tenant_id": str,
                    "client_id": str,
                    "client_secret": str,
                    "allow_anonymous_login": Bool,
                    "exclude_environment_credential": Bool,
                    "exclude_visual_studio_code_credential": Bool,
                    "exclude_shared_token_cache_credential": Bool,
                    "exclude_managed_identity_credential": Bool,
                    Optional("verify", default=False): Bool,
                    **REMOTE_COMMON,
                },
                "oss": {
                    "oss_key_id": str,
                    "oss_key_secret": str,
                    "oss_endpoint": str,
                    Optional("verify", default=True): Bool,
                    **REMOTE_COMMON,
                },
                "gdrive": {
                    "profile": str,
                    "gdrive_use_service_account": Bool,
                    "gdrive_client_id": str,
                    "gdrive_client_secret": str,
                    "gdrive_user_credentials_file": str,
                    "gdrive_service_account_user_email": str,
                    "gdrive_service_account_json_file_path": str,
                    Optional("gdrive_trash_only", default=False): Bool,
                    Optional("gdrive_acknowledge_abuse", default=False): Bool,
                    Optional("verify", default=True): Bool,
                    **REMOTE_COMMON,
                },
                "http": {**HTTP_COMMON, **REMOTE_COMMON},
                "https": {**HTTP_COMMON, **REMOTE_COMMON},
                "webdav": {**WEBDAV_COMMON, **REMOTE_COMMON},
                "webdavs": {**WEBDAV_COMMON, **REMOTE_COMMON},
                "remote": {str: object},  # Any of the above options are valid
            }
        )
    },
    "state": {
        "dir": str,  # obsoleted
        "row_limit": All(Coerce(int), Range(1)),  # obsoleted
        "row_cleanup_quota": All(Coerce(int), Range(0, 100)),  # obsoleted
    },
    "index": {
        "dir": str,  # obsoleted
    },
    "machine": {
        str: {
            "cloud": All(Lower, Choices("aws", "azure")),
            "region": All(Lower, Choices("us-west", "us-east", "eu-west", "eu-north")),
            "image": str,
            "spot": Bool,
            "spot_price": Coerce(float),
            "instance_hdd_size": Coerce(int),
            "instance_type": Lower,
            "instance_gpu": Lower,
            "ssh_private": str,
            "startup_script": str,
            "setup_script": str,
        },
    },
    # section for experimental features
    # only specified keys are validated, others get logged and then ignored/removed
    "feature": FeatureSchema(
        {
            Optional("machine", default=False): Bool,
        },
    ),
    "plots": {
        "html_template": str,
        Optional("auto_open", default=False): Bool,
        "out_dir": str,
    },
    "exp": {
        "code": str,
        "data": str,
        "models": str,
        "metrics": str,
        "params": str,
        "plots": str,
        "live": str,
    },
    "parsing": {
        "bool": All(Lower, Choices("store_true", "boolean_optional")),
        "list": All(Lower, Choices("nargs", "append")),
    },
    "hydra": {
        Optional("enabled", default=False): Bool,
        "config_dir": str,
        "config_name": str,
    },
    "studio": {
        "token": str,
        "url": str,
        "repo_url": str,
        Optional("offline", default=False): Bool,
    },
}




dvc/daemon.py
"""Launch `dvc daemon` command in a separate detached process."""

import inspect
import logging
import os
import platform
import sys
from subprocess import Popen  # nosec B404
from typing import List

from dvc.env import DVC_DAEMON
from dvc.utils import fix_env, is_binary

logger = logging.getLogger(__name__)


def _suppress_resource_warning(popen: Popen):
    """Sets the returncode to avoid ResourceWarning when popen is garbage collected."""
    # only use for daemon processes.
    # See https://bugs.python.org/issue38890.
    popen.returncode = 0


def _popen(cmd, **kwargs) -> Popen:
    prefix = [sys.executable]
    if not is_binary():
        main_entrypoint = os.path.join(
            os.path.abspath(os.path.dirname(__file__)), "__main__.py"
        )
        prefix += [main_entrypoint]
    return Popen(prefix + cmd, close_fds=True, shell=False, **kwargs)  # nosec B603


def _spawn_windows(cmd, env):
    if sys.platform == "win32":
        from subprocess import (  # nosec B404
            CREATE_NEW_PROCESS_GROUP,
            CREATE_NO_WINDOW,
            STARTF_USESHOWWINDOW,
            STARTUPINFO,
        )

        # https://stackoverflow.com/a/7006424
        # https://bugs.python.org/issue41619
        creationflags = CREATE_NEW_PROCESS_GROUP | CREATE_NO_WINDOW

        startupinfo = STARTUPINFO()
        startupinfo.dwFlags |= STARTF_USESHOWWINDOW

        popen = _popen(
            cmd, env=env, creationflags=creationflags, startupinfo=startupinfo
        )
        _suppress_resource_warning(popen)


def _spawn_posix(cmd, env):
    from dvc.cli import main

    # `fork` will copy buffers, so we need to flush them before forking.
    # Otherwise, we will get duplicated outputs.
    if sys.stdout and not sys.stdout.closed:
        sys.stdout.flush()
    if sys.stderr and not sys.stderr.closed:
        sys.stderr.flush()

    # NOTE: using os._exit instead of sys.exit, because dvc built
    # with PyInstaller has trouble with SystemExit exception and throws
    # errors such as "[26338] Failed to execute script __main__"
    try:
        # pylint: disable-next=no-member
        pid = os.fork()  # type: ignore[attr-defined]
        if pid > 0:
            return
    except OSError:
        logger.exception("failed at first fork")
        os._exit(1)  # pylint: disable=protected-access

    os.setsid()  # type: ignore[attr-defined]  # pylint: disable=no-member

    try:
        # pylint: disable-next=no-member
        pid = os.fork()  # type: ignore[attr-defined]
        if pid > 0:
            os._exit(0)  # pylint: disable=protected-access
    except OSError:
        logger.exception("failed at second fork")
        os._exit(1)  # pylint: disable=protected-access

    sys.stdin.close()
    sys.stdout.close()
    sys.stderr.close()
    os.closerange(0, 3)

    if platform.system() == "Darwin":
        # workaround for MacOS bug
        # https://github.com/iterative/dvc/issues/4294
        _popen(cmd, env=env).communicate()
    else:
        os.environ.update(env)
        main(cmd)

    os._exit(0)  # pylint: disable=protected-access


def _spawn(cmd, env):
    logger.debug("Trying to spawn '%s'", cmd)

    if os.name == "nt":
        _spawn_windows(cmd, env)
    elif os.name == "posix":
        _spawn_posix(cmd, env)
    else:
        raise NotImplementedError

    logger.debug("Spawned '%s'", cmd)


def daemon(args):
    """Launch a `dvc daemon` command in a detached process.

    Args:
        args (list): list of arguments to append to `dvc daemon` command.
    """
    daemonize(["daemon", "-q", *args])


def daemonize(cmd: List[str]):
    if os.environ.get(DVC_DAEMON):
        logger.debug("skipping launching a new daemon.")
        return

    env = fix_env()
    if not is_binary():
        file_path = os.path.abspath(inspect.stack()[0][1])
        env["PYTHONPATH"] = os.path.dirname(os.path.dirname(file_path))
    env[DVC_DAEMON] = "1"

    _spawn(cmd, env)




dvc/dagascii.py
"""Draws DAG in ASCII."""

import logging
import math
import os

from grandalf.graphs import Edge, Graph, Vertex
from grandalf.layouts import SugiyamaLayout
from grandalf.routing import EdgeViewer, route_with_lines

logger = logging.getLogger(__name__)


class VertexViewer:
    """Class to define vertex box boundaries that will be accounted for during
    graph building by grandalf.

    Args:
        name (str): name of the vertex.
    """

    HEIGHT = 3  # top and bottom box edges + text

    def __init__(self, name):
        # pylint: disable=invalid-name
        self._h = self.HEIGHT  # top and bottom box edges + text
        self._w = len(name) + 2  # right and left bottom edges + text

    @property
    def h(self):  # pylint: disable=invalid-name
        """Height of the box."""
        return self._h

    @property
    def w(self):  # pylint: disable=invalid-name
        """Width of the box."""
        return self._w


class AsciiCanvas:
    """Class for drawing in ASCII.

    Args:
        cols (int): number of columns in the canvas. Should be > 1.
        lines (int): number of lines in the canvas. Should be > 1.
    """

    TIMEOUT = 10

    def __init__(self, cols, lines):
        assert cols > 1
        assert lines > 1

        self.cols = cols
        self.lines = lines

        self.canvas = [[" "] * cols for line in range(lines)]

    def draw(self):
        """Draws ASCII canvas on the screen."""
        lines = map("".join, self.canvas)
        return os.linesep.join(lines)

    def point(self, x, y, char):
        """Create a point on ASCII canvas.

        Args:
            x (int): x coordinate. Should be >= 0 and < number of columns in
                the canvas.
            y (int): y coordinate. Should be >= 0 an < number of lines in the
                canvas.
            char (str): character to place in the specified point on the
                canvas.
        """
        assert len(char) == 1
        assert x >= 0
        assert x < self.cols
        assert y >= 0
        assert y < self.lines

        self.canvas[y][x] = char

    def line(self, x0, y0, x1, y1, char):  # noqa: C901, PLR0912
        """Create a line on ASCII canvas.

        Args:
            x0 (int): x coordinate where the line should start.
            y0 (int): y coordinate where the line should start.
            x1 (int): x coordinate where the line should end.
            y1 (int): y coordinate where the line should end.
            char (str): character to draw the line with.
        """
        # pylint: disable=too-many-arguments, too-many-branches
        if x0 > x1:
            x1, x0 = x0, x1
            y1, y0 = y0, y1

        dx = x1 - x0
        dy = y1 - y0

        if dx == 0 and dy == 0:
            self.point(x0, y0, char)
        elif abs(dx) >= abs(dy):
            for x in range(x0, x1 + 1):
                if dx == 0:
                    y = y0
                else:
                    y = y0 + int(round((x - x0) * dy / float(dx)))
                self.point(x, y, char)
        elif y0 < y1:
            for y in range(y0, y1 + 1):
                if dy == 0:
                    x = x0
                else:
                    x = x0 + int(round((y - y0) * dx / float(dy)))
                self.point(x, y, char)
        else:
            for y in range(y1, y0 + 1):
                if dy == 0:
                    x = x0
                else:
                    x = x1 + int(round((y - y1) * dx / float(dy)))
                self.point(x, y, char)

    def text(self, x, y, text):
        """Print a text on ASCII canvas.

        Args:
            x (int): x coordinate where the text should start.
            y (int): y coordinate where the text should start.
            text (str): string that should be printed.
        """
        for i, char in enumerate(text):
            self.point(x + i, y, char)

    def box(self, x0, y0, width, height):
        """Create a box on ASCII canvas.

        Args:
            x0 (int): x coordinate of the box corner.
            y0 (int): y coordinate of the box corner.
            width (int): box width.
            height (int): box height.
        """
        assert width > 1
        assert height > 1

        width -= 1
        height -= 1

        for x in range(x0, x0 + width):
            self.point(x, y0, "-")
            self.point(x, y0 + height, "-")

        for y in range(y0, y0 + height):
            self.point(x0, y, "|")
            self.point(x0 + width, y, "|")

        self.point(x0, y0, "+")
        self.point(x0 + width, y0, "+")
        self.point(x0, y0 + height, "+")
        self.point(x0 + width, y0 + height, "+")


def _build_sugiyama_layout(vertices, edges):
    #
    # Just a reminder about naming conventions:
    # +------------X
    # |
    # |
    # |
    # |
    # Y
    #

    vertices = {v: Vertex(f" {v} ") for v in vertices}
    # NOTE: reverting edges to correctly orientate the graph
    edges = [Edge(vertices[e], vertices[s]) for s, e in edges]
    vertices = vertices.values()
    graph = Graph(vertices, edges)

    for vertex in vertices:
        vertex.view = VertexViewer(vertex.data)

    # NOTE: determine min box length to create the best layout
    minw = min(v.view.w for v in vertices)

    for edge in edges:
        edge.view = EdgeViewer()

    sug = SugiyamaLayout(graph.C[0])
    graph = graph.C[0]
    roots = list(filter(lambda x: len(x.e_in()) == 0, graph.sV))

    sug.init_all(roots=roots, optimize=True)

    sug.yspace = VertexViewer.HEIGHT
    sug.xspace = minw
    sug.route_edge = route_with_lines

    sug.draw()

    return sug


def draw(vertices, edges):
    """Build a DAG and draw it in ASCII.

    Args:
        vertices (list): list of graph vertices.
        edges (list): list of graph edges.
    """
    # pylint: disable=too-many-locals
    # NOTE: coordinates might me negative, so we need to shift
    # everything to the positive plane before we actually draw it.
    Xs = []  # noqa: N806, pylint: disable=invalid-name
    Ys = []  # noqa: N806, pylint: disable=invalid-name

    sug = _build_sugiyama_layout(vertices, edges)

    for vertex in sug.g.sV:
        # NOTE: moving boxes w/2 to the left
        Xs.append(vertex.view.xy[0] - vertex.view.w / 2.0)
        Xs.append(vertex.view.xy[0] + vertex.view.w / 2.0)
        Ys.append(vertex.view.xy[1])
        Ys.append(vertex.view.xy[1] + vertex.view.h)

    for edge in sug.g.sE:
        for x, y in edge.view._pts:  # pylint: disable=protected-access
            Xs.append(x)
            Ys.append(y)

    minx = min(Xs)
    miny = min(Ys)
    maxx = max(Xs)
    maxy = max(Ys)

    canvas_cols = int(math.ceil(math.ceil(maxx) - math.floor(minx))) + 1
    canvas_lines = int(round(maxy - miny))

    canvas = AsciiCanvas(canvas_cols, canvas_lines)

    # NOTE: first draw edges so that node boxes could overwrite them
    for edge in sug.g.sE:
        # pylint: disable=protected-access
        assert len(edge.view._pts) > 1
        for index in range(1, len(edge.view._pts)):
            start = edge.view._pts[index - 1]
            end = edge.view._pts[index]

            start_x = int(round(start[0] - minx))
            start_y = int(round(start[1] - miny))
            end_x = int(round(end[0] - minx))
            end_y = int(round(end[1] - miny))

            assert start_x >= 0
            assert start_y >= 0
            assert end_x >= 0
            assert end_y >= 0

            canvas.line(start_x, start_y, end_x, end_y, "*")

    for vertex in sug.g.sV:
        # NOTE: moving boxes w/2 to the left
        x = vertex.view.xy[0] - vertex.view.w / 2.0
        y = vertex.view.xy[1]

        canvas.box(
            int(round(x - minx)),
            int(round(y - miny)),
            vertex.view.w,
            vertex.view.h,
        )

        canvas.text(int(round(x - minx)) + 1, int(round(y - miny)) + 1, vertex.data)

    return canvas.draw()




dvc/data_cloud.py
"""Manages dvc remotes that user can use with push/pull/status commands."""

import logging
from typing import TYPE_CHECKING, Iterable, Optional

from dvc.config import NoRemoteError, RemoteConfigError
from dvc.utils.objects import cached_property
from dvc_data.hashfile.db import get_index

if TYPE_CHECKING:
    from dvc.fs import FileSystem
    from dvc_data.hashfile.db import HashFileDB
    from dvc_data.hashfile.hash_info import HashInfo
    from dvc_data.hashfile.status import CompareStatusResult
    from dvc_data.hashfile.transfer import TransferResult

logger = logging.getLogger(__name__)


class Remote:
    def __init__(self, name: str, path: str, fs: "FileSystem", *, index=None, **config):
        self.path = path
        self.fs = fs
        self.name = name
        self.index = index

        self.worktree: bool = config.pop("worktree", False)
        self.config = config

    @cached_property
    def odb(self) -> "HashFileDB":
        from dvc_data.hashfile.db import get_odb

        path = self.path
        if self.worktree:
            path = self.fs.path.join(path, ".dvc", "cache")

        return get_odb(self.fs, path, hash_name="md5", **self.config)


class DataCloud:
    """Class that manages dvc remotes.

    Args:
        repo (dvc.repo.Repo): repo instance that belongs to the repo that
            we are working on.

    Raises:
        config.ConfigError: thrown when config has invalid format.
    """

    def __init__(self, repo):
        self.repo = repo

    def get_remote(
        self,
        name: Optional[str] = None,
        command: str = "<command>",
    ) -> "Remote":
        if not name:
            name = self.repo.config["core"].get("remote")

        if name:
            from dvc.fs import get_cloud_fs

            cls, config, fs_path = get_cloud_fs(self.repo, name=name)

            if config.get("worktree"):
                version_aware = config.get("version_aware")
                if version_aware is False:
                    raise RemoteConfigError(
                        "worktree remotes require version_aware cloud"
                    )
                if version_aware is None:
                    config["version_aware"] = True

            fs = cls(**config)
            config["tmp_dir"] = self.repo.site_cache_dir
            if self.repo.data_index is not None:
                index = self.repo.data_index.view(("remote", name))
            else:
                index = None
            return Remote(name, fs_path, fs, index=index, **config)

        if bool(self.repo.config["remote"]):
            error_msg = (
                f"no remote specified in {self.repo}. Setup default remote with\n"
                "    dvc remote default <remote name>\n"
                "or use:\n"
                f"    dvc {command} -r <remote name>"
            )
        else:
            error_msg = (
                f"no remote specified in {self.repo}. Create a default remote with\n"
                "    dvc remote add -d <remote name> <remote url>"
            )

        raise NoRemoteError(error_msg)

    def get_remote_odb(
        self,
        name: Optional[str] = None,
        command: str = "<command>",
    ) -> "HashFileDB":
        remote = self.get_remote(name=name, command=command)
        if remote.fs.version_aware or remote.worktree:
            raise NoRemoteError(
                f"'{command}' is unsupported for cloud versioned remotes"
            )
        return remote.odb

    def _log_missing(self, status: "CompareStatusResult"):
        if status.missing:
            missing_desc = "\n".join(
                f"name: {hash_info.obj_name}, {hash_info}"
                for hash_info in status.missing
            )
            logger.warning(
                (
                    "Some of the cache files do not exist neither locally "
                    "nor on remote. Missing cache files:\n%s"
                ),
                missing_desc,
            )

    def transfer(
        self,
        src_odb: "HashFileDB",
        dest_odb: "HashFileDB",
        objs: Iterable["HashInfo"],
        **kwargs,
    ) -> "TransferResult":
        from dvc_data.hashfile.transfer import transfer

        return transfer(src_odb, dest_odb, objs, **kwargs)

    def push(
        self,
        objs: Iterable["HashInfo"],
        jobs: Optional[int] = None,
        remote: Optional[str] = None,
        odb: Optional["HashFileDB"] = None,
    ) -> "TransferResult":
        """Push data items in a cloud-agnostic way.

        Args:
            objs: objects to push to the cloud.
            jobs: number of jobs that can be running simultaneously.
            remote: optional name of remote to push to.
                By default remote from core.remote config option is used.
            odb: optional ODB to push to. Overrides remote.
        """
        odb = odb or self.get_remote_odb(remote, "push")
        return self.transfer(
            self.repo.cache.local,
            odb,
            objs,
            jobs=jobs,
            dest_index=get_index(odb),
            cache_odb=self.repo.cache.local,
            validate_status=self._log_missing,
        )

    def pull(
        self,
        objs: Iterable["HashInfo"],
        jobs: Optional[int] = None,
        remote: Optional[str] = None,
        odb: Optional["HashFileDB"] = None,
    ) -> "TransferResult":
        """Pull data items in a cloud-agnostic way.

        Args:
            objs: objects to pull from the cloud.
            jobs: number of jobs that can be running simultaneously.
            remote: optional name of remote to pull from.
                By default remote from core.remote config option is used.
            odb: optional ODB to pull from. Overrides remote.
        """
        odb = odb or self.get_remote_odb(remote, "pull")
        return self.transfer(
            odb,
            self.repo.cache.local,
            objs,
            jobs=jobs,
            src_index=get_index(odb),
            cache_odb=self.repo.cache.local,
            verify=odb.verify,
            validate_status=self._log_missing,
        )

    def status(
        self,
        objs: Iterable["HashInfo"],
        jobs: Optional[int] = None,
        remote: Optional[str] = None,
        odb: Optional["HashFileDB"] = None,
    ):
        """Check status of data items in a cloud-agnostic way.

        Args:
            objs: objects to check status for.
            jobs: number of jobs that can be running simultaneously.
            remote: optional remote to compare
                cache to. By default remote from core.remote config option
                is used.
            odb: optional ODB to check status from. Overrides remote.
        """
        from dvc_data.hashfile.status import compare_status

        if not odb:
            odb = self.get_remote_odb(remote, "status")
        return compare_status(
            self.repo.cache.local,
            odb,
            objs,
            jobs=jobs,
            dest_index=get_index(odb),
            cache_odb=self.repo.cache.local,
        )

    def get_url_for(self, remote, checksum):
        odb = self.get_remote_odb(remote)
        path = odb.oid_to_path(checksum)
        return odb.fs.unstrip_protocol(path)




dvc/dirs.py
import os

import platformdirs

from . import env

APPNAME = "dvc"
APPAUTHOR = "iterative"


def system_config_dir():
    return os.getenv(env.DVC_SYSTEM_CONFIG_DIR) or platformdirs.site_config_dir(
        APPNAME, APPAUTHOR
    )


def global_config_dir():
    return os.getenv(env.DVC_SYSTEM_CONFIG_DIR) or platformdirs.user_config_dir(
        APPNAME, APPAUTHOR
    )


def site_cache_dir():
    return os.getenv(env.DVC_SITE_CACHE_DIR) or platformdirs.site_cache_dir(
        APPNAME, APPAUTHOR, opinion=True
    )




dvc/dvcfile.py
import contextlib
import logging
import os
from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    Dict,
    List,
    Optional,
    Tuple,
    TypeVar,
    Union,
)

from dvc.exceptions import DvcException
from dvc.stage import serialize
from dvc.stage.exceptions import (
    StageFileBadNameError,
    StageFileDoesNotExistError,
    StageFileIsNotDvcFileError,
)
from dvc.utils import relpath
from dvc.utils.collections import apply_diff
from dvc.utils.objects import cached_property
from dvc.utils.serialize import dump_yaml, modify_yaml

if TYPE_CHECKING:
    from dvc.repo import Repo
    from dvc.types import StrOrBytesPath

    from .parsing import DataResolver
    from .stage import Stage

logger = logging.getLogger(__name__)
_T = TypeVar("_T")

DVC_FILE = "Dvcfile"
DVC_FILE_SUFFIX = ".dvc"
PROJECT_FILE = "dvc.yaml"
LOCK_FILE = "dvc.lock"


class FileIsGitIgnored(DvcException):
    def __init__(self, path, pipeline_file=False):
        super().__init__(
            "{}'{}' is git-ignored.".format(
                "bad DVC file name " if pipeline_file else "", path
            )
        )


class ParametrizedDumpError(DvcException):
    pass


def is_valid_filename(path):
    return path.endswith(DVC_FILE_SUFFIX) or os.path.basename(path) in [
        DVC_FILE,
        PROJECT_FILE,
    ]


def is_dvc_file(path):
    return os.path.isfile(path) and (is_valid_filename(path) or is_lock_file(path))


def is_lock_file(path):
    return os.path.basename(path) == LOCK_FILE


def is_git_ignored(repo, path):
    from dvc.fs import LocalFileSystem
    from dvc.scm import NoSCMError

    try:
        return isinstance(repo.fs, LocalFileSystem) and repo.scm.is_ignored(path)
    except NoSCMError:
        return False


def check_dvcfile_path(repo, path):
    if not is_valid_filename(path):
        raise StageFileBadNameError(
            "bad DVC file name '{}'. DVC files should be named "
            "'{}' or have a '.dvc' suffix (e.g. '{}.dvc').".format(
                relpath(path), PROJECT_FILE, os.path.basename(path)
            )
        )

    if is_git_ignored(repo, path):
        raise FileIsGitIgnored(relpath(path), True)


class FileMixin:
    SCHEMA: Callable[[_T], _T]

    def __init__(self, repo, path, verify=True, **kwargs):
        self.repo = repo
        self.path = path
        self.verify = verify

    def __repr__(self):
        return "{}: {}".format(
            self.__class__.__name__, relpath(self.path, self.repo.root_dir)
        )

    def __hash__(self):
        return hash(self.path)

    def __eq__(self, other):
        return self.repo == other.repo and os.path.abspath(
            self.path
        ) == os.path.abspath(other.path)

    def __str__(self):
        return f"{self.__class__.__name__}: {self.relpath}"

    @property
    def relpath(self):
        return relpath(self.path)

    def exists(self):
        is_ignored = self.repo.dvcignore.is_ignored_file(self.path)
        return self.repo.fs.exists(self.path) and not is_ignored

    def _is_git_ignored(self):
        return is_git_ignored(self.repo, self.path)

    def _verify_filename(self):
        if self.verify:
            check_dvcfile_path(self.repo, self.path)

    def _check_gitignored(self):
        if self._is_git_ignored():
            raise FileIsGitIgnored(self.path)

    def load(self, **kwargs: Any) -> Any:
        d, _ = self._load(**kwargs)
        return d

    def _load(self, **kwargs: Any) -> Tuple[Any, str]:
        # it raises the proper exceptions by priority:
        # 1. when the file doesn't exists
        # 2. filename is not a DVC file
        # 3. path doesn't represent a regular file
        # 4. when the file is git ignored
        if not self.exists():
            dvc_ignored = self.repo.dvcignore.is_ignored_file(self.path)
            raise StageFileDoesNotExistError(self.path, dvc_ignored=dvc_ignored)

        self._verify_filename()
        if not self.repo.fs.isfile(self.path):
            raise StageFileIsNotDvcFileError(self.path)

        self._check_gitignored()
        return self._load_yaml(**kwargs)

    @classmethod
    def validate(cls, d: _T, fname: Optional[str] = None) -> _T:
        from dvc.utils.strictyaml import validate

        return validate(d, cls.SCHEMA, path=fname)  # type: ignore[arg-type]

    def _load_yaml(self, **kwargs: Any) -> Tuple[Any, str]:
        from dvc.utils import strictyaml

        return strictyaml.load(
            self.path,
            self.SCHEMA,  # type: ignore[arg-type]
            self.repo.fs,
            **kwargs,
        )

    # pylint: disable-next=unused-argument
    def remove(self, force=False):  # noqa: ARG002
        with contextlib.suppress(FileNotFoundError):
            os.unlink(self.path)

    def dump(self, stage, **kwargs):
        raise NotImplementedError

    def merge(self, ancestor, other, allowed=None):
        raise NotImplementedError


class SingleStageFile(FileMixin):
    from dvc.schema import COMPILED_SINGLE_STAGE_SCHEMA as SCHEMA
    from dvc.stage.loader import SingleStageLoader as LOADER  # noqa: N814

    metrics: List[str] = []
    plots: Any = {}
    params: List[str] = []
    artifacts: Dict[str, Optional[Dict[str, Any]]] = {}

    @property
    def stage(self) -> "Stage":
        data, raw = self._load()
        return self.LOADER.load_stage(self, data, raw)

    @property
    def stages(self) -> LOADER:
        data, raw = self._load()
        return self.LOADER(self, data, raw)

    def dump(self, stage, **kwargs) -> None:
        """Dumps given stage appropriately in the dvcfile."""
        from dvc.stage import PipelineStage

        assert not isinstance(stage, PipelineStage)
        if self.verify:
            check_dvcfile_path(self.repo, self.path)
        logger.debug("Saving information to '%s'.", relpath(self.path))
        dump_yaml(self.path, serialize.to_single_stage_file(stage, **kwargs))
        self.repo.scm_context.track_file(self.relpath)

    # pylint: disable-next=unused-argument
    def remove_stage(self, stage):  # noqa: ARG002
        self.remove()

    def merge(self, ancestor, other, allowed=None):
        assert isinstance(ancestor, SingleStageFile)
        assert isinstance(other, SingleStageFile)

        stage = self.stage
        stage.merge(ancestor.stage, other.stage, allowed=allowed)
        self.dump(stage)


class ProjectFile(FileMixin):
    """Abstraction for pipelines file, .yaml + .lock combined."""

    from dvc.schema import COMPILED_MULTI_STAGE_SCHEMA as SCHEMA
    from dvc.stage.loader import StageLoader as LOADER  # noqa: N814

    @property
    def _lockfile(self):
        return Lockfile(self.repo, os.path.splitext(self.path)[0] + ".lock")

    def _reset(self):
        self.__dict__.pop("contents", None)
        self.__dict__.pop("lockfile_contents", None)
        self.__dict__.pop("resolver", None)
        self.__dict__.pop("stages", None)

    def dump(
        self, stage, update_pipeline=True, update_lock=True, **kwargs
    ):  # pylint: disable=arguments-differ
        """Dumps given stage appropriately in the dvcfile."""
        from dvc.stage import PipelineStage

        assert isinstance(stage, PipelineStage)
        if self.verify:
            check_dvcfile_path(self.repo, self.path)

        if update_pipeline and not stage.is_data_source:
            self._dump_pipeline_file(stage)

        if update_lock:
            self._dump_lockfile(stage, **kwargs)

    def _dump_lockfile(self, stage, **kwargs):
        self._lockfile.dump(stage, **kwargs)

    @staticmethod
    def _check_if_parametrized(stage, action: str = "dump") -> None:
        if stage.raw_data.parametrized:
            raise ParametrizedDumpError(f"cannot {action} a parametrized {stage}")

    def _dump_pipeline_file(self, stage):
        self._check_if_parametrized(stage)
        stage_data = serialize.to_pipeline_file(stage)

        with modify_yaml(self.path, fs=self.repo.fs) as data:
            if not data:
                logger.info("Creating '%s'", self.relpath)

            data["stages"] = data.get("stages", {})
            existing_entry = stage.name in data["stages"]
            action = "Modifying" if existing_entry else "Adding"
            logger.info("%s stage '%s' in '%s'", action, stage.name, self.relpath)

            if existing_entry:
                orig_stage_data = data["stages"][stage.name]
                apply_diff(stage_data[stage.name], orig_stage_data)
            else:
                data["stages"].update(stage_data)

        self.repo.scm_context.track_file(self.relpath)

    @property
    def stage(self):
        raise DvcException("ProjectFile has multiple stages. Please specify it's name.")

    @cached_property
    def contents(self) -> Dict[str, Any]:
        return self._load()[0]

    @cached_property
    def lockfile_contents(self) -> Dict[str, Any]:
        return self._lockfile.load()

    @cached_property
    def resolver(self) -> "DataResolver":
        from .parsing import DataResolver

        wdir = self.repo.fs.path.parent(self.path)
        return DataResolver(self.repo, wdir, self.contents)

    @cached_property
    def stages(self) -> LOADER:
        return self.LOADER(self, self.contents, self.lockfile_contents)

    @property
    def metrics(self) -> List[str]:
        return self.contents.get("metrics", [])

    @property
    def plots(self) -> Any:
        return self.contents.get("plots", {})

    @property
    def params(self) -> List[str]:
        return self.contents.get("params", [])

    @property
    def artifacts(self) -> Dict[str, Optional[Dict[str, Any]]]:
        return self.contents.get("artifacts", {})

    def remove(self, force=False):
        if not force:
            logger.warning("Cannot remove pipeline file.")
            return

        super().remove()
        self._lockfile.remove()

    def remove_stage(self, stage):
        self._check_if_parametrized(stage, "remove")
        self._lockfile.remove_stage(stage)
        if not self.exists():
            return

        d, _ = self._load_yaml(round_trip=True)
        if stage.name not in d.get("stages", {}):
            return

        logger.debug("Removing '%s' from '%s'", stage.name, self.path)
        del d["stages"][stage.name]

        if d["stages"]:
            dump_yaml(self.path, d)
        else:
            super().remove()

    def merge(self, ancestor, other, allowed=None):
        raise NotImplementedError


class Lockfile(FileMixin):
    from dvc.schema import COMPILED_LOCKFILE_SCHEMA as SCHEMA

    def _verify_filename(self):
        pass  # lockfile path is hardcoded, so no need to verify here

    def _load(self, **kwargs: Any):
        try:
            return super()._load(**kwargs)
        except StageFileDoesNotExistError:
            # we still need to account for git-ignored dvc.lock file
            # even though it may not exist or have been .dvcignored
            self._check_gitignored()
            return {}, ""

    def dump(self, stage, **kwargs):
        stage_data = serialize.to_lockfile(stage, **kwargs)

        with modify_yaml(self.path, fs=self.repo.fs) as data:
            if not data:
                data.update({"schema": "2.0"})
                # order is important, meta should always be at the top
                logger.info("Generating lock file '%s'", self.relpath)

            data["stages"] = data.get("stages", {})
            modified = data["stages"].get(stage.name, {}) != stage_data.get(
                stage.name, {}
            )
            if modified:
                logger.info("Updating lock file '%s'", self.relpath)

            data["stages"].update(stage_data)

        if modified:
            self.repo.scm_context.track_file(self.relpath)

    def remove_stage(self, stage):
        if not self.exists():
            return

        d, _ = self._load_yaml(round_trip=True)
        data = d.get("stages", {})
        if stage.name not in data:
            return

        logger.debug("Removing '%s' from '%s'", stage.name, self.path)
        del data[stage.name]

        if data:
            dump_yaml(self.path, d)
        else:
            self.remove()

    def merge(self, ancestor, other, allowed=None):
        raise NotImplementedError


def load_file(
    repo: "Repo", path: "StrOrBytesPath", **kwargs: Any
) -> Union[ProjectFile, SingleStageFile]:
    _, ext = os.path.splitext(path)
    if ext in (".yaml", ".yml"):
        return ProjectFile(repo, path, **kwargs)
    return SingleStageFile(repo, path, **kwargs)




dvc/env.py
DVC_DAEMON = "DVC_DAEMON"
DVC_EXP_AUTO_PUSH = "DVC_EXP_AUTO_PUSH"
DVC_EXP_BASELINE_REV = "DVC_EXP_BASELINE_REV"
DVC_EXP_GIT_REMOTE = "DVC_EXP_GIT_REMOTE"
DVC_EXP_NAME = "DVC_EXP_NAME"
DVC_GLOBAL_CONFIG_DIR = "DVC_GLOBAL_CONFIG_DIR"
DVC_IGNORE_ISATTY = "DVC_IGNORE_ISATTY"
DVC_NO_ANALYTICS = "DVC_NO_ANALYTICS"
DVC_PAGER = "DVC_PAGER"
DVC_ROOT = "DVC_ROOT"
DVC_SHOW_TRACEBACK = "DVC_SHOW_TRACEBACK"
DVC_SITE_CACHE_DIR = "DVC_SITE_CACHE_DIR"
DVC_STUDIO_OFFLINE = "DVC_STUDIO_OFFLINE"
DVC_STUDIO_REPO_URL = "DVC_STUDIO_REPO_URL"
DVC_STUDIO_TOKEN = "DVC_STUDIO_TOKEN"  # noqa: S105 # nosec B105
DVC_STUDIO_URL = "DVC_STUDIO_URL"
DVC_SYSTEM_CONFIG_DIR = "DVC_SYSTEM_CONFIG_DIR"




dvc/exceptions.py
"""Exceptions raised by the dvc."""
import errno
from typing import Dict, List

from dvc.utils import format_link


class DvcException(Exception):
    """Base class for all dvc exceptions."""

    def __init__(self, msg, *args):
        assert msg
        self.msg = msg
        super().__init__(msg, *args)


class InvalidArgumentError(ValueError, DvcException):
    """Thrown if arguments are invalid."""


class OutputDuplicationError(DvcException):
    """Thrown if a file/directory is specified as an output in more than one
    stage.

    Args:
        output (unicode): path to the file/directory.
        stages (list): list of paths to stages.
    """

    def __init__(self, output, stages):
        from funcy import first

        assert isinstance(output, str)
        assert all(hasattr(stage, "relpath") for stage in stages)
        msg = ""
        stage_names = [s.addressing for s in stages]
        stages_str = " ".join(stage_names)
        if len(stages) == 1:
            stage_name = first(stages)
            msg = f"output '{output}' is already specified in {stage_name}."
        else:
            msg = "output '{}' is already specified in stages:\n{}".format(
                output, "\n".join(f"\t- {s}" for s in stage_names)
            )
        msg += (
            f"\nUse `dvc remove {stages_str}` to stop tracking the overlapping output."
        )
        super().__init__(msg)
        self.stages = stages
        self.output = output


class OutputNotFoundError(DvcException):
    """Thrown if a file/directory is not found as an output in any pipeline.

    Args:
        output (unicode): path to the file/directory.
    """

    def __init__(self, output, repo=None):
        from dvc.utils import relpath

        self.output = output
        self.repo = repo
        super().__init__(
            "Unable to find DVC file with output '{path}'".format(
                path=relpath(self.output)
            )
        )


class StagePathAsOutputError(DvcException):
    """Thrown if directory that stage is going to be saved in is specified as
    an output of another stage.

    Args:
        stage (Stage): a stage that is in some other stages output
        output (str): an output covering the stage above
    """

    def __init__(self, stage, output):
        assert isinstance(output, str)
        super().__init__(
            "{stage} is within an output '{output}' of another stage".format(
                stage=stage, output=output
            )
        )


class CircularDependencyError(DvcException):
    """Thrown if a file/directory specified both as an output and as a
    dependency.

    Args:
        dependency (str): path to the dependency.
    """

    def __init__(self, dependency):
        assert isinstance(dependency, str)

        msg = "'{}' is specified as an output and as a dependency."
        super().__init__(msg.format(dependency))


class ArgumentDuplicationError(DvcException):
    """Thrown if a file/directory is specified as a dependency/output more
    than once.

    Args:
        path (str): path to the file/directory.
    """

    def __init__(self, path):
        assert isinstance(path, str)
        super().__init__(f"file '{path}' is specified more than once.")


class MoveNotDataSourceError(DvcException):
    """Thrown when trying to move a file/directory that is not an output
    in a data source stage.

    Args:
        path (str): path to the file/directory.
    """

    def __init__(self, path):
        msg = (
            "move is not permitted for stages that are not data sources. "
            "You need to either move '{path}' to a new location and edit "
            "it by hand, or remove '{path}' and create a new one at the "
            "desired location."
        )
        super().__init__(msg.format(path=path))


class NotDvcRepoError(DvcException):
    """Thrown if a directory is not a DVC repo"""


class CyclicGraphError(DvcException):
    def __init__(self, stages):
        assert isinstance(stages, list)
        stage_part = "stage" if len(stages) == 1 else "stages"
        msg = (
            "Same item(s) are defined as both a dependency and an output "
            "in {stage_part}: {stage}."
        )
        super().__init__(
            msg.format(
                stage_part=stage_part,
                stage=", ".join(s.addressing for s in stages),
            )
        )


class ConfirmRemoveError(DvcException):
    def __init__(self, path):
        super().__init__(
            "unable to remove '{}' without a confirmation. Use `-f` to force.".format(
                path
            )
        )


class InitError(DvcException):
    pass


class ReproductionError(DvcException):
    def __init__(self, name):
        self.name = name
        super().__init__(f"failed to reproduce '{name}'")


class BadMetricError(DvcException):
    def __init__(self, paths):
        super().__init__(
            "the following metrics do not exist, "
            "are not metrics files or are malformed: {paths}".format(
                paths=", ".join(f"'{path}'" for path in paths)
            )
        )


class OverlappingOutputPathsError(DvcException):
    def __init__(self, parent, overlapping_out, message):
        self.parent = parent
        self.overlapping_out = overlapping_out
        super().__init__(message)


class CheckoutErrorSuggestGit(DvcException):
    def __init__(self, target):
        super().__init__(f"Did you mean `git checkout {target}`?")


class ETagMismatchError(DvcException):
    def __init__(self, etag, cached_etag):
        super().__init__(
            "ETag mismatch detected when copying file to cache! "
            "(expected: '{}', actual: '{}')".format(etag, cached_etag)
        )


class FileExistsLocallyError(FileExistsError, DvcException):
    def __init__(self, path, hint=None):
        import os.path

        self.path = path
        hint = "" if hint is None else f". {hint}"
        path_typ = "directory" if os.path.isdir(path) else "file"
        msg = f"The {path_typ} '{path}' already exists locally{hint}"
        super().__init__(msg)
        self.errno = errno.EEXIST


class FileMissingError(DvcException):
    def __init__(self, path, hint=None):
        self.path = path
        hint = "" if hint is None else f". {hint}"
        super().__init__(f"Can't find '{path}' neither locally nor on remote{hint}")


class FileTransferError(DvcException):
    _METHOD = "transfer"

    def __init__(self, amount):
        self.amount = amount

        super().__init__(f"{amount} files failed to {self._METHOD}")


class DownloadError(FileTransferError):
    _METHOD = "download"


class UploadError(FileTransferError):
    _METHOD = "upload"


class CheckoutError(DvcException):
    def __init__(self, target_infos: List[str], stats: Dict[str, List[str]]):
        from dvc.utils import error_link

        self.target_infos = target_infos
        self.stats = stats
        targets = [str(t) for t in target_infos]
        m = (
            "Checkout failed for following targets:\n{}\nIs your "
            "cache up to date?\n{}".format(
                "\n".join(targets), error_link("missing-files")
            )
        )
        super().__init__(m)


class CollectCacheError(DvcException):
    pass


class NoRemoteInExternalRepoError(DvcException):
    def __init__(self, url):
        super().__init__(f"No DVC remote is specified in target repository '{url}'.")


class NoOutputInExternalRepoError(DvcException):
    def __init__(self, path, external_repo_path, external_repo_url):
        from dvc.utils import relpath

        super().__init__(
            "Output '{}' not found in target repository '{}'".format(
                relpath(path, external_repo_path), external_repo_url
            )
        )


class HTTPError(DvcException):
    def __init__(self, code, reason):
        super().__init__(f"'{code} {reason}'")


class PathMissingError(DvcException):
    default_msg = (
        "The path '{}' does not exist in the target repository '{}'"
        " neither as a DVC output nor as a Git-tracked file."
    )
    default_msg_dvc_only = (
        "The path '{}' does not exist in the target repository '{}' as an DVC output."
    )

    def __init__(self, path, repo, dvc_only=False):
        msg = self.default_msg if not dvc_only else self.default_msg_dvc_only
        super().__init__(msg.format(path, repo))
        self.dvc_only = dvc_only


class URLMissingError(DvcException):
    def __init__(self, url):
        super().__init__(f"The path '{url}' does not exist")


class RemoteCacheRequiredError(DvcException):
    def __init__(self, scheme, fs_path):
        super().__init__(
            (
                "Current operation was unsuccessful because '{}' requires "
                "existing cache on '{}' remote. See {} for information on how "
                "to set up remote cache."
            ).format(
                fs_path,
                scheme,
                format_link("https://man.dvc.org/config#cache"),
            )
        )


class IsADirectoryError(DvcException):  # noqa,pylint:disable=redefined-builtin
    """Raised when a file operation is requested on a directory."""


class NoOutputOrStageError(DvcException):
    """
    Raised when the target is neither an output nor a stage name in dvc.yaml
    """

    def __init__(self, target, file):
        super().__init__(
            f"'{target}' does not exist as an output or a stage name in '{file}'"
        )


class MergeError(DvcException):
    pass


class CacheLinkError(DvcException):
    SUPPORT_LINK = "See {} for more information.".format(
        format_link("https://dvc.org/doc/user-guide/troubleshooting#cache-types")
    )

    def __init__(self, fs_paths):
        msg = "No possible cache link types for '{}'. {}".format(
            ", ".join(fs_paths), self.SUPPORT_LINK
        )
        super().__init__(msg)
        self.fs_paths = fs_paths


class PrettyDvcException(DvcException):
    def __pretty_exc__(self, **kwargs):
        """Print prettier exception message."""




dvc/ignore.py
import logging
import os
import re
from collections import namedtuple
from itertools import chain, groupby, takewhile
from typing import TYPE_CHECKING, List, Optional

from pathspec.patterns import GitWildMatchPattern
from pathspec.util import normalize_file
from pygtrie import Trie

from dvc.fs import Schemes, localfs
from dvc.pathspec_math import PatternInfo, merge_patterns

if TYPE_CHECKING:
    from dvc.fs import AnyFSPath, FileSystem

logger = logging.getLogger(__name__)


class DvcIgnore:
    DVCIGNORE_FILE = ".dvcignore"

    def __call__(self, root, dirs, files):
        raise NotImplementedError


class DvcIgnorePatterns(DvcIgnore):
    def __init__(self, pattern_list, dirname, sep):
        from pathspec.patterns.gitwildmatch import _DIR_MARK

        if pattern_list and isinstance(pattern_list[0], str):
            pattern_list = [PatternInfo(pattern, "") for pattern in pattern_list]

        self.sep = sep
        self.pattern_list = pattern_list
        self.dirname = dirname

        self.regex_pattern_list = []
        for count, pattern in enumerate(pattern_list):
            pattern, group = GitWildMatchPattern.pattern_to_regex(pattern.patterns)
            if pattern:
                pattern = pattern.replace(f"<{_DIR_MARK}>", f"<{_DIR_MARK}{count}>")
                self.regex_pattern_list.append((pattern, group))

        self.ignore_spec = [
            (ignore, re.compile("|".join(item[0] for item in group)))
            for ignore, group in groupby(self.regex_pattern_list, lambda x: x[1])
            if ignore is not None
        ]

    @classmethod
    def from_file(cls, path, fs, name):
        assert fs.path.isabs(path)
        dirname = fs.path.normpath(fs.path.dirname(path))
        with fs.open(path, encoding="utf-8") as fobj:
            path_spec_lines = [
                PatternInfo(line, f"{name}:{line_no + 1}:{line}")
                for line_no, line in enumerate(map(str.strip, fobj.readlines()))
                if line and not (line.strip().startswith("#"))
            ]

        return cls(path_spec_lines, dirname, fs.sep)

    def __call__(self, root: List[str], dirs: List[str], files: List[str]):
        files = [f for f in files if not self.matches(root, f)]
        dirs = [d for d in dirs if not self.matches(root, d, True)]

        return dirs, files

    def _get_normalize_path(self, dirname, basename):
        # NOTE: `relpath` is too slow, so we have to assume that both
        # `dirname` and `self.dirname` are relative or absolute together.

        prefix = self.dirname.rstrip(self.sep) + self.sep

        if dirname == self.dirname:
            path = basename
        elif dirname.startswith(prefix):
            rel = dirname[len(prefix) :]
            # NOTE: `os.path.join` is ~x5.5 slower
            path = f"{rel}{self.sep}{basename}"
        else:
            return False

        if os.name == "nt":
            return normalize_file(path)
        return path

    def matches(self, dirname, basename, is_dir=False, details: bool = False):
        path = self._get_normalize_path(dirname, basename)
        if not path:
            return False

        if details:
            return self._ignore_details(path, is_dir)
        return self.ignore(path, is_dir)

    def ignore(self, path, is_dir):
        def matches(pattern, path, is_dir) -> bool:
            matches_ = bool(pattern.match(path))

            if is_dir:
                matches_ |= bool(pattern.match(f"{path}/"))

            return matches_

        result = False

        for ignore, pattern in self.ignore_spec[::-1]:
            if matches(pattern, path, is_dir):
                result = ignore
                break
        return result

    def _ignore_details(self, path, is_dir: bool):
        result = []
        for (regex, _), pattern_info in list(
            zip(self.regex_pattern_list, self.pattern_list)
        ):
            # skip system pattern
            if not pattern_info.file_info:
                continue

            regex = re.compile(regex)

            matches = bool(regex.match(path))
            if is_dir:
                matches |= bool(regex.match(f"{path}/"))

            if matches:
                result.append(pattern_info.file_info)

        return result

    def __hash__(self):
        return hash(self.dirname + ":" + str(self.pattern_list))

    def __eq__(self, other):
        if not isinstance(other, DvcIgnorePatterns):
            return NotImplemented
        return (self.dirname == other.dirname) & (
            [pattern.patterns for pattern in self.pattern_list]
            == [pattern.patterns for pattern in other.pattern_list]
        )

    def __bool__(self):
        return bool(self.pattern_list)


CheckIgnoreResult = namedtuple("CheckIgnoreResult", ["file", "match", "patterns"])


def _no_match(path):
    return CheckIgnoreResult(path, False, ["::"])


class DvcIgnoreFilter:
    def __init__(self, fs, root_dir):
        from dvc.repo import Repo

        default_ignore_patterns = [
            ".hg/",
            ".git/",
            ".git",
            f"{Repo.DVC_DIR}/",
        ]

        self.fs = fs
        self.root_dir = root_dir
        self.ignores_trie_fs = Trie()
        self._ignores_trie_subrepos = Trie()

        key = self._get_key(root_dir)
        self.ignores_trie_fs[key] = DvcIgnorePatterns(
            default_ignore_patterns,
            root_dir,
            fs.sep,
        )
        self._ignores_trie_subrepos[key] = self.ignores_trie_fs[key]
        self._update(
            self.root_dir,
            self._ignores_trie_subrepos,
            dnames=None,
            ignore_subrepos=False,
        )
        self._update(
            self.root_dir,
            self.ignores_trie_fs,
            dnames=None,
            ignore_subrepos=True,
        )

    def _get_key(self, path):
        parts = self.fs.path.relparts(path, self.root_dir)
        if parts == (os.curdir,):
            return ()
        return parts

    def _update_trie(self, dirname: str, trie: Trie) -> None:
        key = self._get_key(dirname)
        old_pattern = trie.longest_prefix(key).value
        matches = old_pattern.matches(dirname, DvcIgnore.DVCIGNORE_FILE, False)

        path = self.fs.path.join(dirname, DvcIgnore.DVCIGNORE_FILE)
        if not matches and self.fs.exists(path):
            name = self.fs.path.relpath(path, self.root_dir)
            new_pattern = DvcIgnorePatterns.from_file(path, self.fs, name)
            if old_pattern:
                plist, prefix = merge_patterns(
                    self.fs.path.flavour,
                    old_pattern.pattern_list,
                    old_pattern.dirname,
                    new_pattern.pattern_list,
                    new_pattern.dirname,
                )
                trie[key] = DvcIgnorePatterns(plist, prefix, self.fs.sep)
            else:
                trie[key] = new_pattern
        elif old_pattern:
            trie[key] = old_pattern

    def _update(
        self,
        dirname: str,
        ignore_trie: Trie,
        dnames: Optional["List"],
        ignore_subrepos: bool,
    ) -> None:
        self._update_trie(dirname, ignore_trie)

        if ignore_subrepos:
            if dnames is None:
                try:
                    _, dnames, _ = next(self.fs.walk(dirname))
                except StopIteration:
                    dnames = []

            for dname in dnames:
                self._update_sub_repo(self.fs.path.join(dirname, dname), ignore_trie)

    def _update_sub_repo(self, path, ignore_trie: Trie):
        from dvc.repo import Repo

        if path == self.root_dir:
            return

        dvc_dir = self.fs.path.join(path, Repo.DVC_DIR)
        if not self.fs.exists(dvc_dir):
            return

        root, dname = self.fs.path.split(path)
        key = self._get_key(root)
        pattern_info = PatternInfo(f"/{dname}/", f"in sub_repo:{dname}")
        new_pattern = DvcIgnorePatterns([pattern_info], root, self.fs.sep)
        old_pattern = ignore_trie.longest_prefix(key).value
        if old_pattern:
            plist, prefix = merge_patterns(
                self.fs.path.flavour,
                old_pattern.pattern_list,
                old_pattern.dirname,
                new_pattern.pattern_list,
                new_pattern.dirname,
            )
            ignore_trie[key] = DvcIgnorePatterns(plist, prefix, self.fs.sep)
        else:
            ignore_trie[key] = new_pattern

    def __call__(self, root, dirs, files, ignore_subrepos=True):
        abs_root = self.fs.path.abspath(root)
        ignore_pattern = self._get_trie_pattern(
            abs_root, dnames=dirs, ignore_subrepos=ignore_subrepos
        )
        if ignore_pattern:
            dirs, files = ignore_pattern(abs_root, dirs, files)
        return dirs, files

    def ls(self, fs, path, detail=True, **kwargs):
        fs_dict = {}
        dirs = []
        nondirs = []

        for entry in fs.ls(path, detail=True, **kwargs):
            name = fs.path.name(entry["name"])
            fs_dict[name] = entry
            if entry["type"] == "directory":
                dirs.append(name)
            else:
                nondirs.append(name)

        dirs, nondirs = self(path, dirs, nondirs, **kwargs)

        if not detail:
            return dirs + nondirs

        return [fs_dict[name] for name in chain(dirs, nondirs)]

    def walk(self, fs: "FileSystem", path: "AnyFSPath", **kwargs):
        detail = kwargs.get("detail", False)
        ignore_subrepos = kwargs.pop("ignore_subrepos", True)
        if fs.protocol == Schemes.LOCAL:
            for root, dirs, files in fs.walk(path, **kwargs):
                if detail:
                    all_dnames = set(dirs.keys())
                    all_fnames = set(files.keys())
                    dnames, fnames = self(
                        root,
                        all_dnames,
                        all_fnames,
                        ignore_subrepos=ignore_subrepos,
                    )
                    list(map(dirs.pop, all_dnames - set(dnames)))
                    list(map(files.pop, all_fnames - set(fnames)))
                else:
                    dirs[:], files[:] = self(
                        root, dirs, files, ignore_subrepos=ignore_subrepos
                    )
                yield root, dirs, files
        else:
            yield from fs.walk(path, **kwargs)

    def find(self, fs: "FileSystem", path: "AnyFSPath", **kwargs):
        if fs.protocol == Schemes.LOCAL:
            for root, _, files in self.walk(fs, path, **kwargs):
                for file in files:
                    # NOTE: os.path.join is ~5.5 times slower
                    yield f"{root}{fs.sep}{file}"
        else:
            yield from fs.find(path)

    def _get_trie_pattern(
        self, dirname, dnames: Optional["List"] = None, ignore_subrepos=True
    ) -> Optional["DvcIgnorePatterns"]:
        if ignore_subrepos:
            ignores_trie = self.ignores_trie_fs
        else:
            ignores_trie = self._ignores_trie_subrepos

        if not self.fs.path.isin_or_eq(dirname, self.root_dir):
            # outside of the repo
            return None

        key = self._get_key(dirname)

        ignore_pattern = ignores_trie.get(key)
        if ignore_pattern:
            return ignore_pattern

        prefix_key = ignores_trie.longest_prefix(key).key or ()
        prefix = self.fs.path.join(self.root_dir, *prefix_key)

        dirs = list(
            takewhile(
                lambda path: path != prefix,
                (parent for parent in localfs.path.parents(dirname)),
            )
        )
        dirs.reverse()
        dirs.append(dirname)

        for parent in dirs:
            self._update(parent, ignores_trie, dnames, ignore_subrepos)

        return ignores_trie.get(key)

    def _is_ignored(
        self, path: str, is_dir: bool = False, ignore_subrepos: bool = True
    ):
        if self._outside_repo(path):
            return False
        dirname, basename = self.fs.path.split(self.fs.path.normpath(path))
        ignore_pattern = self._get_trie_pattern(dirname, None, ignore_subrepos)
        if ignore_pattern:
            return ignore_pattern.matches(dirname, basename, is_dir)
        return False

    def is_ignored_dir(self, path: str, ignore_subrepos: bool = True) -> bool:
        # only used in LocalFileSystem
        path = self.fs.path.abspath(path)
        if path == self.root_dir:
            return False

        return self._is_ignored(path, True, ignore_subrepos=ignore_subrepos)

    def is_ignored_file(self, path: str, ignore_subrepos: bool = True) -> bool:
        # only used in LocalFileSystem
        path = self.fs.path.abspath(path)
        return self._is_ignored(path, False, ignore_subrepos=ignore_subrepos)

    def _outside_repo(self, path):
        return not self.fs.path.isin_or_eq(path, self.root_dir)

    def check_ignore(self, target):
        # NOTE: can only be used in `dvc check-ignore`, see
        # https://github.com/iterative/dvc/issues/5046
        full_target = self.fs.path.abspath(target)
        if not self._outside_repo(full_target):
            dirname, basename = self.fs.path.split(self.fs.path.normpath(full_target))
            pattern = self._get_trie_pattern(dirname)
            if pattern:
                matches = pattern.matches(
                    dirname, basename, self.fs.isdir(full_target), True
                )

                if matches:
                    return CheckIgnoreResult(target, True, matches)
        return _no_match(target)

    def is_ignored(
        self, fs: "FileSystem", path: str, ignore_subrepos: bool = True
    ) -> bool:
        # NOTE: can't use self.check_ignore(path).match for now, see
        # https://github.com/iterative/dvc/issues/4555
        if fs.protocol != Schemes.LOCAL:
            return False
        if fs.isfile(path):
            return self.is_ignored_file(path, ignore_subrepos)
        if fs.isdir(path):
            return self.is_ignored_dir(path, ignore_subrepos)
        return self.is_ignored_file(path, ignore_subrepos) or self.is_ignored_dir(
            path, ignore_subrepos
        )


def init(path):
    dvcignore = os.path.join(path, DvcIgnore.DVCIGNORE_FILE)
    if os.path.exists(dvcignore):
        return dvcignore

    with open(dvcignore, "w", encoding="utf-8") as fobj:
        fobj.write(
            "# Add patterns of files dvc should ignore, which could improve\n"
            "# the performance. Learn more at\n"
            "# https://dvc.org/doc/user-guide/dvcignore\n"
        )

    return dvcignore


def destroy(path):
    from dvc.utils.fs import remove

    dvcignore = os.path.join(path, DvcIgnore.DVCIGNORE_FILE)
    remove(dvcignore)




dvc/info.py
import importlib.metadata as importlib_metadata
import itertools
import os
import pathlib
import platform

import psutil

from dvc import __version__
from dvc.exceptions import NotDvcRepoError
from dvc.fs import Schemes, generic, get_fs_cls, get_fs_config, registry
from dvc.repo import Repo
from dvc.scm import SCMError
from dvc.utils import error_link
from dvc.utils.pkg import PKG

SUBPROJECTS = (
    "dvc_data",
    "dvc_objects",
    "dvc_render",
    "dvc_task",
    "scmrepo",
)
package = "" if PKG is None else f" ({PKG})"


def get_dvc_info():
    dvc_version = f"DVC version: {__version__}{package}"
    info = [
        dvc_version,
        "-" * len(dvc_version),
        f"Platform: Python {platform.python_version()} on {platform.platform()}",
        f"Subprojects:{_get_subprojects()}",
        f"Supports:{_get_supported_remotes()}",
        f"Config:{_get_config_dirs()}",
    ]

    try:
        with Repo() as repo:
            # cache_dir might not exist yet (e.g. after `dvc init`), and we
            # can't auto-create it, as it might cause issues if the user
            # later decides to enable shared cache mode with
            # `dvc config cache.shared group`.
            if os.path.exists(repo.cache.local.path):
                info.append(f"Cache types: {_get_linktype_support_info(repo)}")
                fs_type = _get_fs_type(repo.cache.local.path)
                info.append(f"Cache directory: {fs_type}")
            else:
                info.append("Cache types: " + error_link("no-dvc-cache"))

            info.append(f"Caches: {_get_caches(repo.cache)}")
            info.append(f"Remotes: {_get_remotes(repo.config)}")

            root_directory = repo.root_dir
            fs_root = _get_fs_type(os.path.abspath(root_directory))
            info.append(f"Workspace directory: {fs_root}")
            info.append(f"Repo: {_get_dvc_repo_info(repo)}")
            info.append(f"Repo.site_cache_dir: {repo.site_cache_dir}")
    except NotDvcRepoError:
        pass
    except SCMError:
        info.append("Repo: dvc, git (broken)")

    return "\n".join(info)


def _get_caches(cache):
    caches = (
        cache_type
        for cache_type, cache_instance in cache.by_scheme()
        if cache_instance and cache_type != "repo"
    )

    # Caches will be always non-empty including the local cache
    return ", ".join(caches)


def _get_remotes(config):
    schemes = (
        get_fs_cls(get_fs_config(config, name=remote)).protocol
        for remote in config["remote"]
    )

    return ", ".join(schemes) or "None"


def _get_linktype_support_info(repo):
    odb = repo.cache.local

    links = generic.test_links(
        ["reflink", "hardlink", "symlink"],
        odb.fs,
        odb.path,
        repo.fs,
        repo.root_dir,
    )

    return ", ".join(links)


def _get_subprojects():
    subprojects = []
    for subproject in SUBPROJECTS:
        try:
            version = importlib_metadata.version(subproject)
            subprojects.append(f"{subproject} = {version}")
        except ImportError:
            pass

    return "\n\t" + "\n\t".join(subprojects)


def _get_supported_remotes():
    supported_remotes = []
    for scheme in registry:
        if scheme in [Schemes.LOCAL, Schemes.MEMORY, "dvc", "git"]:
            continue

        try:
            fs_cls = registry[scheme]
        except ImportError:
            continue

        if not fs_cls.get_missing_deps():
            dependencies = []
            for requirement in fs_cls.REQUIRES:
                dependencies.append(
                    f"{requirement} = {importlib_metadata.version(requirement)}"
                )

            remote_info = scheme
            if dependencies:
                remote_info += " (" + ", ".join(dependencies) + ")"
            supported_remotes.append(remote_info)

    assert len(supported_remotes) >= 1
    return "\n\t" + ",\n\t".join(supported_remotes)


def _get_config_dirs():
    from dvc.config import Config

    dirs = [
        f"Global: {Config.get_dir('global')}",
        f"System: {Config.get_dir('system')}",
    ]

    return "\n\t" + "\n\t".join(dirs)


def _get_fs_type(path):
    partition = {}
    for part in psutil.disk_partitions(all=True):
        if part.fstype:
            try:
                mountpoint = pathlib.Path(part.mountpoint).resolve()
                partition[mountpoint] = part.fstype + " on " + part.device
            except PermissionError:
                pass

    # need to follow the symlink: https://github.com/iterative/dvc/issues/5065
    path = pathlib.Path(path).resolve()

    for parent in itertools.chain([path], path.parents):
        if parent in partition:
            return partition[parent]
    return ("unknown", "none")


def _get_dvc_repo_info(repo):
    if repo.config.get("core", {}).get("no_scm", False):
        return "dvc (no_scm)"

    if repo.root_dir != repo.scm.root_dir:
        return "dvc (subdir), git"

    return "dvc, git"




dvc/lock.py
"""Manages dvc lock file."""

import hashlib
import os
from abc import ABC, abstractmethod
from datetime import timedelta

import flufl.lock
import zc.lockfile
from funcy import retry

from dvc.exceptions import DvcException
from dvc.progress import Tqdm
from dvc.utils import format_link

DEFAULT_TIMEOUT = 3


FAILED_TO_LOCK_MESSAGE = (
    "Unable to acquire lock. Most likely another DVC process is running or "
    "was terminated abruptly. Check the page {} for other possible reasons "
    "and to learn how to resolve this."
).format(format_link("https://dvc.org/doc/user-guide/troubleshooting#lock-issue"))


class LockError(DvcException):
    """Thrown when unable to acquire the lock for DVC repo."""


class LockBase(ABC):
    @abstractmethod
    def __init__(self, lockfile):
        self._lockfile = lockfile

    @property
    def lockfile(self):
        return self._lockfile

    @abstractmethod
    def lock(self):
        pass

    @abstractmethod
    def unlock(self):
        pass

    @property
    @abstractmethod
    def is_locked(self):
        pass

    @abstractmethod
    def __enter__(self):
        pass

    @abstractmethod
    def __exit__(self, typ, value, tbck):
        pass


class LockNoop(LockBase):
    def __init__(self, *args, **kwargs):  # pylint: disable=super-init-not-called
        self._lock = False

    def lock(self):
        self._lock = True

    def unlock(self):
        if not self.is_locked:
            raise DvcException("Unlock called on an unlocked lock")
        self._lock = False

    @property
    def is_locked(self):
        return self._lock

    def __enter__(self):
        self.lock()

    def __exit__(self, typ, value, tbck):
        self.unlock()


class Lock(LockBase):
    """Class for DVC repo lock.

    Uses zc.lockfile as backend.
    """

    def __init__(self, lockfile, friendly=False, **kwargs):
        super().__init__(lockfile)
        self._friendly = friendly
        self._lock = None
        self._lock_failed = False

    @property
    def files(self):
        return [self._lockfile]

    def _do_lock(self):
        try:
            self._lock_failed = False
            with Tqdm(
                bar_format="{desc}",
                disable=not self._friendly,
                desc="If DVC froze, see `hardlink_lock` in {}".format(
                    format_link("https://man.dvc.org/config#core")
                ),
            ):
                self._lock = zc.lockfile.LockFile(self._lockfile)
        except zc.lockfile.LockError:
            self._lock_failed = True
            raise LockError(FAILED_TO_LOCK_MESSAGE)  # noqa: B904

    def lock(self):
        retries = 6
        delay = DEFAULT_TIMEOUT / retries
        lock_retry = retry(retries, LockError, timeout=delay)(self._do_lock)
        lock_retry()

    def unlock(self):
        if self._lock_failed:
            assert self._lock is None
            return

        if not self.is_locked:
            raise DvcException("Unlock called on an unlocked lock")
        assert self._lock
        self._lock.close()
        self._lock = None

    @property
    def is_locked(self):
        return bool(self._lock)

    def __enter__(self):
        self.lock()

    def __exit__(self, typ, value, tbck):
        self.unlock()


class HardlinkLock(flufl.lock.Lock, LockBase):
    """Class for DVC repo lock.

    Args:
        lockfile (str): the lock filename
            in.
        tmp_dir (str): a directory to store claim files.
    """

    def __init__(
        self, lockfile, tmp_dir=None, **kwargs
    ):  # pylint: disable=super-init-not-called
        import socket

        self._tmp_dir = tmp_dir
        super().__init__(lockfile)

        # NOTE: this is basically Lock.__init__ copy-paste, except that
        # instead of using `socket.getfqdn()` we use `socket.gethostname()`
        # to speed this up. We've seen [1] `getfqdn()` take ~5sec to return
        # anything, which is way too slow. `gethostname()` is actually a
        # fallback for `getfqdn()` when it is not able to resolve a
        # canonical hostname through network. The claimfile that uses
        # `self._hostname` is still usable, as it uses `pid` and random
        # number to generate the resulting lock file name, which is unique
        # enough for our application.
        #
        # [1] https://github.com/iterative/dvc/issues/2582
        self._hostname = socket.gethostname()

        self._lifetime = timedelta(days=365)  # Lock for good by default
        self._separator = flufl.lock.SEP
        self._set_claimfile()
        self._owned = True
        self._retry_errnos = []

    def lock(self):  # pylint: disable=arguments-differ
        try:
            super().lock(timedelta(seconds=DEFAULT_TIMEOUT))
        except flufl.lock.TimeOutError:
            raise LockError(FAILED_TO_LOCK_MESSAGE)  # noqa: B904

    def _set_claimfile(self):
        super()._set_claimfile()

        if self._tmp_dir is not None:
            # Under Windows file path length is limited so we hash it
            hasher = hashlib.md5(  # nosec B324, B303  # noqa: S324
                self._claimfile.encode()
            )
            filename = hasher.hexdigest()
            self._claimfile = os.path.join(self._tmp_dir, filename + ".lock")


def make_lock(lockfile, tmp_dir=None, friendly=False, hardlink_lock=False):
    cls = HardlinkLock if hardlink_lock else Lock
    return cls(lockfile, tmp_dir=tmp_dir, friendly=friendly)




dvc/logger.py
"""Manages logging configuration for DVC repo."""

import logging
import logging.config
import logging.handlers
import os
import sys

import colorama

from dvc.env import DVC_SHOW_TRACEBACK
from dvc.progress import Tqdm


def add_logging_level(level_name, level_num, method_name=None):
    """
    Adds a new logging level to the `logging` module and the
    currently configured logging class.

    Uses the existing numeric level_num if already defined.

    Based on https://stackoverflow.com/questions/2183233
    """
    if method_name is None:
        method_name = level_name.lower()

    # If the level name is already defined as a top-level `logging`
    # constant, then adopt the existing numeric level.
    if hasattr(logging, level_name):
        existing_level_num = getattr(logging, level_name)
        assert isinstance(existing_level_num, int)
        level_num = existing_level_num

    def log_for_level(self, message, *args, **kwargs):
        if self.isEnabledFor(level_num):
            # pylint: disable=protected-access
            self._log(level_num, message, args, **kwargs)

    def log_to_root(message, *args, **kwargs):
        logging.log(level_num, message, *args, **kwargs)

    # getLevelName resolves the numeric log level if already defined,
    # otherwise returns a string
    if not isinstance(logging.getLevelName(level_name), int):
        logging.addLevelName(level_num, level_name)

    if not hasattr(logging, level_name):
        setattr(logging, level_name, level_num)

    if not hasattr(logging.getLoggerClass(), method_name):
        setattr(logging.getLoggerClass(), method_name, log_for_level)

    if not hasattr(logging, method_name):
        setattr(logging, method_name, log_to_root)


class LoggingException(Exception):
    def __init__(self, record):
        msg = f"failed to log {str(record)}"
        super().__init__(msg)


def exclude_filter(level: int):
    def filter_fn(record: "logging.LogRecord") -> bool:
        return record.levelno < level

    return filter_fn


class ColorFormatter(logging.Formatter):
    """Spit out colored text in supported terminals.

    colorama__ makes ANSI escape character sequences work under Windows.
    See the colorama documentation for details.

    __ https://pypi.python.org/pypi/colorama

    If record has an extra `tb_only` attribute, it will not show the
    exception cause, just the message and the traceback.
    """

    reset = colorama.Fore.RESET
    color_codes = {
        "TRACE": colorama.Fore.GREEN,
        "DEBUG": colorama.Fore.BLUE,
        "WARNING": colorama.Fore.YELLOW,
        "ERROR": colorama.Fore.RED,
        "CRITICAL": colorama.Fore.RED,
    }

    def __init__(self, log_colors: bool = True, show_traceback: bool = False) -> None:
        super().__init__()
        self.log_colors = log_colors
        self.show_traceback = show_traceback

    def format(self, record) -> str:  # noqa: A003, C901
        record.message = record.getMessage()
        msg = self.formatMessage(record)

        if record.levelno == logging.INFO:
            return msg

        ei = record.exc_info
        if ei:
            cause = ""
            if not getattr(record, "tb_only", False):
                cause = ": ".join(_iter_causes(ei[1]))
            sep = " - " if msg and cause else ""
            msg = msg + sep + cause

        asctime = ""
        verbose = _is_verbose()
        if verbose:
            asctime = self.formatTime(record, self.datefmt)
        if verbose or self.show_traceback:
            if ei and not record.exc_text:
                record.exc_text = self.formatException(ei)
            if record.exc_text:
                if msg[-1:] != "\n":
                    msg = msg + "\n"
                msg = msg + record.exc_text + "\n"
            if record.stack_info:
                if msg[-1:] != "\n":
                    msg = msg + "\n"
                msg = msg + self.formatStack(record.stack_info) + "\n"

        level = record.levelname
        if self.log_colors:
            color = self.color_codes[level]
            if asctime:
                asctime = color + asctime + self.reset
            level = color + level + self.reset
        return asctime + (" " if asctime else "") + level + ": " + msg


class LoggerHandler(logging.StreamHandler):
    def handleError(self, record):  # noqa: N802
        super().handleError(record)
        raise LoggingException(record)

    def emit_pretty_exception(self, exc, verbose: bool = False):
        return exc.__pretty_exc__(verbose=verbose)

    def emit(self, record):
        """Write to Tqdm's stream so as to not break progress-bars"""
        try:
            if record.exc_info:
                _, exc, *_ = record.exc_info
                if hasattr(exc, "__pretty_exc__"):
                    try:
                        self.emit_pretty_exception(exc, verbose=_is_verbose())
                        if not _is_verbose():
                            return
                    # pylint: disable-next=broad-except
                    except Exception:  # noqa: BLE001, S110  # nosec B110
                        pass

            msg = self.format(record)
            Tqdm.write(msg, file=self.stream, end=getattr(self, "terminator", "\n"))
            self.flush()
        except (BrokenPipeError, RecursionError):
            raise
        except Exception:  # noqa, pylint: disable=broad-except
            self.handleError(record)


def _is_verbose():
    return (
        logging.NOTSET < logging.getLogger("dvc").getEffectiveLevel() <= logging.DEBUG
    )


def _iter_causes(exc):
    while exc:
        yield str(exc)
        exc = exc.__cause__


def set_loggers_level(level: int = logging.INFO) -> None:
    for name in ["dvc", "dvc_objects", "dvc_data"]:
        logging.getLogger(name).setLevel(level)


def setup(level: int = logging.INFO, log_colors: bool = True) -> None:
    colorama.init()

    formatter = ColorFormatter(log_colors=log_colors and sys.stdout.isatty())

    console_info = LoggerHandler(sys.stdout)
    console_info.setLevel(logging.INFO)
    console_info.setFormatter(formatter)
    console_info.addFilter(exclude_filter(logging.WARNING))

    console_debug = LoggerHandler(sys.stdout)
    console_debug.setLevel(logging.DEBUG)
    console_debug.setFormatter(formatter)
    console_debug.addFilter(exclude_filter(logging.INFO))

    add_logging_level("TRACE", logging.DEBUG - 5)

    console_trace = LoggerHandler(sys.stdout)
    console_trace.setLevel(logging.TRACE)  # type: ignore[attr-defined]
    console_trace.setFormatter(formatter)
    console_trace.addFilter(exclude_filter(logging.DEBUG))

    show_traceback = bool(os.environ.get(DVC_SHOW_TRACEBACK))
    err_formatter = ColorFormatter(
        log_colors=log_colors and sys.stderr.isatty(), show_traceback=show_traceback
    )
    console_errors = LoggerHandler(sys.stderr)
    console_errors.setLevel(logging.WARNING)
    console_errors.setFormatter(err_formatter)

    for name in ["dvc", "dvc_objects", "dvc_data"]:
        logger = logging.getLogger(name)
        logger.setLevel(level)
        for handler in [console_info, console_debug, console_trace, console_errors]:
            logger.addHandler(handler)

    if level >= logging.DEBUG:
        # Unclosed session errors for asyncio/aiohttp are only available
        # on the tracing mode for extensive debug purposes. They are really
        # noisy, and this is potentially somewhere in the client library
        # not managing their own session. Even though it is the best practice
        # for them to do so, we can be assured that these errors raised when
        # the object is getting deallocated, so no need to take any extensive
        # action.
        logging.getLogger("asyncio").setLevel(logging.CRITICAL)
        logging.getLogger("aiohttp").setLevel(logging.CRITICAL)




dvc/output.py
import errno
import logging
import os
import posixpath
from collections import defaultdict
from contextlib import suppress
from operator import itemgetter
from typing import TYPE_CHECKING, Dict, List, Optional, Set, Tuple, Type, Union
from urllib.parse import urlparse

from funcy import collecting, first, project
from voluptuous import All, And, Any, Coerce, Length, Lower, Required, SetTo

from dvc import prompt
from dvc.exceptions import (
    CacheLinkError,
    CheckoutError,
    CollectCacheError,
    ConfirmRemoveError,
    DvcException,
    MergeError,
    RemoteCacheRequiredError,
)
from dvc.utils.objects import cached_property
from dvc_data.hashfile import check as ocheck
from dvc_data.hashfile import load as oload
from dvc_data.hashfile.build import build
from dvc_data.hashfile.checkout import checkout
from dvc_data.hashfile.db import HashFileDB, add_update_tree
from dvc_data.hashfile.hash_info import HashInfo
from dvc_data.hashfile.istextfile import istextfile
from dvc_data.hashfile.meta import Meta
from dvc_data.hashfile.transfer import transfer as otransfer
from dvc_data.hashfile.tree import Tree, du
from dvc_objects.errors import ObjectFormatError

from .annotations import ANNOTATION_FIELDS, ANNOTATION_SCHEMA, Annotation
from .fs import LocalFileSystem, RemoteMissingDepsError, Schemes, get_cloud_fs
from .fs.callbacks import DEFAULT_CALLBACK, Callback, TqdmCallback
from .utils import relpath
from .utils.fs import path_isin

if TYPE_CHECKING:
    from dvc_data.hashfile.obj import HashFile
    from dvc_data.index import DataIndexKey
    from dvc_objects.db import ObjectDB

    from .ignore import DvcIgnoreFilter

logger = logging.getLogger(__name__)


CHECKSUM_SCHEMA = Any(
    None,
    And(str, Length(max=0), SetTo(None)),
    And(Any(str, And(int, Coerce(str))), Length(min=3), Lower),
)

CASE_SENSITIVE_CHECKSUM_SCHEMA = Any(
    None,
    And(str, Length(max=0), SetTo(None)),
    And(Any(str, And(int, Coerce(str))), Length(min=3)),
)

# NOTE: currently there are only 3 possible checksum names:
#
#    1) md5 (LOCAL, SSH);
#    2) etag (S3, GS, OSS, AZURE, HTTP);
#    3) checksum (HDFS);
#
# so when a few types of outputs share the same name, we only need
# specify it once.
HDFS_PARAM_CHECKSUM = "checksum"
S3_PARAM_CHECKSUM = "etag"
CHECKSUMS_SCHEMA = {
    LocalFileSystem.PARAM_CHECKSUM: CHECKSUM_SCHEMA,
    HDFS_PARAM_CHECKSUM: CHECKSUM_SCHEMA,
    S3_PARAM_CHECKSUM: CASE_SENSITIVE_CHECKSUM_SCHEMA,
}


def _get(stage, path, **kwargs):
    return Output(stage, path, **kwargs)


def loadd_from(stage, d_list):
    ret = []
    for d in d_list:
        p = d.pop(Output.PARAM_PATH)
        cache = d.pop(Output.PARAM_CACHE, True)
        metric = d.pop(Output.PARAM_METRIC, False)
        plot = d.pop(Output.PARAM_PLOT, False)
        persist = d.pop(Output.PARAM_PERSIST, False)
        remote = d.pop(Output.PARAM_REMOTE, None)
        annot = {field: d.pop(field, None) for field in ANNOTATION_FIELDS}
        files = d.pop(Output.PARAM_FILES, None)
        push = d.pop(Output.PARAM_PUSH, True)
        ret.append(
            _get(
                stage,
                p,
                info=d,
                cache=cache,
                metric=metric,
                plot=plot,
                persist=persist,
                remote=remote,
                **annot,
                files=files,
                push=push,
            )
        )
    return ret


def loads_from(
    stage,
    s_list,
    use_cache=True,
    metric=False,
    plot=False,
    persist=False,
    remote=None,
    push=True,
):
    return [
        _get(
            stage,
            s,
            info={},
            cache=use_cache,
            metric=metric,
            plot=plot,
            persist=persist,
            remote=remote,
            push=push,
        )
        for s in s_list
    ]


def _split_dict(d, keys):
    return project(d, keys), project(d, d.keys() - keys)


def _merge_data(s_list):
    d: Dict[str, Dict] = defaultdict(dict)
    for key in s_list:
        if isinstance(key, str):
            d[key].update({})
            continue
        if not isinstance(key, dict):
            raise ValueError(f"'{type(key).__name__}' not supported.")  # noqa: TRY004

        for k, flags in key.items():
            if not isinstance(flags, dict):
                raise ValueError(  # noqa: TRY004
                    f"Expected dict for '{k}', got: '{type(flags).__name__}'"
                )
            d[k].update(flags)
    return d


@collecting
def load_from_pipeline(stage, data, typ="outs"):
    if typ not in (
        stage.PARAM_OUTS,
        stage.PARAM_METRICS,
        stage.PARAM_PLOTS,
    ):
        raise ValueError(f"'{typ}' key is not allowed for pipeline files.")

    metric = typ == stage.PARAM_METRICS
    plot = typ == stage.PARAM_PLOTS

    d = _merge_data(data)

    for path, flags in d.items():
        plt_d = {}
        if plot:
            from dvc.schema import PLOT_PROPS

            plt_d, flags = _split_dict(flags, keys=PLOT_PROPS.keys())

        extra = project(
            flags,
            [
                Output.PARAM_CACHE,
                Output.PARAM_PERSIST,
                Output.PARAM_REMOTE,
                Output.PARAM_PUSH,
                *ANNOTATION_FIELDS,
            ],
        )

        yield _get(
            stage,
            path,
            info={},
            plot=plt_d or plot,
            metric=metric,
            **extra,
        )


def split_file_meta_from_cloud(entry: Dict) -> Dict:
    if remote_name := entry.pop(Meta.PARAM_REMOTE, None):
        remote_meta = {}
        for key in (
            S3_PARAM_CHECKSUM,
            HDFS_PARAM_CHECKSUM,
            Meta.PARAM_VERSION_ID,
        ):
            if value := entry.pop(key, None):
                remote_meta[key] = value

        if remote_meta:
            entry[Output.PARAM_CLOUD] = {remote_name: remote_meta}
    return entry


def merge_file_meta_from_cloud(entry: Dict) -> Dict:
    cloud_meta = entry.pop(Output.PARAM_CLOUD, {})
    if remote_name := first(cloud_meta):
        entry.update(cloud_meta[remote_name])
        entry[Meta.PARAM_REMOTE] = remote_name
    return entry


def _serialize_tree_obj_to_files(obj: "Tree") -> List[Dict[str, Any]]:
    key = obj.PARAM_RELPATH
    return sorted(
        (
            {key: posixpath.sep.join(parts), **hi.to_dict(), **meta.to_dict()}
            for parts, meta, hi in obj
        ),
        key=itemgetter(key),
    )


class OutputDoesNotExistError(DvcException):
    def __init__(self, path):
        msg = f"output '{path}' does not exist"
        super().__init__(msg)


class OutputIsNotFileOrDirError(DvcException):
    def __init__(self, path):
        msg = f"output '{path}' is not a file or directory"
        super().__init__(msg)


class OutputAlreadyTrackedError(DvcException):
    def __init__(self, path):
        msg = f""" output '{path}' is already tracked by SCM (e.g. Git).
    You can remove it from Git, then add to DVC.
        To stop tracking from Git:
            git rm -r --cached '{path}'
            git commit -m "stop tracking {path}" """
        super().__init__(msg)


class OutputIsStageFileError(DvcException):
    def __init__(self, path):
        super().__init__(f"DVC file '{path}' cannot be an output.")


class OutputIsIgnoredError(DvcException):
    def __init__(self, match):
        lines = "\n".join(match.patterns)
        super().__init__(f"Path '{match.file}' is ignored by\n{lines}")


class CheckoutCallback(TqdmCallback):
    # disable branching for checkouts
    branch = Callback.branch  # type: ignore[assignment]


class Output:
    IS_DEPENDENCY = False

    PARAM_PATH = "path"
    PARAM_CACHE = "cache"
    PARAM_FILES = "files"
    PARAM_METRIC = "metric"
    PARAM_METRIC_TYPE = "type"
    PARAM_METRIC_XPATH = "xpath"
    PARAM_PLOT = "plot"
    PARAM_PLOT_TEMPLATE = "template"
    PARAM_PLOT_X = "x"
    PARAM_PLOT_Y = "y"
    PARAM_PLOT_X_LABEL = "x_label"
    PARAM_PLOT_Y_LABEL = "y_label"
    PARAM_PLOT_TITLE = "title"
    PARAM_PLOT_HEADER = "header"
    PARAM_PERSIST = "persist"
    PARAM_REMOTE = "remote"
    PARAM_PUSH = "push"
    PARAM_CLOUD = "cloud"

    METRIC_SCHEMA = Any(
        None,
        bool,
        {
            PARAM_METRIC_TYPE: Any(str, None),
            PARAM_METRIC_XPATH: Any(str, None),
        },
    )

    DoesNotExistError: Type[DvcException] = OutputDoesNotExistError
    IsNotFileOrDirError: Type[DvcException] = OutputIsNotFileOrDirError
    IsStageFileError: Type[DvcException] = OutputIsStageFileError
    IsIgnoredError: Type[DvcException] = OutputIsIgnoredError

    def __init__(  # noqa: PLR0913
        self,
        stage,
        path,
        info=None,
        cache=True,
        metric=False,
        plot=False,
        persist=False,
        desc=None,
        type=None,  # noqa: A002, pylint: disable=redefined-builtin
        labels=None,
        meta=None,
        remote=None,
        repo=None,
        fs_config=None,
        files: Optional[List[Dict[str, Any]]] = None,
        push: bool = True,
    ):
        self.annot = Annotation(
            desc=desc, type=type, labels=labels or [], meta=meta or {}
        )
        self.repo = stage.repo if not repo and stage else repo
        meta_d = merge_file_meta_from_cloud(info or {})
        meta = Meta.from_dict(meta_d)
        # NOTE: when version_aware is not passed into get_cloud_fs, it will be
        # set based on whether or not path is versioned
        fs_kwargs = {}
        if meta.version_id or files:
            fs_kwargs["version_aware"] = True

        if fs_config is not None:
            fs_kwargs.update(**fs_config)

        fs_cls, fs_config, fs_path = get_cloud_fs(self.repo, url=path, **fs_kwargs)
        self.fs = fs_cls(**fs_config)

        if (
            self.fs.protocol == "local"
            and stage
            and isinstance(stage.repo.fs, LocalFileSystem)
            and path_isin(path, stage.repo.root_dir)
        ):
            self.def_path: str = relpath(path, stage.wdir)
            self.fs = stage.repo.fs
        else:
            self.def_path = path

        if (
            self.repo
            and self.fs.protocol == "local"
            and not self.fs.path.isabs(self.def_path)
        ):
            self.fs = self.repo.fs

        self._validate_output_path(path, stage)
        # This output (and dependency) objects have too many paths/urls
        # here is a list and comments:
        #
        #   .def_path - path from definition in DVC file
        #   .fspath - local only, resolved
        #   .__str__ - for presentation purposes, def_path/relpath
        #
        # By resolved path, which contains actual location,
        # should be absolute and don't contain remote:// refs.
        self.stage = stage
        self.meta = meta

        if files is not None:
            files = [merge_file_meta_from_cloud(f) for f in files]
        self.files = files
        self.use_cache = False if self.IS_DEPENDENCY else cache
        self.metric = False if self.IS_DEPENDENCY else metric
        self.plot = False if self.IS_DEPENDENCY else plot
        self.persist = persist
        self.can_push = push

        self.fs_path = self._parse_path(self.fs, fs_path)
        self.obj: Optional["HashFile"] = None

        self.odb: Optional["HashFileDB"] = None
        self.remote = remote

        if self.fs.version_aware:
            _, version_id = self.fs.path.coalesce_version(
                self.def_path, self.meta.version_id
            )
            self.meta.version_id = version_id

        if self.is_in_repo:
            self.hash_name = "md5"
        else:
            self.hash_name = self.fs.PARAM_CHECKSUM

        self.hash_info = HashInfo(
            name=self.hash_name,
            value=getattr(self.meta, self.hash_name, None),
        )
        self._compute_meta_hash_info_from_files()

    def _compute_meta_hash_info_from_files(self) -> None:
        if self.files:
            tree = Tree.from_list(self.files, hash_name=self.hash_name)
            tree.digest(with_meta=True)
            self.odb = HashFileDB(tree.fs, tree.path + ".odb")
            self.odb.add(tree.path, tree.fs, tree.hash_info.value)

            self.hash_info = tree.hash_info
            self.meta.isdir = True
            self.meta.nfiles = len(self.files)
            self.meta.size = sum(filter(None, (f.get("size") for f in self.files)))
            self.meta.remote = first(f.get("remote") for f in self.files)
        elif self.meta.nfiles or self.hash_info and self.hash_info.isdir:
            self.meta.isdir = True
            if not self.hash_info and self.hash_name != "md5":
                md5 = getattr(self.meta, "md5", None)
                if md5:
                    self.hash_info = HashInfo("md5", md5)

    def _parse_path(self, fs, fs_path):
        parsed = urlparse(self.def_path)
        if (
            parsed.scheme != "remote"
            and self.stage
            and self.stage.repo.fs == fs
            and not fs.path.isabs(fs_path)
        ):
            # NOTE: we can path either from command line or .dvc file,
            # so we should expect both posix and windows style paths.
            # paths accepts both, i.e. / works everywhere, \ only on win.
            #
            # FIXME: if we have Windows path containing / or posix one with \
            # then we have #2059 bug and can't really handle that.
            fs_path = fs.path.join(self.stage.wdir, fs_path)

        return fs.path.abspath(fs.path.normpath(fs_path))

    def __repr__(self):
        return "{class_name}: '{def_path}'".format(
            class_name=type(self).__name__, def_path=self.def_path
        )

    def __str__(self):
        if self.fs.protocol != "local":
            return self.def_path

        if (
            not self.repo
            or urlparse(self.def_path).scheme == "remote"
            or os.path.isabs(self.def_path)
        ):
            return str(self.def_path)

        cur_dir = self.fs.path.getcwd()
        if self.fs.path.isin(cur_dir, self.repo.root_dir):
            return self.fs.path.relpath(self.fs_path, cur_dir)

        return self.fs.path.relpath(self.fs_path, self.repo.root_dir)

    def clear(self):
        self.hash_info = HashInfo.from_dict({})
        self.meta = Meta.from_dict({})
        self.obj = None
        self.files = None

    @property
    def protocol(self):
        return self.fs.protocol

    @property
    def is_in_repo(self):
        if urlparse(self.def_path).scheme == "remote":
            return False

        if self.fs.path.isabs(self.def_path):
            return False

        return self.repo and self.fs.path.isin(
            self.fs.path.realpath(self.fs_path),
            self.repo.root_dir,
        )

    @property
    def use_scm_ignore(self):
        if not self.is_in_repo:
            return False

        return self.use_cache or self.stage.is_repo_import

    @property
    def cache(self):
        odb_name = "repo" if self.is_in_repo else self.protocol
        odb = getattr(self.repo.cache, odb_name)
        if self.use_cache and odb is None:
            raise RemoteCacheRequiredError(self.fs.protocol, self.fs_path)
        return odb

    @property
    def cache_path(self):
        return self.cache.fs.unstrip_protocol(
            self.cache.oid_to_path(self.hash_info.value)
        )

    def get_hash(self):
        _, hash_info = self._get_hash_meta()
        return hash_info

    def _get_hash_meta(self):
        if self.use_cache:
            odb = self.cache
        else:
            odb = self.repo.cache.local
        _, meta, obj = build(
            odb,
            self.fs_path,
            self.fs,
            self.hash_name,
            ignore=self.dvcignore,
            dry_run=not self.use_cache,
        )
        return meta, obj.hash_info

    def get_meta(self) -> Meta:
        meta, _ = self._get_hash_meta()
        return meta

    @property
    def is_dir_checksum(self):
        return self.hash_info.isdir

    def _is_path_dvcignore(self, path) -> bool:
        if self.IS_DEPENDENCY or not self.dvcignore:
            return False
        return self.dvcignore.is_ignored(self.fs, path, ignore_subrepos=False)

    @property
    def exists(self):
        if self._is_path_dvcignore(self.fs_path):
            return False

        return self.fs.exists(self.fs_path)

    @cached_property
    def index_key(self) -> Tuple[str, "DataIndexKey"]:
        if self.is_in_repo:
            workspace = "repo"
            key = self.repo.fs.path.relparts(self.fs_path, self.repo.root_dir)
        else:
            workspace = self.fs.protocol
            no_drive = self.fs.path.flavour.splitdrive(self.fs_path)[1]
            key = self.fs.path.parts(no_drive)[1:]
        return workspace, key

    def changed_checksum(self):
        return self.hash_info != self.get_hash()

    def changed_cache(self, filter_info=None):
        if not self.use_cache or not self.hash_info:
            return True

        obj = self.get_obj(filter_info=filter_info)
        if not obj:
            return True

        try:
            ocheck(self.cache, obj)
            return False
        except (FileNotFoundError, ObjectFormatError):
            return True

    def changed_meta(self) -> bool:
        if self.fs.version_aware and self.meta.version_id:
            return self.meta.version_id != self.get_meta().version_id
        return False

    def workspace_status(self) -> Dict[str, str]:
        if not self.exists:
            return {str(self): "deleted"}

        if self.changed_checksum():
            return {str(self): "modified"}

        if not self.hash_info:
            return {str(self): "new"}

        return {}

    def status(self) -> Dict[str, str]:
        if self.hash_info and self.use_cache and self.changed_cache():
            return {str(self): "not in cache"}

        return self.workspace_status()

    def changed(self) -> bool:
        status = self.status()
        logger.debug(str(status))
        return bool(status)

    @property
    def dvcignore(self) -> Optional["DvcIgnoreFilter"]:
        if self.fs.protocol == "local":
            return self.repo.dvcignore
        return None

    @property
    def is_empty(self) -> bool:
        return self.fs.is_empty(self.fs_path)

    def isdir(self) -> bool:
        if self._is_path_dvcignore(self.fs_path):
            return False
        return self.fs.isdir(self.fs_path)

    def isfile(self) -> bool:
        if self._is_path_dvcignore(self.fs_path):
            return False
        return self.fs.isfile(self.fs_path)

    # pylint: disable=no-member

    def ignore(self) -> None:
        if not self.use_scm_ignore:
            return

        if self.repo.scm.is_tracked(self.fspath):
            raise OutputAlreadyTrackedError(self)

        self.repo.scm_context.ignore(self.fspath)

    def ignore_remove(self) -> None:
        if not self.use_scm_ignore:
            return

        self.repo.scm_context.ignore_remove(self.fspath)

    # pylint: enable=no-member

    def save(self) -> None:
        if not self.exists:
            raise self.DoesNotExistError(self)

        if not self.isfile() and not self.isdir():
            raise self.IsNotFileOrDirError(self)

        if self.is_empty:
            logger.warning("'%s' is empty.", self)

        self.ignore()

        if self.metric:
            self.verify_metric()

        if self.use_cache:
            _, self.meta, self.obj = build(
                self.cache,
                self.fs_path,
                self.fs,
                self.hash_name,
                ignore=self.dvcignore,
            )
        else:
            _, self.meta, self.obj = build(
                self.repo.cache.local,
                self.fs_path,
                self.fs,
                self.hash_name,
                ignore=self.dvcignore,
                dry_run=True,
            )
            if not self.IS_DEPENDENCY:
                logger.debug("Output '%s' doesn't use cache. Skipping saving.", self)

        self.hash_info = self.obj.hash_info
        self.files = None

    def set_exec(self) -> None:
        if self.isfile() and self.meta.isexec:
            self.cache.set_exec(self.fs_path)

    def _checkout(self, *args, **kwargs) -> Optional[bool]:
        from dvc_data.hashfile.checkout import CheckoutError as _CheckoutError
        from dvc_data.hashfile.checkout import LinkError, PromptError

        kwargs.setdefault("ignore", self.dvcignore)
        try:
            return checkout(*args, **kwargs)
        except PromptError as exc:
            raise ConfirmRemoveError(exc.path)  # noqa: B904
        except LinkError as exc:
            raise CacheLinkError([exc.path])  # noqa: B904
        except _CheckoutError as exc:
            raise CheckoutError(exc.paths, {})  # noqa: B904

    def commit(self, filter_info=None, relink=True) -> None:
        if not self.exists:
            raise self.DoesNotExistError(self)

        assert self.hash_info

        if self.use_cache:
            granular = (
                self.is_dir_checksum and filter_info and filter_info != self.fs_path
            )
            # NOTE: trying to use hardlink during transfer only if we will be
            # relinking later
            hardlink = relink
            if granular:
                obj = self._commit_granular_dir(filter_info, hardlink)
            else:
                staging, _, obj = build(
                    self.cache,
                    filter_info or self.fs_path,
                    self.fs,
                    self.hash_name,
                    ignore=self.dvcignore,
                )
                otransfer(
                    staging,
                    self.cache,
                    {obj.hash_info},
                    shallow=False,
                    hardlink=hardlink,
                )
            if relink:
                rel = self.fs.path.relpath(filter_info or self.fs_path)
                with CheckoutCallback(desc=f"Checking out {rel}", unit="files") as cb:
                    self._checkout(
                        filter_info or self.fs_path,
                        self.fs,
                        obj,
                        self.cache,
                        relink=True,
                        state=self.repo.state,
                        prompt=prompt.confirm,
                        progress_callback=cb,
                    )
                self.set_exec()

    def _commit_granular_dir(self, filter_info, hardlink) -> Optional["HashFile"]:
        prefix = self.fs.path.parts(self.fs.path.relpath(filter_info, self.fs_path))
        staging, _, obj = build(
            self.cache,
            self.fs_path,
            self.fs,
            self.hash_name,
            ignore=self.dvcignore,
        )
        assert isinstance(obj, Tree)
        save_obj = obj.filter(prefix)
        assert isinstance(save_obj, Tree)
        checkout_obj = save_obj.get_obj(self.cache, prefix)
        otransfer(
            staging,
            self.cache,
            {save_obj.hash_info} | {oid for _, _, oid in save_obj},
            shallow=True,
            hardlink=hardlink,
        )
        return checkout_obj

    def dumpd(self, **kwargs):  # noqa: C901, PLR0912
        ret: Dict[str, Any] = {}
        with_files = (
            (not self.IS_DEPENDENCY or self.stage.is_import)
            and self.hash_info.isdir
            and (kwargs.get("with_files") or self.files is not None)
        )

        if not with_files:
            meta_d = self.meta.to_dict()
            meta_d.pop("isdir", None)
            ret.update(self.hash_info.to_dict())
            ret.update(split_file_meta_from_cloud(meta_d))

        if self.is_in_repo:
            path = self.fs.path.as_posix(relpath(self.fs_path, self.stage.wdir))
        else:
            path = self.def_path

        ret[self.PARAM_PATH] = path

        if not self.IS_DEPENDENCY:
            ret.update(self.annot.to_dict())
            if not self.use_cache:
                ret[self.PARAM_CACHE] = self.use_cache

            if (
                isinstance(self.metric, dict)
                and self.PARAM_METRIC_XPATH in self.metric
                and not self.metric[self.PARAM_METRIC_XPATH]
            ):
                del self.metric[self.PARAM_METRIC_XPATH]

            if self.metric:
                ret[self.PARAM_METRIC] = self.metric

            if self.plot:
                ret[self.PARAM_PLOT] = self.plot

            if self.persist:
                ret[self.PARAM_PERSIST] = self.persist

            if self.remote:
                ret[self.PARAM_REMOTE] = self.remote

            if not self.can_push:
                ret[self.PARAM_PUSH] = self.can_push

        if with_files:
            obj = self.obj or self.get_obj()
            if obj:
                assert isinstance(obj, Tree)
                ret[self.PARAM_FILES] = [
                    split_file_meta_from_cloud(f)
                    for f in _serialize_tree_obj_to_files(obj)
                ]
        return ret

    def verify_metric(self):
        if self.fs.protocol != "local":
            raise DvcException(f"verify metric is not supported for {self.protocol}")
        if not self.metric:
            return

        if not os.path.exists(self.fs_path):
            return

        if os.path.isdir(self.fs_path):
            msg = "directory '%s' cannot be used as %s."
            logger.debug(msg, str(self), "metrics")
            return

        if not istextfile(self.fs_path, self.fs):
            raise DvcException(
                f"binary file '{self.fs_path}' cannot be used as metrics."
            )

    def get_obj(
        self, filter_info: Optional[str] = None, **kwargs
    ) -> Optional["HashFile"]:
        obj: Optional["HashFile"] = None
        if self.obj:
            obj = self.obj
        elif self.files:
            tree = Tree.from_list(self.files, hash_name=self.hash_name)
            tree.digest()
            obj = tree
        elif self.hash_info:
            try:
                obj = oload(self.cache, self.hash_info)
            except (FileNotFoundError, ObjectFormatError):
                return None
        else:
            return None

        assert obj
        fs_path = self.fs.path
        if filter_info and filter_info != self.fs_path:
            prefix = fs_path.relparts(filter_info, self.fs_path)
            assert isinstance(obj, Tree)
            obj = obj.get_obj(self.cache, prefix)

        return obj

    def checkout(
        self,
        force: bool = False,
        progress_callback: "Callback" = DEFAULT_CALLBACK,
        relink: bool = False,
        filter_info: Optional[str] = None,
        allow_missing: bool = False,
        **kwargs,
    ) -> Optional[Tuple[bool, Optional[bool]]]:
        # callback passed act as a aggregate callback.
        # do not let checkout to call set_size and change progressbar.
        class CallbackProxy(Callback):
            def relative_update(self, inc: int = 1) -> None:
                progress_callback.relative_update(inc)
                return super().relative_update(inc)

        callback = CallbackProxy()
        if not self.use_cache:
            callback.relative_update(self.get_files_number(filter_info))
            return None

        obj = self.get_obj(filter_info=filter_info)
        if not obj and (filter_info and filter_info != self.fs_path):
            # backward compatibility
            return None

        added = not self.exists

        try:
            modified = self._checkout(
                filter_info or self.fs_path,
                self.fs,
                obj,
                self.cache,
                force=force,
                progress_callback=callback,
                relink=relink,
                state=self.repo.state,
                prompt=prompt.confirm,
                **kwargs,
            )
        except CheckoutError:
            if allow_missing:
                return None
            raise
        self.set_exec()
        return added, False if added else modified

    def remove(self, ignore_remove=False):
        self.fs.remove(self.fs_path, recursive=True)
        if self.protocol != Schemes.LOCAL:
            return

        if ignore_remove:
            self.ignore_remove()

    def move(self, out):
        # pylint: disable=no-member
        if self.protocol == "local" and self.use_scm_ignore:
            self.repo.scm_context.ignore_remove(self.fspath)

        self.fs.move(self.fs_path, out.fs_path)
        self.def_path = out.def_path
        self.fs_path = out.fs_path
        self.save()
        self.commit()

        if self.protocol == "local" and self.use_scm_ignore:
            self.repo.scm_context.ignore(self.fspath)

    def transfer(
        self, source, odb=None, jobs=None, update=False, no_progress_bar=False
    ):
        if odb is None:
            odb = self.cache

        cls, config, from_info = get_cloud_fs(self.repo, url=source)
        from_fs = cls(**config)

        # When running import-url --to-remote / add --to-remote/-o ... we
        # assume that it is unlikely that the odb will contain majority of the
        # hashes, so we transfer everything as is (even if that file might
        # already be in the cache) and don't waste an upload to scan the layout
        # of the source location. But when doing update --to-remote, there is
        # a high probability that the odb might contain some of the hashes, so
        # we first calculate all the hashes (but don't transfer anything) and
        # then only update the missing cache files.

        upload = not (update and from_fs.isdir(from_info))
        jobs = jobs or min((from_fs.jobs, odb.fs.jobs))
        staging, self.meta, obj = build(
            odb,
            from_info,
            from_fs,
            "md5",
            upload=upload,
            no_progress_bar=no_progress_bar,
        )
        otransfer(
            staging,
            odb,
            {obj.hash_info},
            jobs=jobs,
            hardlink=False,
            shallow=False,
        )

        self.hash_info = obj.hash_info
        self.files = None
        return obj

    def get_files_number(self, filter_info=None):
        if not self.use_cache or not self.hash_info:
            return 0

        if not self.hash_info.isdir:
            return 1

        if not filter_info or filter_info == self.fs_path:
            return self.meta.nfiles or 0

        obj = self.get_obj(filter_info=filter_info)
        return len(obj) if obj else 0

    def unprotect(self):
        if self.exists:
            self.cache.unprotect(self.fs_path)

    def get_dir_cache(self, **kwargs) -> Optional["Tree"]:
        if not self.is_dir_checksum:
            raise DvcException("cannot get dir cache for file checksum")

        obj = self.cache.get(self.hash_info.value)
        try:
            ocheck(self.cache, obj)
        except FileNotFoundError:
            if self.remote:
                kwargs["remote"] = self.remote
            with suppress(Exception):
                self.repo.cloud.pull([obj.hash_info], **kwargs)

        if self.obj:
            assert isinstance(self.obj, Tree)
            return self.obj

        try:
            obj = oload(self.cache, self.hash_info)
            assert isinstance(obj, Tree)
        except (FileNotFoundError, ObjectFormatError):
            obj = None

        self.obj = obj
        return obj

    def _collect_used_dir_cache(
        self, remote=None, force=False, jobs=None, filter_info=None
    ) -> Optional["Tree"]:
        """Fetch dir cache and return used object IDs for this out."""

        try:
            self.get_dir_cache(jobs=jobs, remote=remote)
        except RemoteMissingDepsError:  # pylint: disable=try-except-raise
            raise
        except DvcException:
            logger.debug("failed to pull cache for '%s'", self)

        try:
            ocheck(self.cache, self.cache.get(self.hash_info.value))
        except FileNotFoundError:
            msg = (
                "Missing cache for directory '{}'. "
                "Cache for files inside will be lost. "
                "Would you like to continue? Use '-f' to force."
            )
            if not force and not prompt.confirm(msg.format(self.fs_path)):
                raise CollectCacheError(  # noqa: B904
                    "unable to fully collect used cache"
                    " without cache for directory '{}'".format(self)
                )
            return None

        obj = self.get_obj()
        assert obj is None or isinstance(obj, Tree)
        if filter_info and filter_info != self.fs_path:
            assert obj
            prefix = self.fs.path.parts(self.fs.path.relpath(filter_info, self.fs_path))
            return obj.filter(prefix)
        return obj

    def get_used_objs(  # noqa: C901
        self, **kwargs
    ) -> Dict[Optional["ObjectDB"], Set["HashInfo"]]:
        """Return filtered set of used object IDs for this out."""

        if not self.use_cache:
            return {}

        push: bool = kwargs.pop("push", False)
        if self.stage.is_repo_import:
            return {}

        if push and not self.can_push:
            return {}

        if not self.hash_info:
            msg = (
                "Output '{}'({}) is missing version info. "
                "Cache for it will not be collected. "
                "Use `dvc repro` to get your pipeline up to date.".format(
                    self, self.stage
                )
            )
            if self.exists:
                msg += (
                    "\n"
                    "You can also use `dvc commit {stage.addressing}` "
                    "to associate existing '{out}' with {stage}.".format(
                        out=self, stage=self.stage
                    )
                )
            logger.warning(msg)
            return {}

        obj: Optional["HashFile"]
        if self.is_dir_checksum:
            obj = self._collect_used_dir_cache(**kwargs)
        else:
            obj = self.get_obj(filter_info=kwargs.get("filter_info"))
            if not obj:
                obj = self.cache.get(self.hash_info.value)

        if not obj:
            return {}

        if self.remote:
            remote = self.repo.cloud.get_remote_odb(name=self.remote)
        else:
            remote = None

        return {remote: self._named_obj_ids(obj)}

    def _named_obj_ids(self, obj):
        name = str(self)
        obj.hash_info.obj_name = name
        oids = {obj.hash_info}
        if isinstance(obj, Tree):
            for key, _, oid in obj:
                oid.obj_name = self.fs.sep.join([name, *key])
                oids.add(oid)
        return oids

    def _validate_output_path(self, path, stage=None):
        from dvc.dvcfile import is_valid_filename

        if is_valid_filename(path):
            raise self.IsStageFileError(path)

        if stage:
            abs_path = os.path.join(stage.wdir, path)
            if self._is_path_dvcignore(abs_path):
                check = stage.repo.dvcignore.check_ignore(abs_path)
                raise self.IsIgnoredError(check)

    def _check_can_merge(self, out):
        if self.protocol != out.protocol:
            raise MergeError("unable to auto-merge outputs of different types")

        my = self.dumpd()
        other = out.dumpd()

        ignored = [
            self.hash_name,
            Meta.PARAM_SIZE,
            Meta.PARAM_NFILES,
        ]

        for opt in ignored:
            my.pop(opt, None)
            other.pop(opt, None)

        if my != other:
            raise MergeError("unable to auto-merge outputs with different options")

        if not out.is_dir_checksum:
            raise MergeError("unable to auto-merge outputs that are not directories")

    def merge(self, ancestor, other, allowed=None):
        from dvc_data.hashfile.tree import MergeError as TreeMergeError
        from dvc_data.hashfile.tree import merge

        assert other

        if ancestor:
            self._check_can_merge(ancestor)
            ancestor_info = ancestor.hash_info
        else:
            ancestor_info = None

        self._check_can_merge(self)
        self._check_can_merge(other)

        try:
            merged = merge(
                self.cache,
                ancestor_info,
                self.hash_info,
                other.hash_info,
                allowed=allowed,
            )
        except TreeMergeError as exc:
            raise MergeError(str(exc)) from exc

        self.cache.add(merged.path, merged.fs, merged.oid)

        self.hash_info = merged.hash_info
        self.files = None
        self.meta = Meta(
            size=du(self.cache, merged),
            nfiles=len(merged),
        )

    def unstage(self, path: str) -> Tuple["Meta", "Tree"]:
        from pygtrie import Trie

        from dvc_objects.fs.path import Path

        assert isinstance(self.fs.path, Path)
        rel_key = tuple(self.fs.path.parts(self.fs.path.relpath(path, self.fs_path)))

        if self.hash_info:
            tree = self.get_dir_cache()
            if tree is None:
                raise DvcException(f"could not read {self.hash_info.value!r}")
        else:
            tree = Tree()

        trie = tree.as_trie()
        assert isinstance(trie, Trie)

        try:
            del trie[rel_key:]  # type: ignore[misc]
        except KeyError:
            raise FileNotFoundError(  # noqa: B904
                errno.ENOENT,
                os.strerror(errno.ENOENT),
                self.fs.path.relpath(path),
            )

        new = tree.from_trie(trie)
        new.digest()
        return Meta(nfiles=len(new), isdir=True), new

    def apply(
        self,
        path: str,
        obj: Union["Tree", "HashFile"],
        meta: "Meta",
    ) -> Tuple["Meta", "Tree"]:
        from pygtrie import Trie

        from dvc_objects.fs.path import Path

        assert isinstance(self.fs.path, Path)
        append_only = True
        rel_key = tuple(self.fs.path.parts(self.fs.path.relpath(path, self.fs_path)))

        if self.hash_info:
            tree = self.get_dir_cache()
            if tree is None:
                raise DvcException(f"could not read {self.hash_info.value!r}")
        else:
            tree = Tree()

        trie = tree.as_trie()
        assert isinstance(trie, Trie)

        try:
            del trie[rel_key:]  # type: ignore[misc]
        except KeyError:
            pass
        else:
            append_only = False

        items = {}
        if isinstance(obj, Tree):
            items = {(*rel_key, *key): (m, o) for key, m, o in obj}
        else:
            items = {rel_key: (meta, obj.hash_info)}
        trie.update(items)

        new = Tree.from_trie(trie)
        new.digest()

        size = self.meta.size if self.meta and self.meta.size else None
        if append_only and size and meta.size is not None:
            # if files were only appended, we can sum to the existing size
            size += meta.size
        elif self.hash_info and self.hash_info == new.hash_info:
            # if hashes are same, sizes must have been the same
            size = self.meta.size
        else:
            size = None

        meta = Meta(nfiles=len(new), size=size, isdir=True)
        return meta, new

    def add(  # noqa: C901
        self, path: Optional[str] = None, no_commit: bool = False, relink: bool = True
    ) -> Optional["HashFile"]:
        path = path or self.fs_path
        if self.hash_info and not self.is_dir_checksum and self.fs_path != path:
            raise DvcException(
                f"Cannot modify '{self}' which is being tracked as a file"
            )

        assert self.repo
        cache = self.cache if self.use_cache else self.repo.cache.local
        assert isinstance(cache, HashFileDB)

        new: "HashFile"
        try:
            assert self.hash_name
            staging, meta, obj = build(
                cache,
                path,
                self.fs,
                self.hash_name,
                ignore=self.dvcignore,
                dry_run=not self.use_cache,
            )
        except FileNotFoundError as exc:
            if self.fs_path == path:
                raise self.DoesNotExistError(self) from exc
            if not self.is_dir_checksum:
                raise

            meta, new = self.unstage(path)
            staging, obj = None, None
        else:
            assert obj
            assert staging
            if self.fs_path != path:
                meta, new = self.apply(path, obj, meta)
                add_update_tree(staging, new)
            else:
                new = obj

        self.obj = new
        self.hash_info = self.obj.hash_info
        self.meta = meta
        self.files = None
        self.ignore()

        if no_commit or not self.use_cache:
            return obj

        if isinstance(new, Tree):
            add_update_tree(cache, new)

        if not obj:
            return obj

        assert staging
        assert obj.hash_info
        otransfer(staging, self.cache, {obj.hash_info}, hardlink=relink, shallow=False)

        if relink:
            rel = self.fs.path.relpath(path)
            with CheckoutCallback(desc=f"Checking out {rel}", unit="files") as callback:
                self._checkout(
                    path,
                    self.fs,
                    obj,
                    self.cache,
                    relink=True,
                    state=self.repo.state,
                    prompt=prompt.confirm,
                    progress_callback=callback,
                )
            self.set_exec()
        return obj

    @property
    def fspath(self):
        return self.fs_path

    @property
    def is_decorated(self) -> bool:
        return self.is_metric or self.is_plot

    @property
    def is_metric(self) -> bool:
        return bool(self.metric)

    @property
    def is_plot(self) -> bool:
        return bool(self.plot)

    def restore_fields(self, other: "Output"):
        """Restore attributes that need to be preserved when serialized."""
        self.annot = other.annot
        self.remote = other.remote
        self.can_push = other.can_push

    def merge_version_meta(self, other: "Output"):
        """Merge version meta for files which are unchanged from other."""
        if not self.hash_info:
            return
        if self.hash_info.isdir:
            return self._merge_dir_version_meta(other)
        if self.hash_info != other.hash_info:
            return
        self.meta = other.meta

    def _merge_dir_version_meta(self, other: "Output"):
        from dvc_data.hashfile.tree import update_meta

        if not self.obj or not other.hash_info.isdir:
            return
        other_obj = other.obj if other.obj is not None else other.get_obj()
        assert isinstance(self.obj, Tree)
        assert isinstance(other_obj, Tree)
        updated = update_meta(self.obj, other_obj)
        assert updated.hash_info == self.obj.hash_info
        self.obj = updated
        self.files = updated.as_list(with_meta=True)


META_SCHEMA = {
    Meta.PARAM_SIZE: int,
    Meta.PARAM_NFILES: int,
    Meta.PARAM_ISEXEC: bool,
    Meta.PARAM_VERSION_ID: str,
}

CLOUD_SCHEMA = All({str: {**META_SCHEMA, **CHECKSUMS_SCHEMA}}, Length(max=1))

ARTIFACT_SCHEMA = {
    **CHECKSUMS_SCHEMA,
    **META_SCHEMA,
    Required(Output.PARAM_PATH): str,
    Output.PARAM_PLOT: bool,
    Output.PARAM_PERSIST: bool,
    Output.PARAM_CLOUD: CLOUD_SCHEMA,
}

DIR_FILES_SCHEMA: Dict[str, Any] = {
    **CHECKSUMS_SCHEMA,
    **META_SCHEMA,
    Required(Tree.PARAM_RELPATH): str,
    Output.PARAM_CLOUD: CLOUD_SCHEMA,
}

SCHEMA = {
    **ARTIFACT_SCHEMA,
    **ANNOTATION_SCHEMA,
    Output.PARAM_CACHE: bool,
    Output.PARAM_METRIC: Output.METRIC_SCHEMA,
    Output.PARAM_REMOTE: str,
    Output.PARAM_PUSH: bool,
    Output.PARAM_FILES: [DIR_FILES_SCHEMA],
}




dvc/pathspec_math.py
# Path Specification Pattern Math
# Including changing base dir of path specification patterns and merging
# of two path specification patterns with different base
# All the operations follow the documents of `gitignore`
from collections import namedtuple

from pathspec.util import normalize_file

from dvc.utils import relpath

PatternInfo = namedtuple("PatternInfo", ["patterns", "file_info"])


def _not_ignore(rule):
    return (True, rule[1:]) if rule.startswith("!") else (False, rule)


def _is_comment(rule):
    return rule.startswith("#")


def _remove_slash(rule):
    if rule.startswith("\\"):
        return rule[1:]
    return rule


def _match_all_level(rule):
    if rule[:-1].find("/") >= 0 and not rule.startswith("**/"):
        if rule.startswith("/"):
            rule = rule[1:]
        return False, rule
    if rule.startswith("**/"):
        rule = rule[3:]
    return True, rule


def change_rule(rule, rel):
    rule = rule.strip()
    if _is_comment(rule):
        return rule
    not_ignore, rule = _not_ignore(rule)
    match_all, rule = _match_all_level(rule)
    rule = _remove_slash(rule)
    if not match_all:
        rule = f"/{rule}"
    else:
        rule = f"/**/{rule}"
    if not_ignore:
        rule = f"!/{rel}{rule}"
    else:
        rule = f"/{rel}{rule}"
    return normalize_file(rule)


def _change_dirname(dirname, pattern_list, new_dirname):
    if new_dirname == dirname:
        return pattern_list
    rel = relpath(dirname, new_dirname)
    if rel.startswith(".."):
        raise ValueError("change dirname can only change to parent path")

    return [
        PatternInfo(change_rule(rule.patterns, rel), rule.file_info)
        for rule in pattern_list
    ]


def merge_patterns(flavour, pattern_a, prefix_a, pattern_b, prefix_b):
    """
    Merge two path specification patterns.

    This implementation merge two path specification patterns on different
    bases. It returns the longest common parent directory, and the patterns
    based on this new base directory.
    """
    if not pattern_a:
        return pattern_b, prefix_b
    if not pattern_b:
        return pattern_a, prefix_a

    longest_common_dir = flavour.commonpath([prefix_a, prefix_b])
    new_pattern_a = _change_dirname(prefix_a, pattern_a, longest_common_dir)
    new_pattern_b = _change_dirname(prefix_b, pattern_b, longest_common_dir)

    if len(prefix_a) <= len(prefix_b):
        merged_pattern = new_pattern_a + new_pattern_b
    else:
        merged_pattern = new_pattern_b + new_pattern_a

    return merged_pattern, longest_common_dir




dvc/progress.py
"""Manages progress bars for DVC repo."""
import logging
import sys
from threading import RLock

from tqdm import tqdm

from dvc.env import DVC_IGNORE_ISATTY
from dvc.utils import env2bool

logger = logging.getLogger(__name__)
tqdm.set_lock(RLock())


class Tqdm(tqdm):
    """
    maximum-compatibility tqdm-based progressbars
    """

    BAR_FMT_DEFAULT = (
        "{percentage:3.0f}% {desc}|{bar}|"
        "{postfix[info]}{n_fmt}/{total_fmt}"
        " [{elapsed}<{remaining}, {rate_fmt:>11}]"
    )
    # nested bars should have fixed bar widths to align nicely
    BAR_FMT_DEFAULT_NESTED = (
        "{percentage:3.0f}%|{bar:10}|{desc:{ncols_desc}.{ncols_desc}}"
        "{postfix[info]}{n_fmt}/{total_fmt}"
        " [{elapsed}<{remaining}, {rate_fmt:>11}]"
    )
    BAR_FMT_NOTOTAL = "{desc}{bar:b}|{postfix[info]}{n_fmt} [{elapsed}, {rate_fmt:>11}]"
    BYTES_DEFAULTS = {
        "unit": "B",
        "unit_scale": True,
        "unit_divisor": 1024,
        "miniters": 1,
    }

    def __init__(  # noqa: PLR0913
        self,
        iterable=None,
        disable=None,
        level=logging.ERROR,
        desc=None,
        leave=False,
        bar_format=None,
        bytes=False,  # noqa: A002, pylint: disable=redefined-builtin
        file=None,
        total=None,
        postfix=None,
        **kwargs,
    ):
        """
        bytes   : shortcut for
            `unit='B', unit_scale=True, unit_divisor=1024, miniters=1`
        desc  : persists after `close()`
        level  : effective logging level for determining `disable`;
            used only if `disable` is unspecified
        disable  : If (default: None) or False,
            will be determined by logging level.
            May be overridden to `True` due to non-TTY status.
            Skip override by specifying env var `DVC_IGNORE_ISATTY`.
        kwargs  : anything accepted by `tqdm.tqdm()`
        """
        kwargs = kwargs.copy()
        if bytes:
            kwargs = {**self.BYTES_DEFAULTS, **kwargs}
        else:
            kwargs.setdefault("unit_scale", total > 999 if total else True)
        if file is None:
            file = sys.stderr
        # auto-disable based on `logger.level`
        if not disable:
            disable = logger.getEffectiveLevel() > level
        # auto-disable based on TTY
        if not disable and not env2bool(DVC_IGNORE_ISATTY) and hasattr(file, "isatty"):
            disable = not file.isatty()
        super().__init__(
            iterable=iterable,
            disable=disable,
            leave=leave,
            desc=desc,
            bar_format="!",
            lock_args=(False,),
            total=total,
            **kwargs,
        )
        self.postfix = postfix or {"info": ""}
        if bar_format is None:
            if self.__len__():
                self.bar_format = (
                    self.BAR_FMT_DEFAULT_NESTED if self.pos else self.BAR_FMT_DEFAULT
                )
            else:
                self.bar_format = self.BAR_FMT_NOTOTAL
        else:
            self.bar_format = bar_format
        self.refresh()

    def update_msg(self, msg: str, n: int = 1) -> None:
        """
        Sets `msg` as a postfix and calls `update(n)`.
        """
        self.set_msg(msg)
        self.update(n)

    def set_msg(self, msg: str) -> None:
        self.postfix["info"] = f" {msg} |"

    def update_to(self, current, total=None):
        if total:
            self.total = total
        self.update(current - self.n)

    def wrap_fn(self, fn, callback=None):
        """
        Returns a wrapped `fn` which calls `callback()` on each call.
        `callback` is `self.update` by default.
        """
        if callback is None:
            callback = self.update

        def wrapped(*args, **kwargs):
            res = fn(*args, **kwargs)
            callback()
            return res

        return wrapped

    def close(self):
        self.postfix["info"] = ""
        # remove ETA (either unknown or zero); remove completed bar
        self.bar_format = self.bar_format.replace("<{remaining}", "").replace(
            "|{bar:10}|", " "
        )
        super().close()

    @property
    def format_dict(self):
        """inject `ncols_desc` to fill the display width (`ncols`)"""
        d = super().format_dict
        ncols: int = d["ncols"] or 80
        # assumes `bar_format` has max one of ("ncols_desc" & "ncols_info")
        ncols_left = (
            ncols
            - len(
                self.format_meter(  # type: ignore[call-arg]
                    ncols_desc=1, ncols_info=1, **d
                )
            )
            + 1
        )
        ncols_left = max(ncols_left, 0)
        if ncols_left:
            d["ncols_desc"] = d["ncols_info"] = ncols_left
        else:
            # work-around for zero-width description
            d["ncols_desc"] = d["ncols_info"] = 1
            d["prefix"] = ""
        return d




dvc/prompt.py
"""Manages user prompts."""

import logging
import sys
from getpass import getpass
from typing import Collection, Optional

logger = logging.getLogger(__name__)


def ask(prompt: str, limited_to: Optional[Collection[str]] = None):
    if not sys.stdout.isatty():
        return None

    while True:
        try:
            answer = input(prompt + " ").lower()
        except EOFError:
            return None

        if not limited_to:
            return answer

        if answer in limited_to:
            return answer

        logger.info("Your response must be one of: %s. Please try again.", limited_to)


def confirm(statement: str) -> bool:
    """Ask the user for confirmation about the specified statement.

    Args:
        statement (unicode): statement to ask the user confirmation about.

    Returns:
        bool: whether or not specified statement was confirmed.
    """
    prompt = f"{statement} [y/n]"
    answer = ask(prompt, limited_to=["yes", "no", "y", "n"])
    return answer and answer.startswith("y")


def password(statement: str) -> str:
    """Ask the user for a password.

    Args:
        statement (str): string to prompt the user with.

    Returns:
        str: password entered by the user.
    """
    logger.info("%s: ", statement)
    return getpass("")




dvc/rwlock.py
import json
import logging
import os
from collections import defaultdict
from contextlib import contextmanager

import psutil
from voluptuous import Invalid, Optional, Required, Schema

from .exceptions import DvcException
from .fs import localfs
from .lock import make_lock
from .utils import relpath

logger = logging.getLogger(__name__)


INFO_SCHEMA = {Required("pid"): int, Required("cmd"): str}

SCHEMA = Schema(
    {
        Optional("write", default={}): {str: INFO_SCHEMA},
        Optional("read", default={}): {str: [INFO_SCHEMA]},
    }
)

RWLOCK_FILE = "rwlock"
RWLOCK_LOCK = "rwlock.lock"


class RWLockFileCorruptedError(DvcException):
    def __init__(self, path):
        super().__init__(
            "Unable to read RWLock-file '{}'. JSON structure is corrupted".format(
                relpath(path)
            )
        )


class RWLockFileFormatError(DvcException):
    def __init__(self, path):
        super().__init__(f"RWLock-file '{relpath(path)}' format error.")


@contextmanager
def _edit_rwlock(lock_dir, fs, hardlink):
    path = fs.path.join(lock_dir, RWLOCK_FILE)

    rwlock_guard = make_lock(
        fs.path.join(lock_dir, RWLOCK_LOCK),
        tmp_dir=lock_dir,
        hardlink_lock=hardlink,
    )
    with rwlock_guard:
        try:
            with fs.open(path, encoding="utf-8") as fobj:
                lock = SCHEMA(json.load(fobj))
        except FileNotFoundError:
            lock = SCHEMA({})
        except json.JSONDecodeError as exc:
            raise RWLockFileCorruptedError(path) from exc
        except Invalid as exc:
            raise RWLockFileFormatError(path) from exc
        lock["read"] = defaultdict(list, lock["read"])
        lock["write"] = defaultdict(dict, lock["write"])
        yield lock
        with fs.open(path, "w", encoding="utf-8") as fobj:
            json.dump(lock, fobj)


def _infos_to_str(infos):
    return "\n".join(
        "  (PID {}): {}".format(info["pid"], info["cmd"]) for info in infos
    )


def _check_blockers(tmp_dir, lock, info, *, mode, waiters):  # noqa: C901, PLR0912
    from .lock import LockError

    non_existing_pid = set()

    blockers = []
    to_release = defaultdict(list)
    for path, infos in lock[mode].items():
        for waiter_path in waiters:
            if localfs.path.overlaps(waiter_path, path):
                break
        else:
            continue

        infos = infos if isinstance(infos, list) else [infos]
        for blocker in infos:
            if blocker == info:
                continue

            pid = int(blocker["pid"])

            if pid in non_existing_pid:
                pass
            elif psutil.pid_exists(pid):
                blockers.append(blocker)
                continue
            else:
                non_existing_pid.add(pid)
                cmd = blocker["cmd"]
                logger.warning(
                    (
                        "Process '%s' with (Pid %s), in RWLock-file '%s'"
                        " had been killed. Auto remove it from the lock file."
                    ),
                    cmd,
                    pid,
                    relpath(path),
                )
            to_release[json.dumps(blocker, sort_keys=True)].append(path)

    if to_release:
        for info_json, path_list in to_release.items():
            info = json.loads(info_json)
            if mode == "read":
                _release_read(lock, info, path_list)
            elif mode == "write":
                _release_write(lock, info, path_list)

    if blockers:
        raise LockError(
            f"'{waiter_path}' is busy, it is being blocked by:\n"
            f"{_infos_to_str(blockers)}\n"
            "\n"
            "If there are no processes with such PIDs, you can manually "
            f"remove '{tmp_dir}/rwlock' and try again."
        )


def _acquire_read(lock, info, paths):
    changes = []

    lock["read"] = lock.get("read", defaultdict(list))

    for path in paths:
        readers = lock["read"][path]
        if info in readers:
            continue

        changes.append(path)
        readers.append(info)

    return changes


def _acquire_write(lock, info, paths):
    changes = []

    lock["write"] = lock.get("write", defaultdict(dict))

    for path in paths:
        if lock["write"][path] == info:
            continue

        changes.append(path)
        lock["write"][path] = info

    return changes


def _release_write(lock, info, changes):
    for url in changes:
        assert "write" in lock
        assert url in lock["write"]
        assert lock["write"][url] == info
        del lock["write"][url]
        if not lock["write"]:
            del lock["write"]


def _release_read(lock, info, changes):
    for url in changes:
        assert "read" in lock
        assert url in lock["read"]
        assert info in lock["read"][url]
        lock["read"][url].remove(info)
        if not lock["read"][url]:
            del lock["read"][url]
        if not lock["read"]:
            del lock["read"]


@contextmanager
def rwlock(tmp_dir, fs, cmd, read, write, hardlink):
    """Create non-thread-safe RWLock for file paths.

    Args:
        tmp_dir (str): existing directory where to create the rwlock file.
        fs (FileSystem): fs instance that tmp_dir belongs to.
        cmd (str): command that will be working on these file path.
        read ([str]): file paths that are going to be read.
        write ([str]): file paths that are going to be written.
        hardlink (bool): use hardlink lock to guard rwlock file when on edit.

    Raises:
        LockError: raised if file paths we want to read is being written to by
            another command or if file paths we want to write is being written
            to or read from by another command.
        RWLockFileCorruptedError: raised if rwlock file is not a valid JSON.
        RWLockFileFormatError: raised if rwlock file is a valid JSON, but
            has internal format that doesn't pass our schema validation.
    """
    info = {"pid": os.getpid(), "cmd": cmd}

    with _edit_rwlock(tmp_dir, fs, hardlink) as lock:
        _check_blockers(tmp_dir, lock, info, mode="write", waiters=read + write)
        _check_blockers(tmp_dir, lock, info, mode="read", waiters=write)

        rchanges = _acquire_read(lock, info, read)
        wchanges = _acquire_write(lock, info, write)

    try:
        yield
    finally:
        with _edit_rwlock(tmp_dir, fs, hardlink) as lock:
            _release_write(lock, info, wchanges)
            _release_read(lock, info, rchanges)




dvc/schema.py
from collections.abc import Mapping

from voluptuous import Any, Equal, Optional, Required, Schema

from dvc import dependency, output
from dvc.annotations import ANNOTATION_SCHEMA, ARTIFACT_SCHEMA
from dvc.output import (
    CHECKSUMS_SCHEMA,
    CLOUD_SCHEMA,
    DIR_FILES_SCHEMA,
    META_SCHEMA,
    Output,
)
from dvc.parsing import DO_KWD, FOREACH_KWD, VARS_KWD
from dvc.stage.params import StageParams

STAGES = "stages"
SINGLE_STAGE_SCHEMA = {
    StageParams.PARAM_MD5: output.CHECKSUM_SCHEMA,
    StageParams.PARAM_CMD: Any(str, list, None),
    StageParams.PARAM_WDIR: Any(str, None),
    StageParams.PARAM_DEPS: Any([dependency.SCHEMA], None),
    StageParams.PARAM_OUTS: Any([output.SCHEMA], None),
    StageParams.PARAM_LOCKED: bool,  # backward compatibility
    StageParams.PARAM_FROZEN: bool,
    StageParams.PARAM_META: object,
    StageParams.PARAM_ALWAYS_CHANGED: bool,
    StageParams.PARAM_DESC: str,
}

DATA_SCHEMA = {
    **CHECKSUMS_SCHEMA,
    **META_SCHEMA,
    Required("path"): str,
    Output.PARAM_CLOUD: CLOUD_SCHEMA,
    Output.PARAM_FILES: [DIR_FILES_SCHEMA],
}
LOCK_FILE_STAGE_SCHEMA = {
    Required(StageParams.PARAM_CMD): Any(str, list),
    StageParams.PARAM_DEPS: [DATA_SCHEMA],
    StageParams.PARAM_PARAMS: {str: {str: object}},
    StageParams.PARAM_OUTS: [DATA_SCHEMA],
}

LOCKFILE_STAGES_SCHEMA = {str: LOCK_FILE_STAGE_SCHEMA}
LOCKFILE_SCHEMA = {
    Required("schema"): Equal("2.0", "invalid schema version"),
    STAGES: LOCKFILE_STAGES_SCHEMA,
}

OUT_PSTAGE_DETAILED_SCHEMA = {
    str: {
        **ANNOTATION_SCHEMA,  # type: ignore[arg-type]
        Output.PARAM_CACHE: bool,
        Output.PARAM_PERSIST: bool,
        Output.PARAM_REMOTE: str,
        Output.PARAM_PUSH: bool,
    }
}

PLOTS = "plots"
PLOT_PROPS = {
    Output.PARAM_PLOT_TEMPLATE: str,
    Output.PARAM_PLOT_X: str,
    Output.PARAM_PLOT_Y: str,
    Output.PARAM_PLOT_X_LABEL: str,
    Output.PARAM_PLOT_Y_LABEL: str,
    Output.PARAM_PLOT_TITLE: str,
    Output.PARAM_PLOT_HEADER: bool,
}
PLOT_PROPS_SCHEMA = {**OUT_PSTAGE_DETAILED_SCHEMA[str], **PLOT_PROPS}
PLOT_PSTAGE_SCHEMA = {str: Any(PLOT_PROPS_SCHEMA, [PLOT_PROPS_SCHEMA])}

PARAM_PSTAGE_NON_DEFAULT_SCHEMA = {str: [str]}

VARS_SCHEMA = [str, dict]

STAGE_DEFINITION = {
    Required(StageParams.PARAM_CMD): Any(str, list),
    Optional(StageParams.PARAM_WDIR): str,
    Optional(StageParams.PARAM_DEPS): [str],
    Optional(StageParams.PARAM_PARAMS): [Any(str, dict)],
    Optional(VARS_KWD): VARS_SCHEMA,
    Optional(StageParams.PARAM_FROZEN): bool,
    Optional(StageParams.PARAM_META): object,
    Optional(StageParams.PARAM_DESC): str,
    Optional(StageParams.PARAM_ALWAYS_CHANGED): bool,
    Optional(StageParams.PARAM_OUTS): [Any(str, OUT_PSTAGE_DETAILED_SCHEMA)],
    Optional(StageParams.PARAM_METRICS): [Any(str, OUT_PSTAGE_DETAILED_SCHEMA)],
    Optional(StageParams.PARAM_PLOTS): [Any(str, PLOT_PSTAGE_SCHEMA)],
}


def either_or(primary, fallback, fallback_includes=None):
    def validator(data):
        schema = primary
        if isinstance(data, Mapping) and set(fallback_includes or []) & data.keys():
            schema = fallback
        return Schema(schema)(data)

    return validator


PLOT_DEFINITION = {
    Output.PARAM_PLOT_X: Any(str, {str: str}),
    Output.PARAM_PLOT_Y: Any(str, [str], {str: Any(str, [str])}),
    Output.PARAM_PLOT_X_LABEL: str,
    Output.PARAM_PLOT_Y_LABEL: str,
    Output.PARAM_PLOT_TITLE: str,
    Output.PARAM_PLOT_TEMPLATE: str,
}
SINGLE_PLOT_SCHEMA = {str: Any(PLOT_DEFINITION, None)}
ARTIFACTS = "artifacts"
SINGLE_ARTIFACT_SCHEMA = Schema({str: ARTIFACT_SCHEMA})
FOREACH_IN = {
    Required(FOREACH_KWD): Any(dict, list, str),
    Required(DO_KWD): STAGE_DEFINITION,
}
SINGLE_PIPELINE_STAGE_SCHEMA = {
    str: either_or(STAGE_DEFINITION, FOREACH_IN, [FOREACH_KWD, DO_KWD])
}
MULTI_STAGE_SCHEMA = {
    PLOTS: [Any(str, SINGLE_PLOT_SCHEMA)],
    STAGES: SINGLE_PIPELINE_STAGE_SCHEMA,
    VARS_KWD: VARS_SCHEMA,
    StageParams.PARAM_PARAMS: [str],
    StageParams.PARAM_METRICS: [str],
    ARTIFACTS: SINGLE_ARTIFACT_SCHEMA,
}

COMPILED_SINGLE_STAGE_SCHEMA = Schema(SINGLE_STAGE_SCHEMA)
COMPILED_MULTI_STAGE_SCHEMA = Schema(MULTI_STAGE_SCHEMA)
COMPILED_LOCK_FILE_STAGE_SCHEMA = Schema(LOCK_FILE_STAGE_SCHEMA)
COMPILED_LOCKFILE_SCHEMA = Schema(LOCKFILE_SCHEMA)




dvc/scm.py
"""Manages source control systems (e.g. Git)."""
import os
from contextlib import contextmanager
from functools import partial
from typing import (
    TYPE_CHECKING,
    Iterator,
    List,
    Literal,
    Mapping,
    Optional,
    Union,
    overload,
)

from funcy import group_by
from scmrepo.base import Base  # noqa: F401, pylint: disable=unused-import
from scmrepo.git import Git
from scmrepo.noscm import NoSCM

from dvc.exceptions import DvcException
from dvc.progress import Tqdm

if TYPE_CHECKING:
    from scmrepo.progress import GitProgressEvent


class SCMError(DvcException):
    """Base class for source control management errors."""


class CloneError(SCMError):
    pass


class RevError(SCMError):
    pass


class NoSCMError(SCMError):
    def __init__(self):
        msg = (
            "Only supported for Git repositories. If you're "
            "seeing this error in a Git repo, try updating the DVC "
            "configuration with `dvc config core.no_scm false`."
        )
        super().__init__(msg)


class InvalidRemoteSCMRepo(SCMError):
    pass


class GitAuthError(SCMError):
    def __init__(self, reason: str) -> None:
        doc = "See https://dvc.org/doc/user-guide/troubleshooting#git-auth"
        super().__init__(f"{reason}\n{doc}")


@contextmanager
def map_scm_exception(with_cause: bool = False) -> Iterator[None]:
    from scmrepo.exceptions import SCMError as InternalSCMError

    try:
        yield
    except InternalSCMError as exc:
        into = SCMError(str(exc))
        if with_cause:
            raise into from exc
        raise into


@overload
def SCM(
    root_dir: str,
    *,
    search_parent_directories: bool = ...,
    no_scm: Literal[False] = ...,
) -> "Git":
    ...


@overload
def SCM(
    root_dir: str,
    *,
    search_parent_directories: bool = ...,
    no_scm: Literal[True],
) -> "NoSCM":
    ...


@overload
def SCM(
    root_dir: str,
    *,
    search_parent_directories: bool = ...,
    no_scm: bool = ...,
) -> Union["Git", "NoSCM"]:
    ...


def SCM(
    root_dir, *, search_parent_directories=True, no_scm=False
):  # pylint: disable=invalid-name
    """Returns SCM instance that corresponds to a repo at the specified
    path.

    Args:
        root_dir (str): path to a root directory of the repo.
        search_parent_directories (bool): whether to look for repo root in
        parent directories.
        no_scm (bool): return NoSCM if True.

    Returns:
        dvc.scm.base.Base: SCM instance.
    """
    with map_scm_exception():
        if no_scm:
            return NoSCM(root_dir, _raise_not_implemented_as=NoSCMError)
        return Git(root_dir, search_parent_directories=search_parent_directories)


class TqdmGit(Tqdm):
    BAR_FMT = (
        "{desc}|{bar}|{postfix[info]}{n_fmt}/{total_fmt} [{elapsed}, {rate_fmt:>11}]"
    )

    def __init__(self, *args, **kwargs):
        kwargs.setdefault("unit", "obj")
        kwargs.setdefault("bar_format", self.BAR_FMT)
        super().__init__(*args, **kwargs)
        self._last_phase = None

    def update_git(self, event: "GitProgressEvent") -> None:
        phase, completed, total, message, *_ = event
        if phase:
            message = (phase + " | " + message) if message else phase
        if message:
            self.set_msg(message)
        force_refresh = (  # force-refresh progress bar when:
            (total and completed and completed >= total)  # the task completes
            or total != self.total  # the total changes
            or phase != self._last_phase  # or, the phase changes
        )
        if completed is not None:
            self.update_to(completed, total)
        if force_refresh:
            self.refresh()
        self._last_phase = phase


def clone(url: str, to_path: str, **kwargs):
    from scmrepo.exceptions import CloneError as InternalCloneError

    from dvc.repo.experiments.utils import fetch_all_exps

    with TqdmGit(desc=f"Cloning {os.path.basename(url)}") as pbar:
        try:
            git = Git.clone(url, to_path, progress=pbar.update_git, **kwargs)
            if "shallow_branch" not in kwargs:
                fetch_all_exps(git, url, progress=pbar.update_git)
            return git
        except InternalCloneError as exc:
            raise CloneError("SCM error") from exc


def resolve_rev(scm: Union["Git", "NoSCM"], rev: str) -> str:
    from scmrepo.exceptions import RevError as InternalRevError

    from dvc.repo.experiments.utils import fix_exp_head

    try:
        return scm.resolve_rev(fix_exp_head(scm, rev))
    except InternalRevError as exc:
        assert isinstance(scm, Git)
        # `scm` will only resolve git branch and tag names,
        # if rev is not a sha it may be an abbreviated experiment name
        if not (rev == "HEAD" or rev.startswith("refs/")):
            from dvc.repo.experiments.utils import AmbiguousExpRefInfo, resolve_name

            try:
                ref_infos = resolve_name(scm, rev).get(rev)
            except AmbiguousExpRefInfo:
                raise RevError(f"ambiguous Git revision '{rev}'")  # noqa: B904
            if ref_infos:
                return scm.get_ref(str(ref_infos))

        raise RevError(str(exc))  # noqa: B904


def _get_n_commits(scm: "Git", revs: List[str], num: int) -> List[str]:
    results = []
    for rev in revs:
        if num == 0:
            continue
        results.append(rev)
        n = 1
        while True:
            if num == n:
                break
            try:
                head = f"{rev}~{n}"
                results.append(resolve_rev(scm, head))
            except RevError:
                break
            n += 1
    return results


def iter_revs(
    scm: "Git",
    revs: Optional[List[str]] = None,
    num: int = 1,
    all_branches: bool = False,
    all_tags: bool = False,
    all_commits: bool = False,
    all_experiments: bool = False,
    commit_date: Optional[str] = None,
) -> Mapping[str, List[str]]:
    from scmrepo.exceptions import SCMError as _SCMError

    from dvc.repo.experiments.utils import exp_commits

    if not any(
        [
            revs,
            all_branches,
            all_tags,
            all_commits,
            all_experiments,
            commit_date,
        ]
    ):
        return {}

    revs = revs or []
    results: List[str] = _get_n_commits(scm, revs, num)

    if all_commits:
        results.extend(scm.list_all_commits())
    else:
        if all_branches:
            results.extend(scm.list_branches())

        if all_tags:
            results.extend(scm.list_tags())

        if commit_date:
            from datetime import datetime

            commit_datestamp = datetime.strptime(commit_date, "%Y-%m-%d").timestamp()

            def _time_filter(rev):
                try:
                    return scm.resolve_commit(rev).commit_time >= commit_datestamp
                except _SCMError:
                    return True

            results.extend(filter(_time_filter, scm.list_all_commits()))

    if all_experiments:
        results.extend(exp_commits(scm))

    rev_resolver = partial(resolve_rev, scm)
    return group_by(rev_resolver, results)




dvc/types.py
from typing import TYPE_CHECKING, Any, AnyStr, Dict, List, Union

if TYPE_CHECKING:
    from os import PathLike

StrPath = Union[str, "PathLike[str]"]
BytesPath = Union[bytes, "PathLike[bytes]"]
GenericPath = Union[AnyStr, "PathLike[AnyStr]"]
StrOrBytesPath = Union[str, bytes, "PathLike[str]", "PathLike[bytes]"]

TargetType = Union[List[str], str]
DictStrAny = Dict[str, Any]




dvc/updater.py
import logging
import os
import sys
import time
from typing import TYPE_CHECKING, Optional

from packaging import version

from dvc import __version__
from dvc.utils.pkg import PKG

if TYPE_CHECKING:
    from dvc.ui import RichText

logger = logging.getLogger(__name__)


class Updater:
    URL = "https://updater.dvc.org"
    UPDATER_FILE = "updater"
    TIMEOUT = 24 * 60 * 60  # every day
    TIMEOUT_GET = 10

    def __init__(self, tmp_dir, friendly=False, hardlink_lock=False):
        from dvc.lock import make_lock

        self.updater_file = os.path.join(tmp_dir, self.UPDATER_FILE)
        self.lock = make_lock(
            self.updater_file + ".lock",
            tmp_dir=tmp_dir,
            friendly=friendly,
            hardlink_lock=hardlink_lock,
        )
        self.current = version.parse(__version__).base_version

    def _is_outdated_file(self):
        ctime = os.path.getmtime(self.updater_file)
        outdated = time.time() - ctime >= self.TIMEOUT
        if outdated:
            logger.debug("'%s' is outdated", self.updater_file)
        return outdated

    def _with_lock(self, func, action):
        from dvc.lock import LockError

        try:
            with self.lock:
                func()
        except LockError:
            logger.debug(
                "Failed to acquire '%s' before %s updates",
                self.lock.lockfile,
                action,
            )

    def check(self):
        from dvc.utils import env2bool

        if (
            os.getenv("CI")
            or env2bool("DVC_TEST")
            or PKG == "snap"
            or not self.is_enabled()
        ):
            return

        self._with_lock(self._check, "checking")

    def _check(self):
        if not os.path.exists(self.updater_file) or self._is_outdated_file():
            self.fetch()
            return

        with open(self.updater_file, encoding="utf-8") as fobj:
            import json

            try:
                info = json.load(fobj)
                latest = info["version"]
            except Exception as e:  # noqa: BLE001  # pylint: disable=W0703
                logger.debug("'%s' is not a valid json: %s", self.updater_file, e)
                self.fetch()
                return

        if version.parse(self.current) < version.parse(latest):
            self._notify(latest)

    def fetch(self, detach=True):
        from dvc.daemon import daemon

        if detach:
            daemon(["updater"])
            return

        self._with_lock(self._get_latest_version, "fetching")

    def _get_latest_version(self):
        import json

        import requests

        try:
            resp = requests.get(self.URL, timeout=self.TIMEOUT_GET)
            info = resp.json()
        except requests.exceptions.RequestException as exc:
            logger.debug("Failed to retrieve latest version: %s", exc)
            return

        with open(self.updater_file, "w+", encoding="utf-8") as fobj:
            json.dump(info, fobj)

    def _notify(self, latest: str, pkg: Optional[str] = PKG) -> None:
        from dvc.ui import ui

        if not sys.stdout.isatty():
            return

        message = self._get_message(latest, pkg=pkg)
        return ui.error_write(message, styled=True)

    def _get_message(
        self,
        latest: str,
        current: Optional[str] = None,
        color: str = "yellow",
        pkg: Optional[str] = None,
    ) -> "RichText":
        from dvc.ui import ui

        current = current or self.current
        update_message = ui.rich_text.from_markup(
            f"You are using dvc version [bold]{current}[/]; "
            f"however, version [bold]{latest}[/] is available."
        )
        instruction = ui.rich_text.from_markup(self._get_update_instructions(pkg=pkg))
        return ui.rich_text.assemble(
            "\n", update_message, "\n", instruction, style=color
        )

    @staticmethod
    def _get_update_instructions(pkg: Optional[str] = None) -> str:
        if pkg in ("osxpkg", "exe", "binary"):
            return (
                "To upgrade, uninstall dvc and reinstall from [blue]https://dvc.org[/]."
            )

        instructions = {
            "pip": "pip install --upgrade dvc",
            "rpm": "yum update dvc",
            "brew": "brew upgrade dvc",
            "deb": "apt-get install --only-upgrade dvc",
            "conda": "conda update dvc",
            "choco": "choco upgrade dvc",
        }

        if pkg not in instructions:
            return (
                "Find the latest release at "
                "[blue]https://github.com/iterative/dvc/releases/latest[/]."
            )

        instruction = instructions[pkg]
        return f"To upgrade, run '{instruction}'."

    def is_enabled(self):
        from dvc.config import Config, to_bool

        enabled = to_bool(
            Config.from_cwd(validate=False).get("core", {}).get("check_update", "true")
        )
        logger.debug("Check for update is %sabled.", "en" if enabled else "dis")
        return enabled


def notify_updates():
    from contextlib import suppress

    from dvc.repo import NotDvcRepoError, Repo

    with suppress(NotDvcRepoError), Repo() as repo:
        hardlink_lock = repo.config["core"].get("hardlink_lock", False)
        updater = Updater(repo.tmp_dir, hardlink_lock=hardlink_lock)
        updater.check()




dvc/version.py
# pylint: disable=unused-import
try:
    from ._dvc_version import version as __version__  # type: ignore[import]
    from ._dvc_version import version_tuple  # type: ignore[import]
except ImportError:
    __version__ = "UNKNOWN"
    version_tuple = (0, 0, __version__)  # type: ignore[assignment]




dvc/api/__init__.py
from dvc.fs.dvc import _DVCFileSystem as DVCFileSystem

from .data import open  # pylint: disable=redefined-builtin
from .data import get_url, read
from .experiments import exp_save, exp_show
from .scm import all_branches, all_commits, all_tags
from .show import metrics_show, params_show

__all__ = [
    "all_branches",
    "all_commits",
    "all_tags",
    "exp_save",
    "exp_show",
    "get_url",
    "open",
    "params_show",
    "metrics_show",
    "read",
    "DVCFileSystem",
]




dvc/api/data.py
from contextlib import _GeneratorContextManager as GCM
from contextlib import contextmanager
from typing import Any, Dict, Optional

from funcy import reraise

from dvc.exceptions import FileMissingError, OutputNotFoundError, PathMissingError
from dvc.repo import Repo


@contextmanager
def _wrap_exceptions(repo, url):
    from dvc.config import NoRemoteError
    from dvc.exceptions import NoOutputInExternalRepoError, NoRemoteInExternalRepoError

    try:
        yield
    except NoRemoteError as exc:
        raise NoRemoteInExternalRepoError(url) from exc
    except OutputNotFoundError as exc:
        if exc.repo is repo:
            raise NoOutputInExternalRepoError(exc.output, repo.root_dir, url) from exc
        raise
    except FileMissingError as exc:
        raise PathMissingError(exc.path, url) from exc


def get_url(path, repo=None, rev=None, remote=None):
    """
    Returns the URL to the storage location of a data file or directory tracked
    in a DVC repo. For Git repos, HEAD is used unless a rev argument is
    supplied. The default remote is tried unless a remote argument is supplied.

    Raises OutputNotFoundError if the file is not tracked by DVC.

    NOTE: This function does not check for the actual existence of the file or
    directory in the remote storage.
    """
    with Repo.open(repo, rev=rev, subrepos=True, uninitialized=True) as _repo:
        with _wrap_exceptions(_repo, path):
            fs_path = _repo.dvcfs.from_os_path(path)

            with reraise(FileNotFoundError, PathMissingError(path, repo)):
                info = _repo.dvcfs.info(fs_path)

            dvc_info = info.get("dvc_info")
            if not dvc_info:
                raise OutputNotFoundError(path, repo)

            dvc_repo = info["repo"]  # pylint: disable=unsubscriptable-object
            md5 = dvc_info["md5"]

            return dvc_repo.cloud.get_url_for(remote, checksum=md5)


class _OpenContextManager(GCM):
    def __init__(self, func, args, kwds):  # pylint: disable=super-init-not-called
        self.gen = func(*args, **kwds)
        self.func, self.args, self.kwds = (  # type: ignore[assignment]
            func,
            args,
            kwds,
        )

    def __getattr__(self, name):
        raise AttributeError("dvc.api.open() should be used in a with statement.")


def open(  # noqa, pylint: disable=redefined-builtin
    path: str,
    repo: Optional[str] = None,
    rev: Optional[str] = None,
    remote: Optional[str] = None,
    mode: str = "r",
    encoding: Optional[str] = None,
):
    """
    Opens a file tracked in a DVC project.

    This function may only be used as a context manager (using the `with`
    keyword, as shown in the examples).

    This function makes a direct connection to the remote storage, so the file
    contents can be streamed. Your code can process the data buffer as it's
    streamed, which optimizes memory usage.

    Note:
        Use dvc.api.read() to load the complete file contents
        in a single function call, no context manager involved.
        Neither function utilizes disc space.

    Args:
        path (str): location and file name of the target to open,
        relative to the root of `repo`.
        repo (str, optional): location of the DVC project or Git Repo.
            Defaults to the current DVC project (found by walking up from the
            current working directory tree).
            It can be a URL or a file system path.
            Both HTTP and SSH protocols are supported for online Git repos
            (e.g. [user@]server:project.git).
        rev (str, optional): Any `Git revision`_ such as a branch or tag name,
            a commit hash or a dvc experiment name.
            Defaults to HEAD.
            If `repo` is not a Git repo, this option is ignored.
        remote (str, optional): Name of the `DVC remote`_ used to form the
            returned URL string.
            Defaults to the `default remote`_ of `repo`.
            For local projects, the cache is tried before the default remote.
        mode (str, optional): Specifies the mode in which the file is opened.
            Defaults to "r" (read).
            Mirrors the namesake parameter in builtin `open()`_.
            Only reading `mode` is supported.
        encoding(str, optional): `Codec`_ used to decode the file contents.
            Defaults to None.
            This should only be used in text mode.
            Mirrors the namesake parameter in builtin `open()`_.

    Returns:
        _OpenContextManager: A context manager that generatse a corresponding
            `file object`_.
            The exact type of file object depends on the mode used.
            For more details, please refer to Python's `open()`_ built-in,
            which is used under the hood.

    Raises:
        AttributeError: If this method is not used as a context manager.
        ValueError: If non-read `mode` is used.

    Examples:

        - Use data or models from a DVC repository.

        Any file tracked in a DVC project (and stored remotely) can be
        processed directly in your Python code with this API.
        For example, an XML file tracked in a public DVC repo on GitHub can be
        processed like this:

        >>> from xml.sax import parse
        >>> import dvc.api
        >>> from mymodule import mySAXHandler

        >>> with dvc.api.open(
        ...     'get-started/data.xml',
        ...     repo='https://github.com/iterative/dataset-registry'
        ... ) as fd:
        ...     parse(fd, mySAXHandler)

        We use a SAX XML parser here because dvc.api.open() is able to stream
        the data from remote storage.
        The mySAXHandler object should handle the event-driven parsing of the
        document in this case.
        This increases the performance of the code (minimizing memory usage),
        and is typically faster than loading the whole data into memory.

        - Accessing private repos

        This is just a matter of using the right repo argument, for example an
        SSH URL (requires that the credentials are configured locally):

        >>> import dvc.api

        >>> with dvc.api.open(
        ...     'features.dat',
        ...     repo='git@server.com:path/to/repo.git'
        ... ) as fd:
        ...     # ... Process 'features'
        ...     pass

        - Use different versions of data

        Any git revision (see `rev`) can be accessed programmatically.
        For example, if your DVC repo has tagged releases of a CSV dataset:

        >>> import csv
        >>> import dvc.api
        >>> with dvc.api.open(
        ...     'clean.csv',
        ...     rev='v1.1.0'
        ... ) as fd:
        ...     reader = csv.reader(fd)
        ...     # ... Process 'clean' data from version 1.1.0

    .. _Git revision:
        https://git-scm.com/docs/revisions

    .. _DVC remote:
        https://dvc.org/doc/command-reference/remote

    .. _default remote:
        https://dvc.org/doc/command-reference/remote/default

    .. _open():
        https://docs.python.org/3/library/functions.html#open

    .. _Codec:
        https://docs.python.org/3/library/codecs.html#standard-encodings

    .. _file object:
        https://docs.python.org/3/glossary.html#term-file-object

    """
    if "r" not in mode:
        raise ValueError("Only reading `mode` is supported.")

    args = (path,)
    kwargs = {
        "repo": repo,
        "remote": remote,
        "rev": rev,
        "mode": mode,
        "encoding": encoding,
    }
    return _OpenContextManager(_open, args, kwargs)


def _open(path, repo=None, rev=None, remote=None, mode="r", encoding=None):
    repo_kwargs: Dict[str, Any] = {"subrepos": True, "uninitialized": True}
    if remote:
        repo_kwargs["config"] = {"core": {"remote": remote}}

    with Repo.open(repo, rev=rev, **repo_kwargs) as _repo:
        with _wrap_exceptions(_repo, path):
            import os
            from typing import TYPE_CHECKING, Union

            from dvc.exceptions import IsADirectoryError as DvcIsADirectoryError
            from dvc.fs.data import DataFileSystem
            from dvc.fs.dvc import DVCFileSystem

            if TYPE_CHECKING:
                from dvc.fs import FileSystem

            fs: Union["FileSystem", DataFileSystem, DVCFileSystem]
            if os.path.isabs(path):
                fs = DataFileSystem(index=_repo.index.data["local"])
                fs_path = path
            else:
                fs = DVCFileSystem(repo=_repo, subrepos=True)
                fs_path = fs.from_os_path(path)

            try:
                with fs.open(
                    fs_path,
                    mode=mode,
                    encoding=encoding,
                ) as fobj:
                    yield fobj
            except FileNotFoundError as exc:
                raise FileMissingError(path) from exc
            except IsADirectoryError as exc:
                raise DvcIsADirectoryError(f"'{path}' is a directory") from exc


def read(path, repo=None, rev=None, remote=None, mode="r", encoding=None):
    """
    Returns the contents of a tracked file (by DVC or Git). For Git repos, HEAD
    is used unless a rev argument is supplied. The default remote is tried
    unless a remote argument is supplied.
    """
    with open(
        path, repo=repo, rev=rev, remote=remote, mode=mode, encoding=encoding
    ) as fd:
        return fd.read()




dvc/api/experiments.py
from typing import Dict, List, Optional, Union

from rich.text import Text

from dvc.repo import Repo
from dvc.repo.experiments.show import tabulate


def exp_save(
    name: Optional[str] = None,
    force: bool = False,
    include_untracked: Optional[List[str]] = None,
):
    """
    Create a new DVC experiment using `exp save`.

    See https://dvc.org/doc/command-reference/exp/save.

    Args:
        name (str, optional): specify a name for this experiment.
            If `None`, a default one will be generated, such as `urban-sign`.
            Defaults to `None`.
        force (bool):  overwrite the experiment if an experiment with the same
            name already exists.
            Defaults to `False`.
        include_untracked (List[str], optional): specify untracked file(s) to
            be included in the saved experiment.
            Defaults to `None`.

    Returns:
        str: The `Git revision`_ of the created experiment.

    Raises:
        ExperimentExistsError: If an experiment with `name` already exists and
            `force=False`.

    .. _Git revision:
        https://git-scm.com/docs/revisions
    """
    with Repo() as repo:
        return repo.experiments.save(
            name=name, force=force, include_untracked=include_untracked
        )


def _postprocess(exp_rows):
    for exp_row in exp_rows:
        for k, v in exp_row.items():
            if isinstance(v, Text):
                v_str = str(v)
                try:
                    exp_row[k] = float(v_str)
                except ValueError:
                    exp_row[k] = v_str

            if not exp_row[k]:
                exp_row[k] = None

    return exp_rows


def exp_show(
    repo: Optional[str] = None,
    revs: Optional[Union[str, List[str]]] = None,
    num: int = 1,
    param_deps: bool = False,
    force: bool = False,
) -> List[Dict]:
    """Get DVC experiments tracked in `repo`.

    Without arguments, this function will retrieve all experiments derived from
    the Git `HEAD`.

    See the options below to customize the experiments retrieved.

    Args:
        repo (str, optional): location of the DVC repository.
            Defaults to the current project (found by walking up from the
            current working directory tree).
            It can be a URL or a file system path.
            Both HTTP and SSH protocols are supported for online Git repos
            (e.g. [user@]server:project.git).
        revs (Union[str, List[str]], optional): Git revision(s) (e.g. branch,
            tag, SHA commit) to use as a reference point to start listing
            experiments.
            Defaults to `None`, which will use `HEAD` as starting point.
        num (int, optional): show experiments from the last `num` commits
            (first parents) starting from the `revs` baseline.
            Give a negative value to include all first-parent commits (similar
            to `git log -n`).
            Defaults to 1.
        param_deps (bool, optional): include only parameters that are stage
            dependencies.
            Defaults to `False`.
        force (bool, optional): force re-collection of experiments instead of
            loading from internal experiments cache.
            DVC caches `exp_show` data for completed experiments to improve
            performance of subsequent calls.
            When `force` is specified, DVC will reload all experiment data and
            ignore any previously cached results.
            Defaults to `False`.

    Returns:
        List[Dict]: Each item in the list will contain a dictionary with
            the info for an individual experiment.
            See Examples below.
    """
    with Repo.open(repo) as _repo:
        experiments = _repo.experiments.show(
            revs=revs,
            num=num,
            param_deps=param_deps,
            force=force,
        )
        td, _ = tabulate(experiments, fill_value=None)

        return _postprocess(td.as_dict())




dvc/api/scm.py
from typing import List, Optional

from dvc.repo import Repo


def all_branches(repo: Optional[str] = None) -> List[str]:
    """Get all Git branches in a DVC repository.

    Args:
        repo (str, optional): location of the DVC repository.
            Defaults to the current project (found by walking up from the
            current working directory tree).
            It can be a URL or a file system path.
            Both HTTP and SSH protocols are supported for online Git repos
            (e.g. [user@]server:project.git).
    Returns:
        List[str]: Names of the Git branches.
    """
    with Repo.open(repo) as _repo:
        return _repo.scm.list_branches()


def all_commits(repo: Optional[str] = None) -> List[str]:
    """Get all Git commits in a DVC repository.

    Args:
        repo (str, optional): location of the DVC repository.
            Defaults to the current project (found by walking up from the
            current working directory tree).
            It can be a URL or a file system path.
            Both HTTP and SSH protocols are supported for online Git repos
            (e.g. [user@]server:project.git).
    Returns:
        List[str]: SHAs of the Git commits.
    """
    with Repo.open(repo) as _repo:
        return _repo.scm.list_all_commits()


def all_tags(repo: Optional[str] = None) -> List[str]:
    """Get all Git tags in a DVC repository.

    Args:
        repo (str, optional): location of the DVC repository.
            Defaults to the current project (found by walking up from the
            current working directory tree).
            It can be a URL or a file system path.
            Both HTTP and SSH protocols are supported for online Git repos
            (e.g. [user@]server:project.git).
    Returns:
        List[str]: Names of the Git tags.
    """
    with Repo.open(repo) as _repo:
        return _repo.scm.list_tags()




dvc/api/show.py
import typing
from collections import Counter
from typing import Dict, Iterable, Optional, Union

from funcy import first

from dvc.repo import Repo


def _onerror_raise(exception: Exception, *args, **kwargs):
    raise exception


def _postprocess(results):
    processed: Dict[str, Dict] = {}
    for rev, rev_data in results.items():
        if not rev_data:
            continue

        processed[rev] = {}

        counts: typing.Counter[str] = Counter()
        for file_data in rev_data["data"].values():
            for k in file_data["data"]:
                counts[k] += 1

        for file_name, file_data in rev_data["data"].items():
            to_merge = {
                (k if counts[k] == 1 else f"{file_name}:{k}"): v
                for k, v in file_data["data"].items()
            }
            processed[rev] = {**processed[rev], **to_merge}

    if "workspace" in processed:
        del processed["workspace"]

    return processed


def metrics_show(
    *targets: str,
    repo: Optional[str] = None,
    rev: Optional[str] = None,
) -> Dict:
    """Get metrics tracked in `repo`.

    Without arguments, this function will retrieve all metrics from all tracked
    metric files, for the current working tree.

    See the options below to restrict the metrics retrieved.

    Args:
        *targets (str, optional): Names of the metric files to retrieve
        metrics from. For example, "classifier_eval.json,
        clustering_eval.json".
        If no `targets` are provided, all metric files tracked in `dvc.yaml`
        will be used.
        Note that targets don't necessarily have to be defined in `dvc.yaml`.
        repo (str, optional): Location of the DVC repository.
            Defaults to the current project (found by walking up from the
            current working directory tree).
            It can be a URL or a file system path.
            Both HTTP and SSH protocols are supported for online Git repos
            (e.g. [user@]server:project.git).
        rev (str, optional): Name of the `Git revision`_ to retrieve metrics
            from.
            Defaults to `None`.
            An example of git revision can be a branch or tag name, a commit
            hash or a dvc experiment name.
            If `repo` is not a Git repo, this option is ignored.
            If `None`, the current working tree will be used.

    Returns:
        Dict: See Examples below.

    Examples:

        - No arguments.

        Working on https://github.com/iterative/example-get-started

        >>> import dvc.api
        >>> metrics = dvc.api.metrics_show()
        >>> print(json.dumps(metrics, indent=4))
        {
            "avg_prec": 0.9249974999612706,
            "roc_auc": 0.9460213440787918
        }

        ---

        - Using `rev`.

        Working on https://github.com/iterative/example-get-started

        >>> import json
        >>> import dvc.api
        >>> metrics = dvc.api.metrics_show(rev="tune-hyperparams")
        >>> print(json.dumps(metrics, indent=4))
        {
            "avg_prec": 0.9268792615819422,
            "roc_auc": 0.945093365854111
        }

        ---

        - Using `targets`.

        Working on https://github.com/iterative/example-get-started

        >>> import json
        >>> import dvc.api
        >>> metrics = dvc.api.metrics_show("evaluation.json")
        >>> print(json.dumps(metrics, indent=4))
        {
            "avg_prec": 0.9249974999612706,
            "roc_auc": 0.9460213440787918
        }

        ---

        - Git URL as `repo`.

        >>> import json
        >>> import dvc.api
        >>> metrics = dvc.api.metrics_show(
        ...     repo="https://github.com/iterative/demo-fashion-mnist")
        >>> print(json.dumps(metrics, indent=4))
        {
            "loss": 0.25284987688064575,
            "accuracy": 0.9071000218391418
        }


    .. _Git revision:
        https://git-scm.com/docs/revisions
    """

    with Repo.open(repo) as _repo:
        metrics = _repo.metrics.show(
            targets=targets,
            revs=rev if rev is None else [rev],
            onerror=_onerror_raise,
        )

    metrics = _postprocess(metrics)

    if not metrics:
        return {}

    return metrics[first(metrics)]


def params_show(
    *targets: str,
    repo: Optional[str] = None,
    stages: Optional[Union[str, Iterable[str]]] = None,
    rev: Optional[str] = None,
    deps: bool = False,
) -> Dict:
    """Get parameters tracked in `repo`.

    Without arguments, this function will retrieve all params from all tracked
    parameter files, for the current working tree.

    See the options below to restrict the parameters retrieved.

    Args:
        *targets (str, optional): Names of the parameter files to retrieve
        params from. For example, "params.py, myparams.toml".
        If no `targets` are provided, all parameter files tracked in `dvc.yaml`
        will be used.
        Note that targets don't necessarily have to be defined in `dvc.yaml`.
        repo (str, optional): location of the DVC repository.
            Defaults to the current project (found by walking up from the
            current working directory tree).
            It can be a URL or a file system path.
            Both HTTP and SSH protocols are supported for online Git repos
            (e.g. [user@]server:project.git).
        stages (Union[str, Iterable[str]], optional): Name or names of the
            stages to retrieve parameters from.
            Defaults to `None`.
            If `None`, all parameters from all stages will be retrieved.
            If this method is called from a different location to the one where
            the `dvc.yaml` is found, the relative path to the `dvc.yaml` must
            be provided as a prefix with the syntax `{relpath}:{stage}`.
            For example: `subdir/dvc.yaml:stage-0` or `../dvc.yaml:stage-1`.
        rev (str, optional): Name of the `Git revision`_ to retrieve parameters
            from.
            Defaults to `None`.
            An example of git revision can be a branch or tag name, a commit
            hash or a dvc experiment name.
            If `repo` is not a Git repo, this option is ignored.
            If `None`, the current working tree will be used.
        deps (bool, optional): Whether to retrieve only parameters that are
            stage dependencies or not.
            Defaults to `False`.

    Returns:
        Dict: See Examples below.

    Examples:

        - No arguments.

        Working on https://github.com/iterative/example-get-started

        >>> import json
        >>> import dvc.api
        >>> params = dvc.api.params_show()
        >>> print(json.dumps(params, indent=4))
        {
            "prepare": {
                "split": 0.2,
                "seed": 20170428
            },
            "featurize": {
                "max_features": 200,
                "ngrams": 2
            },
            "train": {
                "seed": 20170428,
                "n_est": 50,
                "min_split": 0.01
            }
        }

        ---

        - Filtering with `stages`.

        Working on https://github.com/iterative/example-get-started

        `stages` can a single string:

        >>> import json
        >>> import dvc.api
        >>> params = dvc.api.params_show(stages="prepare")
        >>> print(json.dumps(params, indent=4))
        {
            "prepare": {
                "split": 0.2,
                "seed": 20170428
            }
        }

        Or an iterable of strings:

        >>> import json
        >>> import dvc.api
        >>> params = dvc.api.params_show(stages=["prepare", "train"])
        >>> print(json.dumps(params, indent=4))
        {
            "prepare": {
                "split": 0.2,
                "seed": 20170428
            },
            "train": {
                "seed": 20170428,
                "n_est": 50,
                "min_split": 0.01
            }
        }

        ---

        - Using `rev`.

        Working on https://github.com/iterative/example-get-started

        >>> import json
        >>> import dvc.api
        >>> params = dvc.api.params_show(rev="tune-hyperparams")
        >>> print(json.dumps(params, indent=4))
        {
            "prepare": {
                "split": 0.2,
                "seed": 20170428
            },
            "featurize": {
                "max_features": 200,
                "ngrams": 2
            },
            "train": {
                "seed": 20170428,
                "n_est": 100,
                "min_split": 8
            }
        }

        ---

        - Using `targets`.

        Working on `multi-params-files` folder of
        https://github.com/iterative/pipeline-conifguration

        You can pass a single target:

        >>> import json
        >>> import dvc.api
        >>> params = dvc.api.params_show("params.yaml")
        >>> print(json.dumps(params, indent=4))
        {
            "run_mode": "prod",
            "configs": {
                "dev": "configs/params_dev.yaml",
                "test": "configs/params_test.yaml",
                "prod": "configs/params_prod.yaml"
            },
            "evaluate": {
                "dataset": "micro",
                "size": 5000,
                "metrics": ["f1", "roc-auc"],
                "metrics_file": "reports/metrics.json",
                "plots_cm": "reports/plot_confusion_matrix.png"
            }
        }


        Or multiple targets:

        >>> import json
        >>> import dvc.api
        >>> params = dvc.api.params_show(
        ...     "configs/params_dev.yaml", "configs/params_prod.yaml")
        >>> print(json.dumps(params, indent=4))
        {
            "configs/params_prod.yaml:run_mode": "prod",
            "configs/params_prod.yaml:config_file": "configs/params_prod.yaml",
            "configs/params_prod.yaml:data_load": {
                "dataset": "large",
                "sampling": {
                "enable": true,
                "size": 50000
                }
            },
            "configs/params_prod.yaml:train": {
                "epochs": 1000
            },
            "configs/params_dev.yaml:run_mode": "dev",
            "configs/params_dev.yaml:config_file": "configs/params_dev.yaml",
            "configs/params_dev.yaml:data_load": {
                "dataset": "development",
                "sampling": {
                "enable": true,
                "size": 1000
                }
            },
            "configs/params_dev.yaml:train": {
                "epochs": 10
            }
        }

        ---

        - Git URL as `repo`.

        >>> import json
        >>> import dvc.api
        >>> params = dvc.api.params_show(
        ...     repo="https://github.com/iterative/demo-fashion-mnist")
        {
            "train": {
                "batch_size": 128,
                "hidden_units": 64,
                "dropout": 0.4,
                "num_epochs": 10,
                "lr": 0.001,
                "conv_activation": "relu"
            }
        }


    .. _Git revision:
        https://git-scm.com/docs/revisions

    """
    if isinstance(stages, str):
        stages = [stages]

    with Repo.open(repo) as _repo:
        params = _repo.params.show(
            revs=rev if rev is None else [rev],
            targets=targets,
            deps=deps,
            onerror=_onerror_raise,
            stages=stages,
        )

    params = _postprocess(params)

    if not params:
        return {}

    return params[first(params)]




dvc/cli/__init__.py
"""This module provides an entrypoint to the dvc cli and parsing utils."""

import logging
import sys

# Workaround for CPython bug. See [1] and [2] for more info.
# [1] https://github.com/aws/aws-cli/blob/1.16.277/awscli/clidriver.py#L55
# [2] https://bugs.python.org/issue29288
from typing import Optional

"".encode("idna")

logger = logging.getLogger("dvc")


class DvcParserError(Exception):
    """Base class for CLI parser errors."""

    def __init__(self):
        super().__init__("parser error")


def parse_args(argv=None):
    """Parses CLI arguments.

    Args:
        argv: optional list of arguments to parse. sys.argv is used by default.

    Raises:
        DvcParserError: raised for argument parsing errors.
    """
    from .parser import get_main_parser

    parser = get_main_parser()
    args = parser.parse_args(argv)
    args.parser = parser
    return args


def _log_unknown_exceptions() -> None:
    from dvc.info import get_dvc_info
    from dvc.ui import ui
    from dvc.utils import colorize

    logger.exception("unexpected error")
    if logger.isEnabledFor(logging.DEBUG):
        logger.debug("Version info for developers:\n%s", get_dvc_info())

    q = colorize("Having any troubles?", "yellow")
    link = colorize("https://dvc.org/support", "blue")
    footer = f"\n{q} Hit us up at {link}, we are always happy to help!"
    ui.error_write(footer)


def _log_exceptions(exc: Exception) -> Optional[int]:
    """Try to log some known exceptions, that are not DVCExceptions."""
    from dvc.utils import error_link, format_link

    if isinstance(exc, OSError):
        import errno

        if exc.errno == errno.EMFILE:
            logger.exception(
                (
                    "too many open files, please visit "
                    "%s to see how to handle this problem"
                ),
                error_link("many-files"),
                extra={"tb_only": True},
            )
        else:
            _log_unknown_exceptions()
        return None

    from dvc.fs import AuthError, ConfigError, RemoteMissingDepsError

    if isinstance(exc, RemoteMissingDepsError):
        from dvc.utils.pkg import PKG

        proto = exc.protocol
        by_pkg = {
            "pip": f"pip install 'dvc[{proto}]'",
            "conda": f"conda install -c conda-forge dvc-{proto}",
        }

        cmd = by_pkg.get(PKG)
        if cmd:
            link = format_link("https://dvc.org/doc/install")
            hint = (
                "To install dvc with those dependencies, run:\n"
                "\n"
                f"\t{cmd}\n"
                "\n"
                f"See {link} for more info."
            )
        else:
            link = format_link("https://github.com/iterative/dvc/issues")
            hint = f"\nPlease report this bug to {link}. Thank you!"

        logger.exception(
            "URL '%s' is supported but requires these missing dependencies: %s. %s",
            exc.url,
            exc.missing_deps,
            hint,
            extra={"tb_only": True},
        )
        return None

    if isinstance(exc, (AuthError, ConfigError)):
        link = format_link("https://man.dvc.org/remote/modify")
        logger.exception("configuration error")
        logger.exception(
            "%s\nLearn more about configuration settings at %s.",
            exc,
            link,
            extra={"tb_only": True},
        )
        return 251

    from dvc_data.hashfile.cache import DiskError

    if isinstance(exc, DiskError):
        from dvc.utils import relpath

        directory = relpath(exc.directory)
        logger.exception(
            (
                "Could not open pickled '%s' cache.\n"
                "Remove the '%s' directory and then retry this command."
                "\nSee %s for more information."
            ),
            exc.type,
            directory,
            error_link("pickle"),
            extra={"tb_only": True},
        )
        return None

    from dvc_data.hashfile.build import IgnoreInCollectedDirError

    if isinstance(exc, IgnoreInCollectedDirError):
        logger.exception("")
        return None

    _log_unknown_exceptions()
    return None


def main(argv=None):  # noqa: C901, PLR0912, PLR0915
    """Main entry point for dvc CLI.

    Args:
        argv: optional list of arguments to parse. sys.argv is used by default.

    Returns:
        int: command's return code.
    """
    from dvc._debug import debugtools
    from dvc.config import ConfigError
    from dvc.exceptions import DvcException, NotDvcRepoError
    from dvc.logger import set_loggers_level

    # NOTE: stderr/stdout may be closed if we are running from dvc.daemon.
    # On Linux we directly call cli.main after double forking and closing
    # the copied parent's standard file descriptors. If we make any logging
    # calls in this state it will cause an exception due to writing to a closed
    # file descriptor.
    if sys.stderr.closed:  # pylint: disable=using-constant-test
        logging.disable()
    elif sys.stdout.closed:  # pylint: disable=using-constant-test
        logging.disable(logging.INFO)

    args = None

    outer_log_level = logger.level
    try:
        args = parse_args(argv)

        level = None
        if args.quiet:
            level = logging.CRITICAL
        elif args.verbose == 1:
            level = logging.DEBUG
        elif args.verbose > 1:
            level = logging.TRACE  # type: ignore[attr-defined]

        if level is not None:
            set_loggers_level(level)

        if level and level <= logging.DEBUG:
            from platform import platform, python_implementation, python_version

            from dvc import __version__
            from dvc.utils.pkg import PKG

            pyv = " ".join([python_implementation(), python_version()])
            pkg = f" ({PKG})" if PKG else ""
            logger.debug("v%s%s, %s on %s", __version__, pkg, pyv, platform())
            logger.debug("command: %s", " ".join(argv or sys.argv))

        logger.trace(args)  # type: ignore[attr-defined]

        if not sys.stdout.closed and not args.quiet:
            from dvc.ui import ui

            ui.enable()

        with debugtools(args):
            cmd = args.func(args)
            ret = cmd.do_run()
    except ConfigError:
        logger.exception("configuration error")
        ret = 251
    except KeyboardInterrupt:
        logger.exception("interrupted by the user")
        ret = 252
    except BrokenPipeError:
        import os

        # Python flushes standard streams on exit; redirect remaining output
        # to devnull to avoid another BrokenPipeError at shutdown
        # See: https://docs.python.org/3/library/signal.html#note-on-sigpipe
        devnull = os.open(os.devnull, os.O_WRONLY)
        os.dup2(devnull, sys.stdout.fileno())
        ret = 141  # 128 + 13 (SIGPIPE)
    except NotDvcRepoError:
        logger.exception("")
        ret = 253
    except DvcException:
        ret = 255
        logger.exception("")
    except DvcParserError:
        ret = 254
    except Exception as exc:  # noqa, pylint: disable=broad-except
        # pylint: disable=no-member
        ret = _log_exceptions(exc) or 255

    try:
        from dvc import analytics

        if analytics.is_enabled():
            analytics.collect_and_send_report(args, ret)

        return ret
    finally:
        logger.setLevel(outer_log_level)

        from dvc.repo.open_repo import clean_repos

        # Remove cached repos in the end of the call, these are anonymous
        # so won't be reused by any other subsequent run anyway.
        clean_repos()




dvc/cli/command.py
import logging
import os
from abc import ABC, abstractmethod
from typing import TYPE_CHECKING, Any

if TYPE_CHECKING:
    from dvc.config import Config

logger = logging.getLogger(__name__)


class CmdBase(ABC):
    UNINITIALIZED = False

    def __init__(self, args: Any):
        from dvc.repo import Repo

        os.chdir(args.cd)

        self.repo: "Repo" = Repo(uninitialized=self.UNINITIALIZED)
        self.config: "Config" = self.repo.config
        self.args = args

    def do_run(self):
        with self.repo:
            return self.run()

    @abstractmethod
    def run(self):
        pass


class CmdBaseNoRepo(CmdBase):
    def __init__(self, args):  # pylint: disable=super-init-not-called
        self.args = args

        os.chdir(args.cd)

    def do_run(self):
        return self.run()




dvc/cli/completion.py
import shtab

BASH_PREAMBLE = """
# $1=COMP_WORDS[1]
_dvc_compgen_DVCFiles() {
  compgen -d -S '/' -- $1  # recurse into subdirs
  compgen -f -X '!*?.dvc' -- $1
  compgen -f -X '!*Dvcfile' -- $1
  compgen -f -X '!*dvc.yaml' -- $1
}

_dvc_compgen_stages() {
    local _dvc_stages=($(dvc stage list -q --names-only))
    compgen -W "${_dvc_stages[*]}" -- $1
}
_dvc_compgen_stages_and_files() {
    _dvc_compgen_DVCFiles $1
    _dvc_compgen_stages $1
}

_dvc_compgen_exps() {
    local _dvc_exps=($(dvc exp list -q --all-commits --names-only))
    compgen -W "${_dvc_exps[*]}" -- $1
}
"""

ZSH_PREAMBLE = """
_dvc_compadd_DVCFiles() {
    _files -g '(*?.dvc|Dvcfile|dvc.yaml)'
}
_dvc_compadd_stages() {
    # this will also show up the description of the stages
    _describe 'stages' "($(_dvc_stages_output))"
}

_dvc_stages_output() {
  dvc stage list -q | awk '{
    # escape possible `:` on the stage name
    sub(/:/, "\\\\\\\\:", $1);
    # read all of the columns except the first
    # reading `out` from $2, so as not to have a leading whitespace
    out=$2; for(i=3;i<=NF;i++){out=out" "$i};
    # print key, ":" and then single-quote the description
    # colon is a delimiter used by `_describe` to separate field/description
    print $1":""\\047"out"\\047"
    # single quote -> \\047
    }'
}

_dvc_compadd_stages_and_files() {
    _dvc_compadd_DVCFiles
    _dvc_compadd_stages
}

_dvc_compadd_exps() {
    _describe 'experiments' "($(dvc exp list -q --all-commits --names-only))"
}
"""

PREAMBLE = {
    "bash": BASH_PREAMBLE,
    "zsh": ZSH_PREAMBLE,
}

FILE = shtab.FILE
DIR = shtab.DIRECTORY
DVC_FILE = {"bash": "_dvc_compgen_DVCFiles", "zsh": "_dvc_compadd_DVCFiles"}
STAGE = {"bash": "_dvc_compgen_stages", "zsh": "_dvc_compadd_stages"}
DVCFILES_AND_STAGE = {
    "bash": "_dvc_compgen_stages_and_files",
    "zsh": "_dvc_compadd_stages_and_files",
}
EXPERIMENT = {"bash": "_dvc_compgen_exps", "zsh": "_dvc_compadd_exps"}




dvc/cli/parser.py
"""Main parser for the dvc cli."""
import argparse
import logging
import os
from functools import lru_cache

from dvc import __version__
from dvc.commands import (
    add,
    cache,
    check_ignore,
    checkout,
    commit,
    completion,
    config,
    daemon,
    dag,
    data,
    data_sync,
    destroy,
    diff,
    experiments,
    freeze,
    gc,
    get,
    get_url,
    git_hook,
    imp,
    imp_url,
    init,
    install,
    ls,
    ls_url,
    machine,
    metrics,
    move,
    params,
    plots,
    queue,
    remote,
    remove,
    repro,
    root,
    stage,
    unprotect,
    update,
    version,
)

from . import DvcParserError

logger = logging.getLogger(__name__)

COMMANDS = [
    init,
    queue,
    get,
    get_url,
    destroy,
    add,
    remove,
    move,
    unprotect,
    repro,
    data_sync,
    gc,
    imp,
    imp_url,
    config,
    checkout,
    remote,
    cache,
    metrics,
    params,
    install,
    root,
    ls,
    ls_url,
    freeze,
    dag,
    daemon,
    commit,
    completion,
    diff,
    version,
    update,
    git_hook,
    plots,
    stage,
    experiments,
    check_ignore,
    machine,
    data,
]


def _find_parser(parser, cmd_cls):
    defaults = parser._defaults  # pylint: disable=protected-access
    if not cmd_cls or cmd_cls == defaults.get("func"):
        parser.print_help()
        raise DvcParserError

    actions = parser._actions  # pylint: disable=protected-access
    for action in actions:
        if not isinstance(action.choices, dict):
            # NOTE: we are only interested in subparsers
            continue
        for subparser in action.choices.values():
            _find_parser(subparser, cmd_cls)


class DvcParser(argparse.ArgumentParser):
    """Custom parser class for dvc CLI."""

    def error(self, message, cmd_cls=None):  # pylint: disable=arguments-differ
        logger.error(message)
        _find_parser(self, cmd_cls)

    def parse_args(self, args=None, namespace=None):
        # NOTE: overriding to provide a more granular help message.
        # E.g. `dvc plots diff --bad-flag` would result in a `dvc plots diff`
        # help message instead of generic `dvc` usage.
        args, argv = self.parse_known_args(args, namespace)
        if argv:
            msg = "unrecognized arguments: %s"
            self.error(msg % " ".join(argv), getattr(args, "func", None))
        return args


def get_parent_parser():
    """Create instances of a parser containing common arguments shared among
    all the commands.

    When overwriting `-q` or `-v`, you need to instantiate a new object
    in order to prevent some weird behavior.
    """
    from dvc._debug import add_debugging_flags

    parent_parser = argparse.ArgumentParser(add_help=False)
    add_debugging_flags(parent_parser)
    log_level_group = parent_parser.add_mutually_exclusive_group()
    log_level_group.add_argument(
        "-q", "--quiet", action="count", default=0, help="Be quiet."
    )
    log_level_group.add_argument(
        "-v", "--verbose", action="count", default=0, help="Be verbose."
    )

    return parent_parser


@lru_cache(maxsize=1)
def get_main_parser():
    parent_parser = get_parent_parser()

    # Main parser
    desc = "Data Version Control"
    parser = DvcParser(
        prog="dvc",
        description=desc,
        parents=[parent_parser],
        formatter_class=argparse.RawTextHelpFormatter,
        add_help=False,
    )

    # NOTE: We are doing this to capitalize help message.
    # Unfortunately, there is no easier and clearer way to do it,
    # as adding this argument in get_parent_parser() either in
    # log_level_group or on parent_parser itself will cause unexpected error.
    parser.add_argument(
        "-h",
        "--help",
        action="help",
        default=argparse.SUPPRESS,
        help="Show this help message and exit.",
    )

    parser.add_argument(
        "-V",
        "--version",
        action="version",
        version=__version__,
        help="Show program's version.",
    )

    parser.add_argument(
        "--cd",
        default=os.path.curdir,
        metavar="<path>",
        help="Change to directory before executing.",
        type=str,
    )

    # Sub commands
    subparsers = parser.add_subparsers(
        title="Available Commands",
        metavar="COMMAND",
        dest="cmd",
        help="Use `dvc COMMAND --help` for command-specific help.",
    )

    from .utils import fix_subparsers

    fix_subparsers(subparsers)

    for cmd in COMMANDS:
        cmd.add_parser(subparsers, parent_parser)

    return parser




dvc/cli/utils.py
def fix_subparsers(subparsers):
    """Workaround for bug in Python 3. See more info at:
    https://bugs.python.org/issue16308
    https://github.com/iterative/dvc/issues/769

    Args:
        subparsers: subparsers to fix.
    """
    subparsers.required = True
    subparsers.dest = "cmd"


def append_doc_link(help_message, path):
    from dvc.utils import format_link

    if not path:
        return help_message
    doc_base = "https://man.dvc.org/"
    return "{message}\nDocumentation: {link}".format(
        message=help_message, link=format_link(doc_base + path)
    )


def hide_subparsers_from_help(subparsers):
    # metavar needs to be explicitly set in order to hide subcommands
    # from the 'positional arguments' choices list
    # see: https://bugs.python.org/issue22848
    # Need to set `add_help=False`, but avoid setting `help`
    # (not even to `argparse.SUPPPRESS`).
    # NOTE: The argument is the parent subparser, not the subcommand parser.
    cmds = [cmd for cmd, parser in subparsers.choices.items() if parser.add_help]
    subparsers.metavar = "{{{}}}".format(",".join(cmds))




dvc/commands/__init__.py




dvc/commands/add.py
import argparse
import logging

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link

logger = logging.getLogger(__name__)


class CmdAdd(CmdBase):
    def run(self):
        from dvc.exceptions import DvcException

        try:
            self.repo.add(
                self.args.targets,
                no_commit=self.args.no_commit,
                file=self.args.file,
                external=self.args.external,
                glob=self.args.glob,
                out=self.args.out,
                remote=self.args.remote,
                to_remote=self.args.to_remote,
                jobs=self.args.remote_jobs,
                force=self.args.force,
            )
        except FileNotFoundError:
            logger.exception("")
            return 1
        except DvcException:
            logger.exception("")
            return 1
        return 0


def add_parser(subparsers, parent_parser):
    ADD_HELP = "Track data files or directories with DVC."

    parser = subparsers.add_parser(
        "add",
        parents=[parent_parser],
        description=append_doc_link(ADD_HELP, "add"),
        help=ADD_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--no-commit",
        action="store_true",
        default=False,
        help="Don't put files/directories into cache.",
    )
    parser.add_argument(
        "--external",
        action="store_true",
        default=False,
        help="Allow targets that are outside of the DVC repository.",
    )
    parser.add_argument(
        "--glob",
        action="store_true",
        default=False,
        help="Allows targets containing shell-style wildcards.",
    )
    parser.add_argument(
        "--file",
        help="Specify name of the .dvc file this command will generate.",
        metavar="<filename>",
    )
    parser.add_argument(
        "-o",
        "--out",
        help="Destination path to put files to.",
        metavar="<path>",
    )
    parser.add_argument(
        "--to-remote",
        action="store_true",
        default=False,
        help="Download it directly to the remote",
    )
    parser.add_argument(
        "-r",
        "--remote",
        help="Remote storage to download to",
        metavar="<name>",
    )
    parser.add_argument(
        "--remote-jobs",
        type=int,
        help=(
            "Only used along with '--to-remote'. "
            "Number of jobs to run simultaneously "
            "when pushing data to remote."
            "The default value is 4 * cpu_count(). "
        ),
        metavar="<number>",
    )
    parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        default=False,
        help="Override local file or folder if exists.",
    )
    parser.add_argument(
        "targets", nargs="+", help="Input files/directories to add."
    ).complete = completion.FILE
    parser.set_defaults(func=CmdAdd)




dvc/commands/cache.py
import argparse
import os

from dvc.cli import completion
from dvc.cli.utils import append_doc_link, fix_subparsers
from dvc.commands.config import CmdConfig
from dvc.ui import ui


class CmdCacheDir(CmdConfig):
    def run(self):
        if self.args.value is None and not self.args.unset:
            from dvc.config import ConfigError

            if self.args.level:
                conf = self.config.read(level=self.args.level)
            else:
                # Use merged config with default values
                conf = self.config
            try:
                self._check(conf, False, "cache", "dir")
                path = conf["cache"]["dir"]
            except ConfigError:
                if not self.config.dvc_dir or self.args.level:
                    raise
                path = os.path.join(self.config.dvc_dir, "cache")
            ui.write(path)
            return 0
        with self.config.edit(level=self.args.level) as conf:
            if self.args.unset:
                self._check(conf, False, "cache", "dir")
                del conf["cache"]["dir"]
            else:
                self._check(conf, False, "cache")
                conf["cache"]["dir"] = self.args.value
        return 0


def add_parser(subparsers, parent_parser):
    from dvc.commands.config import parent_config_parser

    CACHE_HELP = "Manage cache settings."

    cache_parser = subparsers.add_parser(
        "cache",
        parents=[parent_parser],
        description=append_doc_link(CACHE_HELP, "cache"),
        help=CACHE_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    cache_subparsers = cache_parser.add_subparsers(
        dest="cmd",
        help="Use `dvc cache CMD --help` for command-specific help.",
    )

    fix_subparsers(cache_subparsers)

    parent_cache_config_parser = argparse.ArgumentParser(
        add_help=False, parents=[parent_config_parser]
    )
    CACHE_DIR_HELP = "Configure cache directory location."

    cache_dir_parser = cache_subparsers.add_parser(
        "dir",
        parents=[parent_parser, parent_cache_config_parser],
        description=append_doc_link(CACHE_HELP, "cache/dir"),
        help=CACHE_DIR_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    cache_dir_parser.add_argument(
        "-u",
        "--unset",
        default=False,
        action="store_true",
        help="Unset option.",
    )
    cache_dir_parser.add_argument(
        "value",
        help=(
            "Path to cache directory. Relative paths are resolved relative "
            "to the current directory and saved to config relative to the "
            "config file location. If no path is provided, it returns the "
            "current cache directory."
        ),
        nargs="?",
    ).complete = completion.DIR
    cache_dir_parser.set_defaults(func=CmdCacheDir)




dvc/commands/check_ignore.py
import argparse

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.ui import ui


class CmdCheckIgnore(CmdBase):
    def __init__(self, args):
        super().__init__(args)
        self.ignore_filter = self.repo.dvcignore

    def _show_results(self, result):
        if not result.match and not self.args.non_matching:
            return

        if self.args.details:
            patterns = result.patterns
            if not self.args.all:
                patterns = patterns[-1:]

            for pattern in patterns:
                ui.write(pattern, result.file, sep="\t")
        else:
            ui.write(result.file)

    def _check_one_file(self, target):
        result = self.ignore_filter.check_ignore(target)
        self._show_results(result)
        if result.match:
            return 0
        return 1

    def _interactive_mode(self):
        ret = 1
        while True:
            try:
                target = input()
            except (KeyboardInterrupt, EOFError):
                break
            if not target:
                break
            if not self._check_one_file(target):
                ret = 0
        return ret

    def _normal_mode(self):
        ret = 1
        for target in self.args.targets:
            if not self._check_one_file(target):
                ret = 0
        return ret

    def _check_args(self):
        from dvc.exceptions import DvcException

        if not self.args.stdin and not self.args.targets:
            raise DvcException("`targets` or `--stdin` needed")

        if self.args.stdin and self.args.targets:
            raise DvcException("cannot have both `targets` and `--stdin`")

        if self.args.non_matching and not self.args.details:
            raise DvcException("`--non-matching` is only valid with `--details`")

        if self.args.all and not self.args.details:
            raise DvcException("`--all` is only valid with `--details`")

        if self.args.quiet and self.args.details:
            raise DvcException("cannot use both `--details` and `--quiet`")

    def run(self):
        self._check_args()
        if self.args.stdin:
            return self._interactive_mode()
        return self._normal_mode()


def add_parser(subparsers, parent_parser):
    ADD_HELP = "Check whether files or directories are excluded due to `.dvcignore`."

    parser = subparsers.add_parser(
        "check-ignore",
        parents=[parent_parser],
        description=append_doc_link(ADD_HELP, "check-ignore"),
        help=ADD_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "-d",
        "--details",
        action="store_true",
        default=False,
        help="Show the exclude patterns along with each target path.",
    )
    parser.add_argument(
        "-a",
        "--all",
        action="store_true",
        default=False,
        help=(
            "Include the target paths which don't match any pattern "
            "in the `--details` list."
        ),
    )
    parser.add_argument(
        "-n",
        "--non-matching",
        action="store_true",
        default=False,
        help=(
            "Include the target paths which don't match any pattern "
            "in the `--details` list."
        ),
    )
    parser.add_argument(
        "--stdin",
        action="store_true",
        default=False,
        help="Read paths from standard input instead of providing `targets`.",
    )
    parser.add_argument(
        "targets", nargs="*", help="File or directory paths to check"
    ).complete = completion.FILE
    parser.set_defaults(func=CmdCheckIgnore)




dvc/commands/checkout.py
import argparse
import operator

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.exceptions import CheckoutError
from dvc.ui import ui


def log_changes(stats):
    colors = {
        "modified": "yellow",
        "added": "green",
        "deleted": "red",
    }

    for state, color in colors.items():
        entries = stats.get(state)

        if not entries:
            continue

        for entry in entries:
            ui.write(f"[{color}]{state[0].upper()}", entry, styled=True, sep="\t")


class CmdCheckout(CmdBase):
    def run(self):
        from dvc.utils.humanize import get_summary

        stats, exc = None, None
        try:
            stats = self.repo.checkout(
                targets=self.args.targets,
                with_deps=self.args.with_deps,
                force=self.args.force,
                relink=self.args.relink,
                recursive=self.args.recursive,
            )
        except CheckoutError as _exc:
            exc = _exc
            stats = exc.stats

        if self.args.summary:
            default_message = "No changes."
            msg = get_summary(sorted(stats.items(), key=operator.itemgetter(0)))
            ui.write(msg or default_message)
        else:
            log_changes(stats)

        if exc:
            raise exc

        if self.args.relink:
            msg = "Relinked successfully"
            ui.write(msg)
        return 0


def add_parser(subparsers, parent_parser):
    CHECKOUT_HELP = "Checkout data files from cache."

    checkout_parser = subparsers.add_parser(
        "checkout",
        parents=[parent_parser],
        description=append_doc_link(CHECKOUT_HELP, "checkout"),
        help=CHECKOUT_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    checkout_parser.add_argument(
        "--summary",
        action="store_true",
        default=False,
        help="Show summary of the changes.",
    )
    checkout_parser.add_argument(
        "-d",
        "--with-deps",
        action="store_true",
        default=False,
        help="Checkout all dependencies of the specified target.",
    )
    checkout_parser.add_argument(
        "-R",
        "--recursive",
        action="store_true",
        default=False,
        help="Checkout all subdirectories of the specified directory.",
    )
    checkout_parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        default=False,
        help="Do not prompt when removing working directory files.",
    )
    checkout_parser.add_argument(
        "--relink",
        action="store_true",
        default=False,
        help="Recreate links or copies from cache to workspace.",
    )
    checkout_parser.add_argument(
        "targets",
        nargs="*",
        help=(
            "Limit command scope to these tracked files/directories, "
            ".dvc files and stage names."
        ),
    ).complete = completion.DVC_FILE
    checkout_parser.set_defaults(func=CmdCheckout)




dvc/commands/commit.py
import argparse
import logging

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link

logger = logging.getLogger(__name__)


class CmdCommit(CmdBase):
    def run(self):
        from dvc.exceptions import DvcException

        if not self.args.targets:
            self.args.targets = [None]

        for target in self.args.targets:
            try:
                self.repo.commit(
                    target,
                    with_deps=self.args.with_deps,
                    recursive=self.args.recursive,
                    force=self.args.force,
                )
            except DvcException:
                logger.exception("failed to commit%s", (" " + target) if target else "")
                return 1
        return 0


def add_parser(subparsers, parent_parser):
    COMMIT_HELP = (
        "Record changes to files or directories tracked by DVC"
        " by storing the current versions in the cache."
    )

    commit_parser = subparsers.add_parser(
        "commit",
        parents=[parent_parser],
        description=append_doc_link(COMMIT_HELP, "commit"),
        help=COMMIT_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    commit_parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        default=False,
        help="Commit even if hash value for dependencies/outputs changed.",
    )
    commit_parser.add_argument(
        "-d",
        "--with-deps",
        action="store_true",
        default=False,
        help="Commit all dependencies of the specified target.",
    )
    commit_parser.add_argument(
        "-R",
        "--recursive",
        action="store_true",
        default=False,
        help="Commit cache for subdirectories of the specified directory.",
    )
    commit_parser.add_argument(
        "targets",
        nargs="*",
        help=(
            "Limit command scope to these tracked files/directories, "
            ".dvc files and stage names."
        ),
    ).complete = completion.DVCFILES_AND_STAGE
    commit_parser.set_defaults(func=CmdCommit)




dvc/commands/completion.py
import argparse
import logging

from dvc.cli.command import CmdBaseNoRepo
from dvc.cli.completion import PREAMBLE
from dvc.cli.utils import append_doc_link
from dvc.ui import ui

logger = logging.getLogger(__name__)


class CmdCompletion(CmdBaseNoRepo):
    def run(self):
        import shtab

        shell = self.args.shell
        parser = self.args.parser
        script = shtab.complete(parser, shell=shell, preamble=PREAMBLE)  # nosec B604
        ui.write(script, force=True)
        return 0


def add_parser(subparsers, parent_parser):
    COMPLETION_HELP = "Generate shell tab completion."
    COMPLETION_DESCRIPTION = "Prints out shell tab completion scripts."
    completion_parser = subparsers.add_parser(
        "completion",
        parents=[parent_parser],
        description=append_doc_link(COMPLETION_DESCRIPTION, "completion"),
        help=COMPLETION_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    completion_parser.add_argument(
        "-s",
        "--shell",
        help="Shell syntax for completions.",
        default="bash",
        choices=["bash", "zsh"],
    )
    completion_parser.set_defaults(func=CmdCompletion)




dvc/commands/config.py
import argparse
import logging
import os

from dvc.cli.command import CmdBaseNoRepo
from dvc.cli.utils import append_doc_link
from dvc.ui import ui

logger = logging.getLogger(__name__)

NAME_REGEX = r"^(?P<remote>remote\.)?(?P<section>[^\.]*)\.(?P<option>[^\.]*)$"


def _name_type(value):
    import re

    match = re.match(NAME_REGEX, value)
    if not match:
        raise argparse.ArgumentTypeError(
            "name argument should look like remote.name.option or section.option"
        )
    return (
        bool(match.group("remote")),
        match.group("section").lower(),
        match.group("option").lower(),
    )


class CmdConfig(CmdBaseNoRepo):
    def __init__(self, args):
        from dvc.config import Config

        super().__init__(args)

        self.config = Config.from_cwd(validate=False)

    def run(self):
        if self.args.show_origin and (self.args.value or self.args.unset):
            logger.error(
                "--show-origin can't be used together with any of these "
                "options: -u/--unset, value"
            )
            return 1

        if self.args.list:
            return self._list()

        if self.args.name is None:
            logger.error("name argument is required")
            return 1

        remote, section, opt = self.args.name

        if self.args.value is None and not self.args.unset:
            return self._get(remote, section, opt)

        return self._set(remote, section, opt)

    def _list(self):
        if any((self.args.name, self.args.value, self.args.unset)):
            logger.error(
                "-l/--list can't be used together with any of these "
                "options: -u/--unset, name, value"
            )
            return 1

        levels = self._get_appropriate_levels(self.args.level)

        for level in levels:
            conf = self.config.read(level)
            prefix = self._config_file_prefix(self.args.show_origin, self.config, level)
            configs = list(self._format_config(conf, prefix))
            if configs:
                ui.write("\n".join(configs))

        return 0

    def _get(self, remote, section, opt):
        from dvc.config import ConfigError

        levels = self._get_appropriate_levels(self.args.level)[::-1]

        for level in levels:
            conf = self.config.read(level)
            if remote:
                conf = conf["remote"]

            try:
                self._check(conf, remote, section, opt)
            except ConfigError:
                if self.args.level:
                    raise
            else:
                prefix = self._config_file_prefix(
                    self.args.show_origin, self.config, level
                )
                ui.write(prefix, conf[section][opt], sep="")
                break

        return 0

    def _set(self, remote, section, opt):
        with self.config.edit(self.args.level) as conf:
            if remote:
                conf = conf["remote"]
            if self.args.unset:
                self._check(conf, remote, section, opt)
                del conf[section][opt]
            else:
                self._check(conf, remote, section)
                conf[section][opt] = self.args.value

        if self.args.name == "cache.type":
            logger.warning(
                "You have changed the 'cache.type' option. This doesn't update"
                " any existing workspace file links, but it can be done with:"
                "\n             dvc checkout --relink"
            )

        return 0

    def _check(self, conf, remote, section, opt=None):
        from dvc.config import ConfigError

        name = "remote" if remote else "section"
        if section not in conf:
            raise ConfigError(f"{name} '{section}' doesn't exist")

        if opt and opt not in conf[section]:
            raise ConfigError(f"option '{opt}' doesn't exist in {name} '{section}'")

    def _get_appropriate_levels(self, levels):
        if levels:
            self._validate_level_for_non_repo_operation(levels)
            return [levels]
        if self.config.dvc_dir is None:
            return self.config.SYSTEM_LEVELS
        return self.config.LEVELS

    def _validate_level_for_non_repo_operation(self, level):
        from dvc.config import ConfigError

        if self.config.dvc_dir is None and level in self.config.REPO_LEVELS:
            raise ConfigError("Not inside a DVC repo")

    @staticmethod
    def _format_config(config, prefix=""):
        from dvc.utils.flatten import flatten

        for key, value in flatten(config).items():
            yield f"{prefix}{key}={value}"

    @staticmethod
    def _config_file_prefix(show_origin, config, level):
        from dvc.repo import Repo

        if not show_origin:
            return ""

        level = level or "repo"
        fname = config.files[level]

        if level in ["local", "repo"]:
            fname = os.path.relpath(fname, start=Repo.find_root())

        return fname + "\t"


parent_config_parser = argparse.ArgumentParser(add_help=False)
level_group = parent_config_parser.add_mutually_exclusive_group()
level_group.add_argument(
    "--global",
    dest="level",
    action="store_const",
    const="global",
    help="Use global config.",
)
level_group.add_argument(
    "--system",
    dest="level",
    action="store_const",
    const="system",
    help="Use system config.",
)
level_group.add_argument(
    "--project",
    dest="level",
    action="store_const",
    const="repo",
    help="Use project config (.dvc/config).",
)
level_group.add_argument(
    "--local",
    dest="level",
    action="store_const",
    const="local",
    help="Use local config (.dvc/config.local).",
)
parent_config_parser.set_defaults(level=None)


def add_parser(subparsers, parent_parser):
    CONFIG_HELP = "Get or set config options."

    config_parser = subparsers.add_parser(
        "config",
        parents=[parent_config_parser, parent_parser],
        description=append_doc_link(CONFIG_HELP, "config"),
        help=CONFIG_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    config_parser.add_argument(
        "-u",
        "--unset",
        default=False,
        action="store_true",
        help="Unset option.",
    )
    config_parser.add_argument(
        "name",
        nargs="?",
        type=_name_type,
        help="Option name (section.option or remote.name.option).",
    )
    config_parser.add_argument("value", nargs="?", help="Option value.")
    config_parser.add_argument(
        "-l",
        "--list",
        default=False,
        action="store_true",
        help="List all defined config values.",
    )
    config_parser.add_argument(
        "--show-origin",
        default=False,
        action="store_true",
        help="Show the source file containing each config value.",
    )
    config_parser.set_defaults(func=CmdConfig)




dvc/commands/daemon.py
from dvc.cli import completion
from dvc.cli.command import CmdBaseNoRepo
from dvc.cli.utils import fix_subparsers


class CmdDaemonBase(CmdBaseNoRepo):
    pass


class CmdDaemonUpdater(CmdDaemonBase):
    def run(self):
        import os

        from dvc.config import Config
        from dvc.repo import Repo
        from dvc.updater import Updater

        root_dir = Repo.find_root()
        dvc_dir = os.path.join(root_dir, Repo.DVC_DIR)
        tmp_dir = os.path.join(dvc_dir, "tmp")
        config = Config(dvc_dir, validate=False)
        hardlink_lock = config.get("core", {}).get("hardlink_lock", False)
        updater = Updater(tmp_dir, hardlink_lock=hardlink_lock)
        updater.fetch(detach=False)

        return 0


class CmdDaemonAnalytics(CmdDaemonBase):
    def run(self):
        from dvc import analytics

        analytics.send(self.args.target)

        return 0


def add_parser(subparsers, parent_parser):
    DAEMON_HELP = "Service daemon."
    daemon_parser = subparsers.add_parser(
        "daemon",
        parents=[parent_parser],
        description=DAEMON_HELP,
        add_help=False,
    )

    daemon_subparsers = daemon_parser.add_subparsers(
        dest="cmd",
        help="Use `dvc daemon CMD --help` for command-specific help.",
    )

    fix_subparsers(daemon_subparsers)

    DAEMON_UPDATER_HELP = "Fetch latest available version."
    daemon_updater_parser = daemon_subparsers.add_parser(
        "updater",
        parents=[parent_parser],
        description=DAEMON_UPDATER_HELP,
        help=DAEMON_UPDATER_HELP,
    )
    daemon_updater_parser.set_defaults(func=CmdDaemonUpdater)

    DAEMON_ANALYTICS_HELP = "Send dvc usage analytics."
    daemon_analytics_parser = daemon_subparsers.add_parser(
        "analytics",
        parents=[parent_parser],
        description=DAEMON_ANALYTICS_HELP,
        help=DAEMON_ANALYTICS_HELP,
    )
    daemon_analytics_parser.add_argument(
        "target", help="Analytics file."
    ).complete = completion.FILE
    daemon_analytics_parser.set_defaults(func=CmdDaemonAnalytics)




dvc/commands/dag.py
import argparse
from typing import TYPE_CHECKING

from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.ui import ui

if TYPE_CHECKING:
    from networkx import DiGraph


def _show_ascii(graph: "DiGraph"):
    from dvc.dagascii import draw
    from dvc.repo.graph import get_pipelines

    pipelines = get_pipelines(graph)

    ret = []
    for pipeline in pipelines:
        ret.append(draw(pipeline.nodes, pipeline.edges))

    return "\n".join(ret)


def _quote_label(node):
    label = str(node)
    # Node names should not contain ":" unless they are quoted with "".
    # See: https://github.com/pydot/pydot/issues/258.
    if label[0] != '"' and label[-1] != '"':
        return f'"{label}"'
    return label


def _show_dot(graph: "DiGraph"):
    import io

    import networkx as nx
    from networkx.drawing.nx_pydot import write_dot

    dot_file = io.StringIO()

    nx.relabel_nodes(graph, _quote_label, copy=False)
    write_dot(graph.reverse(), dot_file)
    return dot_file.getvalue()


def _show_mermaid(graph, markdown: bool = False):
    from dvc.repo.graph import get_pipelines

    pipelines = get_pipelines(graph)

    graph = "flowchart TD"

    total_nodes = 0
    for pipeline in pipelines:
        node_ids = {}
        nodes = sorted(str(x) for x in pipeline.nodes)
        for node in nodes:
            total_nodes += 1
            node_id = f"node{total_nodes}"
            graph += f'\n\t{node_id}["{node}"]'
            node_ids[node] = node_id
        edges = sorted((str(a), str(b)) for b, a in pipeline.edges)
        for a, b in edges:
            graph += f"\n\t{node_ids[str(a)]}-->{node_ids[str(b)]}"

    if markdown:
        return f"```mermaid\n{graph}\n```"

    return graph


def _collect_targets(repo, target, outs):
    if not target:
        return []

    pairs = repo.stage.collect_granular(target)
    if not outs:
        return [stage.addressing for stage, _ in pairs]

    targets = []

    outs_trie = repo.index.outs_trie
    for stage, path in pairs:
        if not path:
            targets.extend([str(out) for out in stage.outs])
            continue

        for out in outs_trie.itervalues(prefix=repo.fs.path.parts(path)):  # noqa: B301
            targets.extend(str(out))

    return targets


def _transform(index, outs):
    import networkx as nx

    from dvc.stage import Stage

    def _relabel(node) -> str:
        return node.addressing if isinstance(node, Stage) else str(node)

    graph = index.outs_graph if outs else index.graph
    return nx.relabel_nodes(graph, _relabel, copy=True)


def _filter(graph, targets, full):
    import networkx as nx

    if not targets:
        return graph

    new_graph = graph.copy()
    if not full:
        descendants = set()
        for target in targets:
            descendants.update(nx.descendants(graph, target))
            descendants.add(target)
        new_graph.remove_nodes_from(set(graph.nodes()) - descendants)

    undirected = new_graph.to_undirected()
    connected = set()
    for target in targets:
        connected.update(nx.node_connected_component(undirected, target))

    new_graph.remove_nodes_from(set(new_graph.nodes()) - connected)
    return new_graph


def _build(repo, target=None, full=False, outs=False):
    targets = _collect_targets(repo, target, outs)
    graph = _transform(repo.index, outs)
    return _filter(graph, targets, full)


class CmdDAG(CmdBase):
    def run(self):
        graph = _build(
            self.repo,
            target=self.args.target,
            full=self.args.full,
            outs=self.args.outs,
        )

        if self.args.dot:
            ui.write(_show_dot(graph))
        elif self.args.mermaid or self.args.markdown:
            ui.write(_show_mermaid(graph, self.args.markdown))
        else:
            with ui.pager():
                ui.write(_show_ascii(graph))

        return 0


def add_parser(subparsers, parent_parser):
    DAG_HELP = "Visualize DVC project DAG."
    dag_parser = subparsers.add_parser(
        "dag",
        parents=[parent_parser],
        description=append_doc_link(DAG_HELP, "dag"),
        help=DAG_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    dag_parser.add_argument(
        "--dot",
        action="store_true",
        default=False,
        help="Print DAG with .dot format.",
    )
    dag_parser.add_argument(
        "--mermaid",
        action="store_true",
        default=False,
        help="Print DAG with mermaid format.",
    )
    dag_parser.add_argument(
        "--md",
        action="store_true",
        default=False,
        dest="markdown",
        help="Print DAG with mermaid format wrapped in Markdown block.",
    )
    dag_parser.add_argument(
        "--full",
        action="store_true",
        default=False,
        help=(
            "Show full DAG that the target belongs too, instead of "
            "showing DAG consisting only of ancestors."
        ),
    )
    dag_parser.add_argument(
        "-o",
        "--outs",
        action="store_true",
        default=False,
        help="Print output files instead of stages.",
    )
    dag_parser.add_argument(
        "target",
        nargs="?",
        help=(
            "Stage name or output to show pipeline for. "
            "Finds all stages in the workspace by default."
        ),
    )
    dag_parser.set_defaults(func=CmdDAG)




dvc/commands/data.py
import argparse
import logging
from typing import TYPE_CHECKING

from funcy import chunks, compact, log_durations

from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link, fix_subparsers
from dvc.ui import ui
from dvc.utils import colorize

if TYPE_CHECKING:
    from dvc.repo.data import Status as DataStatus


logger = logging.getLogger(__name__)


class CmdDataStatus(CmdBase):
    COLORS = {
        "not_in_remote": "red",
        "not_in_cache": "red",
        "committed": "green",
        "uncommitted": "yellow",
        "untracked": "cyan",
    }
    LABELS = {
        "not_in_remote": "Not in remote",
        "not_in_cache": "Not in cache",
        "committed": "DVC committed changes",
        "uncommitted": "DVC uncommitted changes",
        "untracked": "Untracked files",
        "unchanged": "DVC unchanged files",
    }
    HINTS = {
        "not_in_remote": ('use "dvc push <file>..." to upload files',),
        "not_in_cache": ('use "dvc fetch <file>..." to download files',),
        "committed": ("git commit the corresponding dvc files to update the repo",),
        "uncommitted": (
            'use "dvc commit <file>..." to track changes',
            'use "dvc checkout <file>..." to discard changes',
        ),
        "untracked": (
            (
                'use "git add <file> ..." or '
                '"dvc add <file>..." to commit to git or to dvc'
            ),
        ),
        "git_dirty": (
            'there are {}changes not tracked by dvc, use "git status" to see',
        ),
    }

    @staticmethod
    def _process_status(status: "DataStatus"):
        """Flatten stage status, and filter empty stage status contents."""
        for stage, stage_status in status.items():
            items = stage_status
            if isinstance(stage_status, dict):
                items = {
                    file: state
                    for state, files in stage_status.items()
                    for file in files
                }
            if not items:
                continue
            yield stage, items

    @classmethod
    def _show_status(cls, status: "DataStatus") -> int:  # noqa: C901
        git_info = status.pop("git")  # type: ignore[misc]
        result = dict(cls._process_status(status))
        if not result:
            no_changes = "No changes"
            if git_info.get("is_empty", False):
                no_changes += " in an empty git repo"
            ui.write(f"{no_changes}.")

        for idx, (stage, stage_status) in enumerate(result.items()):
            if idx:
                ui.write()

            label = cls.LABELS.get(stage, stage.capitalize() + " files")
            header = f"{label}:"
            color = cls.COLORS.get(stage, None)

            ui.write(header)
            if hints := cls.HINTS.get(stage):
                for hint in hints:
                    ui.write(f"  ({hint})")

            if isinstance(stage_status, dict):
                items = [
                    ": ".join([state, file]) for file, state in stage_status.items()
                ]
            else:
                items = stage_status

            tabs = "\t".expandtabs(8)
            for chunk in chunks(1000, items):
                out = "\n".join(tabs + item for item in chunk)
                ui.write(colorize(out, color))

        if (hints := cls.HINTS.get("git_dirty")) and git_info.get("is_dirty"):
            for hint in hints:
                message = hint.format("other " if result else "")
                ui.write(f"[blue]({message})[/]", styled=True)
        return 0

    def run(self) -> int:
        with log_durations(
            logger.trace, "in data_status"  # type: ignore[attr-defined]
        ):
            status = self.repo.data_status(
                granular=self.args.granular,
                untracked_files=self.args.untracked_files,
                not_in_remote=self.args.not_in_remote,
                remote_refresh=self.args.remote_refresh,
            )

        if not self.args.unchanged:
            status.pop("unchanged")  # type: ignore[misc]
        if self.args.untracked_files == "no":
            status.pop("untracked")
        if self.args.json:
            status.pop("git")  # type: ignore[misc]
            ui.write_json(compact(status))
            return 0
        return self._show_status(status)


def add_parser(subparsers, parent_parser):
    data_parser = subparsers.add_parser(
        "data",
        parents=[parent_parser],
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    data_subparsers = data_parser.add_subparsers(
        dest="cmd",
        help="Use `dvc data CMD --help` to display command-specific help.",
    )
    fix_subparsers(data_subparsers)

    DATA_STATUS_HELP = (
        "Show changes between the last git commit, the dvcfiles and the workspace."
    )
    data_status_parser = data_subparsers.add_parser(
        "status",
        parents=[parent_parser],
        description=append_doc_link(DATA_STATUS_HELP, "data/status"),
        formatter_class=argparse.RawDescriptionHelpFormatter,
        help=DATA_STATUS_HELP,
    )
    data_status_parser.add_argument(
        "--json",
        action="store_true",
        default=False,
        help="Show output in JSON format.",
    )
    data_status_parser.add_argument(
        "--granular",
        action="store_true",
        default=False,
        help="Show granular file-level info for DVC-tracked directories.",
    )
    data_status_parser.add_argument(
        "--unchanged",
        action="store_true",
        default=False,
        help="Show unmodified DVC-tracked files.",
    )
    data_status_parser.add_argument(
        "--untracked-files",
        choices=["no", "all"],
        default="no",
        const="all",
        nargs="?",
        help="Show untracked files.",
    )
    data_status_parser.add_argument(
        "--not-in-remote",
        action="store_true",
        default=False,
        help="Show files not in remote.",
    )
    data_status_parser.add_argument(
        "--no-remote-refresh",
        dest="remote_refresh",
        action="store_false",
        help="Use cached remote index (don't check remote).",
    )
    data_status_parser.set_defaults(func=CmdDataStatus)




dvc/commands/data_sync.py
import argparse
import logging

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link

logger = logging.getLogger(__name__)


class CmdDataBase(CmdBase):
    def log_summary(self, stats):
        from dvc.ui import ui
        from dvc.utils.humanize import get_summary

        default_msg = "Everything is up to date."
        ui.write(get_summary(stats.items()) or default_msg)


class CmdDataPull(CmdDataBase):
    def log_summary(self, stats):
        from dvc.commands.checkout import log_changes

        log_changes(stats)
        super().log_summary(stats)

    def run(self):
        from dvc.exceptions import CheckoutError, DvcException

        try:
            stats = self.repo.pull(
                targets=self.args.targets,
                jobs=self.args.jobs,
                remote=self.args.remote,
                all_branches=self.args.all_branches,
                all_tags=self.args.all_tags,
                all_commits=self.args.all_commits,
                with_deps=self.args.with_deps,
                force=self.args.force,
                recursive=self.args.recursive,
                run_cache=self.args.run_cache,
                glob=self.args.glob,
                allow_missing=self.args.allow_missing,
            )
            self.log_summary(stats)
        except (CheckoutError, DvcException) as exc:
            if stats := getattr(exc, "stats", {}):
                self.log_summary(stats)
            logger.exception("failed to pull data from the cloud")
            return 1

        return 0


class CmdDataPush(CmdDataBase):
    def run(self):
        from dvc.exceptions import DvcException

        try:
            processed_files_count = self.repo.push(
                targets=self.args.targets,
                jobs=self.args.jobs,
                remote=self.args.remote,
                all_branches=self.args.all_branches,
                all_tags=self.args.all_tags,
                all_commits=self.args.all_commits,
                with_deps=self.args.with_deps,
                recursive=self.args.recursive,
                run_cache=self.args.run_cache,
                glob=self.args.glob,
            )
            self.log_summary({"pushed": processed_files_count})
        except DvcException:
            logger.exception("failed to push data to the cloud")
            return 1
        return 0


class CmdDataFetch(CmdDataBase):
    def run(self):
        from dvc.exceptions import DvcException

        try:
            processed_files_count = self.repo.fetch(
                targets=self.args.targets,
                jobs=self.args.jobs,
                remote=self.args.remote,
                all_branches=self.args.all_branches,
                all_tags=self.args.all_tags,
                all_commits=self.args.all_commits,
                with_deps=self.args.with_deps,
                recursive=self.args.recursive,
                run_cache=self.args.run_cache,
            )
            self.log_summary({"fetched": processed_files_count})
        except DvcException:
            logger.exception("failed to fetch data from the cloud")
            return 1
        return 0


def shared_parent_parser():
    from dvc.cli.parser import get_parent_parser

    # Parent parser used in pull/push/status
    parent_parser = argparse.ArgumentParser(
        add_help=False, parents=[get_parent_parser()]
    )
    parent_parser.add_argument(
        "-j",
        "--jobs",
        type=int,
        help=(
            "Number of jobs to run simultaneously. "
            "The default value is 4 * cpu_count(). "
        ),
        metavar="<number>",
    )
    parent_parser.add_argument(
        "targets",
        nargs="*",
        help=(
            "Limit command scope to these tracked files/directories, "
            ".dvc files and stage names."
        ),
    ).complete = completion.DVC_FILE  # type: ignore[attr-defined]

    return parent_parser


def add_parser(subparsers, _parent_parser):
    from dvc.commands.status import CmdDataStatus

    # Pull
    PULL_HELP = "Download tracked files or directories from remote storage."

    pull_parser = subparsers.add_parser(
        "pull",
        parents=[shared_parent_parser()],
        description=append_doc_link(PULL_HELP, "pull"),
        help=PULL_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    pull_parser.add_argument(
        "-r", "--remote", help="Remote storage to pull from", metavar="<name>"
    )
    pull_parser.add_argument(
        "-a",
        "--all-branches",
        action="store_true",
        default=False,
        help="Fetch cache for all branches.",
    )
    pull_parser.add_argument(
        "-T",
        "--all-tags",
        action="store_true",
        default=False,
        help="Fetch cache for all tags.",
    )
    pull_parser.add_argument(
        "-A",
        "--all-commits",
        action="store_true",
        default=False,
        help="Fetch cache for all commits.",
    )
    pull_parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        default=False,
        help="Do not prompt when removing working directory files.",
    )
    pull_parser.add_argument(
        "-d",
        "--with-deps",
        action="store_true",
        default=False,
        help="Fetch cache for all dependencies of the specified target.",
    )
    pull_parser.add_argument(
        "-R",
        "--recursive",
        action="store_true",
        default=False,
        help="Pull cache for subdirectories of the specified directory.",
    )
    pull_parser.add_argument(
        "--run-cache",
        action="store_true",
        default=False,
        help="Fetch run history for all stages.",
    )
    pull_parser.add_argument(
        "--glob",
        action="store_true",
        default=False,
        help=argparse.SUPPRESS,
    )
    pull_parser.add_argument(
        "--allow-missing",
        action="store_true",
        default=False,
        help="Ignore errors if some of the files or directories are missing.",
    )
    pull_parser.set_defaults(func=CmdDataPull)

    # Push
    PUSH_HELP = "Upload tracked files or directories to remote storage."

    push_parser = subparsers.add_parser(
        "push",
        parents=[shared_parent_parser()],
        description=append_doc_link(PUSH_HELP, "push"),
        help=PUSH_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    push_parser.add_argument(
        "-r", "--remote", help="Remote storage to push to", metavar="<name>"
    )
    push_parser.add_argument(
        "-a",
        "--all-branches",
        action="store_true",
        default=False,
        help="Push cache for all branches.",
    )
    push_parser.add_argument(
        "-T",
        "--all-tags",
        action="store_true",
        default=False,
        help="Push cache for all tags.",
    )
    push_parser.add_argument(
        "-A",
        "--all-commits",
        action="store_true",
        default=False,
        help="Push cache for all commits.",
    )
    push_parser.add_argument(
        "-d",
        "--with-deps",
        action="store_true",
        default=False,
        help="Push cache for all dependencies of the specified target.",
    )
    push_parser.add_argument(
        "-R",
        "--recursive",
        action="store_true",
        default=False,
        help="Push cache for subdirectories of specified directory.",
    )
    push_parser.add_argument(
        "--run-cache",
        action="store_true",
        default=False,
        help="Push run history for all stages.",
    )
    push_parser.add_argument(
        "--glob",
        action="store_true",
        default=False,
        help="Allows targets containing shell-style wildcards.",
    )
    push_parser.set_defaults(func=CmdDataPush)

    # Fetch
    FETCH_HELP = "Download files or directories from remote storage to the cache."

    fetch_parser = subparsers.add_parser(
        "fetch",
        parents=[shared_parent_parser()],
        description=append_doc_link(FETCH_HELP, "fetch"),
        help=FETCH_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    fetch_parser.add_argument(
        "-r", "--remote", help="Remote storage to fetch from", metavar="<name>"
    )
    fetch_parser.add_argument(
        "-a",
        "--all-branches",
        action="store_true",
        default=False,
        help="Fetch cache for all branches.",
    )
    fetch_parser.add_argument(
        "-T",
        "--all-tags",
        action="store_true",
        default=False,
        help="Fetch cache for all tags.",
    )
    fetch_parser.add_argument(
        "-A",
        "--all-commits",
        action="store_true",
        default=False,
        help="Fetch cache for all commits.",
    )
    fetch_parser.add_argument(
        "-d",
        "--with-deps",
        action="store_true",
        default=False,
        help="Fetch cache for all dependencies of the specified target.",
    )
    fetch_parser.add_argument(
        "-R",
        "--recursive",
        action="store_true",
        default=False,
        help="Fetch cache for subdirectories of specified directory.",
    )
    fetch_parser.add_argument(
        "--run-cache",
        action="store_true",
        default=False,
        help="Fetch run history for all stages.",
    )
    fetch_parser.set_defaults(func=CmdDataFetch)

    # Status
    STATUS_HELP = "Show changed stages, compare local cache and a remote storage."

    status_parser = subparsers.add_parser(
        "status",
        parents=[shared_parent_parser()],
        description=append_doc_link(STATUS_HELP, "status"),
        help=STATUS_HELP,
        conflict_handler="resolve",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    status_parser.add_argument(
        "-q",
        "--quiet",
        action="store_true",
        default=False,
        help=(
            "Suppresses all output."
            " Exit with 0 if pipelines are up to date, otherwise 1."
        ),
    )
    status_parser.add_argument(
        "-c",
        "--cloud",
        action="store_true",
        default=False,
        help="Show status of a local cache compared to a remote repository.",
    )
    status_parser.add_argument(
        "-r",
        "--remote",
        help="Remote storage to compare local cache to",
        metavar="<name>",
    )
    status_parser.add_argument(
        "-a",
        "--all-branches",
        action="store_true",
        default=False,
        help=(
            "Show status of a local cache compared to a remote repository "
            "for all branches."
        ),
    )
    status_parser.add_argument(
        "-T",
        "--all-tags",
        action="store_true",
        default=False,
        help=(
            "Show status of a local cache compared to a remote repository for all tags."
        ),
    )
    status_parser.add_argument(
        "-A",
        "--all-commits",
        action="store_true",
        default=False,
        help=(
            "Show status of a local cache compared to a remote repository "
            "for all commits."
        ),
    )
    status_parser.add_argument(
        "-d",
        "--with-deps",
        action="store_true",
        default=False,
        help="Show status for all dependencies of the specified target.",
    )
    status_parser.add_argument(
        "-R",
        "--recursive",
        action="store_true",
        default=False,
        help="Show status of all stages in the specified directory.",
    )
    status_parser.add_argument(
        "--json",
        action="store_true",
        default=False,
        help="Show status in JSON format.",
    )

    status_parser.set_defaults(func=CmdDataStatus)




dvc/commands/destroy.py
import argparse
import logging

from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link

logger = logging.getLogger(__name__)


class CmdDestroy(CmdBase):
    def run(self):
        from dvc.exceptions import DvcException
        from dvc.ui import ui

        try:
            statement = (
                "This will destroy all information about your pipelines,"
                " all data files, as well as cache in .dvc/cache."
                "\n"
                "Are you sure you want to continue?"
            )

            if not self.args.force and not ui.confirm(statement):
                raise DvcException(
                    "cannot destroy without a confirmation from the user."
                    " Use `-f` to force."
                )

            self.repo.destroy()
        except DvcException:
            logger.exception("failed to destroy DVC")
            return 1
        return 0


def add_parser(subparsers, parent_parser):
    DESTROY_HELP = "Remove DVC files, local DVC config and data cache."

    destroy_parser = subparsers.add_parser(
        "destroy",
        parents=[parent_parser],
        description=append_doc_link(DESTROY_HELP, "destroy"),
        help=DESTROY_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    destroy_parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        default=False,
        help="Force destruction.",
    )
    destroy_parser.set_defaults(func=CmdDestroy)




dvc/commands/diff.py
import argparse
import logging
import os

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.ui import ui

logger = logging.getLogger(__name__)


def _digest(checksum):
    if isinstance(checksum, str):
        return checksum[0:8]
    return "{}..{}".format(checksum["old"][0:8], checksum["new"][0:8])


def _show_markdown(diff, show_hash=False, hide_missing=False):
    headers = ["Status", "Hash", "Path"] if show_hash else ["Status", "Path"]
    rows = []
    statuses = ["added", "deleted", "renamed", "modified"]
    if not hide_missing:
        statuses.append("not in cache")

    for status in statuses:
        entries = diff.get(status, [])
        if not entries:
            continue
        for entry in entries:
            path = entry["path"]
            if isinstance(path, dict):
                path = f"{path['old']} -> {path['new']}"
            if show_hash:
                check_sum = _digest(entry.get("hash", ""))
                rows.append([status, check_sum, path])
            else:
                rows.append([status, path])

    ui.table(rows, headers=headers, markdown=True)


class CmdDiff(CmdBase):
    @staticmethod
    def _show_diff(diff, hide_missing=False):
        """
        Given a diff structure, generate a string of paths separated
        by new lines and grouped together by their state.

        A group's header is colored to enhance readability, for example:

            Added:
                another_file.txt
                backup.tar
                dir/
                dir/1

        An example of a diff formatted when entries contain hash:

            Added:
                d3b07384 foo

            Modified:
                c157a790..f98bf6f1 bar

        If a group has no entries, it won't be included in the result.

        At the bottom, include a summary with the number of files per state.
        """

        colors = {
            "added": "green",
            "modified": "yellow",
            "deleted": "red",
            "renamed": "green",
            "not in cache": "yellow",
        }

        summary = {}

        states = ["added", "deleted", "renamed", "modified"]
        if not hide_missing:
            states.append("not in cache")
        for state in states:
            summary[state] = 0
            entries = diff[state]

            if not entries:
                continue

            header = state.capitalize()
            ui.write(f"[{colors[state]}]{header}[/]:", styled=True)

            for entry in entries:
                path = entry["path"]
                if isinstance(path, dict):
                    path = f"{path['old']} -> {path['new']}"
                checksum = entry.get("hash")
                summary[state] += 1 if not path.endswith(os.sep) else 0
                ui.write(
                    "{space}{checksum}{separator}{path}".format(
                        space="    ",
                        checksum=_digest(checksum) if checksum else "",
                        separator="  " if checksum else "",
                        path=path,
                    )
                )

            ui.write()

        if not sum(summary.values()):
            return

        states_summary = ", ".join(
            f"{summary[state]} {state}" for state in states if summary[state] > 0
        )
        ui.write("files summary:", states_summary)

    def run(self):
        from dvc.exceptions import DvcException

        try:
            diff = self.repo.diff(self.args.a_rev, self.args.b_rev, self.args.targets)
            show_hash = self.args.show_hash
            hide_missing = self.args.b_rev or self.args.hide_missing
            if hide_missing:
                diff.pop("not in cache", None)

            for key, entries in diff.items():
                entries = sorted(
                    entries,
                    key=lambda entry: (
                        entry["path"]["old"]
                        if isinstance(entry["path"], dict)
                        else entry["path"]
                    ),
                )
                if not show_hash:
                    for entry in entries:
                        del entry["hash"]
                diff[key] = entries

            if self.args.json:
                ui.write_json(diff)
            elif self.args.markdown:
                _show_markdown(diff, show_hash, hide_missing)
            elif diff:
                self._show_diff(diff, hide_missing)

        except DvcException:
            logger.exception("failed to get diff")
            return 1
        return 0


def add_parser(subparsers, parent_parser):
    DIFF_DESCRIPTION = (
        "Show added, modified, or deleted data between commits in the DVC"
        " repository, or between a commit and the workspace."
    )
    diff_parser = subparsers.add_parser(
        "diff",
        parents=[parent_parser],
        description=append_doc_link(DIFF_DESCRIPTION, "diff"),
        help=DIFF_DESCRIPTION,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    diff_parser.add_argument(
        "--targets",
        nargs="*",
        help="Specific DVC-tracked files to compare. Accepts one or more file paths.",
        metavar="<paths>",
    ).complete = completion.FILE
    diff_parser.add_argument(
        "a_rev",
        help="Old Git commit to compare (defaults to HEAD)",
        nargs="?",
        default="HEAD",
    )
    diff_parser.add_argument(
        "b_rev",
        help="New Git commit to compare (defaults to the current workspace)",
        nargs="?",
    )
    diff_parser.add_argument(
        "--json",
        help="Format the output into a JSON",
        action="store_true",
        default=False,
    )
    diff_parser.add_argument(
        "--show-hash",
        help="Display hash value for each entry",
        action="store_true",
        default=False,
    )
    diff_parser.add_argument(
        "--md",
        help="Show tabulated output in the Markdown format (GFM).",
        action="store_true",
        dest="markdown",
        default=False,
    )
    diff_parser.add_argument(
        "--hide-missing",
        help="Hide missing cache file status.",
        action="store_true",
    )
    diff_parser.set_defaults(func=CmdDiff)




dvc/commands/freeze.py
import argparse
import logging

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.exceptions import DvcException

logger = logging.getLogger(__name__)


class CmdFreezeBase(CmdBase):
    def _run(self, func, name):
        ret = 0
        for target in self.args.targets:
            try:
                func(target)
            except DvcException:
                logger.exception("failed to %s '%s'", name, target)
                ret = 1
        return ret


class CmdFreeze(CmdFreezeBase):
    def run(self):
        return self._run(self.repo.freeze, "freeze")


class CmdUnfreeze(CmdFreezeBase):
    def run(self):
        return self._run(self.repo.unfreeze, "unfreeze")


def add_parser(subparsers, parent_parser):
    FREEZE_HELP = "Freeze stages or .dvc files."
    freeze_parser = subparsers.add_parser(
        "freeze",
        parents=[parent_parser],
        description=append_doc_link(FREEZE_HELP, "freeze"),
        help=FREEZE_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    freeze_parser.add_argument(
        "targets", nargs="+", help="Stages or .dvc files to freeze"
    ).complete = completion.DVC_FILE
    freeze_parser.set_defaults(func=CmdFreeze)

    UNFREEZE_HELP = "Unfreeze stages or .dvc files."
    unfreeze_parser = subparsers.add_parser(
        "unfreeze",
        parents=[parent_parser],
        description=append_doc_link(UNFREEZE_HELP, "unfreeze"),
        help=UNFREEZE_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    unfreeze_parser.add_argument(
        "targets", nargs="+", help="Stages or .dvc files to unfreeze"
    ).complete = completion.DVC_FILE
    unfreeze_parser.set_defaults(func=CmdUnfreeze)




dvc/commands/gc.py
import argparse
import logging
import os

from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.ui import ui

logger = logging.getLogger(__name__)


class CmdGC(CmdBase):
    def run(self):  # noqa: C901, PLR0912
        from dvc.repo.gc import _validate_args

        _validate_args(
            all_branches=self.args.all_branches,
            all_tags=self.args.all_tags,
            all_commits=self.args.all_commits,
            all_experiments=self.args.all_experiments,
            commit_date=self.args.commit_date,
            workspace=self.args.workspace,
            rev=self.args.rev,
            num=self.args.num,
            cloud=self.args.cloud,
            not_in_remote=self.args.not_in_remote,
        )

        if self.args.rev:
            self.args.num = self.args.num or 1

        msg = "This will remove all cache except items used in "

        msg += "the workspace"
        if self.args.all_commits:
            msg += " and all git commits"
        else:
            if self.args.all_branches and self.args.all_tags:
                msg += " and all git branches and tags"
            elif self.args.all_branches:
                msg += " and all git branches"
            elif self.args.all_tags:
                msg += " and all git tags"
            if self.args.commit_date:
                msg += f" and all git commits before date {self.args.commit_date}"
            if self.args.rev:
                msg += f" and last {self.args.num} commits from {self.args.rev}"

        if self.args.all_experiments:
            msg += " and all experiments"

        if self.args.not_in_remote:
            msg += " that are present in the DVC remote"

        if self.args.repos:
            msg += " of the current and the following repos:"

            for repo_path in self.args.repos:
                msg += "\n  - %s" % os.path.abspath(repo_path)
        else:
            msg += " of the current repo."

        logger.warning(msg)

        msg = "Are you sure you want to proceed?"
        if not self.args.force and not ui.confirm(msg):
            return 1

        self.repo.gc(
            all_branches=self.args.all_branches,
            all_tags=self.args.all_tags,
            all_commits=self.args.all_commits,
            all_experiments=self.args.all_experiments,
            commit_date=self.args.commit_date,
            cloud=self.args.cloud,
            remote=self.args.remote,
            force=self.args.force,
            jobs=self.args.jobs,
            repos=self.args.repos,
            workspace=self.args.workspace,
            rev=self.args.rev,
            num=self.args.num,
            not_in_remote=self.args.not_in_remote,
        )
        return 0


def add_parser(subparsers, parent_parser):
    GC_HELP = "Garbage collect unused objects from cache or remote storage."
    GC_DESCRIPTION = (
        "Removes all files in the cache or a remote which are not in\n"
        "use by the specified Git revisions (defaults to just HEAD)."
    )
    gc_parser = subparsers.add_parser(
        "gc",
        parents=[parent_parser],
        description=append_doc_link(GC_DESCRIPTION, "gc"),
        help=GC_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    gc_parser.add_argument(
        "-w",
        "--workspace",
        action="store_true",
        default=False,
        help="Keep data files used in the current workspace.",
    )
    gc_parser.add_argument(
        "--rev",
        type=str,
        default=None,
        help="Keep data files used in the specified <commit>.",
        metavar="<commit>",
    )
    gc_parser.add_argument(
        "-n",
        "--num",
        type=int,
        dest="num",
        metavar="<num>",
        help=(
            "Keep data files used in the last `num` commits "
            "starting from the `--rev` <commit>. "
            "Only used if `--rev` is also provided. "
            "Defaults to `1`."
        ),
    )
    gc_parser.add_argument(
        "-a",
        "--all-branches",
        action="store_true",
        default=False,
        help="Keep data files for the tips of all Git branches.",
    )
    gc_parser.add_argument(
        "-T",
        "--all-tags",
        action="store_true",
        default=False,
        help="Keep data files for all Git tags.",
    )
    gc_parser.add_argument(
        "-A",
        "--all-commits",
        action="store_true",
        default=False,
        help="Keep data files for all Git commits.",
    )
    gc_parser.add_argument(
        "--date",
        type=str,
        dest="commit_date",
        metavar="<YYYY-MM-DD>",
        default=None,
        help=(
            "Keep cached data referenced in the commits after ( inclusive )"
            " a certain time. Date must match the extended ISO 8601 format "
            "(YYYY-MM-DD)."
        ),
    )
    gc_parser.add_argument(
        "--all-experiments",
        action="store_true",
        default=False,
        help="Keep data files for all experiments.",
    )
    gc_parser.add_argument(
        "--not-in-remote",
        action="store_true",
        default=False,
        help="Keep data files that are not present in the remote.",
    )
    gc_parser.add_argument(
        "-c",
        "--cloud",
        action="store_true",
        default=False,
        help="Collect garbage in remote storage in addition to local cache.",
    )
    gc_parser.add_argument(
        "-r",
        "--remote",
        help="Remote storage to collect garbage in",
        metavar="<name>",
    )
    gc_parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        default=False,
        help="Force garbage collection - automatically agree to all prompts.",
    )
    gc_parser.add_argument(
        "-j",
        "--jobs",
        type=int,
        help=(
            "Number of jobs to run simultaneously. "
            "The default value is 4 * cpu_count(). "
        ),
        metavar="<number>",
    )
    gc_parser.add_argument(
        "-p",
        "--projects",
        dest="repos",
        type=str,
        nargs="*",
        help=(
            "Keep data files required by these projects "
            "in addition to the current one. "
            "Useful if you share a single cache across repos."
        ),
        metavar="<paths>",
    )
    gc_parser.set_defaults(func=CmdGC)




dvc/commands/get.py
import argparse
import logging

from dvc.cli import completion
from dvc.cli.command import CmdBaseNoRepo
from dvc.cli.utils import append_doc_link
from dvc.exceptions import DvcException

logger = logging.getLogger(__name__)


class CmdGet(CmdBaseNoRepo):
    def _show_url(self):
        from dvc.api import get_url
        from dvc.ui import ui

        url = get_url(self.args.path, repo=self.args.url, rev=self.args.rev)
        ui.write(url, force=True)

        return 0

    def run(self):
        if self.args.show_url:
            return self._show_url()

        return self._get_file_from_repo()

    def _get_file_from_repo(self):
        from dvc.repo import Repo
        from dvc.scm import CloneError

        try:
            Repo.get(
                self.args.url,
                path=self.args.path,
                out=self.args.out,
                rev=self.args.rev,
                jobs=self.args.jobs,
                force=self.args.force,
            )
            return 0
        except CloneError:
            logger.exception("failed to get '%s'", self.args.path)
            return 1
        except DvcException:
            logger.exception(
                "failed to get '%s' from '%s'", self.args.path, self.args.url
            )
            return 1


def add_parser(subparsers, parent_parser):
    GET_HELP = "Download file or directory tracked by DVC or by Git."
    get_parser = subparsers.add_parser(
        "get",
        parents=[parent_parser],
        description=append_doc_link(GET_HELP, "get"),
        help=GET_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    get_parser.add_argument(
        "url", help="Location of DVC or Git repository to download from"
    )
    get_parser.add_argument(
        "path", help="Path to a file or directory within the repository"
    ).complete = completion.FILE
    get_parser.add_argument(
        "-o",
        "--out",
        nargs="?",
        help="Destination path to download files to",
        metavar="<path>",
    ).complete = completion.DIR
    get_parser.add_argument(
        "--rev",
        nargs="?",
        help="Git revision (e.g. SHA, branch, tag)",
        metavar="<commit>",
    )
    get_parser.add_argument(
        "--show-url",
        action="store_true",
        help=(
            "Print the storage location (URL) the target data would be "
            "downloaded from, and exit."
        ),
    )
    get_parser.add_argument(
        "-j",
        "--jobs",
        type=int,
        help=(
            "Number of jobs to run simultaneously. "
            "The default value is 4 * cpu_count(). "
        ),
        metavar="<number>",
    )
    get_parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        default=False,
        help="Override local file or folder if exists.",
    )
    get_parser.set_defaults(func=CmdGet)




dvc/commands/get_url.py
import argparse
import logging

from dvc.cli import completion
from dvc.cli.command import CmdBaseNoRepo
from dvc.cli.utils import append_doc_link
from dvc.exceptions import DvcException

logger = logging.getLogger(__name__)


class CmdGetUrl(CmdBaseNoRepo):
    def run(self):
        from dvc.repo import Repo

        try:
            Repo.get_url(
                self.args.url,
                out=self.args.out,
                jobs=self.args.jobs,
                force=self.args.force,
            )
            return 0
        except DvcException:
            logger.exception("failed to get '%s'", self.args.url)
            return 1


def add_parser(subparsers, parent_parser):
    GET_HELP = "Download or copy files from URL."
    get_parser = subparsers.add_parser(
        "get-url",
        parents=[parent_parser],
        description=append_doc_link(GET_HELP, "get-url"),
        help=GET_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    get_parser.add_argument(
        "url", help="See `dvc import-url -h` for full list of supported URLs."
    )
    get_parser.add_argument(
        "out", nargs="?", help="Destination path to put data to."
    ).complete = completion.DIR
    get_parser.add_argument(
        "-j",
        "--jobs",
        type=int,
        help=(
            "Number of jobs to run simultaneously. "
            "The default value is 4 * cpu_count(). "
        ),
        metavar="<number>",
    )
    get_parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        default=False,
        help="Override local file or folder if exists.",
    )
    get_parser.set_defaults(func=CmdGetUrl)




dvc/commands/git_hook.py
import logging
import os

from dvc.cli.command import CmdBaseNoRepo
from dvc.cli.utils import fix_subparsers
from dvc.exceptions import NotDvcRepoError

logger = logging.getLogger(__name__)


class CmdHookBase(CmdBaseNoRepo):
    def run(self):
        from dvc.repo import Repo

        try:
            repo = Repo()
            repo.close()
        except NotDvcRepoError:
            return 0

        return self._run()

    def _run(self):
        raise NotImplementedError


class CmdPreCommit(CmdHookBase):
    def _run(self):
        from dvc.cli import main

        return main(["status"])


class CmdPostCheckout(CmdHookBase):
    def _run(self):
        # when we are running from pre-commit tool, it doesn't provide CLI
        # flags, but instead provides respective env vars that we could use.
        flag = os.environ.get("PRE_COMMIT_CHECKOUT_TYPE")
        if flag is None and len(self.args.args) >= 3:
            # see https://git-scm.com/docs/githooks#_post_checkout
            flag = self.args.args[2]

        # checking out some reference and not specific file.
        if flag != "1":
            return 0

        # make sure we are not in the middle of a rebase/merge, so we
        # don't accidentally break it with an unsuccessful checkout.
        # Note that git hooks are always running in repo root.
        if os.path.isdir(os.path.join(".git", "rebase-merge")):
            return 0

        from dvc.cli import main

        return main(["checkout"])


class CmdPrePush(CmdHookBase):
    def _run(self):
        from dvc.cli import main

        return main(["push"])


class CmdMergeDriver(CmdHookBase):
    def _run(self):
        from dvc.dvcfile import load_file
        from dvc.repo import Repo

        dvc = Repo()

        try:
            ancestor = load_file(dvc, self.args.ancestor, verify=False)
            our = load_file(dvc, self.args.our, verify=False)
            their = load_file(dvc, self.args.their, verify=False)

            our.merge(ancestor, their, allowed=["add", "remove", "change"])

            return 0
        finally:
            dvc.close()


def add_parser(subparsers, parent_parser):
    GIT_HOOK_HELP = "Run GIT hook."

    git_hook_parser = subparsers.add_parser(
        "git-hook",
        parents=[parent_parser],
        description=GIT_HOOK_HELP,
        add_help=False,
    )

    git_hook_subparsers = git_hook_parser.add_subparsers(
        dest="cmd",
        help="Use `dvc daemon CMD --help` for command-specific help.",
    )

    fix_subparsers(git_hook_subparsers)

    PRE_COMMIT_HELP = "Run pre-commit GIT hook."
    pre_commit_parser = git_hook_subparsers.add_parser(
        "pre-commit",
        parents=[parent_parser],
        description=PRE_COMMIT_HELP,
        help=PRE_COMMIT_HELP,
    )
    pre_commit_parser.add_argument(
        "args", nargs="*", help="Arguments passed by GIT or pre-commit tool."
    )
    pre_commit_parser.set_defaults(func=CmdPreCommit)

    POST_CHECKOUT_HELP = "Run post-checkout GIT hook."
    post_checkout_parser = git_hook_subparsers.add_parser(
        "post-checkout",
        parents=[parent_parser],
        description=POST_CHECKOUT_HELP,
        help=POST_CHECKOUT_HELP,
    )
    post_checkout_parser.add_argument(
        "args", nargs="*", help="Arguments passed by GIT or pre-commit tool."
    )
    post_checkout_parser.set_defaults(func=CmdPostCheckout)

    PRE_PUSH_HELP = "Run pre-push GIT hook."
    pre_push_parser = git_hook_subparsers.add_parser(
        "pre-push",
        parents=[parent_parser],
        description=PRE_PUSH_HELP,
        help=PRE_PUSH_HELP,
    )
    pre_push_parser.add_argument(
        "args", nargs="*", help="Arguments passed by GIT or pre-commit tool."
    )
    pre_push_parser.set_defaults(func=CmdPrePush)

    MERGE_DRIVER_HELP = "Run GIT merge driver."
    merge_driver_parser = git_hook_subparsers.add_parser(
        "merge-driver",
        parents=[parent_parser],
        description=MERGE_DRIVER_HELP,
        help=MERGE_DRIVER_HELP,
    )
    merge_driver_parser.add_argument(
        "--ancestor",
        required=True,
        help="Ancestor's version of the conflicting file.",
    )
    merge_driver_parser.add_argument(
        "--our", required=True, help="Current version of the conflicting file."
    )
    merge_driver_parser.add_argument(
        "--their",
        required=True,
        help="Other branch's version of the conflicting file.",
    )
    merge_driver_parser.set_defaults(func=CmdMergeDriver)




dvc/commands/imp.py
import argparse
import logging

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.exceptions import DvcException

logger = logging.getLogger(__name__)


class CmdImport(CmdBase):
    def run(self):
        from dvc.scm import CloneError

        try:
            self.repo.imp(
                self.args.url,
                self.args.path,
                out=self.args.out,
                fname=self.args.file,
                rev=self.args.rev,
                no_exec=self.args.no_exec,
                no_download=self.args.no_download,
                jobs=self.args.jobs,
            )
        except CloneError:
            logger.exception("failed to import '%s'", self.args.path)
            return 1
        except DvcException:
            logger.exception(
                "failed to import '%s' from '%s'.",
                self.args.path,
                self.args.url,
            )
            return 1
        return 0


def add_parser(subparsers, parent_parser):
    IMPORT_HELP = (
        "Download file or directory tracked by DVC or by Git "
        "into the workspace, and track it."
    )

    import_parser = subparsers.add_parser(
        "import",
        parents=[parent_parser],
        description=append_doc_link(IMPORT_HELP, "import"),
        help=IMPORT_HELP,
        formatter_class=argparse.RawTextHelpFormatter,
    )
    import_parser.add_argument(
        "url", help="Location of DVC or Git repository to download from"
    )
    import_parser.add_argument(
        "path", help="Path to a file or directory within the repository"
    ).complete = completion.FILE
    import_parser.add_argument(
        "-o",
        "--out",
        nargs="?",
        help="Destination path to download files to",
        metavar="<path>",
    ).complete = completion.DIR
    import_parser.add_argument(
        "--rev",
        nargs="?",
        help="Git revision (e.g. SHA, branch, tag)",
        metavar="<commit>",
    )
    import_parser.add_argument(
        "--file",
        help="Specify name of the .dvc file this command will generate.",
        metavar="<filename>",
    )
    no_download_exec_group = import_parser.add_mutually_exclusive_group()
    no_download_exec_group.add_argument(
        "--no-exec",
        action="store_true",
        default=False,
        help="Only create .dvc file without actually importing target data.",
    )
    no_download_exec_group.add_argument(
        "--no-download",
        action="store_true",
        default=False,
        help=(
            "Create .dvc file including target data hash value(s)"
            " but do not actually download the file(s)."
        ),
    )
    import_parser.add_argument(
        "-j",
        "--jobs",
        type=int,
        help=(
            "Number of jobs to run simultaneously. "
            "The default value is 4 * cpu_count(). "
        ),
        metavar="<number>",
    )
    import_parser.set_defaults(func=CmdImport)




dvc/commands/imp_url.py
import argparse
import logging

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.exceptions import DvcException

logger = logging.getLogger(__name__)


class CmdImportUrl(CmdBase):
    def run(self):
        try:
            self.repo.imp_url(
                self.args.url,
                out=self.args.out,
                fname=self.args.file,
                no_exec=self.args.no_exec,
                no_download=self.args.no_download,
                remote=self.args.remote,
                to_remote=self.args.to_remote,
                jobs=self.args.jobs,
                force=self.args.force,
                version_aware=self.args.version_aware,
            )
        except DvcException:
            logger.exception(
                (
                    "failed to import %s. You could also try downloading "
                    "it manually, and adding it with `dvc add`."
                ),
                self.args.url,
            )
            return 1
        return 0


def add_parser(subparsers, parent_parser):
    IMPORT_HELP = "Download or copy file from URL and take it under DVC control."

    import_parser = subparsers.add_parser(
        "import-url",
        parents=[parent_parser],
        description=append_doc_link(IMPORT_HELP, "import-url"),
        help=IMPORT_HELP,
        formatter_class=argparse.RawTextHelpFormatter,
    )
    import_parser.add_argument(
        "url",
        help=(
            "Location of the data to download. Supported URLs:\n"
            "/absolute/path/to/file/or/dir\n"
            "relative/path/to/file/or/dir\n"
            "C:\\\\path\\to\\file\\or\\dir\n"
            "https://example.com/path/to/file\n"
            "s3://bucket/key/path\n"
            "gs://bucket/path/to/file/or/dir\n"
            "hdfs://example.com/path/to/file\n"
            "ssh://example.com/absolute/path/to/file/or/dir\n"
            "remote://remote_name/path/to/file/or/dir (see `dvc remote`)"
        ),
    )
    import_parser.add_argument(
        "out", nargs="?", help="Destination path to put files to."
    ).complete = completion.DIR
    import_parser.add_argument(
        "--file",
        help="Specify name of the .dvc file this command will generate.",
        metavar="<filename>",
    ).complete = completion.DIR
    import_parser.add_argument(
        "--to-remote",
        action="store_true",
        default=False,
        help="Download it directly to the remote",
    )
    import_parser.add_argument(
        "-r",
        "--remote",
        help="Remote storage to download to",
        metavar="<name>",
    )
    no_download_exec_group = import_parser.add_mutually_exclusive_group()
    no_download_exec_group.add_argument(
        "--no-exec",
        action="store_true",
        default=False,
        help="Only create .dvc file without actually importing target data.",
    )
    no_download_exec_group.add_argument(
        "--no-download",
        action="store_true",
        default=False,
        help=(
            "Create .dvc file including target data hash value(s)"
            " but do not actually download the file(s)."
        ),
    )
    import_parser.add_argument(
        "-j",
        "--jobs",
        type=int,
        help=(
            "Number of jobs to run simultaneously. "
            "The default value is 4 * cpu_count(). "
        ),
        metavar="<number>",
    )
    import_parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        default=False,
        help="Override local file or folder if exists.",
    )
    import_parser.add_argument(
        "--version-aware",
        action="store_true",
        default=False,
        help="Import using cloud versioning. Implied if the URL contains a version ID.",
    )
    import_parser.set_defaults(func=CmdImportUrl)




dvc/commands/init.py
import argparse
import logging

import colorama

from dvc import analytics
from dvc.cli.command import CmdBaseNoRepo
from dvc.cli.utils import append_doc_link
from dvc.utils import boxify
from dvc.utils import format_link as fmt_link

logger = logging.getLogger(__name__)


def _welcome_message():
    from dvc.ui import ui

    if analytics.is_enabled():
        ui.write(
            boxify(
                "DVC has enabled anonymous aggregate usage analytics.\n"
                "Read the analytics documentation (and how to opt-out) here:\n"
                + fmt_link("https://dvc.org/doc/user-guide/analytics"),
                border_color="red",
            )
        )

    msg = (
        "{yellow}What's next?{nc}\n"
        "{yellow}------------{nc}\n"
        f"- Check out the documentation: {fmt_link('https://dvc.org/doc')}\n"
        f"- Get help and share ideas: {fmt_link('https://dvc.org/chat')}\n"
        f"- Star us on GitHub: {fmt_link('https://github.com/iterative/dvc')}"
    ).format(yellow=colorama.Fore.YELLOW, nc=colorama.Fore.RESET)

    ui.write(msg)


class CmdInit(CmdBaseNoRepo):
    def run(self):
        from dvc.exceptions import InitError
        from dvc.repo import Repo

        try:
            with Repo.init(
                ".",
                no_scm=self.args.no_scm,
                force=self.args.force,
                subdir=self.args.subdir,
            ) as repo:
                self.config = repo.config
                _welcome_message()
        except InitError:
            logger.exception("failed to initiate DVC")
            return 1
        return 0


def add_parser(subparsers, parent_parser):
    """Setup parser for `dvc init`."""
    INIT_HELP = "Initialize DVC in the current directory."
    INIT_DESCRIPTION = (
        "Initialize DVC in the current directory. Expects directory\n"
        "to be a Git repository unless --no-scm option is specified."
    )

    init_parser = subparsers.add_parser(
        "init",
        parents=[parent_parser],
        description=append_doc_link(INIT_DESCRIPTION, "init"),
        help=INIT_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    init_parser.add_argument(
        "--no-scm",
        action="store_true",
        default=False,
        help=(
            "Initiate DVC in directory that is not tracked by any SCM tool (e.g. Git)."
        ),
    )
    init_parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        default=False,
        help=(
            "Overwrite existing '.dvc/' directory. This operation removes local cache."
        ),
    )
    init_parser.add_argument(
        "--subdir",
        action="store_true",
        default=False,
        help=(
            "Necessary for running this command inside a subdirectory of a "
            "parent SCM repository."
        ),
    )
    init_parser.set_defaults(func=CmdInit)




dvc/commands/install.py
import argparse
import logging

from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.exceptions import DvcException

logger = logging.getLogger(__name__)


class CmdInstall(CmdBase):
    def run(self):
        try:
            self.repo.install(self.args.use_pre_commit_tool)
        except DvcException:
            logger.exception("failed to install DVC Git hooks")
            return 1
        return 0


def add_parser(subparsers, parent_parser):
    INSTALL_HELP = "Install DVC git hooks into the repository."
    install_parser = subparsers.add_parser(
        "install",
        parents=[parent_parser],
        description=append_doc_link(INSTALL_HELP, "install"),
        help=INSTALL_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    install_parser.add_argument(
        "--use-pre-commit-tool",
        action="store_true",
        default=False,
        help=(
            "Install DVC hooks using pre-commit "
            "(https://pre-commit.com) if it is installed."
        ),
    )
    install_parser.set_defaults(func=CmdInstall)




dvc/commands/ls_url.py
import argparse
import logging

from dvc.cli.command import CmdBaseNoRepo
from dvc.cli.utils import append_doc_link
from dvc.ui import ui

from .ls import _prettify

logger = logging.getLogger(__name__)


class CmdListUrl(CmdBaseNoRepo):
    def run(self):
        from dvc.repo import Repo

        entries = Repo.ls_url(self.args.url, recursive=self.args.recursive)
        if entries:
            entries = _prettify(entries, with_color=True)
            ui.write("\n".join(entries))
        return 0


def add_parser(subparsers, parent_parser):
    LS_HELP = "List directory contents from URL."
    lsurl_parser = subparsers.add_parser(
        "list-url",
        aliases=["ls-url"],
        parents=[parent_parser],
        description=append_doc_link(LS_HELP, "list-url"),
        help=LS_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    lsurl_parser.add_argument(
        "url", help="See `dvc import-url -h` for full list of supported URLs."
    )
    lsurl_parser.add_argument(
        "-R",
        "--recursive",
        action="store_true",
        help="Recursively list files.",
    )
    lsurl_parser.set_defaults(func=CmdListUrl)




dvc/commands/machine.py
import argparse
from typing import Dict, List

from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link, fix_subparsers
from dvc.commands.config import CmdConfig
from dvc.compare import TabularData
from dvc.config import ConfigError
from dvc.exceptions import DvcException
from dvc.ui import ui
from dvc.utils import format_link


class MachineDisabledError(ConfigError):
    def __init__(self):
        super().__init__("Machine feature is disabled")


class CmdMachineConfig(CmdConfig):
    def __init__(self, args):
        super().__init__(args)
        if not self.config["feature"].get("machine", False):
            raise MachineDisabledError

        if getattr(self.args, "name", None):
            self.args.name = self.args.name.lower()

    def _check_exists(self, conf):
        if self.args.name not in conf["machine"]:
            raise ConfigError(f"machine '{self.args.name}' doesn't exist.")


class CmdMachineAdd(CmdMachineConfig):
    def run(self):
        from dvc.machine import validate_name

        validate_name(self.args.name)

        if self.args.default:
            ui.write(f"Setting '{self.args.name}' as a default machine.")

        with self.config.edit(self.args.level) as conf:
            if self.args.name in conf["machine"] and not self.args.force:
                raise ConfigError(
                    "machine '{}' already exists. Use `-f|--force` to "
                    "overwrite it.".format(self.args.name)
                )

            conf["machine"][self.args.name] = {"cloud": self.args.cloud}
            if self.args.default:
                conf["core"]["machine"] = self.args.name

        return 0


class CmdMachineRemove(CmdMachineConfig):
    def run(self):
        with self.config.edit(self.args.level) as conf:
            self._check_exists(conf)
            del conf["machine"][self.args.name]

        up_to_level = self.args.level or "repo"
        # Remove core.machine refs to this machine in any shadowing configs
        for level in reversed(self.config.LEVELS):
            with self.config.edit(level) as conf:
                if conf["core"].get("machine") == self.args.name:
                    del conf["core"]["machine"]

            if level == up_to_level:
                break

        return 0


class CmdMachineList(CmdMachineConfig):
    TABLE_COLUMNS = [
        "name",
        "cloud",
        "region",
        "image",
        "spot",
        "spot_price",
        "instance_hdd_size",
        "instance_type",
        "ssh_private",
        "startup_script",
    ]

    PRIVATE_COLUMNS = ["ssh_private", "startup_script"]

    def _hide_private(self, conf):
        for machine in conf:
            for column in self.PRIVATE_COLUMNS:
                if column in conf[machine]:
                    conf[machine][column] = "***"

    def _show_origin(self):
        levels = [self.args.level] if self.args.level else self.config.LEVELS
        for level in levels:
            conf = self.config.read(level)["machine"]
            if self.args.name:
                conf = conf.get(self.args.name, {})
            self._hide_private(conf)
            prefix = self._config_file_prefix(True, self.config, level)
            configs = list(self._format_config(conf, prefix))
            if configs:
                ui.write("\n".join(configs))

    def _show_table(self):
        td = TabularData(self.TABLE_COLUMNS, fill_value="-")
        conf = self.config.read()["machine"]
        if self.args.name:
            conf = {self.args.name: conf.get(self.args.name, {})}
        self._hide_private(conf)
        for machine, machine_config in conf.items():
            machine_config["name"] = machine
            td.row_from_dict(machine_config)
        td.dropna("cols", "all")
        td.render()

    def run(self):
        if self.args.show_origin:
            self._show_origin()
        else:
            self._show_table()
        return 0


class CmdMachineModify(CmdMachineConfig):
    def run(self):
        from dvc.config import merge

        with self.config.edit(self.args.level) as conf:
            merged = self.config.load_config_to_level(self.args.level)
            merge(merged, conf)
            self._check_exists(merged)

            if self.args.name not in conf["machine"]:
                conf["machine"][self.args.name] = {}
            section = conf["machine"][self.args.name]
            if self.args.unset:
                section.pop(self.args.option, None)
            else:
                section[self.args.option] = self.args.value
        return 0


class CmdMachineRename(CmdBase):
    def _check_exists(self, conf):
        if self.args.name not in conf["machine"]:
            raise ConfigError(f"machine '{self.args.name}' doesn't exist.")

    def _rename_default(self, conf):
        if conf["core"].get("machine") == self.args.name:
            conf["core"]["machine"] = self.args.new

    def _check_before_rename(self):
        from dvc.machine import validate_name

        validate_name(self.args.new)

        all_config = self.config.load_config_to_level(None)
        if self.args.new in all_config.get("machine", {}):
            raise ConfigError(
                f"Rename failed. Machine '{self.args.new}' already exists."
            )
        ui.write(f"Rename machine '{self.args.name}' to '{self.args.new}'.")

    def run(self):
        self._check_before_rename()

        with self.config.edit(self.args.level) as conf:
            self._check_exists(conf)
            conf["machine"][self.args.new] = conf["machine"][self.args.name]
            try:
                assert self.repo.machine
                self.repo.machine.rename(self.args.name, self.args.new)
            except DvcException as error:
                del conf["machine"][self.args.new]
                raise ConfigError("terraform rename failed") from error
            del conf["machine"][self.args.name]
            self._rename_default(conf)

        up_to_level = self.args.level or "repo"
        for level in reversed(self.config.LEVELS):
            if level == up_to_level:
                break
            with self.config.edit(level) as level_conf:
                self._rename_default(level_conf)

        return 0


class CmdMachineDefault(CmdMachineConfig):
    def run(self):
        if self.args.name is None and not self.args.unset:
            conf = self.config.read(self.args.level)
            try:
                ui.write(conf["core"]["machine"])
            except KeyError:
                ui.write("No default machine set")
                return 1
        else:
            with self.config.edit(self.args.level) as conf:
                if self.args.unset:
                    conf["core"].pop("machine", None)
                else:
                    merged_conf = self.config.load_config_to_level(self.args.level)
                    if (
                        self.args.name in conf["machine"]
                        or self.args.name in merged_conf["machine"]
                    ):
                        conf["core"]["machine"] = self.args.name
                    else:
                        raise ConfigError(
                            "default machine must be present in machine list."
                        )
        return 0


class CmdMachineCreate(CmdBase):
    def run(self):
        if self.repo.machine is None:
            raise MachineDisabledError

        self.repo.machine.create(self.args.name)
        return 0


class CmdMachineStatus(CmdBase):
    INSTANCE_FIELD = ["name", "instance", "status"]
    SHOWN_FIELD = [
        "cloud",
        "instance_ip",
        "instance_type",
        "instance_hdd_size",
        "instance_gpu",
    ]
    FILL_VALUE = "-"

    def _add_row(
        self,
        name: str,
        all_status: List[Dict],
        td: TabularData,
    ):
        if not all_status:
            row = [
                name,
                self.FILL_VALUE,
                "offline",
            ]  # back to `None` after #7167
            td.append(row)
        for i, status in enumerate(all_status, start=1):
            row = [name, f"num_{i}", "running" if status else "offline"]
            for field in self.SHOWN_FIELD:
                value = str(status.get(field, ""))
                row.append(value)
            td.append(row)

    def run(self):
        if self.repo.machine is None:
            raise MachineDisabledError

        td = TabularData(
            self.INSTANCE_FIELD + self.SHOWN_FIELD, fill_value=self.FILL_VALUE
        )

        if self.args.name:
            all_status = list(self.repo.machine.status(self.args.name))
            self._add_row(self.args.name, all_status, td)
        else:
            name_set = set()
            for level in self.repo.config.LEVELS:
                conf = self.repo.config.read(level)["machine"]
                name_set.update(conf.keys())
            name_list = list(name_set)
            for name in sorted(name_list):
                all_status = list(self.repo.machine.status(name))
                self._add_row(name, all_status, td)

        td.dropna("cols", "all")
        td.render()
        return 0


class CmdMachineDestroy(CmdBase):
    def run(self):
        if self.repo.machine is None:
            raise MachineDisabledError

        self.repo.machine.destroy(self.args.name)
        return 0


class CmdMachineSsh(CmdBase):
    def run(self):
        if self.repo.machine is None:
            raise MachineDisabledError

        self.repo.machine.run_shell(self.args.name)
        return 0


def add_parser(subparsers, parent_parser):  # noqa: PLR0915
    from dvc.commands.config import parent_config_parser

    machine_HELP = "Set up and manage cloud machines."
    machine_parser = subparsers.add_parser(
        "machine",
        parents=[parent_parser],
        description=append_doc_link(machine_HELP, "machine"),
        # NOTE: suppress help during development to hide command
        # help=machine_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    machine_subparsers = machine_parser.add_subparsers(
        dest="cmd",
        help="Use `dvc machine CMD --help` for command-specific help.",
    )

    fix_subparsers(machine_subparsers)

    machine_ADD_HELP = "Add a new data machine."
    machine_add_parser = machine_subparsers.add_parser(
        "add",
        parents=[parent_config_parser, parent_parser],
        description=append_doc_link(machine_ADD_HELP, "machine/add"),
        help=machine_ADD_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    machine_add_parser.add_argument("name", help="Name of the machine")
    machine_add_parser.add_argument(
        "cloud",
        help="Machine cloud. See full list of supported clouds at {}".format(
            format_link(
                "https://github.com/iterative/terraform-provider-iterative#machine"
            )
        ),
    )
    machine_add_parser.add_argument(
        "-d",
        "--default",
        action="store_true",
        default=False,
        help="Set as default machine.",
    )
    machine_add_parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        default=False,
        help="Force overwriting existing configs",
    )
    machine_add_parser.set_defaults(func=CmdMachineAdd)

    machine_DEFAULT_HELP = "Set/unset the default machine."
    machine_default_parser = machine_subparsers.add_parser(
        "default",
        parents=[parent_config_parser, parent_parser],
        description=append_doc_link(machine_DEFAULT_HELP, "machine/default"),
        help=machine_DEFAULT_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    machine_default_parser.add_argument("name", nargs="?", help="Name of the machine")
    machine_default_parser.add_argument(
        "-u",
        "--unset",
        action="store_true",
        default=False,
        help="Unset default machine.",
    )
    machine_default_parser.set_defaults(func=CmdMachineDefault)

    machine_LIST_HELP = "List the configuration of one/all machines."
    machine_list_parser = machine_subparsers.add_parser(
        "list",
        parents=[parent_config_parser, parent_parser],
        description=append_doc_link(machine_LIST_HELP, "machine/list"),
        help=machine_LIST_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    machine_list_parser.add_argument(
        "--show-origin",
        default=False,
        action="store_true",
        help="Show the source file containing each config value.",
    )
    machine_list_parser.add_argument(
        "name",
        nargs="?",
        type=str,
        help="name of machine to specify",
    )
    machine_list_parser.set_defaults(func=CmdMachineList)
    machine_MODIFY_HELP = "Modify the configuration of an machine."
    machine_modify_parser = machine_subparsers.add_parser(
        "modify",
        parents=[parent_config_parser, parent_parser],
        description=append_doc_link(machine_MODIFY_HELP, "machine/modify"),
        help=machine_MODIFY_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    machine_modify_parser.add_argument("name", help="Name of the machine")
    machine_modify_parser.add_argument("option", help="Name of the option to modify.")
    machine_modify_parser.add_argument(
        "value", nargs="?", help="(optional) Value of the option."
    )
    machine_modify_parser.add_argument(
        "-u",
        "--unset",
        default=False,
        action="store_true",
        help="Unset option.",
    )
    machine_modify_parser.set_defaults(func=CmdMachineModify)

    machine_RENAME_HELP = "Rename a machine "
    machine_rename_parser = machine_subparsers.add_parser(
        "rename",
        parents=[parent_config_parser, parent_parser],
        description=append_doc_link(machine_RENAME_HELP, "remote/rename"),
        help=machine_RENAME_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    machine_rename_parser.add_argument("name", help="Machine to be renamed")
    machine_rename_parser.add_argument("new", help="New name of the machine")
    machine_rename_parser.set_defaults(func=CmdMachineRename)

    machine_REMOVE_HELP = "Remove an machine."
    machine_remove_parser = machine_subparsers.add_parser(
        "remove",
        parents=[parent_config_parser, parent_parser],
        description=append_doc_link(machine_REMOVE_HELP, "machine/remove"),
        help=machine_REMOVE_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    machine_remove_parser.add_argument("name", help="Name of the machine to remove.")
    machine_remove_parser.set_defaults(func=CmdMachineRemove)

    machine_CREATE_HELP = "Create and start a machine instance."
    machine_create_parser = machine_subparsers.add_parser(
        "create",
        parents=[parent_parser],
        description=append_doc_link(machine_CREATE_HELP, "machine/create"),
        help=machine_CREATE_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    machine_create_parser.add_argument("name", help="Name of the machine to create.")
    machine_create_parser.set_defaults(func=CmdMachineCreate)

    machine_STATUS_HELP = "List the status of running instances for one/all machines."
    machine_status_parser = machine_subparsers.add_parser(
        "status",
        parents=[parent_parser],
        description=append_doc_link(machine_STATUS_HELP, "machine/status"),
        help=machine_STATUS_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    machine_status_parser.add_argument(
        "name", nargs="?", help="(optional) Name of the machine."
    )
    machine_status_parser.set_defaults(func=CmdMachineStatus)

    machine_DESTROY_HELP = "Destroy an machine instance."
    machine_destroy_parser = machine_subparsers.add_parser(
        "destroy",
        parents=[parent_parser],
        description=append_doc_link(machine_DESTROY_HELP, "machine/destroy"),
        help=machine_DESTROY_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    machine_destroy_parser.add_argument(
        "name", help="Name of the machine instance to destroy."
    )
    machine_destroy_parser.set_defaults(func=CmdMachineDestroy)

    machine_SSH_HELP = "Connect to a machine via SSH."
    machine_ssh_parser = machine_subparsers.add_parser(
        "ssh",
        parents=[parent_parser],
        description=append_doc_link(machine_SSH_HELP, "machine/ssh"),
        help=machine_SSH_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    machine_ssh_parser.add_argument(
        "name", help="Name of the machine instance to connect to."
    )
    machine_ssh_parser.set_defaults(func=CmdMachineSsh)




dvc/commands/metrics.py
import argparse
import logging

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link, fix_subparsers
from dvc.exceptions import DvcException
from dvc.ui import ui
from dvc.utils.serialize import encode_exception

logger = logging.getLogger(__name__)


DEFAULT_PRECISION = 5


class CmdMetricsBase(CmdBase):
    UNINITIALIZED = True


class CmdMetricsShow(CmdMetricsBase):
    def run(self):
        try:
            metrics = self.repo.metrics.show(
                self.args.targets,
                all_branches=self.args.all_branches,
                all_tags=self.args.all_tags,
                all_commits=self.args.all_commits,
                recursive=self.args.recursive,
            )
        except DvcException:
            logger.exception("")
            return 1

        if self.args.json:
            ui.write_json(metrics, default=encode_exception)
        else:
            from dvc.compare import show_metrics

            show_metrics(
                metrics,
                markdown=self.args.markdown,
                all_branches=self.args.all_branches,
                all_tags=self.args.all_tags,
                all_commits=self.args.all_commits,
                precision=self.args.precision or DEFAULT_PRECISION,
                round_digits=True,
            )

        return 0


class CmdMetricsDiff(CmdMetricsBase):
    def run(self):
        try:
            diff = self.repo.metrics.diff(
                a_rev=self.args.a_rev,
                b_rev=self.args.b_rev,
                targets=self.args.targets,
                recursive=self.args.recursive,
                all=self.args.all,
            )
        except DvcException:
            logger.exception("failed to show metrics diff")
            return 1

        if self.args.json:
            ui.write_json(diff)
        else:
            from dvc.compare import show_diff

            show_diff(
                diff,
                title="Metric",
                markdown=self.args.markdown,
                no_path=self.args.no_path,
                precision=self.args.precision or DEFAULT_PRECISION,
                round_digits=True,
                a_rev=self.args.a_rev,
                b_rev=self.args.b_rev,
            )

        return 0


def add_parser(subparsers, parent_parser):
    METRICS_HELP = "Commands to display and compare metrics."

    metrics_parser = subparsers.add_parser(
        "metrics",
        parents=[parent_parser],
        description=append_doc_link(METRICS_HELP, "metrics"),
        help=METRICS_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    metrics_subparsers = metrics_parser.add_subparsers(
        dest="cmd",
        help="Use `dvc metrics CMD --help` to display command-specific help.",
    )

    fix_subparsers(metrics_subparsers)

    METRICS_SHOW_HELP = "Print metrics, with optional formatting."
    metrics_show_parser = metrics_subparsers.add_parser(
        "show",
        parents=[parent_parser],
        description=append_doc_link(METRICS_SHOW_HELP, "metrics/show"),
        help=METRICS_SHOW_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    metrics_show_parser.add_argument(
        "targets",
        nargs="*",
        help=(
            "Limit command scope to these metrics files. Using -R, "
            "directories to search metrics files in can also be given."
        ),
    ).complete = completion.FILE
    metrics_show_parser.add_argument(
        "-a",
        "--all-branches",
        action="store_true",
        default=False,
        help="Show metrics for all branches.",
    )
    metrics_show_parser.add_argument(
        "-T",
        "--all-tags",
        action="store_true",
        default=False,
        help="Show metrics for all tags.",
    )
    metrics_show_parser.add_argument(
        "-A",
        "--all-commits",
        action="store_true",
        default=False,
        help="Show metrics for all commits.",
    )
    metrics_show_parser.add_argument(
        "--json",
        action="store_true",
        default=False,
        help="Show output in JSON format.",
    )
    metrics_show_parser.add_argument(
        "--md",
        action="store_true",
        default=False,
        dest="markdown",
        help="Show tabulated output in the Markdown format (GFM).",
    )
    metrics_show_parser.add_argument(
        "-R",
        "--recursive",
        action="store_true",
        default=False,
        help=(
            "If any target is a directory, recursively search and process "
            "metrics files."
        ),
    )
    metrics_show_parser.add_argument(
        "--precision",
        type=int,
        help=(
            "Round metrics to `n` digits precision after the decimal point. "
            f"Rounds to {DEFAULT_PRECISION} digits by default."
        ),
        metavar="<n>",
    )
    metrics_show_parser.set_defaults(func=CmdMetricsShow)

    METRICS_DIFF_HELP = (
        "Show changes in metrics between commits in the DVC repository, or "
        "between a commit and the workspace."
    )
    metrics_diff_parser = metrics_subparsers.add_parser(
        "diff",
        parents=[parent_parser],
        description=append_doc_link(METRICS_DIFF_HELP, "metrics/diff"),
        help=METRICS_DIFF_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    metrics_diff_parser.add_argument(
        "a_rev", nargs="?", help="Old Git commit to compare (defaults to HEAD)"
    )
    metrics_diff_parser.add_argument(
        "b_rev",
        nargs="?",
        help="New Git commit to compare (defaults to the current workspace)",
    )
    metrics_diff_parser.add_argument(
        "--targets",
        nargs="*",
        help=(
            "Specific metrics file(s) to compare "
            "(even if not found as `metrics` in `dvc.yaml`). "
            "Using -R, directories to search metrics files in "
            "can also be given."
            "Shows all tracked metrics by default."
        ),
        metavar="<paths>",
    ).complete = completion.FILE
    metrics_diff_parser.add_argument(
        "-R",
        "--recursive",
        action="store_true",
        default=False,
        help=(
            "If any target is a directory, recursively search and process "
            "metrics files."
        ),
    )
    metrics_diff_parser.add_argument(
        "--all",
        action="store_true",
        default=False,
        help="Show unchanged metrics as well.",
    )
    metrics_diff_parser.add_argument(
        "--json",
        action="store_true",
        default=False,
        help="Show output in JSON format.",
    )
    metrics_diff_parser.add_argument(
        "--md",
        action="store_true",
        default=False,
        dest="markdown",
        help="Show tabulated output in the Markdown format (GFM).",
    )
    metrics_diff_parser.add_argument(
        "--no-path",
        action="store_true",
        default=False,
        help="Don't show metric path.",
    )
    metrics_diff_parser.add_argument(
        "--precision",
        type=int,
        help=(
            "Round metrics to `n` digits precision after the decimal point. "
            f"Rounds to {DEFAULT_PRECISION} digits by default."
        ),
        metavar="<n>",
    )
    metrics_diff_parser.set_defaults(func=CmdMetricsDiff)




dvc/commands/move.py
import argparse
import logging

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.exceptions import DvcException

logger = logging.getLogger(__name__)


class CmdMove(CmdBase):
    def run(self):
        try:
            self.repo.move(self.args.src, self.args.dst)
        except DvcException:
            msg = f"failed to move '{self.args.src}' -> '{self.args.dst}'"
            logger.exception(msg)
            return 1
        return 0


def add_parser(subparsers, parent_parser):
    MOVE_HELP = "Rename or move a DVC controlled data file or a directory."
    MOVE_DESCRIPTION = (
        "Rename or move a DVC controlled data file or a directory.\n"
        "It renames and modifies the corresponding .dvc file to reflect the"
        " changes."
    )

    move_parser = subparsers.add_parser(
        "move",
        parents=[parent_parser],
        description=append_doc_link(MOVE_DESCRIPTION, "move"),
        help=MOVE_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    move_parser.add_argument(
        "src", help="Source path to a data file or directory."
    ).complete = completion.FILE
    move_parser.add_argument("dst", help="Destination path.").complete = completion.FILE
    move_parser.set_defaults(func=CmdMove)




dvc/commands/params.py
import argparse
import logging

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link, fix_subparsers
from dvc.exceptions import DvcException
from dvc.ui import ui

logger = logging.getLogger(__name__)


class CmdParamsDiff(CmdBase):
    UNINITIALIZED = True

    def run(self):
        try:
            diff = self.repo.params.diff(
                a_rev=self.args.a_rev,
                b_rev=self.args.b_rev,
                targets=self.args.targets,
                all=self.args.all,
                deps=self.args.deps,
            )
        except DvcException:
            logger.exception("failed to show params diff")
            return 1

        if self.args.json:
            ui.write_json(diff)
        else:
            from dvc.compare import show_diff

            show_diff(
                diff,
                title="Param",
                markdown=self.args.markdown,
                no_path=self.args.no_path,
                show_changes=False,
                a_rev=self.args.a_rev,
                b_rev=self.args.b_rev,
            )

        return 0


def add_parser(subparsers, parent_parser):
    PARAMS_HELP = "Commands to display params."

    params_parser = subparsers.add_parser(
        "params",
        parents=[parent_parser],
        description=append_doc_link(PARAMS_HELP, "params"),
        help=PARAMS_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    params_subparsers = params_parser.add_subparsers(
        dest="cmd",
        help="Use `dvc params CMD --help` to display command-specific help.",
    )

    fix_subparsers(params_subparsers)

    PARAMS_DIFF_HELP = (
        "Show changes in params between commits in the DVC repository, or "
        "between a commit and the workspace."
    )
    params_diff_parser = params_subparsers.add_parser(
        "diff",
        parents=[parent_parser],
        description=append_doc_link(PARAMS_DIFF_HELP, "params/diff"),
        help=PARAMS_DIFF_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    params_diff_parser.add_argument(
        "a_rev", nargs="?", help="Old Git commit to compare (defaults to HEAD)"
    )
    params_diff_parser.add_argument(
        "b_rev",
        nargs="?",
        help="New Git commit to compare (defaults to the current workspace)",
    )
    params_diff_parser.add_argument(
        "--targets",
        nargs="*",
        help=(
            "Specific params file(s) to compare "
            "(even if not found as `params` in `dvc.yaml`). "
            "Shows all tracked params by default."
        ),
        metavar="<paths>",
    ).complete = completion.FILE
    params_diff_parser.add_argument(
        "--all",
        action="store_true",
        default=False,
        help="Show unchanged params as well.",
    )
    params_diff_parser.add_argument(
        "--deps",
        action="store_true",
        default=False,
        help="Show only params that are stage dependencies.",
    )
    params_diff_parser.add_argument(
        "--json",
        action="store_true",
        default=False,
        help="Show output in JSON format.",
    )
    params_diff_parser.add_argument(
        "--md",
        action="store_true",
        default=False,
        dest="markdown",
        help="Show tabulated output in the Markdown format (GFM).",
    )
    params_diff_parser.add_argument(
        "--no-path",
        action="store_true",
        default=False,
        help="Don't show params path.",
    )
    params_diff_parser.set_defaults(func=CmdParamsDiff)




dvc/commands/plots.py
import argparse
import logging
import os
from typing import TYPE_CHECKING, Dict, List, Optional

from funcy import compact, first, get_in

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link, fix_subparsers
from dvc.exceptions import DvcException
from dvc.ui import ui
from dvc.utils import format_link

if TYPE_CHECKING:
    from dvc.render.match import RendererWithErrors


logger = logging.getLogger(__name__)


def _show_json(
    renderers_with_errors: List["RendererWithErrors"],
    split=False,
    errors: Optional[Dict[str, Exception]] = None,
):
    from dvc.render.convert import to_json
    from dvc.utils.serialize import encode_exception

    all_errors: List[Dict] = []
    data = {}

    for renderer, src_errors, def_errors in renderers_with_errors:
        name = renderer.name
        data[name] = to_json(renderer, split)
        all_errors.extend(
            {
                "name": name,
                "rev": rev,
                "source": source,
                **encode_exception(e),
            }
            for rev, per_rev_src_errors in src_errors.items()
            for source, e in per_rev_src_errors.items()
        )
        all_errors.extend(
            {
                "name": name,
                "rev": rev,
                **encode_exception(e),
            }
            for rev, e in def_errors.items()
        )

    # these errors are not tied to any renderers
    errors = errors or {}
    all_errors.extend({"rev": rev, **encode_exception(e)} for rev, e in errors.items())

    ui.write_json(compact({"errors": all_errors, "data": data}), highlight=False)


def _adjust_vega_renderers(renderers):
    from dvc.render import REVISION_FIELD, VERSION_FIELD
    from dvc_render import VegaRenderer

    for r in renderers:
        if isinstance(r, VegaRenderer):
            if _data_versions_count(r) > 1:
                summary = _summarize_version_infos(r)
                for dp in r.datapoints:
                    vi = dp.pop(VERSION_FIELD, {})
                    keys = list(vi.keys())
                    for key in keys:
                        if not (len(summary.get(key, set())) > 1):
                            vi.pop(key)
                    if vi:
                        dp["rev"] = "::".join(vi.values())
            else:
                for dp in r.datapoints:
                    dp[REVISION_FIELD] = dp[VERSION_FIELD]["revision"]
                    dp.pop(VERSION_FIELD, {})


def _summarize_version_infos(renderer):
    from collections import defaultdict

    from dvc.render import VERSION_FIELD

    result = defaultdict(set)

    for dp in renderer.datapoints:
        for key, value in dp.get(VERSION_FIELD, {}).items():
            result[key].add(value)
    return dict(result)


def _data_versions_count(renderer):
    from itertools import product

    summary = _summarize_version_infos(renderer)
    x = product(summary.get("filename", {None}), summary.get("field", {None}))
    return len(set(x))


class CmdPlots(CmdBase):
    def _func(self, *args, **kwargs):
        raise NotImplementedError

    def _props(self):
        from dvc.schema import PLOT_PROPS

        # Pass only props specified by user, to not shadow ones from plot def
        props = {p: getattr(self.args, p) for p in PLOT_PROPS}
        return {k: v for k, v in props.items() if v is not None}

    def _html_template_path(self):
        html_template_path = self.args.html_template
        if not html_template_path:
            html_template_path = self.repo.config.get("plots", {}).get(
                "html_template", None
            )
            if html_template_path and not os.path.isabs(html_template_path):
                assert self.repo.dvc_dir
                html_template_path = os.path.join(self.repo.dvc_dir, html_template_path)
        return html_template_path

    def run(self) -> int:  # noqa: C901, PLR0911, PLR0912
        from pathlib import Path

        from dvc.render.match import match_defs_renderers
        from dvc_render import render_html

        if self.args.show_vega:
            if not self.args.targets:
                logger.error("please specify a target for `--show-vega`")
                return 1
            if len(self.args.targets) > 1:
                logger.error("you can only specify one target for `--show-vega`")
                return 1
            if self.args.json:
                logger.error(
                    "'--show-vega' and '--json' are mutually exclusive options."
                )
                return 1

        try:
            plots_data = self._func(
                targets=self.args.targets,
                props=self._props(),
            )

            if not plots_data and not self.args.json:
                ui.error_write(
                    "No plots were loaded, visualization file will not be created."
                )

            out: str = self.args.out or self.repo.config.get("plots", {}).get(
                "out_dir", "dvc_plots"
            )

            renderers_out = out if self.args.json else os.path.join(out, "static")
            renderers_with_errors = match_defs_renderers(
                data=plots_data,
                out=renderers_out,
                templates_dir=self.repo.plots.templates_dir,
            )
            if self.args.json:
                errors = compact(
                    {
                        rev: get_in(data, ["definitions", "error"])
                        for rev, data in plots_data.items()
                    }
                )
                _show_json(renderers_with_errors, self.args.split, errors=errors)
                return 0

            renderers = [r.renderer for r in renderers_with_errors]
            _adjust_vega_renderers(renderers)
            if self.args.show_vega:
                renderer = first(filter(lambda r: r.TYPE == "vega", renderers))
                if renderer:
                    ui.write_json(renderer.get_filled_template(as_string=False))
                return 0

            output_file: Path = (Path.cwd() / out).resolve() / "index.html"

            if renderers:
                render_html(
                    renderers=renderers,
                    output_file=output_file,
                    html_template=self._html_template_path(),
                )

                ui.write(output_file.as_uri())
                auto_open = self.repo.config["plots"].get("auto_open", False)
                if self.args.open or auto_open:
                    if not auto_open:
                        ui.write(
                            "To enable auto opening, you can run:\n"
                            "\n"
                            "\tdvc config plots.auto_open true"
                        )
                    return ui.open_browser(output_file)

            return 0

        except DvcException:
            logger.exception("")
            return 1


class CmdPlotsShow(CmdPlots):
    UNINITIALIZED = True

    def _func(self, *args, **kwargs):
        return self.repo.plots.show(*args, **kwargs)


class CmdPlotsDiff(CmdPlots):
    UNINITIALIZED = True

    def _func(self, *args, **kwargs):
        return self.repo.plots.diff(
            *args,
            revs=self.args.revisions,
            experiment=self.args.experiment,
            **kwargs,
        )


class CmdPlotsModify(CmdPlots):
    def run(self):
        self.repo.plots.modify(
            self.args.target, props=self._props(), unset=self.args.unset
        )
        return 0


class CmdPlotsTemplates(CmdBase):
    def run(self):
        from dvc.exceptions import InvalidArgumentError
        from dvc_render.vega_templates import TEMPLATES

        try:
            target = self.args.template
            if target:
                for template in TEMPLATES:
                    if target == template.DEFAULT_NAME:
                        ui.write_json(template.DEFAULT_CONTENT)
                        return 0
                raise InvalidArgumentError(f"Unexpected template: {target}.")

            for template in TEMPLATES:
                ui.write(template.DEFAULT_NAME)

            return 0
        except DvcException:
            logger.exception("")
            return 1


def add_parser(subparsers, parent_parser):
    PLOTS_HELP = "Commands to visualize and compare plot data."

    plots_parser = subparsers.add_parser(
        "plots",
        parents=[parent_parser],
        description=append_doc_link(PLOTS_HELP, "plots"),
        help=PLOTS_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    plots_subparsers = plots_parser.add_subparsers(
        dest="cmd",
        help="Use `dvc plots CMD --help` to display command-specific help.",
    )

    fix_subparsers(plots_subparsers)

    SHOW_HELP = (
        "Generate plots from target files or from `plots` definitions in `dvc.yaml`."
    )
    plots_show_parser = plots_subparsers.add_parser(
        "show",
        parents=[parent_parser],
        description=append_doc_link(SHOW_HELP, "plots/show"),
        help=SHOW_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    plots_show_parser.add_argument(
        "targets",
        nargs="*",
        help=(
            "Plots files or plot IDs from `dvc.yaml` to visualize. "
            "Shows all plots by default."
        ),
    ).complete = completion.FILE
    _add_props_arguments(plots_show_parser)
    _add_output_argument(plots_show_parser)
    _add_ui_arguments(plots_show_parser)
    plots_show_parser.set_defaults(func=CmdPlotsShow)

    PLOTS_DIFF_HELP = (
        "Show multiple versions of a plot by overlaying them in a single image."
    )
    plots_diff_parser = plots_subparsers.add_parser(
        "diff",
        parents=[parent_parser],
        description=append_doc_link(PLOTS_DIFF_HELP, "plots/diff"),
        help=PLOTS_DIFF_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    plots_diff_parser.add_argument(
        "--targets",
        nargs="*",
        help=(
            "Specific plots to visualize. "
            "Accepts any file path or plot name from `dvc.yaml` file. "
            "Shows all tracked plots by default."
        ),
        metavar="<paths>",
    ).complete = completion.FILE
    plots_diff_parser.add_argument(
        "-e",
        "--experiment",
        action="store_true",
        default=False,
        help=argparse.SUPPRESS,
    )
    plots_diff_parser.add_argument(
        "revisions", nargs="*", default=None, help="Git commits to plot from"
    )
    _add_props_arguments(plots_diff_parser)
    _add_output_argument(plots_diff_parser)
    _add_ui_arguments(plots_diff_parser)
    plots_diff_parser.set_defaults(func=CmdPlotsDiff)

    PLOTS_MODIFY_HELP = (
        "Modify display properties of data-series plots "
        "defined in stages (has no effect on image plots)."
    )
    plots_modify_parser = plots_subparsers.add_parser(
        "modify",
        parents=[parent_parser],
        description=append_doc_link(PLOTS_MODIFY_HELP, "plots/modify"),
        help=PLOTS_MODIFY_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    plots_modify_parser.add_argument(
        "target",
        help="Plots file to set properties for (defined at the stage level).",
    ).complete = completion.FILE
    _add_props_arguments(plots_modify_parser)
    plots_modify_parser.add_argument(
        "--unset",
        nargs="*",
        metavar="<property>",
        help="Unset one or more display properties.",
    )
    plots_modify_parser.set_defaults(func=CmdPlotsModify)

    TEMPLATES_HELP = "List built-in plots templates or show JSON specification for one."
    plots_templates_parser = plots_subparsers.add_parser(
        "templates",
        parents=[parent_parser],
        description=append_doc_link(TEMPLATES_HELP, "plots/templates"),
        help=TEMPLATES_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    plots_templates_parser.add_argument(
        "template",
        default=None,
        nargs="?",
        help=(
            "Template for which to show JSON specification. "
            "List all template names by default."
        ),
    )
    plots_templates_parser.set_defaults(func=CmdPlotsTemplates)


def _add_props_arguments(parser):
    parser.add_argument(
        "-t",
        "--template",
        nargs="?",
        default=None,
        help="Special JSON or HTML schema file to inject with the data. See {}".format(
            format_link("https://man.dvc.org/plots#plot-templates")
        ),
        metavar="<path>",
    ).complete = completion.FILE
    parser.add_argument(
        "-x", default=None, help="Field name for X axis.", metavar="<field>"
    )
    parser.add_argument(
        "-y", default=None, help="Field name for Y axis.", metavar="<field>"
    )
    parser.add_argument(
        "--no-header",
        action="store_false",
        dest="header",
        default=None,  # Use default None to distinguish when it's not used
        help="Provided CSV or TSV datafile does not have a header.",
    )
    parser.add_argument("--title", default=None, metavar="<text>", help="Plot title.")
    parser.add_argument(
        "--x-label", default=None, help="X axis label", metavar="<text>"
    )
    parser.add_argument(
        "--y-label", default=None, help="Y axis label", metavar="<text>"
    )


def _add_output_argument(parser, typ="plots"):
    parser.add_argument(
        "-o",
        "--out",
        default=None,
        help=f"Directory to save {typ} to.",
        metavar="<path>",
    ).complete = completion.DIR


def _add_ui_arguments(parser):
    parser.add_argument(
        "--show-vega",
        action="store_true",
        default=False,
        help="Show output in Vega format.",
    )
    parser.add_argument(
        "--json",
        action="store_true",
        default=False,
        help=argparse.SUPPRESS,
    )
    parser.add_argument(
        "--split", action="store_true", default=False, help=argparse.SUPPRESS
    )
    parser.add_argument(
        "--open",
        action="store_true",
        default=False,
        help="Open plot file directly in the browser.",
    )
    parser.add_argument(
        "--html-template",
        default=None,
        help="Custom HTML template for VEGA visualization.",
        metavar="<path>",
    )




dvc/commands/remote.py
import argparse

from dvc.cli.utils import append_doc_link, fix_subparsers
from dvc.commands.config import CmdConfig
from dvc.ui import ui
from dvc.utils import format_link


class CmdRemote(CmdConfig):
    def __init__(self, args):
        super().__init__(args)

        if getattr(self.args, "name", None):
            self.args.name = self.args.name.lower()

    def _check_exists(self, conf):
        from dvc.config import ConfigError

        if self.args.name not in conf["remote"]:
            raise ConfigError(f"remote '{self.args.name}' doesn't exist.")


class CmdRemoteAdd(CmdRemote):
    def run(self):
        from dvc.config import ConfigError

        if self.args.default:
            ui.write(f"Setting '{self.args.name}' as a default remote.")

        with self.config.edit(self.args.level) as conf:
            if self.args.name in conf["remote"] and not self.args.force:
                raise ConfigError(
                    "remote '{}' already exists. Use `-f|--force` to "
                    "overwrite it.".format(self.args.name)
                )

            conf["remote"][self.args.name] = {"url": self.args.url}
            if self.args.default:
                conf["core"]["remote"] = self.args.name

        return 0


class CmdRemoteRemove(CmdRemote):
    def run(self):
        with self.config.edit(self.args.level) as conf:
            self._check_exists(conf)
            del conf["remote"][self.args.name]

        up_to_level = self.args.level or "repo"
        # Remove core.remote refs to this remote in any shadowing configs
        for level in reversed(self.config.LEVELS):
            with self.config.edit(level) as conf:
                if conf["core"].get("remote") == self.args.name:
                    del conf["core"]["remote"]

            if level == up_to_level:
                break

        return 0


class CmdRemoteModify(CmdRemote):
    def run(self):
        from dvc.config import merge

        with self.config.edit(self.args.level) as conf:
            merged = self.config.load_config_to_level(self.args.level)
            merge(merged, conf)
            self._check_exists(merged)

            if self.args.name not in conf["remote"]:
                conf["remote"][self.args.name] = {}
            section = conf["remote"][self.args.name]
            if self.args.unset:
                section.pop(self.args.option, None)
            else:
                section[self.args.option] = self.args.value
        return 0


class CmdRemoteDefault(CmdRemote):
    def run(self):
        from dvc.config import ConfigError

        if self.args.name is None and not self.args.unset:
            conf = self.config.read(self.args.level)
            try:
                ui.write(conf["core"]["remote"])
            except KeyError:
                ui.write("No default remote set")
                return 1
        else:
            with self.config.edit(self.args.level) as conf:
                if self.args.unset:
                    conf["core"].pop("remote", None)
                else:
                    merged_conf = self.config.load_config_to_level(self.args.level)
                    if (
                        self.args.name in conf["remote"]
                        or self.args.name in merged_conf["remote"]
                    ):
                        conf["core"]["remote"] = self.args.name
                    else:
                        raise ConfigError(
                            "default remote must be present in remote list."
                        )
        return 0


class CmdRemoteList(CmdRemote):
    def run(self):
        conf = self.config.read(self.args.level)
        for name, remote_conf in conf["remote"].items():
            ui.write(name, remote_conf["url"], sep="\t")
        return 0


class CmdRemoteRename(CmdRemote):
    def _rename_default(self, conf):
        if conf["core"].get("remote") == self.args.name:
            conf["core"]["remote"] = self.args.new

    def run(self):
        from dvc.config import ConfigError

        all_config = self.config.load_config_to_level(None)
        if self.args.new in all_config.get("remote", {}):
            raise ConfigError(
                "Rename failed. Remote name '{}' already exists.".format(
                    {self.args.new}
                )
            )

        with self.config.edit(self.args.level) as conf:
            self._check_exists(conf)
            conf["remote"][self.args.new] = conf["remote"][self.args.name]
            del conf["remote"][self.args.name]
            self._rename_default(conf)

        up_to_level = self.args.level or "repo"
        for level in reversed(self.config.LEVELS):
            if level == up_to_level:
                break
            with self.config.edit(level) as level_conf:
                self._rename_default(level_conf)

        return 0


def add_parser(subparsers, parent_parser):
    from dvc.commands.config import parent_config_parser

    REMOTE_HELP = "Set up and manage data remotes."
    remote_parser = subparsers.add_parser(
        "remote",
        parents=[parent_parser],
        description=append_doc_link(REMOTE_HELP, "remote"),
        help=REMOTE_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    remote_subparsers = remote_parser.add_subparsers(
        dest="cmd",
        help="Use `dvc remote CMD --help` for command-specific help.",
    )

    fix_subparsers(remote_subparsers)

    REMOTE_ADD_HELP = "Add a new data remote."
    remote_add_parser = remote_subparsers.add_parser(
        "add",
        parents=[parent_config_parser, parent_parser],
        description=append_doc_link(REMOTE_ADD_HELP, "remote/add"),
        help=REMOTE_ADD_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    remote_add_parser.add_argument("name", help="Name of the remote")
    remote_add_parser.add_argument(
        "url",
        help="Remote location. See full list of supported URLs at {}".format(
            format_link("https://man.dvc.org/remote")
        ),
    )
    remote_add_parser.add_argument(
        "-d",
        "--default",
        action="store_true",
        default=False,
        help="Set as default remote.",
    )
    remote_add_parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        default=False,
        help="Force overwriting existing configs",
    )
    remote_add_parser.set_defaults(func=CmdRemoteAdd)

    REMOTE_DEFAULT_HELP = "Set/unset the default data remote."
    remote_default_parser = remote_subparsers.add_parser(
        "default",
        parents=[parent_config_parser, parent_parser],
        description=append_doc_link(REMOTE_DEFAULT_HELP, "remote/default"),
        help=REMOTE_DEFAULT_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    remote_default_parser.add_argument("name", nargs="?", help="Name of the remote")
    remote_default_parser.add_argument(
        "-u",
        "--unset",
        action="store_true",
        default=False,
        help="Unset default remote.",
    )
    remote_default_parser.set_defaults(func=CmdRemoteDefault)

    REMOTE_MODIFY_HELP = "Modify the configuration of a data remote."
    remote_modify_parser = remote_subparsers.add_parser(
        "modify",
        parents=[parent_config_parser, parent_parser],
        description=append_doc_link(REMOTE_MODIFY_HELP, "remote/modify"),
        help=REMOTE_MODIFY_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    remote_modify_parser.add_argument("name", help="Name of the remote")
    remote_modify_parser.add_argument("option", help="Name of the option to modify.")
    remote_modify_parser.add_argument(
        "value", nargs="?", help="(optional) Value of the option."
    )
    remote_modify_parser.add_argument(
        "-u",
        "--unset",
        default=False,
        action="store_true",
        help="Unset option.",
    )
    remote_modify_parser.set_defaults(func=CmdRemoteModify)

    REMOTE_LIST_HELP = "List all available data remotes."
    remote_list_parser = remote_subparsers.add_parser(
        "list",
        parents=[parent_config_parser, parent_parser],
        description=append_doc_link(REMOTE_LIST_HELP, "remote/list"),
        help=REMOTE_LIST_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    remote_list_parser.set_defaults(func=CmdRemoteList)

    REMOTE_REMOVE_HELP = "Remove a data remote."
    remote_remove_parser = remote_subparsers.add_parser(
        "remove",
        parents=[parent_config_parser, parent_parser],
        description=append_doc_link(REMOTE_REMOVE_HELP, "remote/remove"),
        help=REMOTE_REMOVE_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    remote_remove_parser.add_argument("name", help="Name of the remote to remove.")
    remote_remove_parser.set_defaults(func=CmdRemoteRemove)
    REMOTE_RENAME_HELP = "Rename a DVC remote"
    remote_rename_parser = remote_subparsers.add_parser(
        "rename",
        parents=[parent_config_parser, parent_parser],
        description=append_doc_link(REMOTE_RENAME_HELP, "remote/rename"),
        help=REMOTE_RENAME_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    remote_rename_parser.add_argument("name", help="Remote to be renamed")
    remote_rename_parser.add_argument("new", help="New name of the remote")
    remote_rename_parser.set_defaults(func=CmdRemoteRename)




dvc/commands/remove.py
import argparse
import logging

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.exceptions import DvcException

logger = logging.getLogger(__name__)


class CmdRemove(CmdBase):
    def run(self):
        for target in self.args.targets:
            try:
                self.repo.remove(target, outs=self.args.outs)
            except DvcException:
                logger.exception("")
                return 1
        return 0


def add_parser(subparsers, parent_parser):
    REMOVE_HELP = (
        "Remove stages from dvc.yaml and/or stop tracking files or directories."
    )
    remove_parser = subparsers.add_parser(
        "remove",
        parents=[parent_parser],
        description=append_doc_link(REMOVE_HELP, "remove"),
        help=REMOVE_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    remove_parser.add_argument(
        "--outs",
        action="store_true",
        default=False,
        help="Remove outputs as well.",
    )
    remove_parser.add_argument(
        "targets",
        nargs="+",
        help=".dvc files or stages from dvc.yaml to remove.",
    ).complete = completion.DVC_FILE
    remove_parser.set_defaults(func=CmdRemove)




dvc/commands/repro.py
import argparse

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.commands.status import CmdDataStatus


class CmdRepro(CmdBase):
    def run(self):
        from dvc.ui import ui

        stages = self.repo.reproduce(**self._common_kwargs, **self._repro_kwargs)
        if len(stages) == 0:
            ui.write(CmdDataStatus.UP_TO_DATE_MSG)
        else:
            ui.write("Use `dvc push` to send your updates to remote storage.")

        return 0

    @property
    def _common_kwargs(self):
        return {
            "targets": self.args.targets,
            "single_item": self.args.single_item,
            "force": self.args.force,
            "dry": self.args.dry,
            "interactive": self.args.interactive,
            "pipeline": self.args.pipeline,
            "all_pipelines": self.args.all_pipelines,
            "downstream": self.args.downstream,
            "recursive": self.args.recursive,
            "force_downstream": self.args.force_downstream,
            "pull": self.args.pull,
            "allow_missing": self.args.allow_missing,
        }

    @property
    def _repro_kwargs(self):
        return {
            "run_cache": not self.args.no_run_cache,
            "no_commit": self.args.no_commit,
            "glob": self.args.glob,
        }


def add_arguments(repro_parser):
    repro_parser.add_argument(
        "targets",
        nargs="*",
        help="""\
Stages to reproduce. 'dvc.yaml' by default.
The targets can be path to a dvc.yaml file or `.dvc` file,
or a stage name from dvc.yaml file from
current working directory. To run a stage from dvc.yaml
from other directories, the target must be a path followed by colon `:`
and then the stage name name.
""",
    ).complete = completion.DVCFILES_AND_STAGE
    repro_parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        default=False,
        help="Reproduce even if dependencies were not changed.",
    )
    repro_parser.add_argument(
        "-i",
        "--interactive",
        action="store_true",
        default=False,
        help="Ask for confirmation before reproducing each stage.",
    )
    repro_parser.add_argument(
        "-s",
        "--single-item",
        action="store_true",
        default=False,
        help="Reproduce only single data item without recursive dependencies check.",
    )
    repro_parser.add_argument(
        "-p",
        "--pipeline",
        action="store_true",
        default=False,
        help="Reproduce the whole pipeline that the specified targets belong to.",
    )
    repro_parser.add_argument(
        "-P",
        "--all-pipelines",
        action="store_true",
        default=False,
        help="Reproduce all pipelines in the repo.",
    )
    repro_parser.add_argument(
        "-R",
        "--recursive",
        action="store_true",
        default=False,
        help="Reproduce all stages in the specified directory.",
    )
    repro_parser.add_argument(
        "--downstream",
        action="store_true",
        default=False,
        help="Start from the specified stages when reproducing pipelines.",
    )
    repro_parser.add_argument(
        "--force-downstream",
        action="store_true",
        default=False,
        help=(
            "Reproduce all descendants of a changed stage even if their "
            "direct dependencies didn't change."
        ),
    )
    repro_parser.add_argument(
        "--pull",
        action="store_true",
        default=False,
        help=(
            "Try automatically pulling missing cache for outputs restored "
            "from the run-cache."
        ),
    )
    repro_parser.add_argument(
        "--allow-missing",
        action="store_true",
        default=False,
        help=("Skip stages with missing data but no other changes."),
    )
    repro_parser.add_argument(
        "--dry",
        action="store_true",
        default=False,
        help=(
            "Only print the commands that would be executed without actually executing."
        ),
    )


def add_parser(subparsers, parent_parser):
    REPRO_HELP = "Reproduce complete or partial pipelines by executing their stages."
    repro_parser = subparsers.add_parser(
        "repro",
        parents=[parent_parser],
        description=append_doc_link(REPRO_HELP, "repro"),
        help=REPRO_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    # repro/exp run shared args
    add_arguments(repro_parser)
    # repro only args
    repro_parser.add_argument(
        "--glob",
        action="store_true",
        default=False,
        help="Allows targets containing shell-style wildcards.",
    )
    repro_parser.add_argument(
        "--no-commit",
        action="store_true",
        default=False,
        help="Don't put files/directories into cache.",
    )
    repro_parser.add_argument(
        "--no-run-cache",
        action="store_true",
        default=False,
        help=(
            "Execute stage commands even if they have already been run with "
            "the same command/dependencies/outputs/etc before."
        ),
    )
    repro_parser.set_defaults(func=CmdRepro)




dvc/commands/root.py
import argparse
import logging

from dvc.cli.command import CmdBaseNoRepo
from dvc.cli.utils import append_doc_link
from dvc.utils import relpath

logger = logging.getLogger(__name__)


class CmdRoot(CmdBaseNoRepo):
    def run(self):
        from dvc.repo import Repo
        from dvc.ui import ui

        ui.write(relpath(Repo.find_root()))
        return 0


def add_parser(subparsers, parent_parser):
    ROOT_HELP = "Return the relative path to the root of the DVC project."
    root_parser = subparsers.add_parser(
        "root",
        parents=[parent_parser],
        description=append_doc_link(ROOT_HELP, "root"),
        help=ROOT_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    root_parser.set_defaults(func=CmdRoot)




dvc/commands/stage.py
import argparse
import logging
from contextlib import contextmanager
from itertools import chain, filterfalse
from typing import TYPE_CHECKING, Dict, Iterable, List

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link, fix_subparsers
from dvc.utils.cli_parse import parse_params
from dvc.utils.humanize import truncate_text

if TYPE_CHECKING:
    from dvc.output import Output
    from dvc.stage import Stage

logger = logging.getLogger(__name__)

MAX_TEXT_LENGTH = 80


def generate_description(stage: "Stage") -> str:
    def part_desc(outs: Iterable["Output"]) -> str:
        return ", ".join(out.def_path for out in outs)

    if not stage.deps and not stage.outs:
        return "No outputs or dependencies"

    if not stage.outs and stage.deps:
        return "Depends on " + part_desc(stage.deps)

    def is_plot_or_metric(out: "Output"):
        return bool(out.plot) or bool(out.metric)

    desc: List[str] = []

    outs = list(filterfalse(is_plot_or_metric, stage.outs))
    if outs:
        desc.append("Outputs " + part_desc(outs))

    plots_and_metrics = list(filter(is_plot_or_metric, stage.outs))
    if plots_and_metrics:
        desc.append("Reports " + part_desc(plots_and_metrics))

    return "; ".join(desc)


def prepare_description(stage: "Stage", max_length: int = MAX_TEXT_LENGTH) -> str:
    desc = stage.short_description() or generate_description(stage)
    return truncate_text(desc, max_length)


def prepare_stages_data(
    stages: Iterable["Stage"],
    description: bool = True,
    max_length: int = MAX_TEXT_LENGTH,
) -> Dict[str, str]:
    return {
        stage.addressing: (
            prepare_description(stage, max_length=max_length) if description else ""
        )
        for stage in stages
    }


class CmdStageList(CmdBase):
    def _get_stages(self) -> Iterable["Stage"]:
        if self.args.all:
            stages: List["Stage"] = self.repo.index.stages
            logger.trace(  # type: ignore[attr-defined]
                "%d no. of stages found", len(stages)
            )
            return stages

        # removing duplicates while maintaining order
        collected = chain.from_iterable(
            self.repo.stage.collect(target=target, recursive=self.args.recursive)
            for target in self.args.targets
        )
        return dict.fromkeys(collected).keys()

    def run(self):
        from dvc.ui import ui

        def log_error(relpath: str, exc: Exception):
            if self.args.fail:
                raise exc
            logger.debug("Stages from %s failed to load", relpath)

        # silence stage collection error by default
        self.repo.stage_collection_error_handler = log_error

        stages = self._get_stages()
        data = prepare_stages_data(stages, description=not self.args.name_only)
        ui.table(list(data.items()))

        return 0


def parse_cmd(commands: List[str]) -> str:
    """
    We need to take into account two cases:

    - ['python code.py foo bar']: Used mainly with dvc as a library
    - ['echo', 'foo bar']: List of arguments received from the CLI

    The second case would need quoting, as it was passed through:
            dvc run echo "foo bar"
    """

    def quote_argument(arg: str):
        if not arg:
            return '""'
        if " " in arg and '"' not in arg:
            return f'"{arg}"'
        return arg

    if len(commands) < 2:
        return " ".join(commands)
    return " ".join(map(quote_argument, commands))


@contextmanager
def _disable_logging(highest_level=logging.CRITICAL):
    previous_level = logging.root.manager.disable

    logging.disable(highest_level)

    try:
        yield
    finally:
        logging.disable(previous_level)


class CmdStageAdd(CmdBase):
    def run(self):
        from dvc.repo import lock_repo

        kwargs = vars(self.args)
        kwargs.update(
            {
                "cmd": parse_cmd(kwargs.pop("command")),
                "params": parse_params(self.args.params),
            }
        )

        with self.repo.scm_context, lock_repo(self.repo):
            with _disable_logging(logging.INFO):
                stage = self.repo.stage.add(**kwargs)
            logger.info("Added stage %r in %r", stage.addressing, stage.relpath)
            if self.args.run:
                stage.run()
                stage.dump(update_pipeline=False)

        return 0


def _add_common_args(parser):
    parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        default=False,
        help="Overwrite existing stage",
    )
    parser.add_argument(
        "-d",
        "--deps",
        action="append",
        default=[],
        help="Declare dependencies for reproducible cmd.",
        metavar="<path>",
    ).complete = completion.FILE
    parser.add_argument(
        "-p",
        "--params",
        action="append",
        default=[],
        help="Declare parameter to use as additional dependency.",
        metavar="[<filename>:]<params_list>",
    ).complete = completion.FILE
    parser.add_argument(
        "-o",
        "--outs",
        action="append",
        default=[],
        help="Declare output file or directory.",
        metavar="<filename>",
    ).complete = completion.FILE
    parser.add_argument(
        "-O",
        "--outs-no-cache",
        action="append",
        default=[],
        help="Declare output file or directory (do not put into DVC cache).",
        metavar="<filename>",
    ).complete = completion.FILE
    parser.add_argument(
        "--external",
        action="store_true",
        default=False,
        help="Allow outputs that are outside of the DVC repository.",
    )
    parser.add_argument(
        "--outs-persist",
        action="append",
        default=[],
        help="Declare output file or directory that will not be removed upon repro.",
        metavar="<filename>",
    )
    parser.add_argument(
        "--outs-persist-no-cache",
        action="append",
        default=[],
        help=(
            "Declare output file or directory that will not be "
            "removed upon repro (do not put into DVC cache)."
        ),
        metavar="<filename>",
    )
    parser.add_argument(
        "-m",
        "--metrics",
        action="append",
        default=[],
        help="Declare output metrics file.",
        metavar="<path>",
    )
    parser.add_argument(
        "-M",
        "--metrics-no-cache",
        action="append",
        default=[],
        help="Declare output metrics file (do not put into DVC cache).",
        metavar="<path>",
    )
    parser.add_argument(
        "--plots",
        action="append",
        default=[],
        help="Declare output plot file.",
        metavar="<path>",
    )
    parser.add_argument(
        "--plots-no-cache",
        action="append",
        default=[],
        help="Declare output plot file (do not put into DVC cache).",
        metavar="<path>",
    )
    parser.add_argument(
        "-w",
        "--wdir",
        help="Directory within your repo to run your command in.",
        metavar="<path>",
    )
    parser.add_argument(
        "--always-changed",
        action="store_true",
        default=False,
        help="Always consider this DVC-file as changed.",
    )
    parser.add_argument(
        "--desc",
        type=str,
        metavar="<text>",
        help=(
            "User description of the stage (optional). "
            "This doesn't affect any DVC operations."
        ),
    )
    parser.add_argument(
        "--run",
        action="store_true",
        default=False,
        help="Execute the stage after generating it.",
    )
    parser.add_argument(
        "command",
        nargs=argparse.REMAINDER,
        help="Command to execute.",
        metavar="command",
    )


def add_parser(subparsers, parent_parser):
    STAGES_HELP = "Commands to list and create stages."

    stage_parser = subparsers.add_parser(
        "stage",
        parents=[parent_parser],
        description=append_doc_link(STAGES_HELP, "stage"),
        help=STAGES_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    stage_subparsers = stage_parser.add_subparsers(
        dest="cmd",
        help="Use `dvc stage CMD --help` to display command-specific help.",
    )

    fix_subparsers(stage_subparsers)

    STAGE_ADD_HELP = "Create stage"
    stage_add_parser = stage_subparsers.add_parser(
        "add",
        parents=[parent_parser],
        description=append_doc_link(STAGE_ADD_HELP, "stage/add"),
        help=STAGE_ADD_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    stage_add_parser.add_argument(
        "-n", "--name", help="Name of the stage to add", required=True
    )
    _add_common_args(stage_add_parser)
    stage_add_parser.set_defaults(func=CmdStageAdd)

    STAGE_LIST_HELP = "List stages."
    stage_list_parser = stage_subparsers.add_parser(
        "list",
        parents=[parent_parser],
        description=append_doc_link(STAGE_LIST_HELP, "stage/list"),
        help=STAGE_LIST_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    stage_list_parser.add_argument(
        "targets",
        nargs="*",
        default=["dvc.yaml"],
        help=(
            "Show stages from a dvc.yaml/.dvc file or a directory. "
            "'dvc.yaml' by default"
        ),
    )
    stage_list_parser.add_argument(
        "--all",
        action="store_true",
        default=False,
        help="List all of the stages in the repo.",
    )
    stage_list_parser.add_argument(
        "--fail",
        action="store_true",
        default=False,
        help="Fail immediately, do not suppress any syntax errors.",
    )
    stage_list_parser.add_argument(
        "-R",
        "--recursive",
        action="store_true",
        default=False,
        help="List all stages inside the specified directory.",
    )
    stage_list_parser.add_argument(
        "--name-only",
        "--names-only",
        action="store_true",
        default=False,
        help="List only stage names.",
    )
    stage_list_parser.set_defaults(func=CmdStageList)




dvc/commands/status.py
import logging

from dvc.commands.data_sync import CmdDataBase
from dvc.exceptions import DvcException
from dvc.ui import ui
from dvc.utils import format_link

logger = logging.getLogger(__name__)


class CmdDataStatus(CmdDataBase):
    STATUS_LEN = 20
    STATUS_INDENT = "\t"
    UP_TO_DATE_MSG = "Data and pipelines are up to date."
    IN_SYNC_MSG = "Cache and remote '{remote}' are in sync."
    EMPTY_PROJECT_MSG = (
        "There are no data or pipelines tracked in this project yet.\n"
        "See {link} to get started!"
    ).format(link=format_link("https://dvc.org/doc/start"))

    def _normalize(self, s):
        s += ":"
        assert len(s) < self.STATUS_LEN
        return s + (self.STATUS_LEN - len(s)) * " "

    def _show(self, status, indent=0):
        ind = indent * self.STATUS_INDENT

        if isinstance(status, str):
            ui.write(f"{ind}{status}")
            return

        if isinstance(status, list):
            for entry in status:
                self._show(entry, indent)
            return

        assert isinstance(status, dict)

        for key, value in status.items():
            if isinstance(value, str):
                ui.write(f"{ind}{self._normalize(value)}{key}")
            elif value:
                ui.write(f"{ind}{key}:")
                self._show(value, indent + 1)

    def run(self):
        from dvc.repo import lock_repo

        indent = 1 if self.args.cloud else 0

        with lock_repo(self.repo):
            try:
                st = self.repo.status(
                    targets=self.args.targets,
                    jobs=self.args.jobs,
                    cloud=self.args.cloud,
                    remote=self.args.remote,
                    all_branches=self.args.all_branches,
                    all_tags=self.args.all_tags,
                    all_commits=self.args.all_commits,
                    with_deps=self.args.with_deps,
                    recursive=self.args.recursive,
                )
            except DvcException:
                logger.exception("")
                return 1

            if self.args.quiet:
                return bool(st)

            if self.args.json:
                ui.write_json(st)
                return 0

            if st:
                self._show(st, indent)
                return 0

            # additional hints for the user
            if not self.repo.index.stages:
                ui.write(self.EMPTY_PROJECT_MSG)
            elif self.args.cloud or self.args.remote:
                remote = self.args.remote or self.repo.config["core"].get("remote")
                ui.write(self.IN_SYNC_MSG.format(remote=remote))
            else:
                ui.write(self.UP_TO_DATE_MSG)

        return 0




dvc/commands/unprotect.py
import argparse
import logging

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.exceptions import DvcException

logger = logging.getLogger(__name__)


class CmdUnprotect(CmdBase):
    def run(self):
        for target in self.args.targets:
            try:
                self.repo.unprotect(target)
            except DvcException:
                msg = f"failed to unprotect '{target}'"
                logger.exception(msg)
                return 1
        return 0


def add_parser(subparsers, parent_parser):
    UNPROTECT_HELP = (
        "Unprotect tracked files or directories (when hardlinks or symlinks "
        "have been enabled with `dvc config cache.type`)."
    )
    unprotect_parser = subparsers.add_parser(
        "unprotect",
        parents=[parent_parser],
        description=append_doc_link(UNPROTECT_HELP, "unprotect"),
        help=UNPROTECT_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    unprotect_parser.add_argument(
        "targets", nargs="+", help="Data files/directories to unprotect."
    ).complete = completion.FILE
    unprotect_parser.set_defaults(func=CmdUnprotect)




dvc/commands/update.py
import argparse
import logging

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.exceptions import DvcException

logger = logging.getLogger(__name__)


class CmdUpdate(CmdBase):
    def run(self):
        ret = 0
        try:
            self.repo.update(
                targets=self.args.targets,
                rev=self.args.rev,
                recursive=self.args.recursive,
                to_remote=self.args.to_remote,
                no_download=self.args.no_download,
                remote=self.args.remote,
                jobs=self.args.jobs,
            )
        except DvcException:
            logger.exception("failed update data")
            ret = 1
        return ret


def add_parser(subparsers, parent_parser):
    UPDATE_HELP = (
        "Update data artifact imported (via dvc import or dvc import-url) "
        "from an external DVC repository or URL."
    )
    update_parser = subparsers.add_parser(
        "update",
        parents=[parent_parser],
        description=append_doc_link(UPDATE_HELP, "update"),
        help=UPDATE_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    update_parser.add_argument(
        "targets", nargs="+", help=".dvc files to update."
    ).complete = completion.DVC_FILE
    update_parser.add_argument(
        "--rev",
        nargs="?",
        help="Git revision (e.g. SHA, branch, tag)",
        metavar="<commit>",
    )
    update_parser.add_argument(
        "-R",
        "--recursive",
        action="store_true",
        default=False,
        help="Update all stages in the specified directory.",
    )
    update_parser.add_argument(
        "--no-download",
        action="store_true",
        default=False,
        help=(
            "Update .dvc file git revision/hash value(s)"
            " but do not download the file(s)."
        ),
    )
    update_parser.add_argument(
        "--to-remote",
        action="store_true",
        default=False,
        help="Update data directly on the remote",
    )
    update_parser.add_argument(
        "-r",
        "--remote",
        help="Remote storage to perform updates to",
        metavar="<name>",
    )
    update_parser.add_argument(
        "-j",
        "--jobs",
        type=int,
        help=(
            "Number of jobs to run simultaneously. "
            "The default value is 4 * cpu_count(). "
        ),
        metavar="<number>",
    )
    update_parser.set_defaults(func=CmdUpdate)




dvc/commands/version.py
import argparse
import logging

from dvc.cli.command import CmdBaseNoRepo
from dvc.cli.utils import append_doc_link
from dvc.ui import ui

logger = logging.getLogger(__name__)


class CmdVersion(CmdBaseNoRepo):
    def run(self):
        from dvc.info import get_dvc_info
        from dvc.updater import notify_updates

        dvc_info = get_dvc_info()
        ui.write(dvc_info, force=True)

        notify_updates()
        return 0


def add_parser(subparsers, parent_parser):
    VERSION_HELP = "Display the DVC version and system/environment information."
    version_parser = subparsers.add_parser(
        "version",
        parents=[parent_parser],
        description=append_doc_link(VERSION_HELP, "version"),
        help=VERSION_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        aliases=["doctor"],
    )
    version_parser.set_defaults(func=CmdVersion)




dvc/commands/experiments/__init__.py
import argparse

from dvc.cli.utils import append_doc_link, fix_subparsers, hide_subparsers_from_help
from dvc.commands.experiments import (
    apply,
    branch,
    clean,
    diff,
    exec_run,
    ls,
    pull,
    push,
    queue_worker,
    remove,
    run,
    save,
    show,
)

SUB_COMMANDS = [
    apply,
    branch,
    clean,
    diff,
    exec_run,
    ls,
    pull,
    push,
    queue_worker,
    remove,
    run,
    save,
    show,
]


def add_parser(subparsers, parent_parser):
    EXPERIMENTS_HELP = "Commands to run and compare experiments."

    experiments_parser = subparsers.add_parser(
        "experiments",
        parents=[parent_parser],
        aliases=["exp"],
        description=append_doc_link(EXPERIMENTS_HELP, "exp"),
        formatter_class=argparse.RawDescriptionHelpFormatter,
        help=EXPERIMENTS_HELP,
    )

    experiments_subparsers = experiments_parser.add_subparsers(
        dest="cmd",
        help="Use `dvc experiments CMD --help` to display command-specific help.",
    )

    fix_subparsers(experiments_subparsers)
    for cmd in SUB_COMMANDS:
        cmd.add_parser(experiments_subparsers, parent_parser)
    hide_subparsers_from_help(experiments_subparsers)


def add_rev_selection_flags(
    experiments_subcmd_parser, command: str, default: bool = True
):
    experiments_subcmd_parser.add_argument(
        "-A",
        "--all-commits",
        action="store_true",
        default=False,
        help=(
            f"{command} all experiments in the repository "
            "(overrides `--rev` and `--num`)."
        ),
    )
    default_msg = " (HEAD by default)" if default else ""
    msg = (
        f"{command} experiments derived from the specified `<commit>` as "
        f"baseline{default_msg}."
    )
    experiments_subcmd_parser.add_argument(
        "--rev",
        type=str,
        action="append",
        default=None,
        help=msg,
        metavar="<commit>",
    )
    experiments_subcmd_parser.add_argument(
        "-n",
        "--num",
        type=int,
        default=1,
        dest="num",
        metavar="<num>",
        help=(
            f"{command} experiments from the last `num` commits "
            "(first parents) starting from the `--rev` baseline. "
            "Give a negative value to include all first-parent commits "
            "(similar to `git log -n`)."
        ),
    )




dvc/commands/experiments/apply.py
import argparse
import logging

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.ui import ui

logger = logging.getLogger(__name__)


class CmdExperimentsApply(CmdBase):
    def run(self):
        if not self.args.force:
            ui.write(
                "The --no-force option is deprecated and will be removed in a future"
                " DVC release. To revert the result of 'exp apply', run:\n"
                "\n\tgit reset --hard\n"
                "\tgit stash apply refs/exps/apply/stash\n"
            )
        self.repo.experiments.apply(self.args.experiment)

        return 0


def add_parser(experiments_subparsers, parent_parser):
    EXPERIMENTS_APPLY_HELP = "Apply the changes from an experiment to your workspace."
    experiments_apply_parser = experiments_subparsers.add_parser(
        "apply",
        parents=[parent_parser],
        description=append_doc_link(EXPERIMENTS_APPLY_HELP, "exp/apply"),
        help=EXPERIMENTS_APPLY_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    experiments_apply_parser.add_argument(
        "--no-force",
        action="store_false",
        dest="force",
        help="Fail if this command would overwrite conflicting changes.",
    )
    experiments_apply_parser.add_argument(
        "experiment", help="Experiment to be applied."
    ).complete = completion.EXPERIMENT
    experiments_apply_parser.set_defaults(func=CmdExperimentsApply)




dvc/commands/experiments/branch.py
import argparse
import logging

from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link

logger = logging.getLogger(__name__)


class CmdExperimentsBranch(CmdBase):
    def run(self):
        self.repo.experiments.branch(self.args.experiment, self.args.branch)

        return 0


def add_parser(experiments_subparsers, parent_parser):
    EXPERIMENTS_BRANCH_HELP = "Promote an experiment to a Git branch."
    experiments_branch_parser = experiments_subparsers.add_parser(
        "branch",
        parents=[parent_parser],
        description=append_doc_link(EXPERIMENTS_BRANCH_HELP, "exp/branch"),
        help=EXPERIMENTS_BRANCH_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    experiments_branch_parser.add_argument(
        "experiment", help="Experiment to be promoted."
    )
    experiments_branch_parser.add_argument(
        "branch",
        nargs="?",
        default=None,
        help=(
            "Optional name for the new Git branch. "
            "Defaults to '{experiment-name}-branch'."
        ),
    )
    experiments_branch_parser.set_defaults(func=CmdExperimentsBranch)




dvc/commands/experiments/clean.py
import argparse
import logging

from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link

logger = logging.getLogger(__name__)


class CmdExperimentsClean(CmdBase):
    def run(self):
        self.repo.experiments.clean()
        return 0


def add_parser(experiments_subparsers, parent_parser):
    EXPERIMENTS_CLEAN_HELP = "Cleanup experiments temporary internal files."
    experiments_clean_parser = experiments_subparsers.add_parser(
        "clean",
        parents=[parent_parser],
        description=append_doc_link(EXPERIMENTS_CLEAN_HELP, "exp/clean"),
        help=EXPERIMENTS_CLEAN_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    experiments_clean_parser.set_defaults(func=CmdExperimentsClean)




dvc/commands/experiments/diff.py
import argparse
import logging

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.commands.metrics import DEFAULT_PRECISION
from dvc.exceptions import DvcException
from dvc.ui import ui

logger = logging.getLogger(__name__)


class CmdExperimentsDiff(CmdBase):
    def run(self):
        try:
            diff = self.repo.experiments.diff(
                a_rev=self.args.a_rev,
                b_rev=self.args.b_rev,
                all=self.args.all,
                param_deps=self.args.param_deps,
            )
        except DvcException:
            logger.exception("failed to show experiments diff")
            return 1

        if self.args.json:
            ui.write_json(diff)
        elif diff:
            from dvc.compare import show_diff

            precision = self.args.precision or DEFAULT_PRECISION
            diffs = [("metrics", "Metric"), ("params", "Param")]
            for idx, (key, title) in enumerate(diffs):
                if idx:
                    # we are printing tables even in `--quiet` mode
                    # so we should also be printing the "table" separator
                    ui.write(force=True)

                show_diff(
                    diff[key],
                    title=title,
                    markdown=self.args.markdown,
                    no_path=self.args.no_path,
                    on_empty_diff="diff not supported",
                    precision=precision if key == "metrics" else None,
                    a_rev=self.args.a_rev,
                    b_rev=self.args.b_rev,
                )

        return 0


def add_parser(experiments_subparsers, parent_parser):
    EXPERIMENTS_DIFF_HELP = "Show changes between experiments."

    experiments_diff_parser = experiments_subparsers.add_parser(
        "diff",
        parents=[parent_parser],
        description=append_doc_link(EXPERIMENTS_DIFF_HELP, "exp/diff"),
        help=EXPERIMENTS_DIFF_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    experiments_diff_parser.add_argument(
        "a_rev", nargs="?", help="Old experiment to compare (defaults to HEAD)"
    ).complete = completion.EXPERIMENT
    experiments_diff_parser.add_argument(
        "b_rev",
        nargs="?",
        help="New experiment to compare (defaults to the current workspace)",
    ).complete = completion.EXPERIMENT
    experiments_diff_parser.add_argument(
        "--all",
        action="store_true",
        default=False,
        help="Show unchanged metrics/params as well.",
    )
    experiments_diff_parser.add_argument(
        "--param-deps",
        action="store_true",
        default=False,
        help="Show only params that are stage dependencies.",
    )
    experiments_diff_parser.add_argument(
        "--json",
        action="store_true",
        default=False,
        help="Show output in JSON format.",
    )
    experiments_diff_parser.add_argument(
        "--md",
        action="store_true",
        default=False,
        dest="markdown",
        help="Show tabulated output in the Markdown format (GFM).",
    )
    experiments_diff_parser.add_argument(
        "--no-path",
        action="store_true",
        default=False,
        help="Don't show metric/param path.",
    )
    experiments_diff_parser.add_argument(
        "--precision",
        type=int,
        help=(
            "Round metrics/params to `n` digits precision after the decimal "
            f"point. Rounds to {DEFAULT_PRECISION} digits by default."
        ),
        metavar="<n>",
    )
    experiments_diff_parser.set_defaults(func=CmdExperimentsDiff)




dvc/commands/experiments/exec_run.py
import logging

from dvc.cli.command import CmdBaseNoRepo

logger = logging.getLogger(__name__)


class CmdExecutorRun(CmdBaseNoRepo):
    """Run an experiment executor."""

    def run(self):
        from dvc.repo.experiments.executor.base import BaseExecutor, ExecutorInfo
        from dvc.utils.serialize import load_json

        info = ExecutorInfo.from_dict(load_json(self.args.infofile))
        BaseExecutor.reproduce(
            info=info,
            rev="",
            queue=None,
            log_level=logger.getEffectiveLevel(),
            infofile=self.args.infofile,
            copy_paths=self.args.copy_paths,
            message=self.args.message,
        )
        return 0


def add_parser(experiments_subparsers, parent_parser):
    EXEC_RUN_HELP = "Run an experiment executor."
    exec_run_parser = experiments_subparsers.add_parser(
        "exec-run",
        parents=[parent_parser],
        description=EXEC_RUN_HELP,
        add_help=False,
    )
    exec_run_parser.add_argument(
        "--infofile",
        help="Path to executor info file",
        default=None,
    )
    exec_run_parser.add_argument(
        "-C",
        "--copy-paths",
        action="append",
        default=[],
        help=(
            "List of ignored or untracked paths to copy into the temp directory."
            " Only used if `--temp` or `--queue` is specified."
        ),
    )
    exec_run_parser.add_argument(
        "-M",
        "--message",
        type=str,
        default=None,
        help="Custom commit message to use when committing the experiment.",
    )
    exec_run_parser.set_defaults(func=CmdExecutorRun)




dvc/commands/experiments/ls.py
import argparse
import logging

from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.exceptions import InvalidArgumentError
from dvc.ui import ui

logger = logging.getLogger(__name__)


class CmdExperimentsList(CmdBase):
    def run(self):
        name_only = self.args.name_only
        sha_only = self.args.sha_only
        git_remote = self.args.git_remote
        if sha_only and git_remote:
            raise InvalidArgumentError("--sha-only not supported with git_remote.")
        exps = self.repo.experiments.ls(
            all_commits=self.args.all_commits,
            rev=self.args.rev,
            num=self.args.num,
            git_remote=git_remote,
        )

        for baseline in exps:
            if not (name_only or sha_only):
                ui.write(f"{baseline[:7]}:")
            for exp_name, rev in exps[baseline]:
                if name_only:
                    ui.write(exp_name)
                elif sha_only:
                    ui.write(rev)
                elif rev:
                    ui.write(f"\t{rev[:7]} [{exp_name}]")
                else:
                    ui.write(f"\t{exp_name}")

        return 0


def add_parser(experiments_subparsers, parent_parser):
    from . import add_rev_selection_flags

    EXPERIMENTS_LIST_HELP = "List local and remote experiments."
    experiments_list_parser = experiments_subparsers.add_parser(
        "list",
        parents=[parent_parser],
        description=append_doc_link(EXPERIMENTS_LIST_HELP, "exp/list"),
        help=EXPERIMENTS_LIST_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    add_rev_selection_flags(experiments_list_parser, "List")
    display_group = experiments_list_parser.add_mutually_exclusive_group()
    display_group.add_argument(
        "--name-only",
        "--names-only",
        action="store_true",
        help="Only output experiment names (without SHAs or parent commits).",
    )
    display_group.add_argument(
        "--sha-only",
        "--shas-only",
        action="store_true",
        help="Only output experiment commit SHAs (without names or parent commits).",
    )
    experiments_list_parser.add_argument(
        "git_remote",
        nargs="?",
        default=None,
        help=(
            "Optional Git remote name or Git URL. "
            "If provided, experiments from the specified Git repository "
            " will be listed instead of local ones."
        ),
        metavar="<git_remote>",
    )
    experiments_list_parser.set_defaults(func=CmdExperimentsList)




dvc/commands/experiments/pull.py
import argparse
import logging

from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.exceptions import InvalidArgumentError
from dvc.ui import ui

logger = logging.getLogger(__name__)


class CmdExperimentsPull(CmdBase):
    def raise_error_if_all_disabled(self):
        if not any([self.args.experiment, self.args.all_commits, self.args.rev]):
            raise InvalidArgumentError(
                "Either provide an `experiment` argument, or use the "
                "`--rev` or `--all-commits` flag."
            )

    def run(self):
        self.raise_error_if_all_disabled()

        pulled_exps = self.repo.experiments.pull(
            self.args.git_remote,
            self.args.experiment,
            all_commits=self.args.all_commits,
            rev=self.args.rev,
            num=self.args.num,
            force=self.args.force,
            pull_cache=self.args.pull_cache,
            dvc_remote=self.args.dvc_remote,
            jobs=self.args.jobs,
            run_cache=self.args.run_cache,
        )

        if pulled_exps:
            ui.write(
                f"Pulled experiment '{pulled_exps}'",
                f"from Git remote '{self.args.git_remote}'.",
            )
        else:
            ui.write("No experiments to pull.")
        if not self.args.pull_cache:
            ui.write(
                "To pull cached outputs for this experiment"
                "from DVC remote storage,"
                "re-run this command without '--no-cache'."
            )

        return 0


def add_parser(experiments_subparsers, parent_parser):
    from . import add_rev_selection_flags

    EXPERIMENTS_PULL_HELP = "Pull an experiment from a Git remote."
    experiments_pull_parser = experiments_subparsers.add_parser(
        "pull",
        parents=[parent_parser],
        description=append_doc_link(EXPERIMENTS_PULL_HELP, "exp/pull"),
        help=EXPERIMENTS_PULL_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    add_rev_selection_flags(experiments_pull_parser, "Pull", False)
    experiments_pull_parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        help="Replace local experiment if it already exists.",
    )
    experiments_pull_parser.add_argument(
        "--no-cache",
        action="store_false",
        dest="pull_cache",
        help="Do not pull cached outputs for this experiment from DVC remote storage.",
    )
    experiments_pull_parser.add_argument(
        "-r",
        "--remote",
        dest="dvc_remote",
        metavar="<name>",
        help="Name of the DVC remote to use when pulling cached outputs.",
    )
    experiments_pull_parser.add_argument(
        "-j",
        "--jobs",
        type=int,
        metavar="<number>",
        help=(
            "Number of jobs to run simultaneously when pulling from DVC remote storage."
        ),
    )
    experiments_pull_parser.add_argument(
        "--run-cache",
        action="store_true",
        default=False,
        help="Pull run history for all stages.",
    )
    experiments_pull_parser.add_argument(
        "git_remote",
        help="Git remote name or Git URL.",
        metavar="<git_remote>",
    )
    experiments_pull_parser.add_argument(
        "experiment",
        nargs="*",
        default=None,
        help="Experiments to pull.",
        metavar="<experiment>",
    )
    experiments_pull_parser.set_defaults(func=CmdExperimentsPull)




dvc/commands/experiments/push.py
import argparse
import logging
from typing import Any, Dict

from dvc.cli import completion
from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.exceptions import InvalidArgumentError
from dvc.ui import ui

logger = logging.getLogger(__name__)


class CmdExperimentsPush(CmdBase):
    def raise_error_if_all_disabled(self):
        if not any([self.args.experiment, self.args.all_commits, self.args.rev]):
            raise InvalidArgumentError(
                "Either provide an `experiment` argument, or use the "
                "`--rev` or `--all-commits` flag."
            )

    @staticmethod
    def log_result(result: Dict[str, Any], remote: str):
        from dvc.utils import humanize

        def join_exps(exps):
            return humanize.join([f"[bold]{e}[/]" for e in exps])

        if diverged_exps := result.get("diverged"):
            exps = join_exps(diverged_exps)
            ui.error_write(
                f"[yellow]Local experiment {exps} has diverged "
                "from remote experiment with the same name.\n"
                "To override the remote experiment re-run with '--force'.",
                styled=True,
            )
        if uptodate_exps := result.get("up_to_date"):
            exps = join_exps(uptodate_exps)
            verb = "are" if len(uptodate_exps) > 1 else "is"
            ui.write(
                f"Experiment {exps} {verb} up to date on Git remote {remote!r}.",
                styled=True,
            )
        if pushed_exps := result.get("success"):
            exps = join_exps(pushed_exps)
            ui.write(f"Pushed experiment {exps} to Git remote {remote!r}.", styled=True)
        if not uptodate_exps and not pushed_exps:
            ui.write("No experiments to push.")

        if uploaded := result.get("uploaded"):
            stats = {"uploaded": uploaded}
            ui.write(humanize.get_summary(stats.items()))

        if project_url := result.get("url"):
            ui.rich_print(
                "View your experiments at", project_url, style="yellow", soft_wrap=True
            )

    def run(self):
        from dvc.repo.experiments.push import UploadError

        self.raise_error_if_all_disabled()

        try:
            result = self.repo.experiments.push(
                self.args.git_remote,
                self.args.experiment,
                all_commits=self.args.all_commits,
                rev=self.args.rev,
                num=self.args.num,
                force=self.args.force,
                push_cache=self.args.push_cache,
                dvc_remote=self.args.dvc_remote,
                jobs=self.args.jobs,
                run_cache=self.args.run_cache,
            )
        except UploadError as e:
            self.log_result(e.result, self.args.git_remote)
            raise

        self.log_result(result, self.args.git_remote)
        if not self.args.push_cache:
            ui.write(
                "To push cached outputs",
                (
                    "for this experiment to DVC remote storage,"
                    "re-run this command without '--no-cache'."
                ),
            )

        return 0


def add_parser(experiments_subparsers, parent_parser):
    from . import add_rev_selection_flags

    EXPERIMENTS_PUSH_HELP = "Push a local experiment to a Git remote."
    experiments_push_parser = experiments_subparsers.add_parser(
        "push",
        parents=[parent_parser],
        description=append_doc_link(EXPERIMENTS_PUSH_HELP, "exp/push"),
        help=EXPERIMENTS_PUSH_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    add_rev_selection_flags(experiments_push_parser, "Push", False)
    experiments_push_parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        help="Replace experiment in the Git remote if it already exists.",
    )
    experiments_push_parser.add_argument(
        "--no-cache",
        action="store_false",
        dest="push_cache",
        help="Do not push cached outputs for this experiment to DVC remote storage.",
    )
    experiments_push_parser.add_argument(
        "-r",
        "--remote",
        dest="dvc_remote",
        metavar="<name>",
        help="Name of the DVC remote to use when pushing cached outputs.",
    )
    experiments_push_parser.add_argument(
        "-j",
        "--jobs",
        type=int,
        metavar="<number>",
        help="Number of jobs to run simultaneously when pushing to DVC remote storage.",
    )
    experiments_push_parser.add_argument(
        "--run-cache",
        action="store_true",
        default=False,
        help="Push run history for all stages.",
    )
    experiments_push_parser.add_argument(
        "git_remote",
        help="Git remote name or Git URL.",
        metavar="<git_remote>",
    )
    experiments_push_parser.add_argument(
        "experiment",
        nargs="*",
        default=None,
        help="Experiments to push.",
        metavar="<experiment>",
    ).complete = completion.EXPERIMENT
    experiments_push_parser.set_defaults(func=CmdExperimentsPush)




dvc/commands/experiments/queue_worker.py
import logging

from dvc.cli.command import CmdBase

logger = logging.getLogger(__name__)


class CmdQueueWorker(CmdBase):
    """Run the exp queue worker."""

    def run(self):
        self.repo.experiments.celery_queue.worker.start(
            self.args.name, fsapp_clean=self.args.clean
        )
        return 0


def add_parser(experiments_subparsers, parent_parser):
    QUEUE_WORKER_HELP = "Run the exp queue worker."
    parser = experiments_subparsers.add_parser(
        "queue-worker",
        parents=[parent_parser],
        description=QUEUE_WORKER_HELP,
        add_help=False,
    )
    parser.add_argument("name", help="Celery worker name.")
    parser.add_argument(
        "--clean",
        action="store_true",
        help="Automatically cleanup celery broker on shutdown.",
    )
    parser.set_defaults(func=CmdQueueWorker)




dvc/commands/experiments/remove.py
import argparse
import logging

from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.exceptions import InvalidArgumentError
from dvc.ui import ui

logger = logging.getLogger(__name__)


class CmdExperimentsRemove(CmdBase):
    def check_arguments(self):
        if not any(
            [
                self.args.all_commits,
                self.args.rev,
                self.args.queue,
            ]
        ) ^ bool(self.args.experiment):
            raise InvalidArgumentError(
                "Either provide an `experiment` argument, or use the "
                "`--rev` or `--all-commits` or `--queue` flag."
            )

    def run(self):
        from dvc.utils import humanize

        self.check_arguments()

        removed = self.repo.experiments.remove(
            exp_names=self.args.experiment,
            all_commits=self.args.all_commits,
            rev=self.args.rev,
            num=self.args.num,
            queue=self.args.queue,
            git_remote=self.args.git_remote,
        )
        if removed:
            ui.write(f"Removed experiments: {humanize.join(map(repr, removed))}")
        else:
            ui.write("No experiments to remove.")

        return 0


def add_parser(experiments_subparsers, parent_parser):
    from . import add_rev_selection_flags

    EXPERIMENTS_REMOVE_HELP = "Remove experiments."
    experiments_remove_parser = experiments_subparsers.add_parser(
        "remove",
        parents=[parent_parser],
        description=append_doc_link(EXPERIMENTS_REMOVE_HELP, "exp/remove"),
        help=EXPERIMENTS_REMOVE_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    remove_group = experiments_remove_parser.add_mutually_exclusive_group()
    add_rev_selection_flags(experiments_remove_parser, "Remove", False)
    remove_group.add_argument(
        "--queue", action="store_true", help="Remove all queued experiments."
    )
    remove_group.add_argument(
        "-g",
        "--git-remote",
        metavar="<git_remote>",
        help="Name or URL of the Git remote to remove the experiment from",
    )
    experiments_remove_parser.add_argument(
        "experiment",
        nargs="*",
        help="Experiments to remove.",
        metavar="<experiment>",
    )
    experiments_remove_parser.set_defaults(func=CmdExperimentsRemove)




dvc/commands/experiments/run.py
import argparse
import logging

from dvc.cli.utils import append_doc_link
from dvc.commands.repro import CmdRepro
from dvc.commands.repro import add_arguments as add_repro_arguments

logger = logging.getLogger(__name__)


class CmdExperimentsRun(CmdRepro):
    def run(self):
        self.repo.experiments.run(
            name=self.args.name,
            queue=self.args.queue,
            run_all=self.args.run_all,
            jobs=self.args.jobs,
            params=self.args.set_param,
            tmp_dir=self.args.tmp_dir,
            machine=self.args.machine,
            copy_paths=self.args.copy_paths,
            message=self.args.message,
            **self._common_kwargs,
        )

        return 0


def add_parser(experiments_subparsers, parent_parser):
    EXPERIMENTS_RUN_HELP = "Run or resume an experiment."
    experiments_run_parser = experiments_subparsers.add_parser(
        "run",
        parents=[parent_parser],
        description=append_doc_link(EXPERIMENTS_RUN_HELP, "exp/run"),
        help=EXPERIMENTS_RUN_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    _add_run_common(experiments_run_parser)
    experiments_run_parser.set_defaults(func=CmdExperimentsRun)


def _add_run_common(parser):
    """Add common args for 'exp run'."""
    # inherit arguments from `dvc repro`
    add_repro_arguments(parser)
    parser.add_argument(
        "-n",
        "--name",
        default=None,
        help=(
            "Human-readable experiment name. If not specified, a name will "
            "be auto-generated."
        ),
        metavar="<name>",
    )
    parser.add_argument(
        "-S",
        "--set-param",
        action="append",
        default=[],
        help="Use the specified param value when reproducing pipelines.",
        metavar="[<filename>:]<param_name>=<param_value>",
    )
    parser.add_argument(
        "--queue",
        action="store_true",
        default=False,
        help="Stage this experiment in the run queue for future execution.",
    )
    parser.add_argument(
        "--run-all",
        action="store_true",
        default=False,
        help="Execute all experiments in the run queue. Implies --temp.",
    )
    parser.add_argument(
        "-j",
        "--jobs",
        type=int,
        default=1,
        help="Run the specified number of experiments at a time in parallel.",
        metavar="<number>",
    )
    parser.add_argument(
        "--temp",
        action="store_true",
        dest="tmp_dir",
        help=(
            "Run this experiment in a separate temporary directory instead of "
            "your workspace."
        ),
    )
    parser.add_argument(
        "--machine",
        default=None,
        help=argparse.SUPPRESS,
        # help=(
        #     "Run this experiment on the specified 'dvc machine' instance."
        # )
        # metavar="<name>",
    )
    parser.add_argument(
        "-C",
        "--copy-paths",
        action="append",
        default=[],
        help=(
            "List of ignored or untracked paths to copy into the temp directory."
            " Only used if `--temp` or `--queue` is specified."
        ),
    )
    parser.add_argument(
        "-m",
        "--message",
        type=str,
        default=None,
        help="Custom commit message to use when committing the experiment.",
    )




dvc/commands/experiments/save.py
import argparse
import logging

from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.exceptions import DvcException
from dvc.ui import ui

logger = logging.getLogger(__name__)


class CmdExperimentsSave(CmdBase):
    def run(self):
        try:
            ref = self.repo.experiments.save(
                name=self.args.name,
                force=self.args.force,
                include_untracked=self.args.include_untracked,
                message=self.args.message,
            )
        except DvcException:
            logger.exception("failed to save experiment")
            return 1

        if self.args.json:
            ui.write_json({"ref": ref})
        else:
            name = self.repo.experiments.get_exact_name([ref])[ref]
            ui.write(f"Experiment has been saved as: {name}")
            ui.write(
                "\nTo promote an experiment to a Git branch run:\n\n"
                "\tdvc exp branch <exp> <branch>\n"
            )

        return 0


def add_parser(experiments_subparsers, parent_parser):
    EXPERIMENTS_SAVE_HELP = "Save current workspace as an experiment."
    save_parser = experiments_subparsers.add_parser(
        "save",
        parents=[parent_parser],
        description=append_doc_link(EXPERIMENTS_SAVE_HELP, "exp/save"),
        help=EXPERIMENTS_SAVE_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    save_parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        default=False,
        help="Replace experiment if it already exists.",
    )
    save_parser.add_argument(
        "--json",
        action="store_true",
        default=False,
        help="Show output in JSON format.",
    )
    save_parser.add_argument(
        "-n",
        "--name",
        default=None,
        help=(
            "Human-readable experiment name. If not specified, a name will "
            "be auto-generated."
        ),
        metavar="<name>",
    )
    save_parser.add_argument(
        "-I",
        "--include-untracked",
        action="append",
        default=[],
        help="List of untracked paths to include in the experiment.",
        metavar="<path>",
    )
    save_parser.add_argument(
        "-M",
        "--message",
        type=str,
        default=None,
        help="Custom commit message to use when committing the experiment.",
    )
    save_parser.set_defaults(func=CmdExperimentsSave)




dvc/commands/experiments/show.py
import argparse
import logging
import re
from datetime import date, datetime
from typing import TYPE_CHECKING, Dict, Iterable

from funcy import lmap

from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.commands.metrics import DEFAULT_PRECISION
from dvc.exceptions import DvcException
from dvc.ui import ui
from dvc.utils.serialize import encode_exception

if TYPE_CHECKING:
    from dvc.compare import TabularData
    from dvc.ui import RichText

FILL_VALUE = "-"
FILL_VALUE_ERRORED = "!"


logger = logging.getLogger(__name__)


experiment_types = {
    "branch_commit": "‚îú‚îÄ‚îÄ",
    "branch_base": "‚îî‚îÄ‚îÄ",
    "baseline": "",
}


def prepare_exp_id(kwargs) -> "RichText":
    exp_name = kwargs["Experiment"]
    rev = kwargs["rev"]
    typ = kwargs.get("typ", "baseline")

    if typ == "baseline" or not exp_name:
        text = ui.rich_text(exp_name or rev)
    else:
        text = ui.rich_text.assemble(rev, " [", (exp_name, "bold"), "]")

    parent = kwargs.get("parent")
    suff = f" ({parent})" if parent else ""
    text.append(suff)

    tree = experiment_types[typ]
    pref = f"{tree} " if tree else ""
    return ui.rich_text(pref) + text


def baseline_styler(typ):
    return {"style": "bold"} if typ == "baseline" else {}


def show_experiments(  # noqa: C901
    td: "TabularData",
    headers: Dict[str, Iterable[str]],
    keep=None,
    drop=None,
    pager=True,
    csv=False,
    markdown=False,
    **kwargs,
):
    if keep:
        for col in td.keys():  # noqa: SIM118
            if re.match(keep, col):
                td.protect(col)

    for col in ("State", "Executor"):
        if td.is_empty(col):
            td.drop(col)

    row_styles = lmap(baseline_styler, td.column("typ"))

    if not csv:
        merge_headers = ["Experiment", "rev", "typ", "parent"]
        td.column("Experiment")[:] = map(prepare_exp_id, td.as_dict(merge_headers))
        td.drop(*merge_headers[1:])

    styles = {
        "Experiment": {"no_wrap": True, "header_style": "black on grey93"},
        "Created": {"header_style": "black on grey93"},
        "State": {"header_style": "black on grey93"},
        "Executor": {"header_style": "black on grey93"},
    }
    header_bg_colors = {
        "metrics": "cornsilk1",
        "params": "light_cyan1",
        "deps": "plum2",
    }
    styles.update(
        {
            header: {
                "justify": "right" if typ == "metrics" else "left",
                "header_style": f"black on {header_bg_colors[typ]}",
                "collapse": idx != 0,
                "no_wrap": typ == "metrics",
            }
            for typ, hs in headers.items()
            for idx, header in enumerate(hs)
        }
    )

    if kwargs.get("only_changed", False):
        td.drop_duplicates("cols", ignore_empty=False)

    cols_to_drop = set()
    if drop is not None:
        cols_to_drop = {col for col in td.keys() if re.match(drop, col)}  # noqa: SIM118
    td.drop(*cols_to_drop)

    td.render(
        pager=pager,
        borders="horizontals",
        rich_table=True,
        header_styles=styles,
        row_styles=row_styles,
        csv=csv,
        markdown=markdown,
    )


def _normalize_headers(names, count):
    return [
        name if count[name] == 1 else f"{path}:{name}"
        for path in names
        for name in names[path]
    ]


def _format_json(item):
    if isinstance(item, (date, datetime)):
        return item.isoformat()
    return encode_exception(item)


class CmdExperimentsShow(CmdBase):
    def run(self):
        from dvc.repo.experiments.show import tabulate

        try:
            exps = self.repo.experiments.show(
                all_branches=self.args.all_branches,
                all_tags=self.args.all_tags,
                all_commits=self.args.all_commits,
                hide_queued=self.args.hide_queued,
                hide_failed=self.args.hide_failed,
                revs=self.args.rev,
                num=self.args.num,
                sha_only=self.args.sha,
                param_deps=self.args.param_deps,
                fetch_running=self.args.fetch_running,
                force=self.args.force,
            )
        except DvcException:
            logger.exception("failed to show experiments")
            return 1

        if self.args.json:
            ui.write_json([exp.dumpd() for exp in exps], default=_format_json)
        else:
            precision = (
                self.args.precision or None if self.args.csv else DEFAULT_PRECISION
            )
            fill_value = "" if self.args.csv else FILL_VALUE
            iso = self.args.csv
            td, headers = tabulate(
                exps,
                precision=precision,
                fill_value=fill_value,
                iso=iso,
                sort_by=self.args.sort_by,
                sort_order=self.args.sort_order,
            )

            show_experiments(
                td,
                headers,
                keep=self.args.keep,
                drop=self.args.drop,
                sort_by=self.args.sort_by,
                sort_order=self.args.sort_order,
                pager=not self.args.no_pager,
                csv=self.args.csv,
                markdown=self.args.markdown,
                only_changed=self.args.only_changed,
            )
        return 0


def add_parser(experiments_subparsers, parent_parser):
    from . import add_rev_selection_flags

    EXPERIMENTS_SHOW_HELP = "Print experiments."
    experiments_show_parser = experiments_subparsers.add_parser(
        "show",
        parents=[parent_parser],
        description=append_doc_link(EXPERIMENTS_SHOW_HELP, "exp/show"),
        help=EXPERIMENTS_SHOW_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    add_rev_selection_flags(experiments_show_parser, "Show")
    experiments_show_parser.add_argument(
        "-a",
        "--all-branches",
        action="store_true",
        default=False,
        help="Show experiments derived from the tip of all Git branches.",
    )
    experiments_show_parser.add_argument(
        "-T",
        "--all-tags",
        action="store_true",
        default=False,
        help="Show experiments derived from all Git tags.",
    )
    experiments_show_parser.add_argument(
        "--no-pager",
        action="store_true",
        default=False,
        help="Do not pipe output into a pager.",
    )
    experiments_show_parser.add_argument(
        "--only-changed",
        action="store_true",
        default=False,
        help=(
            "Only show metrics/params with values varying "
            "across the selected experiments."
        ),
    )
    experiments_show_parser.add_argument(
        "--drop",
        help="Remove the columns matching the specified regex pattern.",
        metavar="<regex_pattern>",
    )
    experiments_show_parser.add_argument(
        "--keep",
        help="Preserve the columns matching the specified regex pattern.",
        metavar="<regex_pattern>",
    )
    experiments_show_parser.add_argument(
        "--param-deps",
        action="store_true",
        default=False,
        help="Show only params that are stage dependencies.",
    )
    experiments_show_parser.add_argument(
        "--sort-by",
        help="Sort related experiments by the specified metric or param.",
        metavar="<metric/param>",
    )
    experiments_show_parser.add_argument(
        "--sort-order",
        help="Sort order to use with --sort-by. Defaults to ascending ('asc').",
        choices=("asc", "desc"),
        default="asc",
    )
    experiments_show_parser.add_argument(
        "--sha",
        action="store_true",
        default=False,
        help="Always show git commit SHAs instead of branch/tag names.",
    )
    experiments_show_parser.add_argument(
        "--hide-failed",
        action="store_true",
        default=False,
        help="Hide failed experiments in the table.",
    )
    experiments_show_parser.add_argument(
        "--hide-queued",
        action="store_true",
        default=False,
        help="Hide queued experiments in the table.",
    )
    experiments_show_parser.add_argument(
        "--json",
        action="store_true",
        default=False,
        help="Print output in JSON format instead of a human-readable table.",
    )
    experiments_show_parser.add_argument(
        "--csv",
        action="store_true",
        default=False,
        help="Print output in csv format instead of a human-readable table.",
    )
    experiments_show_parser.add_argument(
        "--md",
        action="store_true",
        default=False,
        dest="markdown",
        help="Show tabulated output in the Markdown format (GFM).",
    )
    experiments_show_parser.add_argument(
        "--precision",
        type=int,
        help=(
            "Round metrics/params to `n` digits precision after the decimal "
            f"point. Rounds to {DEFAULT_PRECISION} digits by default."
        ),
        metavar="<n>",
    )
    experiments_show_parser.add_argument(
        "--no-fetch",
        dest="fetch_running",
        action="store_false",
        help=argparse.SUPPRESS,
    )
    experiments_show_parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        help="Force re-collection of experiments instead of loading from exp cache.",
    )
    experiments_show_parser.set_defaults(func=CmdExperimentsShow)




dvc/commands/ls/__init__.py
import argparse
import logging

from dvc.cli import completion
from dvc.cli.command import CmdBaseNoRepo
from dvc.cli.utils import append_doc_link
from dvc.commands.ls.ls_colors import LsColors
from dvc.exceptions import DvcException
from dvc.ui import ui

logger = logging.getLogger(__name__)


def _prettify(entries, with_color=False):
    if with_color:
        ls_colors = LsColors()
        fmt = ls_colors.format
    else:

        def fmt(entry):
            return entry["path"]

    return [fmt(entry) for entry in entries]


class CmdList(CmdBaseNoRepo):
    def run(self):
        from dvc.repo import Repo

        try:
            entries = Repo.ls(
                self.args.url,
                self.args.path,
                rev=self.args.rev,
                recursive=self.args.recursive,
                dvc_only=self.args.dvc_only,
            )
            if self.args.json:
                ui.write_json(entries)
            elif entries:
                entries = _prettify(entries, with_color=True)
                ui.write("\n".join(entries))
            return 0
        except DvcException:
            logger.exception("failed to list '%s'", self.args.url)
            return 1


def add_parser(subparsers, parent_parser):
    LIST_HELP = (
        "List repository contents, including files"
        " and directories tracked by DVC and by Git."
    )
    list_parser = subparsers.add_parser(
        "list",
        aliases=["ls"],
        parents=[parent_parser],
        description=append_doc_link(LIST_HELP, "list"),
        help=LIST_HELP,
        formatter_class=argparse.RawTextHelpFormatter,
    )
    list_parser.add_argument("url", help="Location of DVC repository to list")
    list_parser.add_argument(
        "-R",
        "--recursive",
        action="store_true",
        help="Recursively list files.",
    )
    list_parser.add_argument(
        "--dvc-only", action="store_true", help="Show only DVC outputs."
    )
    list_parser.add_argument(
        "--json",
        action="store_true",
        help="Show output in JSON format.",
    )
    list_parser.add_argument(
        "--rev",
        nargs="?",
        help="Git revision (e.g. SHA, branch, tag)",
        metavar="<commit>",
    )
    list_parser.add_argument(
        "path",
        nargs="?",
        help="Path to directory within the repository to list outputs for",
    ).complete = completion.DIR
    list_parser.set_defaults(func=CmdList)




dvc/commands/ls/ls_colors.py
import os


class LsColors:
    default = "rs=0:di=01;34:ex=01;32"

    def __init__(self, lscolors=None):
        self._extensions = {}
        self._codes = {}
        self._load(lscolors or os.environ.get("LS_COLORS") or LsColors.default)

    def _load(self, lscolors):
        for item in lscolors.split(":"):
            try:
                code, color = item.split("=", 1)
            except ValueError:
                continue
            if code.startswith("*."):
                self._extensions[code[1:]] = color
            else:
                self._codes[code] = color

    def format(self, entry):  # noqa: A003
        text = entry["path"]

        if entry.get("isout", False) and "out" in self._codes:
            return self._format(text, code="out")

        if entry.get("isdir", False):
            return self._format(text, code="di")

        if entry.get("isexec", False):
            return self._format(text, code="ex")

        _, ext = os.path.splitext(text)
        return self._format(text, ext=ext)

    def _format(self, text, code=None, ext=None):
        val = None
        if ext:
            val = self._extensions.get(ext, None)
        if code:
            val = self._codes.get(code, None)

        if not val:
            return text
        rs = self._codes.get("rs", 0)
        return f"\033[{val}m{text}\033[{rs}m"




dvc/commands/queue/__init__.py
import argparse

from dvc.cli.utils import append_doc_link, fix_subparsers
from dvc.commands.queue import kill, logs, remove, start, status, stop

SUB_COMMANDS = [
    start,
    stop,
    status,
    logs,
    remove,
    kill,
]


def add_parser(subparsers, parent_parser):
    QUEUE_HELP = "Commands to manage experiments queue."

    queue_parser = subparsers.add_parser(
        "queue",
        parents=[parent_parser],
        description=append_doc_link(QUEUE_HELP, "queue"),
        formatter_class=argparse.RawDescriptionHelpFormatter,
        help=QUEUE_HELP,
    )

    queue_subparsers = queue_parser.add_subparsers(
        dest="cmd",
        help="Use `dvc queue CMD --help` to display command-specific help.",
    )

    fix_subparsers(queue_subparsers)
    for cmd in SUB_COMMANDS:
        cmd.add_parser(queue_subparsers, parent_parser)




dvc/commands/queue/kill.py
import argparse
import logging

from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link

logger = logging.getLogger(__name__)


class CmdQueueKill(CmdBase):
    """Kill exp task in queue."""

    def run(self):
        self.repo.experiments.celery_queue.kill(
            revs=self.args.task, force=self.args.force
        )

        return 0


def add_parser(queue_subparsers, parent_parser):
    QUEUE_KILL_HELP = (
        "Gracefully interrupt running experiment queue tasks (equivalent to Ctrl-C)"
    )
    queue_kill_parser = queue_subparsers.add_parser(
        "kill",
        parents=[parent_parser],
        description=append_doc_link(QUEUE_KILL_HELP, "queue/kill"),
        help=QUEUE_KILL_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    queue_kill_parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        default=False,
        help="Forcefully and immediately kill running experiment queue tasks",
    )
    queue_kill_parser.add_argument(
        "task",
        nargs="*",
        help="Tasks in queue to kill.",
        metavar="<task>",
    )
    queue_kill_parser.set_defaults(func=CmdQueueKill)




dvc/commands/queue/logs.py
import argparse
import logging

from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link

logger = logging.getLogger(__name__)


class CmdQueueLogs(CmdBase):
    """Show output logs for a queued experiment."""

    def run(self):
        self.repo.experiments.celery_queue.logs(
            rev=self.args.task,
            encoding=self.args.encoding,
            follow=self.args.follow,
        )

        return 0


def add_parser(queue_subparsers, parent_parser):
    QUEUE_LOGS_HELP = (
        "Show output logs for running and completed experiment queue tasks."
    )
    queue_logs_parser = queue_subparsers.add_parser(
        "logs",
        parents=[parent_parser],
        description=append_doc_link(QUEUE_LOGS_HELP, "queue/logs"),
        help=QUEUE_LOGS_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    queue_logs_parser.add_argument(
        "-e",
        "--encoding",
        help="Text encoding for log output. Defaults to system locale encoding.",
        metavar="<encoding>",
    )
    queue_logs_parser.add_argument(
        "-f",
        "--follow",
        help=(
            "Attach to task and follow additional live output. Only "
            "applicable if the task is still running."
        ),
        action="store_true",
    )
    queue_logs_parser.add_argument(
        "task",
        help="Task to show.",
        metavar="<task>",
    )
    queue_logs_parser.set_defaults(func=CmdQueueLogs)




dvc/commands/queue/remove.py
import argparse
import logging

from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.exceptions import InvalidArgumentError
from dvc.ui import ui

logger = logging.getLogger(__name__)


class CmdQueueRemove(CmdBase):
    """Remove exp in queue."""

    def check_arguments(self):
        clear_flag = any(
            [
                self.args.all,
                self.args.queued,
                self.args.failed,
                self.args.success,
            ]
        )
        if not (clear_flag ^ bool(self.args.task)):
            raise InvalidArgumentError(
                "Either provide an `tasks` argument, or use the "
                "`--all`, `--queued`, `--failed`, `--success` flag."
            )

    def run(self):
        self.check_arguments()

        if self.args.all:
            self.args.queued = True
            self.args.failed = True
            self.args.success = True

        if self.args.queued or self.args.failed or self.args.success:
            removed_list = self.repo.experiments.celery_queue.clear(
                success=self.args.success,
                queued=self.args.queued,
                failed=self.args.failed,
            )
        else:
            removed_list = self.repo.experiments.celery_queue.remove(
                revs=self.args.task,
            )

        if removed_list:
            removed = ", ".join(removed_list)
            ui.write(f"Removed tasks in queue: {removed}")
        else:
            ui.write(f"No tasks found named {self.args.task}")

        return 0


def add_parser(queue_subparsers, parent_parser):
    QUEUE_REMOVE_HELP = "Remove queued and completed tasks from the queue."
    queue_remove_parser = queue_subparsers.add_parser(
        "remove",
        parents=[parent_parser],
        description=append_doc_link(QUEUE_REMOVE_HELP, "queue/remove"),
        help=QUEUE_REMOVE_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    queue_remove_parser.add_argument(
        "--all",
        action="store_true",
        help="Remove all queued and completed tasks from the queue.",
    )
    queue_remove_parser.add_argument(
        "--queued",
        action="store_true",
        help="Remove all queued tasks from the queue.",
    )
    queue_remove_parser.add_argument(
        "--success",
        action="store_true",
        help="Remove all successful tasks from the queue.",
    )
    queue_remove_parser.add_argument(
        "--failed",
        action="store_true",
        help="Remove all failed tasks from the queue.",
    )
    queue_remove_parser.add_argument(
        "task",
        nargs="*",
        help="Tasks to remove.",
        metavar="<task>",
    )
    queue_remove_parser.set_defaults(func=CmdQueueRemove)




dvc/commands/queue/start.py
import argparse
import logging

from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.ui import ui

logger = logging.getLogger(__name__)


class CmdQueueStart(CmdBase):
    """Start exp queue workers."""

    def run(self):
        started = self.repo.experiments.celery_queue.start_workers(self.args.jobs)

        suffix = "s" if started > 1 else ""
        ui.write(f"Started '{started}' new experiments task queue worker{suffix}.")

        return 0


def add_parser(queue_subparsers, parent_parser):
    QUEUE_START_HELP = "Start the experiments task queue worker."
    queue_start_parser = queue_subparsers.add_parser(
        "start",
        parents=[parent_parser],
        description=append_doc_link(QUEUE_START_HELP, "queue/start"),
        help=QUEUE_START_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    queue_start_parser.add_argument(
        "-j",
        "--jobs",
        type=int,
        default=1,
        help="Maximum number of concurrent queue workers to start. Defaults to 1.",
        metavar="<number>",
    )
    queue_start_parser.set_defaults(func=CmdQueueStart)




dvc/commands/queue/status.py
import argparse
import logging

from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.compare import TabularData
from dvc.ui import ui

logger = logging.getLogger(__name__)


class CmdQueueStatus(CmdBase):
    """Show queue task and worker status."""

    def run(self) -> int:
        from dvc.repo.experiments.show import format_time

        result = self.repo.experiments.celery_queue.status()
        if result:
            all_headers = ["Task", "Name", "Created", "Status"]
            td = TabularData(all_headers)
            for exp in result:
                created = format_time(exp.get("timestamp"))
                assert exp["rev"]
                assert exp["status"]
                td.append(
                    [
                        exp["rev"][:7],
                        exp.get("name") or "",
                        created,
                        exp["status"],
                    ]
                )
            td.render()
        else:
            ui.write("No experiment tasks in the queue.")
        ui.write()

        worker_status = self.repo.experiments.celery_queue.worker_status()
        active_count = len([name for name, task in worker_status.items() if task])
        idle_count = len(worker_status) - active_count

        ui.write(f"Worker status: {active_count} active, {idle_count} idle")

        return 0


def add_parser(queue_subparsers, parent_parser):
    QUEUE_STATUS_HELP = "Show the status of experiments queue tasks and workers."
    queue_status_parser = queue_subparsers.add_parser(
        "status",
        parents=[parent_parser],
        description=append_doc_link(QUEUE_STATUS_HELP, "queue/status"),
        help=QUEUE_STATUS_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    queue_status_parser.set_defaults(func=CmdQueueStatus)




dvc/commands/queue/stop.py
import argparse
import logging

from dvc.cli.command import CmdBase
from dvc.cli.utils import append_doc_link
from dvc.ui import ui

logger = logging.getLogger(__name__)


class CmdQueueStop(CmdBase):
    """Stop exp queue workers."""

    def run(self):
        self.repo.experiments.celery_queue.shutdown(kill=self.args.kill)

        if self.args.kill:
            ui.write(
                "All running tasks in the queue have been killed."
                "Queue workers are stopping."
            )
        else:
            ui.write("Queue workers will stop after running tasks finish.")

        return 0


def add_parser(queue_subparsers, parent_parser):
    QUEUE_STOP_HELP = "Stop all experiments task queue workers."
    queue_stop_parser = queue_subparsers.add_parser(
        "stop",
        parents=[parent_parser],
        description=append_doc_link(QUEUE_STOP_HELP, "queue/stop"),
        help=QUEUE_STOP_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    queue_stop_parser.add_argument(
        "--kill",
        action="store_true",
        help="Kill all running tasks before stopping the queue workers.",
    )
    queue_stop_parser.set_defaults(func=CmdQueueStop)




dvc/dependency/__init__.py
from collections import defaultdict
from typing import Any, Mapping, Set

from dvc.output import ARTIFACT_SCHEMA, DIR_FILES_SCHEMA, Output

from .base import Dependency
from .param import ParamsDependency
from .repo import RepoDependency

# NOTE: schema for dependencies is basically the same as for outputs, but
# without output-specific entries like 'cache' (whether or not output is
# cached, see -o and -O flags for `dvc run`) and 'metric' (whether or not
# output is a metrics file and how to parse it, see `-M` flag for `dvc run`).
SCHEMA: Mapping[str, Any] = {
    **ARTIFACT_SCHEMA,
    **RepoDependency.REPO_SCHEMA,
    **ParamsDependency.PARAM_SCHEMA,
    Output.PARAM_FILES: [DIR_FILES_SCHEMA],
}


def _get(stage, p, info, **kwargs):
    if info and info.get(RepoDependency.PARAM_REPO):
        repo = info.pop(RepoDependency.PARAM_REPO)
        return RepoDependency(repo, stage, p, info)

    if info and info.get(ParamsDependency.PARAM_PARAMS):
        params = info.pop(ParamsDependency.PARAM_PARAMS)
        return ParamsDependency(stage, p, params)

    return Dependency(stage, p, info, **kwargs)


def loadd_from(stage, d_list):
    ret = []
    for d in d_list:
        p = d.pop(Output.PARAM_PATH, None)
        files = d.pop(Output.PARAM_FILES, None)
        ret.append(_get(stage, p, d, files=files))
    return ret


def loads_from(stage, s_list, erepo=None, fs_config=None):
    assert isinstance(s_list, list)
    info = {RepoDependency.PARAM_REPO: erepo} if erepo else {}
    return [
        _get(
            stage,
            s,
            info.copy(),
            fs_config=fs_config,
        )
        for s in s_list
    ]


def _merge_params(s_list):
    d = defaultdict(list)
    default_file = ParamsDependency.DEFAULT_PARAMS_FILE

    # figure out completely tracked params file, and ignore specific keys
    wholly_tracked: Set[str] = set()
    for key in s_list:
        if not isinstance(key, dict):
            continue
        wholly_tracked.update(k for k, params in key.items() if not params)

    for key in s_list:
        if isinstance(key, str):
            if default_file not in wholly_tracked:
                d[default_file].append(key)
            continue

        if not isinstance(key, dict):
            msg = "Only list of str/dict is supported. Got: "
            msg += f"'{type(key).__name__}'."
            raise ValueError(msg)  # noqa: TRY004

        for k, params in key.items():
            if k in wholly_tracked:
                d[k] = []
                continue
            if not isinstance(params, list):
                msg = "Expected list of params for custom params file "
                msg += f"'{k}', got '{type(params).__name__}'."
                raise ValueError(msg)  # noqa: TRY004
            d[k].extend(params)
    return d


def loads_params(stage, s_list):
    d = _merge_params(s_list)
    return [ParamsDependency(stage, path, params) for path, params in d.items()]




dvc/dependency/base.py
from typing import Dict, Type

from dvc.exceptions import DvcException
from dvc.fs import download as fs_download
from dvc.output import Output


class DependencyDoesNotExistError(DvcException):
    def __init__(self, path):
        msg = f"dependency '{path}' does not exist"
        super().__init__(msg)


class DependencyIsNotFileOrDirError(DvcException):
    def __init__(self, path):
        msg = f"dependency '{path}' is not a file or directory"
        super().__init__(msg)


class DependencyIsStageFileError(DvcException):
    def __init__(self, path):
        super().__init__(f"DVC file '{path}' cannot be a dependency.")


class Dependency(Output):
    IS_DEPENDENCY = True

    DoesNotExistError: Type[DvcException] = DependencyDoesNotExistError
    IsNotFileOrDirError: Type[DvcException] = DependencyIsNotFileOrDirError
    IsStageFileError: Type[DvcException] = DependencyIsStageFileError

    def workspace_status(self) -> Dict[str, str]:
        if self.fs.version_aware:
            old_fs_path = self.fs_path
            try:
                self.fs_path = self.fs.path.version_path(self.fs_path, None)
                if self.changed_meta():
                    return {str(self): "update available"}
            finally:
                self.fs_path = old_fs_path
        return super().workspace_status()

    def update(self, rev=None):
        if self.fs.version_aware:
            self.fs_path = self.fs.path.version_path(self.fs_path, rev)
            self.meta = self.get_meta()
            self.def_path = self.fs.path.version_path(
                self.def_path, self.meta.version_id
            )
            self.fs_path = self.fs.path.version_path(self.fs_path, self.meta.version_id)

    def download(self, to, jobs=None):
        fs_download(self.fs, self.fs_path, to, jobs=jobs)

    def save(self):
        super().save()
        if self.fs.version_aware:
            self.fs_path = self.fs.path.version_path(self.fs_path, self.meta.version_id)

    def dumpd(self, **kwargs):
        if self.fs.version_aware:
            kwargs["with_files"] = True
        return super().dumpd(**kwargs)




dvc/dependency/param.py
import logging
import os
import typing
from collections import defaultdict
from typing import Dict

import dpath
from voluptuous import Any

from dvc.exceptions import DvcException
from dvc.utils.serialize import ParseError, load_path
from dvc_data.hashfile.hash_info import HashInfo

from .base import Dependency

logger = logging.getLogger(__name__)


class MissingParamsError(DvcException):
    pass


class MissingParamsFile(DvcException):
    pass


class ParamsIsADirectoryError(DvcException):
    pass


class BadParamFileError(DvcException):
    pass


class ParamsDependency(Dependency):
    PARAM_PARAMS = "params"
    PARAM_SCHEMA = {PARAM_PARAMS: Any(dict, list, None)}
    DEFAULT_PARAMS_FILE = "params.yaml"

    def __init__(self, stage, path, params=None, repo=None):
        self.params = list(params) if params else []
        hash_info = HashInfo()
        if isinstance(params, dict):
            hash_info = HashInfo(
                self.PARAM_PARAMS,
                params,  # type: ignore[arg-type]
            )
        repo = repo or stage.repo
        path = path or os.path.join(repo.root_dir, self.DEFAULT_PARAMS_FILE)
        super().__init__(stage, path, repo=repo)
        self.hash_info = hash_info

    def dumpd(self, **kwargs):
        ret = super().dumpd()
        if not self.hash_info:
            ret[self.PARAM_PARAMS] = self.params or {}
        return ret

    def fill_values(self, values=None):
        """Load params values dynamically."""
        if values is None:
            return

        info = {}
        if not self.params:
            info.update(values)
        for param in self.params:
            if param in values:
                info[param] = values[param]
        self.hash_info = HashInfo(
            self.PARAM_PARAMS,
            info,  # type: ignore[arg-type]
        )

    def read_params(
        self, flatten: bool = True, **kwargs: typing.Any
    ) -> Dict[str, typing.Any]:
        try:
            config = self.read_file()
        except MissingParamsFile:
            config = {}

        if not self.params:
            return config

        ret = {}
        if flatten:
            for param in self.params:
                try:
                    ret[param] = dpath.get(config, param, separator=".")
                except KeyError:
                    continue
            return ret

        from dpath import merge

        for param in self.params:
            merge(
                ret,
                dpath.search(config, param, separator="."),
                separator=".",
            )
        return ret

    def workspace_status(self):
        if not self.exists:
            return {str(self): "deleted"}
        if self.hash_info.value is None:
            return {str(self): "new"}

        from funcy import ldistinct

        status: Dict[str, Any] = defaultdict(dict)
        assert isinstance(self.hash_info.value, dict)
        info = self.hash_info.value if self.hash_info else {}
        actual = self.read_params()

        # NOTE: we want to preserve the order of params as specified in the
        # status. In case of tracking the whole file, the order is top-level
        # keys in the file and then the keys in the `info` from `dvc.lock`
        # (which are alphabetically sorted).
        params = self.params or ldistinct([*actual.keys(), *info.keys()])
        for param in params:
            if param not in actual:
                st = "deleted"
            elif param not in info:
                st = "new"
            elif actual[param] != info[param]:
                if (
                    isinstance(actual[param], tuple)
                    and list(actual[param]) == info[param]
                ):
                    continue
                st = "modified"
            else:
                continue

            status[str(self)][param] = st

        return status

    def status(self):
        return self.workspace_status()

    def validate_filepath(self):
        if not self.exists:
            raise MissingParamsFile(f"Parameters file '{self}' does not exist")
        if self.isdir():
            raise ParamsIsADirectoryError(
                f"'{self}' is a directory, expected a parameters file"
            )

    def read_file(self):
        self.validate_filepath()
        try:
            return load_path(self.fs_path, self.repo.fs)
        except ParseError as exc:
            raise BadParamFileError(f"Unable to read parameters from '{self}'") from exc

    def get_hash(self):
        info = self.read_params()

        missing_params = set(self.params) - set(info.keys())
        if missing_params:
            raise MissingParamsError(
                "Parameters '{}' are missing from '{}'.".format(
                    ", ".join(missing_params), self
                )
            )

        return HashInfo(self.PARAM_PARAMS, info)  # type: ignore[arg-type]

    def save(self):
        if not self.exists:
            raise self.DoesNotExistError(self)

        if not self.isfile() and not self.isdir():
            raise self.IsNotFileOrDirError(self)

        self.ignore()
        self.hash_info = self.get_hash()




dvc/dependency/repo.py
from typing import TYPE_CHECKING, Dict, Optional, Union

from voluptuous import Required

from dvc.utils import as_posix

from .base import Dependency

if TYPE_CHECKING:
    from dvc.fs import DVCFileSystem
    from dvc.stage import Stage


class RepoDependency(Dependency):
    PARAM_REPO = "repo"
    PARAM_URL = "url"
    PARAM_REV = "rev"
    PARAM_REV_LOCK = "rev_lock"

    REPO_SCHEMA = {
        PARAM_REPO: {
            Required(PARAM_URL): str,
            PARAM_REV: str,
            PARAM_REV_LOCK: str,
        }
    }

    def __init__(self, def_repo: Dict[str, str], stage: "Stage", *args, **kwargs):
        self.def_repo = def_repo
        super().__init__(stage, *args, **kwargs)

        self.fs = self._make_fs()
        self.fs_path = as_posix(self.def_path)

    def _parse_path(self, fs, fs_path):  # noqa: ARG002
        return None

    @property
    def is_in_repo(self):
        return False

    def __str__(self):
        return f"{self.def_path} ({self.def_repo[self.PARAM_URL]})"

    def workspace_status(self):
        current = self._make_fs(locked=True).repo.get_rev()
        updated = self._make_fs(locked=False).repo.get_rev()

        if current != updated:
            return {str(self): "update available"}

        return {}

    def status(self):
        return self.workspace_status()

    def save(self):
        rev = self.fs.repo.get_rev()
        if self.def_repo.get(self.PARAM_REV_LOCK) is None:
            self.def_repo[self.PARAM_REV_LOCK] = rev

    def dumpd(self, **kwargs) -> Dict[str, Union[str, Dict[str, str]]]:
        return {self.PARAM_PATH: self.def_path, self.PARAM_REPO: self.def_repo}

    def update(self, rev: Optional[str] = None):
        if rev:
            self.def_repo[self.PARAM_REV] = rev
        self.fs = self._make_fs(rev=rev, locked=False)
        self.def_repo[self.PARAM_REV_LOCK] = self.fs.repo.get_rev()

    def changed_checksum(self) -> bool:
        # From current repo point of view what describes RepoDependency is its
        # origin project url and rev_lock, and it makes RepoDependency
        # immutable, hence its impossible for checksum to change.
        return False

    def _make_fs(
        self, rev: Optional[str] = None, locked: bool = True
    ) -> "DVCFileSystem":
        from dvc.fs import DVCFileSystem

        config = {"cache": self.repo.config["cache"]}
        config["cache"]["dir"] = self.repo.cache.local.path

        return DVCFileSystem(
            url=self.def_repo[self.PARAM_URL],
            rev=rev or self._get_rev(locked=locked),
            subrepos=True,
            config=config,
        )

    def _get_rev(self, locked: bool = True):
        d = self.def_repo
        return (d.get(self.PARAM_REV_LOCK) if locked else None) or d.get(self.PARAM_REV)




dvc/fs/__init__.py
from urllib.parse import urlparse

from dvc_http import HTTPFileSystem, HTTPSFileSystem  # noqa: F401

from dvc.config import ConfigError as RepoConfigError
from dvc.config_schema import SCHEMA, Invalid

# pylint: disable=unused-import
from dvc_objects.fs import (  # noqa: F401
    LocalFileSystem,
    MemoryFileSystem,
    Schemes,
    generic,
    get_fs_cls,
    known_implementations,
    localfs,
    registry,
    system,
    utils,
)
from dvc_objects.fs.base import AnyFSPath, FileSystem  # noqa: F401
from dvc_objects.fs.errors import (  # noqa: F401
    AuthError,
    ConfigError,
    RemoteMissingDepsError,
)
from dvc_objects.fs.path import Path  # noqa: F401

from .callbacks import Callback
from .data import DataFileSystem  # noqa: F401
from .dvc import DVCFileSystem  # noqa: F401
from .git import GitFileSystem  # noqa: F401

known_implementations.update(
    {
        "dvc": {
            "class": "dvc.fs.dvc.DVCFileSystem",
            "err": "dvc is supported, but requires 'dvc' to be installed",
        },
        "git": {
            "class": "dvc.fs.git.GitFileSystem",
            "err": "git is supported, but requires 'dvc' to be installed",
        },
    }
)


# pylint: enable=unused-import


def download(fs, fs_path, to, jobs=None):
    with Callback.as_tqdm_callback(
        desc=f"Downloading {fs.path.name(fs_path)}",
        unit="files",
    ) as cb:
        fs.get(fs_path, to.fs_path, batch_size=jobs, callback=cb)


def parse_external_url(url, config=None):
    remote_config = dict(config) if config else {}
    remote_config["url"] = url
    fs_cls, fs_config, fs_path = get_cloud_fs(None, **remote_config)
    fs = fs_cls(**fs_config)
    return fs, fs_path


def get_fs_config(config, **kwargs):
    name = kwargs.get("name")
    if name:
        try:
            remote_conf = config["remote"][name.lower()]
        except KeyError:
            from dvc.config import RemoteNotFoundError

            raise RemoteNotFoundError(f"remote '{name}' doesn't exist")  # noqa: B904
    else:
        remote_conf = kwargs
    return _resolve_remote_refs(config, remote_conf)


def _resolve_remote_refs(config, remote_conf):
    # Support for cross referenced remotes.
    # This will merge the settings, shadowing base ref with remote_conf.
    # For example, having:
    #
    #       dvc remote add server ssh://localhost
    #       dvc remote modify server user root
    #       dvc remote modify server ask_password true
    #
    #       dvc remote add images remote://server/tmp/pictures
    #       dvc remote modify images user alice
    #       dvc remote modify images ask_password false
    #       dvc remote modify images password asdf1234
    #
    # Results on a config dictionary like:
    #
    #       {
    #           "url": "ssh://localhost/tmp/pictures",
    #           "user": "alice",
    #           "password": "asdf1234",
    #           "ask_password": False,
    #       }
    parsed = urlparse(remote_conf["url"])
    if parsed.scheme != "remote":
        return remote_conf

    base = get_fs_config(config, name=parsed.netloc)
    cls, _, _ = _get_cloud_fs(config, **base)
    relpath = parsed.path.lstrip("/").replace("/", cls.sep)
    url = cls.sep.join((base["url"], relpath))
    return {**base, **remote_conf, "url": url}


def get_cloud_fs(repo, **kwargs):
    repo_config = repo.config if repo else {}
    return _get_cloud_fs(repo_config, **kwargs)


def _get_cloud_fs(repo_config, **kwargs):
    core_config = repo_config.get("core", {})

    remote_conf = get_fs_config(repo_config, **kwargs)
    try:
        remote_conf = SCHEMA["remote"][str](remote_conf)  # type: ignore[index]
    except Invalid as exc:
        raise RepoConfigError(str(exc)) from None

    if "checksum_jobs" not in remote_conf:
        checksum_jobs = core_config.get("checksum_jobs")
        if checksum_jobs:
            remote_conf["checksum_jobs"] = checksum_jobs

    cls = get_fs_cls(remote_conf)

    url = remote_conf.pop("url")
    if cls.protocol in ["webdav", "webdavs"]:
        # For WebDAVFileSystem, provided url is the base path itself, so it
        # should be treated as being a root path.
        fs_path = cls.root_marker
    else:
        fs_path = cls._strip_protocol(url)  # pylint:disable=protected-access

    extras = cls._get_kwargs_from_urls(url)  # pylint:disable=protected-access
    conf = {**extras, **remote_conf}  # remote config takes priority
    return cls, conf, fs_path




dvc/fs/callbacks.py
# pylint: disable=unused-import
from contextlib import ExitStack
from typing import TYPE_CHECKING, Any, BinaryIO, Dict, Optional, Union

from dvc.utils.objects import cached_property
from dvc_objects.fs.callbacks import (  # noqa: F401
    DEFAULT_CALLBACK,
    Callback,
    TqdmCallback,
)

if TYPE_CHECKING:
    from rich.progress import TaskID

    from dvc.ui._rich_progress import RichTransferProgress


class RichCallback(Callback):
    def __init__(
        self,
        size: Optional[int] = None,
        value: int = 0,
        progress: Optional["RichTransferProgress"] = None,
        desc: Optional[str] = None,
        bytes: bool = False,  # noqa: A002, pylint: disable=redefined-builtin
        unit: Optional[str] = None,
        disable: bool = False,
        transient: bool = True,
    ) -> None:
        self._progress = progress
        self.disable = disable
        self._task_kwargs = {
            "description": desc or "",
            "bytes": bytes,
            "unit": unit,
            "total": size or 0,
            "visible": False,
            "progress_type": None if bytes else "summary",
        }
        self._transient = transient
        self._stack = ExitStack()
        super().__init__(size=size, value=value)

    @cached_property
    def progress(self) -> "RichTransferProgress":
        from dvc.ui import ui
        from dvc.ui._rich_progress import RichTransferProgress

        if self._progress is not None:
            return self._progress

        progress = RichTransferProgress(
            transient=self._transient,
            disable=self.disable,
            console=ui.error_console,
        )
        self._stack.enter_context(progress)
        return progress

    @cached_property
    def task(self) -> "TaskID":
        return self.progress.add_task(**self._task_kwargs)  # type: ignore[arg-type]

    def __enter__(self):
        return self

    def close(self):
        if self._transient:
            self.progress.clear_task(self.task)
        self._stack.close()

    def call(self, hook_name=None, **kwargs):  # noqa: ARG002
        self.progress.update(
            self.task,
            completed=self.value,
            total=self.size,
            visible=not self.disable,
        )

    def branch(
        self,
        path_1: Union[str, BinaryIO],
        path_2: str,
        kwargs: Dict[str, Any],
        child: Optional["Callback"] = None,
    ):
        child = child or RichCallback(
            progress=self.progress,
            desc=path_1 if isinstance(path_1, str) else path_2,
            bytes=True,
            transient=self._transient,
        )
        return super().branch(path_1, path_2, kwargs, child=child)




dvc/fs/data.py
import functools
import logging
import os
from typing import TYPE_CHECKING

from dvc.utils import as_posix
from dvc_objects.fs.base import FileSystem

if TYPE_CHECKING:
    from dvc_data.fs import DataFileSystem as _DataFileSystem


logger = logging.getLogger(__name__)


class DataFileSystem(FileSystem):
    protocol = "local"

    PARAM_CHECKSUM = "md5"

    def _prepare_credentials(self, **config):
        return config

    @functools.cached_property
    def fs(  # pylint: disable=invalid-overridden-method
        self,
    ) -> "_DataFileSystem":
        from dvc_data.fs import DataFileSystem as _DataFileSystem

        return _DataFileSystem(**self.fs_args)

    def isdvc(self, path, **kwargs):
        return self.fs.isdvc(path, **kwargs)

    def from_os_path(self, path):
        if os.path.isabs(path):
            path = os.path.splitdrive(path)[1]

        return as_posix(path)




dvc/fs/dvc.py
import errno
import functools
import logging
import ntpath
import os
import posixpath
import threading
from contextlib import suppress
from typing import TYPE_CHECKING, Any, Callable, Dict, Optional, Tuple, Type, Union

from fsspec.spec import AbstractFileSystem
from funcy import wrap_with

from dvc_objects.fs.base import FileSystem
from dvc_objects.fs.path import Path

from .data import DataFileSystem

if TYPE_CHECKING:
    from dvc.repo import Repo
    from dvc.types import StrPath

logger = logging.getLogger(__name__)

RepoFactory = Union[Callable[..., "Repo"], Type["Repo"]]
Key = Tuple[str, ...]


def as_posix(path: str) -> str:
    return path.replace(ntpath.sep, posixpath.sep)


# NOT the same as dvc.dvcfile.is_dvc_file()!
def _is_dvc_file(fname):
    from dvc.dvcfile import is_valid_filename
    from dvc.ignore import DvcIgnore

    return is_valid_filename(fname) or fname == DvcIgnore.DVCIGNORE_FILE


def _merge_info(repo, key, fs_info, dvc_info):
    from . import utils

    ret = {"repo": repo}

    if dvc_info:
        dvc_info["isout"] = any(
            (len(out_key) <= len(key) and key[: len(out_key)] == out_key)
            for out_key in repo.index.data_keys["repo"]
        )
        dvc_info["isdvc"] = dvc_info["isout"]
        ret["dvc_info"] = dvc_info
        ret["type"] = dvc_info["type"]
        ret["size"] = dvc_info["size"]
        if not fs_info and "md5" in dvc_info:
            ret["md5"] = dvc_info["md5"]

    if fs_info:
        ret["type"] = fs_info["type"]
        ret["size"] = fs_info["size"]
        isexec = False
        if fs_info["type"] == "file":
            isexec = utils.is_exec(fs_info["mode"])
        ret["isexec"] = isexec

    return ret


def _get_dvc_path(dvc_fs, subkey):
    return dvc_fs.path.join(*subkey) if subkey else ""


class _DVCFileSystem(AbstractFileSystem):  # pylint:disable=abstract-method
    cachable = False
    root_marker = "/"

    def __init__(
        self,
        url: Optional[str] = None,
        rev: Optional[str] = None,
        repo: Optional["Repo"] = None,
        subrepos: bool = False,
        repo_factory: Optional[RepoFactory] = None,
        **repo_kwargs: Any,
    ) -> None:
        """DVC + git-tracked files fs.

        Args:
            path (str, optional): URL or path to a DVC/Git repository.
                Defaults to a DVC repository in the current working directory.
                Both HTTP and SSH protocols are supported for remote Git repos
                (e.g. [user@]server:project.git).
            rev (str, optional): Any Git revision such as a branch or tag name,
                a commit hash or a dvc experiment name.
                Defaults to the default branch in case of remote repositories.
                In case of a local repository, if rev is unspecified, it will
                default to the working directory.
                If the repo is not a Git repo, this option is ignored.
            repo (:obj:`Repo`, optional): `Repo` instance.
            subrepos (bool): traverse to subrepos.
                By default, it ignores subrepos.
            repo_factory (callable): A function to initialize subrepo with.
                The default is `Repo`.

        Examples:
            - Opening a filesystem from repo in current working directory

            >>> fs = DVCFileSystem()

            - Opening a filesystem from local repository

            >>> fs = DVCFileSystem("path/to/local/repository")

            - Opening a remote repository

            >>> fs = DVCFileSystem(
            ...    "https://github.com/iterative/example-get-started",
            ...    rev="main",
            ... )
        """
        from pygtrie import Trie

        super().__init__()
        if repo is None:
            repo = self._make_repo(url=url, rev=rev, subrepos=subrepos, **repo_kwargs)
            assert repo is not None
            # pylint: disable=protected-access
            repo_factory = repo._fs_conf["repo_factory"]

        if not repo_factory:
            from dvc.repo import Repo

            self.repo_factory: RepoFactory = Repo
        else:
            self.repo_factory = repo_factory

        def _getcwd():
            relparts = ()
            assert repo is not None
            if repo.fs.path.isin(repo.fs.path.getcwd(), repo.root_dir):
                relparts = repo.fs.path.relparts(repo.fs.path.getcwd(), repo.root_dir)
            return self.root_marker + self.sep.join(relparts)

        self.path = Path(self.sep, getcwd=_getcwd)
        self.repo = repo
        self.hash_jobs = repo.fs.hash_jobs
        self._traverse_subrepos = subrepos

        self._subrepos_trie = Trie()
        """Keeps track of each and every path with the corresponding repo."""

        key = self._get_key(self.repo.root_dir)
        self._subrepos_trie[key] = repo

        self._datafss = {}
        """Keep a datafs instance of each repo."""

        if hasattr(repo, "dvc_dir"):
            self._datafss[key] = DataFileSystem(index=repo.index.data["repo"])

    def _get_key(self, path: "StrPath") -> Key:
        parts = self.repo.fs.path.relparts(path, self.repo.root_dir)
        if parts == (os.curdir,):
            return ()
        return parts

    def _get_key_from_relative(self, path) -> Key:
        parts = self.path.relparts(path, self.root_marker)
        if parts and parts[0] == os.curdir:
            return parts[1:]
        return parts

    def _from_key(self, parts: Key) -> str:
        return self.repo.fs.path.join(self.repo.root_dir, *parts)

    @property
    def repo_url(self):
        return self.repo.url

    @classmethod
    def _make_repo(cls, **kwargs) -> "Repo":
        from dvc.repo import Repo

        with Repo.open(uninitialized=True, **kwargs) as repo:
            return repo

    def _get_repo(self, key: Key) -> "Repo":
        """Returns repo that the path falls in, using prefix.

        If the path is already tracked/collected, it just returns the repo.

        Otherwise, it collects the repos that might be in the path's parents
        and then returns the appropriate one.
        """
        repo = self._subrepos_trie.get(key)
        if repo:
            return repo

        prefix_key, repo = self._subrepos_trie.longest_prefix(key)
        dir_keys = (key[:i] for i in range(len(prefix_key) + 1, len(key) + 1))
        self._update(dir_keys, starting_repo=repo)
        return self._subrepos_trie.get(key) or self.repo

    @wrap_with(threading.Lock())
    def _update(self, dir_keys, starting_repo):
        """Checks for subrepo in directories and updates them."""
        repo = starting_repo
        for key in dir_keys:
            d = self._from_key(key)
            if self._is_dvc_repo(d):
                repo = self.repo_factory(
                    d,
                    fs=self.repo.fs,
                    scm=self.repo.scm,
                    repo_factory=self.repo_factory,
                )
                self._datafss[key] = DataFileSystem(index=repo.index.data["repo"])
            self._subrepos_trie[key] = repo

    def _is_dvc_repo(self, dir_path):
        """Check if the directory is a dvc repo."""
        if not self._traverse_subrepos:
            return False

        from dvc.repo import Repo

        repo_path = self.repo.fs.path.join(dir_path, Repo.DVC_DIR)
        return self.repo.fs.isdir(repo_path)

    def _get_subrepo_info(
        self, key: Key
    ) -> Tuple["Repo", Optional[DataFileSystem], Key]:
        """
        Returns information about the subrepo the key is part of.
        """
        repo = self._get_repo(key)
        repo_key: Key
        if repo is self.repo:
            repo_key = ()
            subkey = key
        else:
            repo_key = self._get_key(repo.root_dir)
            subkey = key[len(repo_key) :]

        dvc_fs = self._datafss.get(repo_key)
        return repo, dvc_fs, subkey

    def _open(
        self, path, mode="rb", **kwargs
    ):  # pylint: disable=arguments-renamed, arguments-differ
        if mode != "rb":
            raise OSError(errno.EROFS, os.strerror(errno.EROFS))

        key = self._get_key_from_relative(path)
        fs_path = self._from_key(key)
        try:
            return self.repo.fs.open(fs_path, mode=mode)
        except FileNotFoundError:
            repo, dvc_fs, subkey = self._get_subrepo_info(key)
            if not dvc_fs:
                raise

        dvc_path = _get_dvc_path(dvc_fs, subkey)
        kw = {}
        if kwargs.get("cache_remote_stream", False):
            kw["cache_odb"] = repo.cache.local
        return dvc_fs.open(dvc_path, mode=mode, **kw)

    def isdvc(self, path, **kwargs) -> bool:
        """Is this entry dvc-tracked?"""
        try:
            return self.info(path).get("dvc_info", {}).get("isout", False)
        except FileNotFoundError:
            return False

    def ls(  # pylint: disable=arguments-differ # noqa: C901
        self, path, detail=True, dvc_only=False, **kwargs
    ):
        key = self._get_key_from_relative(path)
        repo, dvc_fs, subkey = self._get_subrepo_info(key)

        dvc_exists = False
        dvc_infos = {}
        if dvc_fs:
            dvc_path = _get_dvc_path(dvc_fs, subkey)
            with suppress(FileNotFoundError):
                for info in dvc_fs.ls(dvc_path, detail=True):
                    dvc_infos[dvc_fs.path.name(info["name"])] = info
            dvc_exists = bool(dvc_infos) or dvc_fs.exists(dvc_path)

        fs_exists = False
        fs_infos = {}
        ignore_subrepos = kwargs.get("ignore_subrepos", True)
        if not dvc_only:
            fs = self.repo.fs
            fs_path = self._from_key(key)
            try:
                for info in repo.dvcignore.ls(
                    fs, fs_path, detail=True, ignore_subrepos=ignore_subrepos
                ):
                    fs_infos[fs.path.name(info["name"])] = info
            except (FileNotFoundError, NotADirectoryError):
                pass

            fs_exists = bool(fs_infos) or fs.exists(fs_path)

        dvcfiles = kwargs.get("dvcfiles", False)

        infos = []
        paths = []
        names = set(dvc_infos.keys()) | set(fs_infos.keys())

        if not names and (dvc_exists or fs_exists):
            # broken symlink or TreeError
            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), path)

        for name in names:
            if not dvcfiles and _is_dvc_file(name):
                continue

            entry_path = self.path.join(path, name)
            info = _merge_info(
                repo, (*subkey, name), fs_infos.get(name), dvc_infos.get(name)
            )
            info["name"] = entry_path
            infos.append(info)
            paths.append(entry_path)

        if not detail:
            return paths

        return infos

    def info(self, path, **kwargs):
        key = self._get_key_from_relative(path)
        ignore_subrepos = kwargs.get("ignore_subrepos", True)
        return self._info(key, path, ignore_subrepos=ignore_subrepos)

    def _info(  # noqa: C901, PLR0912
        self, key, path, ignore_subrepos=True, check_ignored=True
    ):
        repo, dvc_fs, subkey = self._get_subrepo_info(key)

        dvc_info = None
        if dvc_fs:
            try:
                dvc_info = dvc_fs.fs.index.info(subkey)
                dvc_path = _get_dvc_path(dvc_fs, subkey)
                dvc_info["name"] = dvc_path
            except FileNotFoundError:
                pass

        fs_info = None
        fs = self.repo.fs
        fs_path = self._from_key(key)
        try:
            fs_info = fs.info(fs_path)
            if check_ignored and repo.dvcignore.is_ignored(
                fs, fs_path, ignore_subrepos=ignore_subrepos
            ):
                fs_info = None
        except (FileNotFoundError, NotADirectoryError):
            if not dvc_info:
                raise

        # NOTE: if some parent in fs_path turns out to be a file, it means
        # that the whole repofs branch doesn't exist.
        if dvc_info and not fs_info:
            for parent in fs.path.parents(fs_path):
                try:
                    if fs.info(parent)["type"] != "directory":
                        dvc_info = None
                        break
                except FileNotFoundError:
                    continue

        if not dvc_info and not fs_info:
            raise FileNotFoundError

        info = _merge_info(repo, subkey, fs_info, dvc_info)
        info["name"] = path
        return info

    def get_file(self, rpath, lpath, **kwargs):  # pylint: disable=arguments-differ
        key = self._get_key_from_relative(rpath)
        fs_path = self._from_key(key)
        try:
            return self.repo.fs.get_file(fs_path, lpath, **kwargs)
        except FileNotFoundError:
            _, dvc_fs, subkey = self._get_subrepo_info(key)
            if not dvc_fs:
                raise

        dvc_path = _get_dvc_path(dvc_fs, subkey)
        return dvc_fs.get_file(dvc_path, lpath, **kwargs)


class DVCFileSystem(FileSystem):
    protocol = "local"
    PARAM_CHECKSUM = "md5"

    def _prepare_credentials(self, **config) -> Dict[str, Any]:
        return config

    @functools.cached_property
    # pylint: disable-next=invalid-overridden-method
    def fs(self) -> "DVCFileSystem":
        return _DVCFileSystem(**self.fs_args)

    def isdvc(self, path, **kwargs) -> bool:
        return self.fs.isdvc(path, **kwargs)

    @property
    def path(self) -> Path:  # pylint: disable=invalid-overridden-method
        return self.fs.path

    @property
    def repo(self) -> "Repo":
        return self.fs.repo

    @property
    def repo_url(self) -> str:
        return self.fs.repo_url

    def from_os_path(self, path: str) -> str:
        if os.path.isabs(path):
            path = os.path.relpath(path, self.repo.root_dir)

        return as_posix(path)




dvc/fs/git.py
import functools
from typing import TYPE_CHECKING, Any, Optional

from . import FileSystem

if TYPE_CHECKING:
    from scmrepo.fs import GitFileSystem as FsspecGitFileSystem
    from scmrepo.git.objects import GitTrie

    from dvc.scm import Git


class GitFileSystem(FileSystem):  # pylint:disable=abstract-method
    """Proxies the repo file access methods to Git objects"""

    protocol = "local"
    PARAM_CHECKSUM = "md5"

    def __init__(
        self,
        path: Optional[str] = None,
        rev: Optional[str] = None,
        scm: Optional["Git"] = None,
        trie: Optional["GitTrie"] = None,
        **kwargs: Any,
    ) -> None:
        from dvc.scm import resolve_rev

        super().__init__()
        self.fs_args.update(
            {
                "path": path,
                "rev": rev,
                "scm": scm,
                "trie": trie,
                "rev_resolver": resolve_rev,
                **kwargs,
            }
        )

    @functools.cached_property
    def fs(  # pylint: disable=invalid-overridden-method
        self,
    ) -> "FsspecGitFileSystem":
        from scmrepo.fs import GitFileSystem as FsspecGitFileSystem

        return FsspecGitFileSystem(**self.fs_args)

    @functools.cached_property
    def path(self):  # pylint: disable=invalid-overridden-method
        return self.fs.path

    @property
    def rev(self) -> str:
        return self.fs.rev

    def ls(self, path, detail=True, **kwargs):
        return self.fs.ls(path, detail=detail, **kwargs) or []




dvc/machine/__init__.py
import logging
import os
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Iterable,
    Iterator,
    Mapping,
    Optional,
    Tuple,
    Type,
)

from dvc.exceptions import DvcException

if TYPE_CHECKING:
    from dvc.repo import Repo
    from dvc.types import StrPath

    from .backend.base import BaseMachineBackend

    BackendCls = Type[BaseMachineBackend]

logger = logging.getLogger(__name__)


RESERVED_NAMES = {"local", "localhost"}

DEFAULT_STARTUP_SCRIPT = """#!/bin/bash
sudo apt-get update
sudo apt-get install --yes python3 python3-pip
python3 -m pip install --upgrade pip

pushd /etc/apt/sources.list.d
sudo wget https://dvc.org/deb/dvc.list
sudo apt-get update
sudo apt-get install --yes dvc
popd

sudo echo "OK" > /var/log/dvc-machine-init.log
"""


def validate_name(name: str):
    from dvc.exceptions import InvalidArgumentError

    name = name.lower()
    if name in RESERVED_NAMES:
        raise InvalidArgumentError(
            f"Machine name '{name}' is reserved for internal DVC use."
        )


class MachineBackends(Mapping):
    try:
        from .backend.terraform import TerraformBackend
    except ImportError:
        TerraformBackend = None  # type: ignore[assignment, misc]

    DEFAULT: Dict[str, Optional["BackendCls"]] = {
        "terraform": TerraformBackend,
    }

    def __getitem__(self, key: str) -> "BaseMachineBackend":
        """Lazily initialize backends and cache it afterwards"""
        initialized = self.initialized.get(key)
        if not initialized:
            backend = self.backends[key]
            initialized = backend(os.path.join(self.tmp_dir, key), **self.kwargs)
            self.initialized[key] = initialized
        return initialized

    def __init__(
        self,
        selected: Optional[Iterable[str]],
        tmp_dir: "StrPath",
        **kwargs,
    ) -> None:
        selected = selected or list(self.DEFAULT)
        self.backends: Dict[str, "BackendCls"] = {}
        for key in selected:
            cls = self.DEFAULT.get(key)
            if cls is None:
                raise DvcException(
                    f"'dvc machine' backend '{key}' is missing required "
                    "dependencies. Install them with:\n"
                    f"\tpip install dvc[{key}]"
                )
            self.backends[key] = cls

        self.initialized: Dict[str, "BaseMachineBackend"] = {}

        self.tmp_dir = tmp_dir
        self.kwargs = kwargs

    def __iter__(self):
        return iter(self.backends)

    def __len__(self) -> int:
        return len(self.backends)

    def close_initialized(self) -> None:
        for backend in self.initialized.values():
            backend.close()


class MachineManager:
    """Class that manages dvc cloud machines.

    Args:
        repo (dvc.repo.Repo): repo instance that belongs to the repo that
            we are working on.

    Raises:
        config.ConfigError: thrown when config has invalid format.
    """

    CLOUD_BACKENDS = {
        "aws": "terraform",
        "azure": "terraform",
    }

    def __init__(
        self, repo: "Repo", backends: Optional[Iterable[str]] = None, **kwargs
    ):
        self.repo = repo
        assert self.repo.tmp_dir
        tmp_dir = os.path.join(self.repo.tmp_dir, "machine")
        self.backends = MachineBackends(backends, tmp_dir=tmp_dir, **kwargs)

    def get_config_and_backend(
        self,
        name: Optional[str] = None,
    ) -> Tuple[dict, "BaseMachineBackend"]:
        from dvc.config import NoMachineError

        if not name:
            name = self.repo.config["core"].get("machine")

        if name:
            config = self._get_config(name=name)
            backend = self._get_backend(config["cloud"])
            return config, backend

        if bool(self.repo.config["machine"]):
            error_msg = (
                "no machine specified. Setup default machine with\n"
                "    dvc machine default <name>\n"
            )
        else:
            error_msg = (
                "no machine specified. Create a default machine with\n"
                "    dvc machine add -d <name> <cloud>"
            )

        raise NoMachineError(error_msg)

    def _get_config(self, **kwargs):
        config = self.repo.config
        name = kwargs.get("name")
        if name:
            try:
                conf = config["machine"][name.lower()]
                conf["name"] = name
            except KeyError:
                from dvc.config import MachineNotFoundError

                raise MachineNotFoundError(  # noqa: B904
                    f"machine '{name}' doesn't exist"
                )
        else:
            conf = kwargs
        return conf

    def _get_backend(self, cloud: str) -> "BaseMachineBackend":
        from dvc.config import NoMachineError

        try:
            backend = self.CLOUD_BACKENDS[cloud]
            return self.backends[backend]
        except KeyError:
            raise NoMachineError(  # noqa: B904
                f"Machine platform '{cloud}' unsupported"
            )

    def create(self, name: Optional[str]):
        """Create and start the specified machine instance."""
        config, backend = self.get_config_and_backend(name)
        if "startup_script" in config:
            with open(config["startup_script"], encoding="utf-8") as fobj:
                startup_script = fobj.read()
        else:
            startup_script = DEFAULT_STARTUP_SCRIPT
        config["startup_script"] = startup_script
        config.pop("setup_script", None)
        return backend.create(**config)

    def destroy(self, name: Optional[str]):
        """Destroy the specified machine instance."""
        config, backend = self.get_config_and_backend(name)
        return backend.destroy(**config)

    def get_sshfs(self, name: Optional[str]):
        config, backend = self.get_config_and_backend(name)
        return backend.get_sshfs(**config)

    def run_shell(self, name: Optional[str]):
        config, backend = self.get_config_and_backend(name)
        return backend.run_shell(**config)

    def status(self, name: str) -> Iterator[Dict[Any, Any]]:
        config, backend = self.get_config_and_backend(name)
        return backend.instances(**config)

    def rename(self, name: str, new: str):
        """move configuration to a new location if the machine is running."""
        config, backend = self.get_config_and_backend(name)
        return backend.rename(new=new, **config)

    def get_executor_kwargs(self, name: Optional[str]):
        config, backend = self.get_config_and_backend(name)
        return backend.get_executor_kwargs(**config)

    def get_setup_script(self, name: Optional[str]) -> Optional[str]:
        config, _backend = self.get_config_and_backend(name)
        return config.get("setup_script")




dvc/machine/backend/__init__.py




dvc/machine/backend/base.py
from abc import ABC, abstractmethod
from contextlib import contextmanager
from typing import TYPE_CHECKING, Iterator, Optional

if TYPE_CHECKING:
    from dvc_ssh import SSHFileSystem

    from dvc.types import StrPath


class BaseMachineBackend(ABC):
    def __init__(self, tmp_dir: "StrPath", **kwargs):
        self.tmp_dir = tmp_dir

    @abstractmethod
    def create(self, name: Optional[str] = None, **config):
        """Create and start an instance of the specified machine."""

    @abstractmethod
    def destroy(self, name: Optional[str] = None, **config):
        """Stop and destroy all instances of the specified machine."""

    @abstractmethod
    def instances(self, name: Optional[str] = None, **config) -> Iterator[dict]:
        """Iterate over status of all instances of the specified machine."""

    def close(self):  # noqa: B027
        pass

    @abstractmethod
    def run_shell(self, name: Optional[str] = None, **config):
        """Spawn an interactive SSH shell for the specified machine."""

    @abstractmethod
    def get_executor_kwargs(self, name: str, **config) -> dict:
        """Return SSHExecutor kwargs which can be used for DVC
        experiment/pipeline execution on the specified machine.
        """

    @abstractmethod
    @contextmanager
    def get_sshfs(
        self, name: Optional[str] = None, **config
    ) -> Iterator["SSHFileSystem"]:
        """Return an sshfs instance for the default directory on the
        specified machine."""

    @abstractmethod
    def rename(self, name: str, new: str, **config):
        """Rename a machine instance."""




dvc/machine/backend/terraform.py
import os
from contextlib import contextmanager
from functools import partial, partialmethod
from typing import TYPE_CHECKING, Iterator, Optional

from dvc_ssh import DEFAULT_PORT, SSHFileSystem

from dvc.exceptions import DvcException

from .base import BaseMachineBackend

if TYPE_CHECKING:
    from dvc.types import StrPath


@contextmanager
def _sshfs(resource: dict):
    from tpi import TerraformProviderIterative

    with TerraformProviderIterative.pemfile(resource) as pem:
        fs = SSHFileSystem(
            host=resource["instance_ip"],
            user="ubuntu",
            keyfile=pem,
        )
        yield fs


class TerraformBackend(BaseMachineBackend):
    def __init__(self, tmp_dir: "StrPath", **kwargs):
        super().__init__(tmp_dir, **kwargs)
        os.makedirs(tmp_dir, exist_ok=True)

    @contextmanager
    def make_tpi(self, name: str):
        from tpi import TPIError
        from tpi.terraform import TerraformBackend as TPIBackend

        try:
            working_dir = os.path.join(self.tmp_dir, name)
            os.makedirs(working_dir, exist_ok=True)
            yield TPIBackend(working_dir=working_dir)
        except TPIError as exc:
            raise DvcException("TPI operation failed") from exc

    def _tpi_func(self, fname, name: Optional[str] = None, **config):
        from tpi import TPIError

        assert name
        with self.make_tpi(name) as tpi:
            func = getattr(tpi, fname)
            try:
                return func(name=name, **config)
            except TPIError as exc:
                raise DvcException(f"TPI {fname} failed") from exc

    create = partialmethod(_tpi_func, "create")  # type: ignore[assignment]
    destroy = partialmethod(_tpi_func, "destroy")  # type: ignore[assignment]
    instances = partialmethod(_tpi_func, "instances")  # type: ignore[assignment]
    run_shell = partialmethod(_tpi_func, "run_shell")  # type: ignore[assignment]

    def get_executor_kwargs(self, name: str, **config) -> dict:
        with self.make_tpi(name) as tpi:
            resource = tpi.default_resource(name)
        return {
            "host": resource["instance_ip"],
            "port": DEFAULT_PORT,
            "username": "ubuntu",
            "fs_factory": partial(_sshfs, dict(resource)),
        }

    @contextmanager
    def get_sshfs(  # pylint: disable=unused-argument
        self, name: Optional[str] = None, **config
    ) -> Iterator["SSHFileSystem"]:
        assert name
        with self.make_tpi(name) as tpi:
            resource = tpi.default_resource(name)
        with _sshfs(resource) as fs:
            yield fs

    def rename(self, name: str, new: str, **config):
        """rename a dvc machine instance to a new name"""
        import shutil

        mtype = "iterative_machine"

        new_dir = os.path.join(self.tmp_dir, new)
        old_dir = os.path.join(self.tmp_dir, name)
        if os.path.exists(new_dir):
            raise DvcException(f"rename failed: path {new_dir} already exists")

        if not os.path.exists(old_dir):
            return

        with self.make_tpi(name) as tpi:
            tpi.state_mv(f"{mtype}.{name}", f"{mtype}.{new}", **config)

        shutil.move(old_dir, new_dir)




dvc/parsing/__init__.py
import logging
import os
from collections.abc import Mapping, Sequence
from copy import deepcopy
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    List,
    NamedTuple,
    Optional,
    Tuple,
    Type,
    Union,
)

from funcy import collecting, first, isa, join, reraise

from dvc.exceptions import DvcException
from dvc.parsing.interpolate import ParseError
from dvc.utils.objects import cached_property

from .context import (
    Context,
    ContextError,
    KeyNotInContext,
    MergeError,
    Node,
    VarsAlreadyLoaded,
)
from .interpolate import (
    check_recursive_parse_errors,
    is_interpolated_string,
    recurse,
    to_str,
)

if TYPE_CHECKING:
    from typing import NoReturn

    from dvc.repo import Repo
    from dvc.types import DictStrAny

    from .context import SeqOrMap


logger = logging.getLogger(__name__)

STAGES_KWD = "stages"
VARS_KWD = "vars"
WDIR_KWD = "wdir"
PARAMS_KWD = "params"
FOREACH_KWD = "foreach"
DO_KWD = "do"

DEFAULT_PARAMS_FILE = "params.yaml"

JOIN = "@"


class ResolveError(DvcException):
    pass


class EntryNotFound(DvcException):
    pass


def _format_preamble(msg: str, path: str, spacing: str = " ") -> str:
    return f"failed to parse {msg} in '{path}':{spacing}"


def format_and_raise(exc: Exception, msg: str, path: str) -> "NoReturn":
    spacing = (
        "\n" if isinstance(exc, (ParseError, MergeError, VarsAlreadyLoaded)) else " "
    )
    message = _format_preamble(msg, path, spacing) + str(exc)

    # FIXME: cannot reraise because of how we log "cause" of the exception
    # the error message is verbose, hence need control over the spacing
    _reraise_err(ResolveError, message, from_exc=exc)


def _reraise_err(
    exc_cls: Type[Exception], *args, from_exc: Optional[Exception] = None
) -> "NoReturn":
    err = exc_cls(*args)
    if from_exc and logger.isEnabledFor(logging.DEBUG):
        raise err from from_exc
    raise err


def check_syntax_errors(
    definition: "DictStrAny", name: str, path: str, where: str = "stages"
):
    for key, d in definition.items():
        try:
            check_recursive_parse_errors(d)
        except ParseError as exc:
            format_and_raise(exc, f"'{where}.{name}.{key}'", path)


def is_map_or_seq(data: Any) -> bool:
    _is_map_or_seq = isa(Mapping, Sequence)
    return not isinstance(data, str) and _is_map_or_seq(data)


def split_foreach_name(name: str) -> Tuple[str, Optional[str]]:
    group, *keys = name.rsplit(JOIN, maxsplit=1)
    return group, first(keys)


def check_interpolations(data: "DictStrAny", where: str, path: str):
    def func(s: "DictStrAny") -> None:
        if is_interpolated_string(s):
            raise ResolveError(
                _format_preamble(f"'{where}'", path) + "interpolating is not allowed"
            )

    return recurse(func)(data)


Definition = Union["ForeachDefinition", "EntryDefinition"]


def make_definition(
    resolver: "DataResolver", name: str, definition: "DictStrAny", **kwargs
) -> Definition:
    args = resolver, resolver.context, name, definition
    if FOREACH_KWD in definition:
        return ForeachDefinition(*args, **kwargs)
    return EntryDefinition(*args, **kwargs)


class DataResolver:
    def __init__(self, repo: "Repo", wdir: str, d: dict):
        self.fs = fs = repo.fs

        if os.path.isabs(wdir):
            wdir = fs.path.relpath(wdir)
            wdir = "" if wdir == os.curdir else wdir

        self.wdir = wdir
        self.relpath = fs.path.normpath(fs.path.join(self.wdir, "dvc.yaml"))

        vars_ = d.get(VARS_KWD, [])
        check_interpolations(vars_, VARS_KWD, self.relpath)
        self.context: Context = Context()

        try:
            args = fs, vars_, wdir  # load from `vars` section
            self.context.load_from_vars(*args, default=DEFAULT_PARAMS_FILE)
        except ContextError as exc:
            format_and_raise(exc, "'vars'", self.relpath)

        # we use `tracked_vars` to keep a dictionary of used variables
        # by the interpolated entries.
        self.tracked_vars: Dict[str, Mapping] = {}

        stages_data = d.get(STAGES_KWD, {})
        # we wrap the definitions into ForeachDefinition and EntryDefinition,
        # that helps us to optimize, cache and selectively load each one of
        # them as we need, and simplify all of this DSL/parsing logic.
        self.definitions: Dict[str, Definition] = {
            name: make_definition(self, name, definition)
            for name, definition in stages_data.items()
        }

    def resolve_one(self, name: str):
        group, key = split_foreach_name(name)

        if not self._has_group_and_key(group, key):
            raise EntryNotFound(f"Could not find '{name}'")

        # all of the checks for `key` not being None for `ForeachDefinition`
        # and/or `group` not existing in the `interim`, etc. should be
        # handled by the `self.has_key()` above.
        definition = self.definitions[group]
        if isinstance(definition, EntryDefinition):
            return definition.resolve()

        assert key
        return definition.resolve_one(key)

    def resolve(self):
        """Used for testing purposes, otherwise use resolve_one()."""
        data = join(map(self.resolve_one, self.get_keys()))
        logger.trace("Resolved dvc.yaml:\n%s", data)  # type: ignore[attr-defined]
        return {STAGES_KWD: data}

    def has_key(self, key: str):
        return self._has_group_and_key(*split_foreach_name(key))

    def _has_group_and_key(self, group: str, key: Optional[str] = None):
        try:
            definition = self.definitions[group]
        except KeyError:
            return False

        if key:
            return isinstance(definition, ForeachDefinition) and definition.has_member(
                key
            )
        return not isinstance(definition, ForeachDefinition)

    @collecting
    def get_keys(self):
        for name, definition in self.definitions.items():
            if isinstance(definition, ForeachDefinition):
                yield from definition.get_generated_names()
                continue
            yield name

    def track_vars(self, name: str, vars_) -> None:
        self.tracked_vars[name] = vars_


class EntryDefinition:
    def __init__(
        self,
        resolver: DataResolver,
        context: Context,
        name: str,
        definition: "DictStrAny",
        where: str = STAGES_KWD,
    ):
        self.resolver = resolver
        self.wdir = self.resolver.wdir
        self.relpath = self.resolver.relpath
        self.context = context
        self.name = name
        self.definition = definition
        self.where = where

    def _resolve_wdir(
        self, context: Context, name: str, wdir: Optional[str] = None
    ) -> str:
        if not wdir:
            return self.wdir

        try:
            wdir = to_str(context.resolve_str(wdir))
        except (ContextError, ParseError) as exc:
            format_and_raise(exc, f"'{self.where}.{name}.wdir'", self.relpath)
        return self.resolver.fs.path.join(self.wdir, wdir)

    def resolve(self, **kwargs):
        try:
            return self.resolve_stage(**kwargs)
        except ContextError as exc:
            format_and_raise(exc, f"stage '{self.name}'", self.relpath)

    def resolve_stage(self, skip_checks: bool = False) -> "DictStrAny":
        context = self.context
        name = self.name
        if not skip_checks:
            # we can check for syntax errors as we go for interpolated entries,
            # but for foreach-generated ones, once is enough, which it does
            # that itself. See `ForeachDefinition.do_definition`.
            check_syntax_errors(self.definition, name, self.relpath)

        # we need to pop vars from generated/evaluated data
        definition = deepcopy(self.definition)

        wdir = self._resolve_wdir(context, name, definition.get(WDIR_KWD))
        vars_ = definition.pop(VARS_KWD, [])
        # FIXME: Should `vars` be templatized?
        check_interpolations(vars_, f"{self.where}.{name}.vars", self.relpath)
        if vars_:
            # Optimization: Lookahead if it has any vars, if it does not, we
            # don't need to clone them.
            context = Context.clone(context)

        try:
            fs = self.resolver.fs
            context.load_from_vars(fs, vars_, wdir, stage_name=name)
        except VarsAlreadyLoaded as exc:
            format_and_raise(exc, f"'{self.where}.{name}.vars'", self.relpath)

        logger.trace(  # type: ignore[attr-defined]
            "Context during resolution of stage %s:\n%s", name, context
        )

        with context.track() as tracked_data:
            # NOTE: we do not pop "wdir", and resolve it again
            # this does not affect anything and is done to try to
            # track the source of `wdir` interpolation.
            # This works because of the side-effect that we do not
            # allow overwriting and/or str interpolating complex objects.
            # Fix if/when those assumptions are no longer valid.
            resolved = {
                key: self._resolve(context, value, key, skip_checks)
                for key, value in definition.items()
            }

        self.resolver.track_vars(name, tracked_data)
        return {name: resolved}

    def _resolve(
        self, context: "Context", value: Any, key: str, skip_checks: bool
    ) -> "DictStrAny":
        try:
            return context.resolve(
                value, skip_interpolation_checks=skip_checks, key=key
            )
        except (ParseError, KeyNotInContext) as exc:
            format_and_raise(exc, f"'{self.where}.{self.name}.{key}'", self.relpath)


class IterationPair(NamedTuple):
    key: str = "key"
    value: str = "item"


class ForeachDefinition:
    def __init__(
        self,
        resolver: DataResolver,
        context: Context,
        name: str,
        definition: "DictStrAny",
        where: str = STAGES_KWD,
    ):
        self.resolver = resolver
        self.relpath = self.resolver.relpath
        self.context = context
        self.name = name

        assert DO_KWD in definition
        self.foreach_data = definition[FOREACH_KWD]
        self._do_definition = definition[DO_KWD]

        self.pair = IterationPair()
        self.where = where

    @cached_property
    def do_definition(self):
        # optimization: check for syntax errors only once for `foreach` stages
        check_syntax_errors(self._do_definition, self.name, self.relpath)
        return self._do_definition

    @cached_property
    def resolved_iterable(self):
        return self._resolve_foreach_data()

    def _resolve_foreach_data(self) -> "SeqOrMap":
        try:
            iterable = self.context.resolve(self.foreach_data, unwrap=False)
        except (ContextError, ParseError) as exc:
            format_and_raise(exc, f"'{self.where}.{self.name}.foreach'", self.relpath)

        # foreach data can be a resolved dictionary/list.
        self._check_is_map_or_seq(iterable)
        # foreach stages will have `item` and `key` added to the context
        # so, we better warn them if they have them already in the context
        # from the global vars. We could add them in `set_temporarily`, but
        # that'd make it display for each iteration.
        self._warn_if_overwriting(self._inserted_keys(iterable))
        return iterable

    def _check_is_map_or_seq(self, iterable):
        if not is_map_or_seq(iterable):
            node = iterable.value if isinstance(iterable, Node) else iterable
            typ = type(node).__name__
            raise ResolveError(
                f"failed to resolve '{self.where}.{self.name}.foreach'"
                f" in '{self.relpath}': expected list/dictionary, got " + typ
            )

    def _warn_if_overwriting(self, keys: List[str]):
        warn_for = [k for k in keys if k in self.context]
        if warn_for:
            linking_verb = "is" if len(warn_for) == 1 else "are"
            logger.warning(
                (
                    "%s %s already specified, "
                    "will be overwritten for stages generated from '%s'"
                ),
                " and ".join(warn_for),
                linking_verb,
                self.name,
            )

    def _inserted_keys(self, iterable) -> List[str]:
        keys = [self.pair.value]
        if isinstance(iterable, Mapping):
            keys.append(self.pair.key)
        return keys

    @cached_property
    def normalized_iterable(self):
        """Convert sequence to Mapping with keys normalized."""
        iterable = self.resolved_iterable
        if isinstance(iterable, Mapping):
            return {to_str(k): v for k, v in iterable.items()}

        assert isinstance(iterable, Sequence)
        if any(map(is_map_or_seq, iterable)):
            # if the list contains composite data, index are the keys
            return {to_str(idx): value for idx, value in enumerate(iterable)}

        # for simple lists, eg: ["foo", "bar"],  contents are the key itself
        return {to_str(value): value for value in iterable}

    def has_member(self, key: str) -> bool:
        return key in self.normalized_iterable

    def get_generated_names(self):
        return list(map(self._generate_name, self.normalized_iterable))

    def _generate_name(self, key: str) -> str:
        return f"{self.name}{JOIN}{key}"

    def resolve_all(self) -> "DictStrAny":
        return join(map(self.resolve_one, self.normalized_iterable))

    def resolve_one(self, key: str) -> "DictStrAny":
        return self._each_iter(key)

    def _each_iter(self, key: str) -> "DictStrAny":
        err_message = f"Could not find '{key}' in foreach group '{self.name}'"
        with reraise(KeyError, EntryNotFound(err_message)):
            value = self.normalized_iterable[key]

        # NOTE: we need to use resolved iterable/foreach-data,
        # not the normalized ones to figure out whether to make item/key
        # available
        inserted = self._inserted_keys(self.resolved_iterable)
        temp_dict = {self.pair.value: value}
        key_str = self.pair.key
        if key_str in inserted:
            temp_dict[key_str] = key

        with self.context.set_temporarily(temp_dict, reserve=True):
            # optimization: item and key can be removed on __exit__() as they
            # are top-level values, and are not merged recursively.
            # This helps us avoid cloning context, which is slower
            # (increasing the size of the context might increase
            # the no. of items to be generated which means more cloning,
            # i.e. quadratic complexity).
            generated = self._generate_name(key)
            entry = EntryDefinition(
                self.resolver, self.context, generated, self.do_definition
            )
            try:
                # optimization: skip checking for syntax errors on each foreach
                # generated stages. We do it once when accessing do_definition.
                return entry.resolve_stage(skip_checks=True)
            except ContextError as exc:
                format_and_raise(exc, f"stage '{generated}'", self.relpath)

            # let mypy know that this state is unreachable as format_and_raise
            # does not return at all (it's not able to understand it for some
            # reason)
            raise AssertionError("unreachable")




dvc/parsing/context.py
import logging
from abc import ABC, abstractmethod
from collections import defaultdict
from collections.abc import Mapping, MutableMapping, MutableSequence, Sequence
from contextlib import contextmanager
from copy import deepcopy
from dataclasses import dataclass, field, replace
from typing import Any, Dict, List, Optional, Union

from funcy import identity, lfilter, nullcontext, select

from dvc.exceptions import DvcException
from dvc.parsing.interpolate import (
    get_expression,
    get_matches,
    is_exact_string,
    normalize_key,
    recurse,
    str_interpolate,
    validate_value,
)

logger = logging.getLogger(__name__)
SeqOrMap = Union[Sequence, Mapping]
DictStr = Dict[str, Any]


class ContextError(DvcException):
    pass


class ReservedKeyError(ContextError):
    def __init__(self, keys, path=None):
        from dvc.utils.humanize import join

        self.keys = keys
        self.path = path

        n = "key" + ("s" if len(keys) > 1 else "")
        msg = f"attempted to modify reserved {n} {join(keys)}"
        if path:
            msg += f" in '{path}'"
        super().__init__(msg)


class MergeError(ContextError):
    def __init__(self, key, new, into):
        self.key = key
        to_node = into[key]
        if not isinstance(to_node, Node) or not isinstance(new, Node):
            super().__init__(f"cannot merge '{key}' as it already exists in {into}")
            return

        assert isinstance(to_node, Node)
        assert isinstance(new, Node)
        preexisting = to_node.meta.source
        new_src = new.meta.source
        path = new.meta.path()
        super().__init__(
            f"cannot redefine '{path}' from '{new_src}'"
            f" as it already exists in '{preexisting}'"
        )


class ParamsLoadError(ContextError):
    pass


class KeyNotInContext(ContextError, KeyError):
    def __init__(self, key: str) -> None:
        self.key: str = key
        super().__init__(f"Could not find '{key}'")

    def __str__(self):
        return self.msg


class VarsAlreadyLoaded(ContextError):
    pass


def _merge(into, update, overwrite):
    for key, val in update.items():
        if isinstance(into.get(key), Mapping) and isinstance(val, Mapping):
            _merge(into[key], val, overwrite)
        else:
            if key in into and not overwrite:
                raise MergeError(key, val, into)
            into[key] = val
            assert isinstance(into[key], Node)


def recurse_not_a_node(data: dict):
    def func(item):
        assert not isinstance(item, Node)

    return recurse(func)(data)


@dataclass
class Meta:
    source: Optional[str] = None
    dpaths: List[str] = field(default_factory=list)
    local: bool = True

    @staticmethod
    def update_path(meta: "Meta", path: Union[str, int]):
        dpaths = meta.dpaths[:] + [str(path)]
        return replace(meta, dpaths=dpaths)

    def __str__(self):
        string = self.source or "<local>"
        string += ":" + self.path()
        return string

    def path(self):
        return ".".join(self.dpaths)


def _default_meta() -> Meta:
    return Meta()


class Node:
    meta: Meta

    def get_sources(self):
        raise NotImplementedError

    @property
    @abstractmethod
    def value(self):
        pass


@dataclass
class Value(Node):
    _value: Any
    meta: Meta = field(compare=False, default_factory=_default_meta, repr=False)

    def __repr__(self):
        return repr(self._value)

    def __str__(self) -> str:
        return str(self._value)

    def get_sources(self):
        return {self.meta.source: self.meta.path()}

    @property
    def value(self):
        return self._value


PRIMITIVES = (int, float, str, bytes, bool)


class Container(Node, ABC):
    meta: Meta
    data: Union[list, dict]
    _key_transform = staticmethod(identity)

    def __init__(self, meta=None) -> None:
        self.meta = meta or _default_meta()

    def _convert(self, key, value):
        meta = Meta.update_path(self.meta, key)
        return self._convert_with_meta(value, meta)

    @staticmethod
    def _convert_with_meta(value, meta: Optional[Meta] = None):
        if value is None or isinstance(value, PRIMITIVES):
            assert meta
            return Value(value, meta=meta)
        if isinstance(value, Node):
            return value
        if isinstance(value, (list, dict)):
            assert meta
            container = CtxDict if isinstance(value, dict) else CtxList
            return container(value, meta=meta)
        msg = f"Unsupported value of type '{type(value).__name__}' in '{meta}'"
        raise TypeError(msg)

    def __repr__(self):
        return repr(self.data)

    def __getitem__(self, key):
        return self.data[key]

    def __setitem__(self, key, value):
        self.data[key] = self._convert(key, value)

    def __delitem__(self, key):
        del self.data[key]

    def __len__(self):
        return len(self.data)

    def __iter__(self):
        return iter(self.data)

    def __eq__(self, o):
        container = type(self)
        if isinstance(o, container):
            return o.data == self.data
        return container(o) == self

    def select(self, key: str):
        index, *rems = key.split(sep=".", maxsplit=1)
        index = index.strip()
        index = self._key_transform(index)
        try:
            d = self[index]
        except LookupError as exc:
            raise ValueError(f"Could not find '{index}' in {self.data}") from exc

        if not rems:
            return d

        rem = rems[0]
        if not isinstance(d, Container):
            raise ValueError(  # noqa: TRY004
                f"{index} is a primitive value, cannot get '{rem}'"
            )
        return d.select(rem)

    def get_sources(self):
        return {}


class CtxList(Container, MutableSequence):
    _key_transform = staticmethod(int)

    def __init__(self, values: Sequence, meta: Optional[Meta] = None):
        super().__init__(meta=meta)
        self.data: list = []
        self.extend(values)

    def insert(self, index: int, value):
        self.data.insert(index, self._convert(index, value))

    def get_sources(self):
        return {self.meta.source: self.meta.path()}

    @property
    def value(self):
        return [node.value for node in self]

    def __deepcopy__(self, _):
        # optimization: we don't support overriding a list
        new = CtxList([])
        new.data = self.data[:]  # Short-circuiting __setitem__
        return new


class CtxDict(Container, MutableMapping):
    def __init__(
        self,
        mapping: Optional[Mapping] = None,
        meta: Optional[Meta] = None,
        **kwargs,
    ):
        super().__init__(meta=meta)

        self.data: dict = {}
        if mapping:
            self.update(mapping)
        self.update(kwargs)

    def __setitem__(self, key, value):
        if not isinstance(key, str):
            # limitation for the interpolation
            # ignore other kinds of keys
            return
        return super().__setitem__(key, value)

    def merge_update(self, other, overwrite=False):
        _merge(self, other, overwrite=overwrite)

    @property
    def value(self):
        return {key: node.value for key, node in self.items()}

    def __deepcopy__(self, _):
        new = CtxDict()
        for k, v in self.items():
            new.data[k] = (
                deepcopy(v) if isinstance(v, Container) else v
            )  # short-circuiting __setitem__
        return new


class Context(CtxDict):
    def __init__(self, *args, **kwargs):
        """
        Top level mutable dict, with some helpers to create context and track
        """
        super().__init__(*args, **kwargs)
        self._track = False
        self._tracked_data: Dict[str, Dict] = defaultdict(dict)
        self.imports = {}
        self._reserved_keys = {}

    @contextmanager
    def track(self):
        self._track = True
        yield self._tracked_data

        self._track = False
        self._tracked_data = defaultdict(dict)

    def _track_data(self, node):
        if not self._track or not isinstance(node, Node):
            return

        assert isinstance(node, Node)
        if node.meta and node.meta.local:
            return

        for source, keys in node.get_sources().items():
            if not source:
                continue
            params_file = self._tracked_data[source]
            keys = [keys] if isinstance(keys, str) else keys
            params_file.update({key: node.value for key in keys})

    def select(
        self, key: str, unwrap: bool = False
    ):  # pylint: disable=arguments-differ
        """Select the item using key, similar to `__getitem__`
           but can track the usage of the data on interpolation
           as well and can get from nested data structure by using
           "." separated key (eg: "key1.key2.key3")

        Args:
            key: key to select value from
            unwrap: Convert CtxList/CtxDict/Value items to it's original data
                    Defaults to False. Note that the default is different from
                    `resolve`.
        """
        normalized = normalize_key(key)
        try:
            node = super().select(normalized)
        except ValueError as exc:
            raise KeyNotInContext(key) from exc

        assert isinstance(node, Node)
        self._track_data(node)
        return node.value if unwrap else node

    @classmethod
    def load_from(
        cls, fs, path: str, select_keys: Optional[List[str]] = None
    ) -> "Context":
        from dvc.utils.serialize import load_path

        if not fs.exists(path):
            raise ParamsLoadError(f"'{path}' does not exist")
        if fs.isdir(path):
            raise ParamsLoadError(f"'{path}' is a directory")

        data = load_path(path, fs)
        if not isinstance(data, Mapping):
            typ = type(data).__name__
            raise ParamsLoadError(
                f"expected a dictionary, got '{typ}' in file '{path}'"
            )

        if select_keys:
            try:
                data = {key: data[key] for key in select_keys}
            except KeyError as exc:
                key, *_ = exc.args
                raise ParamsLoadError(f"could not find '{key}' in '{path}'") from exc

        meta = Meta(source=path, local=False)
        ctx = cls(data, meta=meta)
        ctx.imports[path] = select_keys
        return ctx

    def merge_update(self, other: "Context", overwrite=False):
        matches = select(lambda key: key in other, self._reserved_keys.keys())
        if matches:
            raise ReservedKeyError(matches)
        return super().merge_update(other, overwrite=overwrite)

    def merge_from(self, fs, item: str, wdir: str, overwrite=False):
        path, _, keys_str = item.partition(":")
        path = fs.path.normpath(fs.path.join(wdir, path))

        select_keys = lfilter(bool, keys_str.split(",")) if keys_str else None
        if path in self.imports:
            if not select_keys and self.imports[path] is None:
                return  # allow specifying complete filepath multiple times
            self.check_loaded(path, item, select_keys)

        ctx = Context.load_from(fs, path, select_keys)

        try:
            self.merge_update(ctx, overwrite=overwrite)
        except ReservedKeyError as exc:
            raise ReservedKeyError(exc.keys, item) from exc

        cp = ctx.imports[path]
        if path not in self.imports:
            self.imports[path] = cp
        elif cp:
            self.imports[path].extend(cp)

    def check_loaded(self, path, item, keys):
        imported = self.imports[path]
        if not keys and isinstance(imported, list):
            raise VarsAlreadyLoaded(
                f"cannot load '{item}' as it's partially loaded already"
            )
        if keys and imported is None:
            raise VarsAlreadyLoaded(
                f"cannot partially load '{item}' as it's already loaded."
            )
        if isinstance(imported, list) and set(keys) & set(imported):
            raise VarsAlreadyLoaded(
                f"cannot load '{item}' as it's partially loaded already"
            )

    def load_from_vars(
        self,
        fs,
        vars_: List,
        wdir: str,
        stage_name: Optional[str] = None,
        default: Optional[str] = None,
    ):
        if default:
            to_import = fs.path.join(wdir, default)
            if fs.exists(to_import):
                self.merge_from(fs, default, wdir)
            else:
                msg = "%s does not exist, it won't be used in parametrization"
                logger.trace(msg, to_import)  # type: ignore[attr-defined]

        stage_name = stage_name or ""
        for index, item in enumerate(vars_):
            assert isinstance(item, (str, dict))
            if isinstance(item, str):
                self.merge_from(fs, item, wdir)
            else:
                joiner = "." if stage_name else ""
                meta = Meta(source=f"{stage_name}{joiner}vars[{index}]")
                self.merge_update(Context(item, meta=meta))

    def __deepcopy__(self, _):
        new = Context(super().__deepcopy__(_))
        new.meta = deepcopy(self.meta)
        new.imports = deepcopy(self.imports)
        new._reserved_keys = deepcopy(self._reserved_keys)
        return new

    @classmethod
    def clone(cls, ctx: "Context") -> "Context":
        """Clones given context."""
        return deepcopy(ctx)

    @contextmanager
    def reserved(self, *keys: str):
        """Allow reserving some keys so that they cannot be overwritten.

        Ideally, we should delegate this to a separate container
        and support proper namespacing so that we could support `env` features.
        But for now, just `item` and `key`, this should do.
        """
        # using dict to make the error messages ordered
        new = dict.fromkeys([key for key in keys if key not in self._reserved_keys])
        self._reserved_keys.update(new)
        try:
            yield
        finally:
            for key in new:
                self._reserved_keys.pop(key)

    @contextmanager
    def set_temporarily(self, to_set: DictStr, reserve: bool = False):
        cm = self.reserved(*to_set) if reserve else nullcontext()

        non_existing = frozenset(to_set.keys() - self.keys())
        prev = {key: self[key] for key in to_set if key not in non_existing}
        temp = CtxDict(to_set)
        self.update(temp)

        try:
            with cm:
                yield
        finally:
            self.update(prev)
            for key in non_existing:
                self.data.pop(key, None)

    def resolve(
        self, src, unwrap=True, skip_interpolation_checks=False, key=None
    ) -> Any:
        """Recursively resolves interpolation and returns resolved data.

        Args:
            src: Data (str/list/dict etc.) to resolve
            unwrap: Unwrap CtxDict/CtxList/Value to it's original data if
                inside `src`. Defaults to True.
            skip_interpolation_checks: Skip interpolation checks for error
                The callee is responsible to check for errors in advance.

        >>> c = Context({"three": 3})
        >>> c.resolve({"lst": [1, 2, "${three}"]})
        {'lst': [1, 2, 3]}
        """
        func = recurse(self.resolve_str)
        return func(src, unwrap, skip_interpolation_checks, key)

    def resolve_str(
        self, src: str, unwrap=True, skip_interpolation_checks=False, key=None
    ) -> str:
        """Resolves interpolated string to it's original value,
        or in case of multiple interpolations, a combined string.

        >>> c = Context({"enabled": True})
        >>> c.resolve_str("${enabled}")
        True
        >>> c.resolve_str("enabled? ${enabled}")
        'enabled? true'
        """
        matches = get_matches(src)
        if is_exact_string(src, matches):
            # replace "${enabled}", if `enabled` is a boolean, with it's actual
            # value rather than it's string counterparts.
            expr = get_expression(matches[0], skip_checks=skip_interpolation_checks)
            value = self.select(expr, unwrap=unwrap)
            validate_value(value, key)
            return value
        # but not "${num} days"
        return str_interpolate(
            src, matches, self, skip_checks=skip_interpolation_checks, key=key
        )


if __name__ == "__main__":
    import doctest

    doctest.testmod()




dvc/parsing/interpolate.py
import os
import re
import typing
from collections.abc import Iterable, Mapping
from functools import singledispatch

from funcy import memoize, rpartial

from dvc.exceptions import DvcException
from dvc.utils.flatten import flatten

if typing.TYPE_CHECKING:
    from typing import List, Match, NoReturn

    from pyparsing import ParseException

    from .context import Context

BRACE_OPEN = "${"
BRACE_CLOSE = "}"
LBRACK = "["
RBRACK = "]"
PERIOD = "."
KEYCRE = re.compile(
    r"""
    (?<!\\)                            # escape \${}
    \${                                # starts with ${
    (?P<inner>.*?)                     # match every char inside
    }                                  # end with {
""",
    re.VERBOSE,
)


@memoize
def get_parser():
    from pyparsing import CharsNotIn, ParserElement, Suppress, ZeroOrMore

    ParserElement.enablePackrat()

    word = CharsNotIn(f"{PERIOD}{LBRACK}{RBRACK}")
    idx = Suppress(LBRACK) + word + Suppress(RBRACK)
    attr = Suppress(PERIOD) + word
    parser = word + ZeroOrMore(attr ^ idx)
    parser.setParseAction(PERIOD.join)

    return parser


class ParseError(DvcException):
    pass


def get_matches(template: str):
    return list(KEYCRE.finditer(template))


def is_interpolated_string(val):
    return isinstance(val, str) and bool(get_matches(val))


def normalize_key(key: str):
    return key.replace(LBRACK, PERIOD).replace(RBRACK, "")


def format_and_raise_parse_error(exc) -> "NoReturn":
    raise ParseError(_format_exc_msg(exc))


def embrace(s: str):
    return BRACE_OPEN + s + BRACE_CLOSE


def escape_str(value):
    if os.name == "nt":
        from subprocess import list2cmdline  # nosec B404

        return list2cmdline([value])
    from shlex import quote

    return quote(value)


@singledispatch
def to_str(obj) -> str:
    return str(obj)


@to_str.register(bool)
def _(obj: bool):
    return "true" if obj else "false"


@to_str.register(dict)
def _(obj: dict):  # noqa: C901
    from dvc.config import Config

    config = Config.from_cwd().get("parsing", {})

    result = ""
    for k, v in flatten(obj).items():
        if isinstance(v, bool):
            if v:
                result += f"--{k} "
            elif config.get("bool", "store_true") == "boolean_optional":
                result += f"--no-{k} "

        elif isinstance(v, str):
            result += f"--{k} {escape_str(v)} "

        elif isinstance(v, Iterable):
            for n, i in enumerate(v):
                if isinstance(i, str):
                    i = escape_str(i)
                elif isinstance(i, Iterable):
                    raise ParseError(f"Cannot interpolate nested iterable in '{k}'")

                if config.get("list", "nargs") == "append":
                    result += f"--{k} {i} "
                else:
                    result += f"{i} " if n > 0 else f"--{k} {i} "

        else:
            result += f"--{k} {v} "

    return result.rstrip()


def _format_exc_msg(exc: "ParseException"):
    from pyparsing import ParseException

    from dvc.utils import colorize

    exc.loc += 2  # 2 because we append `${` at the start of expr below

    expr = exc.pstr
    exc.pstr = embrace(exc.pstr)
    error = ParseException.explain(exc, depth=0)

    _, pointer, *explains = error.splitlines()
    pstr = "{brace_open}{expr}{brace_close}".format(
        brace_open=colorize(BRACE_OPEN, color="blue"),
        expr=colorize(expr, color="magenta"),
        brace_close=colorize(BRACE_CLOSE, color="blue"),
    )
    msg = "\n".join(explains)
    pointer = colorize(pointer, color="red")
    return "\n".join([pstr, pointer, colorize(msg, color="red", style="bold")])


def recurse(f):
    seq = (list, tuple, set)

    def wrapper(data, *args):
        g = rpartial(wrapper, *args)
        if isinstance(data, Mapping):
            return {g(k): g(v) for k, v in data.items()}
        if isinstance(data, seq):
            return type(data)(map(g, data))
        if isinstance(data, str):
            return f(data, *args)
        return data

    return wrapper


def check_recursive_parse_errors(data):
    func = recurse(check_expression)
    return func(data)


def check_expression(s: str):
    matches = get_matches(s)
    for match in matches:
        get_expression(match)


def parse_expr(s: str):
    from pyparsing import ParseException

    try:
        result = get_parser().parseString(s, parseAll=True)
    except ParseException as exc:
        format_and_raise_parse_error(exc)
        raise AssertionError("unreachable")  # noqa: B904

    joined = result.asList()
    assert len(joined) == 1
    return joined[0]


def get_expression(match: "Match", skip_checks: bool = False):
    inner = match["inner"]
    return inner if skip_checks else parse_expr(inner)


def validate_value(value, key):
    from .context import PRIMITIVES

    not_primitive = value is not None and not isinstance(value, PRIMITIVES)
    not_foreach = key is not None and "foreach" not in key
    if not_primitive and not_foreach:
        if isinstance(value, dict) and key == "cmd":
            return True
        raise ParseError(f"Cannot interpolate data of type '{type(value).__name__}'")


def str_interpolate(
    template: str,
    matches: "List[Match]",
    context: "Context",
    skip_checks: bool = False,
    key=None,
):
    index, buf = 0, ""
    for match in matches:
        start, end = match.span(0)
        expr = get_expression(match, skip_checks=skip_checks)
        value = context.select(expr, unwrap=True)
        validate_value(value, key)
        buf += template[index:start] + to_str(value)
        index = end
    buf += template[index:]
    # regex already backtracks and avoids any `${` starting with
    # backslashes(`\`). We just need to replace those by `${`.
    return buf.replace(r"\${", BRACE_OPEN)


def is_exact_string(src: str, matches: "List[Match]"):
    return len(matches) == 1 and src == matches[0].group(0)




dvc/render/__init__.py
INDEX_FIELD = "step"
REVISION_FIELD = "rev"
FILENAME_FIELD = "filename"
VERSION_FIELD = "dvc_data_version_info"
REVISIONS_KEY = "revisions"
TYPE_KEY = "type"
SRC_FIELD = "src"




dvc/render/convert.py
from collections import defaultdict
from typing import Dict, List, Union

from dvc.render import REVISION_FIELD, REVISIONS_KEY, SRC_FIELD, TYPE_KEY, VERSION_FIELD
from dvc.render.converter.image import ImageConverter
from dvc.render.converter.vega import VegaConverter


def _get_converter(
    renderer_class, renderer_id, props, data
) -> Union[VegaConverter, ImageConverter]:
    from dvc_render import ImageRenderer, VegaRenderer

    if renderer_class.TYPE == VegaRenderer.TYPE:
        return VegaConverter(renderer_id, data, props)
    if renderer_class.TYPE == ImageRenderer.TYPE:
        return ImageConverter(renderer_id, data, props)

    raise ValueError(f"Invalid renderer class {renderer_class}")


def _group_by_rev(datapoints):
    grouped = defaultdict(list)
    for datapoint in datapoints:
        rev = datapoint.get(VERSION_FIELD, {}).get("revision")
        grouped[rev].append(datapoint)
    return dict(grouped)


def to_json(renderer, split: bool = False) -> List[Dict]:
    if renderer.TYPE == "vega":
        grouped = _group_by_rev(renderer.datapoints)
        if split:
            content = renderer.get_filled_template(
                skip_anchors=["data"], as_string=False
            )
        else:
            content = renderer.get_filled_template(as_string=False)
        if grouped:
            return [
                {
                    TYPE_KEY: renderer.TYPE,
                    REVISIONS_KEY: sorted(grouped.keys()),
                    "content": content,
                    "datapoints": grouped,
                }
            ]
        return []
    if renderer.TYPE == "image":
        return [
            {
                TYPE_KEY: renderer.TYPE,
                REVISIONS_KEY: [datapoint.get(REVISION_FIELD)],
                "url": datapoint.get(SRC_FIELD),
            }
            for datapoint in renderer.datapoints
        ]
    raise ValueError(f"Invalid renderer: {renderer.TYPE}")




dvc/render/match.py
import os
from collections import defaultdict
from typing import TYPE_CHECKING, DefaultDict, Dict, List, NamedTuple, Optional

import dpath
import dpath.options
from funcy import get_in, last

from dvc.repo.plots import _normpath, infer_data_sources
from dvc.utils.plots import get_plot_id

from .convert import _get_converter

if TYPE_CHECKING:
    from dvc.types import StrPath
    from dvc_render.base import Renderer


dpath.options.ALLOW_EMPTY_STRING_KEYS = True


def _squash_plots_properties(data: List) -> Dict:
    configs = [last(group) for group in data]
    resolved: Dict = {}
    for config in reversed(configs):
        resolved = {**resolved, **config}
    return resolved


class PlotsData:
    def __init__(self, data: Dict):
        self.data = data

    def group_definitions(self):
        groups = defaultdict(list)
        for rev, rev_content in self.data.items():
            for config_file, config_file_content in (
                rev_content.get("definitions", {}).get("data", {}).items()
            ):
                for plot_id, plot_definition in config_file_content.get(
                    "data", {}
                ).items():
                    full_id = get_plot_id(plot_id, config_file)
                    groups[full_id].append((rev, plot_id, plot_definition))
        return dict(groups)

    def get_definition_data(self, target_files, rev):
        result = {}
        for definition_file in target_files:
            if os.name == "nt":
                source_file = _normpath(definition_file).replace("\\", "/")
            else:
                source_file = definition_file
            file_content = (
                self.data.get(rev, {})
                .get("sources", {})
                .get("data", {})
                .get(source_file, {})
                .get("data", {})
            )
            if file_content:
                result[definition_file] = file_content
        return result


class RendererWithErrors(NamedTuple):
    renderer: "Renderer"
    source_errors: Dict[str, Dict[str, Exception]]
    definition_errors: Dict[str, Exception]


def match_defs_renderers(  # noqa: C901, PLR0912
    data,
    out=None,
    templates_dir: Optional["StrPath"] = None,
) -> List[RendererWithErrors]:
    from dvc_render import ImageRenderer, VegaRenderer

    plots_data = PlotsData(data)
    renderers = []
    renderer_cls = None

    for plot_id, group in plots_data.group_definitions().items():
        plot_datapoints: List[Dict] = []
        props = _squash_plots_properties(group)
        final_props: Dict = {}

        def_errors: Dict[str, Exception] = {}
        src_errors: DefaultDict[str, Dict[str, Exception]] = defaultdict(dict)

        if out is not None:
            props["out"] = out
        if templates_dir is not None:
            props["template_dir"] = templates_dir

        for rev, inner_id, plot_definition in group:
            plot_sources = infer_data_sources(inner_id, plot_definition)
            definitions_data = plots_data.get_definition_data(plot_sources, rev)

            if ImageRenderer.matches(inner_id, None):
                renderer_cls = ImageRenderer
                renderer_id = inner_id
            else:
                renderer_cls = VegaRenderer
                renderer_id = plot_id

            converter = _get_converter(renderer_cls, inner_id, props, definitions_data)

            for src in plot_sources:
                if error := get_in(data, [rev, "sources", "data", src, "error"]):
                    src_errors[rev][src] = error

            try:
                dps, rev_props = converter.flat_datapoints(rev)
            except Exception as e:  # noqa: BLE001, pylint: disable=broad-except
                def_errors[rev] = e
                continue

            if not final_props and rev_props:
                final_props = rev_props
            plot_datapoints.extend(dps)

        if "title" not in final_props:
            final_props["title"] = renderer_id

        if renderer_cls is not None:
            renderer = renderer_cls(plot_datapoints, renderer_id, **final_props)
            renderers.append(RendererWithErrors(renderer, dict(src_errors), def_errors))
    return renderers




dvc/render/converter/__init__.py
from typing import Any, Dict, List, Optional, Tuple


class Converter:
    def __init__(
        self,
        plot_id: str,
        data: Optional[Dict[str, Any]] = None,
        properties: Optional[Dict] = None,
    ):
        self.plot_id = plot_id
        self.properties = properties or {}
        self.data = data or {}

    def convert(self) -> Tuple[List[Tuple[str, str, Any]], Dict]:
        raise NotImplementedError

    def flat_datapoints(self, revision: str) -> Tuple[List[Dict], Dict]:
        raise NotImplementedError




dvc/render/converter/image.py
import base64
import os
from typing import TYPE_CHECKING, Any, Dict, List, Tuple

from dvc.render import FILENAME_FIELD, REVISION_FIELD, SRC_FIELD

from . import Converter

if TYPE_CHECKING:
    from dvc.types import StrPath


class ImageConverter(Converter):
    @staticmethod
    def _write_image(
        path: "StrPath",
        revision: str,
        filename: str,
        image_data: bytes,
    ) -> "StrPath":
        img_path = os.path.join(
            path,
            f"{revision}_{filename}".replace(os.sep, "_").replace("/", "_"),
        )
        with open(img_path, "wb") as fd:
            fd.write(image_data)

        return img_path

    @staticmethod
    def _encode_image(
        image_data: bytes,
    ) -> str:
        base64_str = base64.b64encode(image_data).decode()
        return f"data:image;base64,{base64_str}"

    def convert(self) -> Tuple[List[Tuple[str, str, Any]], Dict]:
        datas = []
        for filename, image_data in self.data.items():
            datas.append((filename, "", image_data))
        return datas, self.properties

    def flat_datapoints(self, revision: str) -> Tuple[List[Dict], Dict]:
        """
        Convert the DVC Plots content to DVC Render datapoints.
        Return both generated datapoints and updated properties.
        """
        path = self.properties.get("out")
        datapoints = []
        datas, properties = self.convert()
        for filename, _, image_data in datas:
            if path:
                if not os.path.isdir(path):
                    os.makedirs(path, exist_ok=True)
                src = self._write_image(
                    os.path.abspath(path), revision, filename, image_data
                )
            else:
                src = self._encode_image(image_data)
            datapoint = {
                REVISION_FIELD: revision,
                FILENAME_FIELD: filename,
                SRC_FIELD: src,
            }
            datapoints.append(datapoint)
        return datapoints, properties




dvc/render/converter/vega.py
import os
from typing import Any, Dict, Iterable, List, Optional, Tuple, Union

from funcy import first, last

from dvc.exceptions import DvcException
from dvc.render import FILENAME_FIELD, INDEX_FIELD, VERSION_FIELD

from . import Converter


class FieldNotFoundError(DvcException):
    def __init__(self, expected_field, found_fields):
        found_str = ", ".join(found_fields)
        super().__init__(
            f"Could not find provided field ('{expected_field}') "
            f"in data fields ('{found_str}')."
        )


def _lists(blob: Union[Dict, List]) -> Iterable[List]:
    if isinstance(blob, list):
        yield blob
    else:
        for _, value in blob.items():
            if isinstance(value, dict):
                yield from _lists(value)
            elif isinstance(value, list):
                yield value


def _file_field(*args):
    for axis_def in args:
        if axis_def is not None:
            for file, val in axis_def.items():
                if isinstance(val, str):
                    yield file, val
                elif isinstance(val, list):
                    for field in val:
                        yield file, field


def _find(
    filename: str,
    field: str,
    data_series: List[Tuple[str, str, Any]],
):
    for data_file, data_field, data in data_series:
        if data_file == filename and data_field == field:
            return data_file, data_field, data
    return None


def _verify_field(file2datapoints: Dict[str, List], filename: str, field: str):
    if filename in file2datapoints:
        datapoint = first(file2datapoints[filename])
        if field not in datapoint:
            raise FieldNotFoundError(field, datapoint.keys())


def _get_xs(properties: Dict, file2datapoints: Dict[str, List[Dict]]):
    x = properties.get("x", None)
    if x is not None and isinstance(x, dict):
        for filename, field in _file_field(x):
            _verify_field(file2datapoints, filename, field)
            yield filename, field


def _get_ys(properties, file2datapoints: Dict[str, List[Dict]]):
    y = properties.get("y", None)
    if y is not None:
        for filename, field in _file_field(y):
            _verify_field(file2datapoints, filename, field)
            yield filename, field


def _is_datapoints(lst: List[Dict]):
    """
    check if dict keys match, datapoints with different keys mgiht lead
    to unexpected behavior
    """

    return all(isinstance(item, dict) for item in lst) and set(first(lst).keys()) == {
        key for keys in lst for key in keys
    }


def get_datapoints(file_content: Dict):
    result: List[Dict[str, Any]] = []
    for lst in _lists(file_content):
        if _is_datapoints(lst):
            for index, datapoint in enumerate(lst):
                if len(result) <= index:
                    result.append({})
                result[index].update(datapoint)
    return result


class VegaConverter(Converter):
    """
    Class that takes care of converting unspecified data blob
    (Dict or List[Dict]) into datapoints (List[Dict]).
    If some properties that are required by Template class are missing
    ('x', 'y') it will attempt to fill in the blanks.
    """

    def __init__(
        self,
        plot_id: str,
        data: Optional[Dict] = None,
        properties: Optional[Dict] = None,
    ):
        super().__init__(plot_id, data, properties)
        self.plot_id = plot_id
        self.inferred_properties: Dict = {}

        # TODO we should be handling that in `convert`,
        #      to avoid stateful `self.inferred_properties`
        self._infer_x_y()

    def _infer_y_from_data(self):
        if self.plot_id in self.data:
            for lst in _lists(self.data[self.plot_id]):
                if all(isinstance(item, dict) for item in lst):
                    datapoint = first(lst)
                    field = last(datapoint.keys())
                    self.inferred_properties["y"] = {self.plot_id: field}
                    break

    def _infer_x_y(self):
        x = self.properties.get("x", None)
        y = self.properties.get("y", None)

        # Infer x.
        if isinstance(x, str):
            self.inferred_properties["x"] = {}
            # If multiple y files, duplicate x for each file.
            if isinstance(y, dict):
                for file, fields in y.items():
                    # Duplicate x for each y.
                    if isinstance(fields, list):
                        self.inferred_properties["x"][file] = [x] * len(fields)
                    else:
                        self.inferred_properties["x"][file] = x
            # Otherwise use plot ID as file.
            else:
                self.inferred_properties["x"][self.plot_id] = x

        # Infer y.
        if y is None:
            self._infer_y_from_data()
        # If y files not provided, use plot ID as file.
        elif not isinstance(y, dict):
            self.inferred_properties["y"] = {self.plot_id: y}

    def _find_datapoints(self):
        result = {}
        for file, content in self.data.items():
            result[file] = get_datapoints(content)

        return result

    @staticmethod
    def infer_y_label(properties):
        y_label = properties.get("y_label", None)
        if y_label is not None:
            return y_label
        y = properties.get("y", None)
        if isinstance(y, str):
            return y
        if isinstance(y, list):
            return "y"
        if not isinstance(y, dict):
            return

        fields = {field for _, field in _file_field(y)}
        if len(fields) == 1:
            return first(fields)
        return "y"

    @staticmethod
    def infer_x_label(properties):
        x_label = properties.get("x_label", None)
        if x_label is not None:
            return x_label

        x = properties.get("x", None)
        if not isinstance(x, dict):
            return INDEX_FIELD

        fields = {field for _, field in _file_field(x)}
        if len(fields) == 1:
            return first(fields)
        return "x"

    def flat_datapoints(self, revision):  # noqa: C901, PLR0912
        file2datapoints, properties = self.convert()

        props_update = {}

        xs = list(_get_xs(properties, file2datapoints))

        # assign "step" if no x provided
        if not xs:
            x_file, x_field = (
                None,
                INDEX_FIELD,
            )
        else:
            x_file, x_field = xs[0]
        props_update["x"] = x_field

        ys = list(_get_ys(properties, file2datapoints))

        num_xs = len(xs)
        num_ys = len(ys)
        if num_xs > 1 and num_xs != num_ys:
            raise DvcException(
                "Cannot have different number of x and y data sources. Found "
                f"{num_xs} x and {num_ys} y data sources."
            )

        all_datapoints = []
        if ys:
            all_y_files, all_y_fields = list(zip(*ys))
            all_y_fields = set(all_y_fields)
            all_y_files = set(all_y_files)
        else:
            all_y_files = set()
            all_y_fields = set()

        # override to unified y field name if there are different y fields
        if len(all_y_fields) > 1:
            props_update["y"] = "dvc_inferred_y_value"
        else:
            props_update["y"] = first(all_y_fields)

        # get common prefix to drop from file names
        if len(all_y_files) > 1:
            common_prefix_len = len(os.path.commonpath(all_y_files))
        else:
            common_prefix_len = 0

        for i, (y_file, y_field) in enumerate(ys):
            if num_xs > 1:
                x_file, x_field = xs[i]
            datapoints = [{**d} for d in file2datapoints.get(y_file, [])]

            if props_update.get("y", None) == "dvc_inferred_y_value":
                _update_from_field(
                    datapoints,
                    field="dvc_inferred_y_value",
                    source_field=y_field,
                )

            if x_field == INDEX_FIELD and x_file is None:
                _update_from_index(datapoints, INDEX_FIELD)
            else:
                x_datapoints = file2datapoints.get(x_file, [])
                try:
                    _update_from_field(
                        datapoints,
                        field=x_field,
                        source_datapoints=x_datapoints,
                    )
                except IndexError:
                    raise DvcException(  # noqa: B904
                        f"Cannot join '{x_field}' from '{x_file}' and "
                        f"'{y_field}' from '{y_file}'. "
                        "They have to have same length."
                    )

            y_file_short = y_file[common_prefix_len:].strip("/\\")
            _update_all(
                datapoints,
                update_dict={
                    VERSION_FIELD: {
                        "revision": revision,
                        FILENAME_FIELD: y_file_short,
                        "field": y_field,
                    }
                },
            )

            all_datapoints.extend(datapoints)

        if not all_datapoints:
            return [], {}

        properties = {**properties, **props_update}

        return all_datapoints, properties

    def convert(
        self,
    ):
        """
        Convert the data. Fill necessary fields ('x', 'y') and return both
        generated datapoints and updated properties. If `x` is not provided,
        leave it as None, fronteds should handle it.

        NOTE: Studio uses this method.
              The only thing studio FE handles is filling `x` and `y`.
              `x/y_label` should be filled here.

              Datapoints are not stripped according to config, because users
              might be utilizing other fields in their custom plots.
        """
        datapoints = self._find_datapoints()
        properties = {**self.properties, **self.inferred_properties}

        properties["y_label"] = self.infer_y_label(properties)
        properties["x_label"] = self.infer_x_label(properties)

        return datapoints, properties


def _update_from_field(
    target_datapoints: List[Dict],
    field: str,
    source_datapoints: Optional[List[Dict]] = None,
    source_field: Optional[str] = None,
):
    if source_datapoints is None:
        source_datapoints = target_datapoints
    if source_field is None:
        source_field = field

    if len(source_datapoints) != len(target_datapoints):
        raise IndexError("Source and target datapoints must have the same length")

    for index, datapoint in enumerate(target_datapoints):
        source_datapoint = source_datapoints[index]
        if source_field in source_datapoint:
            datapoint[field] = source_datapoint[source_field]


def _update_from_index(datapoints: List[Dict], new_field: str):
    for index, datapoint in enumerate(datapoints):
        datapoint[new_field] = index


def _update_all(datapoints: List[Dict], update_dict: Dict):
    for datapoint in datapoints:
        datapoint.update(update_dict)




dvc/repo/__init__.py
import logging
import os
from collections import defaultdict
from contextlib import contextmanager
from functools import wraps
from typing import (
    TYPE_CHECKING,
    Callable,
    ContextManager,
    Iterable,
    Optional,
    Tuple,
    Union,
)

from dvc.exceptions import NotDvcRepoError, OutputNotFoundError
from dvc.ignore import DvcIgnoreFilter
from dvc.utils import env2bool
from dvc.utils.fs import path_isin
from dvc.utils.objects import cached_property

if TYPE_CHECKING:
    from dvc.fs import FileSystem
    from dvc.fs.data import DataFileSystem
    from dvc.fs.dvc import DVCFileSystem
    from dvc.lock import LockBase
    from dvc.machine import MachineManager
    from dvc.scm import Git, NoSCM
    from dvc.stage import Stage
    from dvc.types import DictStrAny
    from dvc_data.hashfile.state import StateBase
    from dvc_data.index import DataIndex

    from .experiments import Experiments
    from .index import Index
    from .scm_context import SCMContext

logger = logging.getLogger(__name__)


@contextmanager
def lock_repo(repo: "Repo"):
    # pylint: disable=protected-access
    depth: int = repo._lock_depth
    repo._lock_depth += 1

    try:
        if depth > 0:
            yield
        else:
            with repo.lock:
                repo._reset()
                yield
                # Graph cache is no longer valid after we release the repo.lock
                repo._reset()
    finally:
        repo._lock_depth = depth


def locked(f):
    @wraps(f)
    def wrapper(repo, *args, **kwargs):
        with lock_repo(repo):
            return f(repo, *args, **kwargs)

    return wrapper


class Repo:
    DVC_DIR = ".dvc"

    from dvc.repo.add import add  # type: ignore[misc]
    from dvc.repo.checkout import checkout  # type: ignore[misc]
    from dvc.repo.commit import commit  # type: ignore[misc]
    from dvc.repo.destroy import destroy  # type: ignore[misc]
    from dvc.repo.diff import diff  # type: ignore[misc]
    from dvc.repo.fetch import fetch  # type: ignore[misc]
    from dvc.repo.freeze import freeze, unfreeze  # type: ignore[misc]
    from dvc.repo.gc import gc  # type: ignore[misc]
    from dvc.repo.get import get as _get  # type: ignore[misc]
    from dvc.repo.get_url import get_url as _get_url  # type: ignore[misc]
    from dvc.repo.imp import imp  # type: ignore[misc]
    from dvc.repo.imp_url import imp_url  # type: ignore[misc]
    from dvc.repo.install import install  # type: ignore[misc]
    from dvc.repo.ls import ls as _ls  # type: ignore[misc]
    from dvc.repo.ls_url import ls_url as _ls_url  # type: ignore[misc]
    from dvc.repo.move import move  # type: ignore[misc]
    from dvc.repo.pull import pull  # type: ignore[misc]
    from dvc.repo.push import push  # type: ignore[misc]
    from dvc.repo.remove import remove  # type: ignore[misc]
    from dvc.repo.reproduce import reproduce  # type: ignore[misc]
    from dvc.repo.run import run  # type: ignore[misc]
    from dvc.repo.status import status  # type: ignore[misc]
    from dvc.repo.update import update  # type: ignore[misc]

    from .data import status as data_status  # type: ignore[misc]

    ls = staticmethod(_ls)
    ls_url = staticmethod(_ls_url)
    get = staticmethod(_get)
    get_url = staticmethod(_get_url)

    def _get_repo_dirs(
        self,
        root_dir: Optional[str] = None,
        fs: Optional["FileSystem"] = None,
        uninitialized: bool = False,
        scm: Optional[Union["Git", "NoSCM"]] = None,
    ) -> Tuple[str, Optional[str]]:
        from dvc.fs import localfs
        from dvc.scm import SCM, SCMError

        dvc_dir: Optional[str] = None
        try:
            root_dir = self.find_root(root_dir, fs)
            fs = fs or localfs
            dvc_dir = fs.path.join(root_dir, self.DVC_DIR)
        except NotDvcRepoError:
            if not uninitialized:
                raise

            if not scm:
                try:
                    scm = SCM(root_dir or os.curdir)
                    if scm.dulwich.repo.bare:
                        raise NotDvcRepoError(f"{scm.root_dir} is a bare git repo")
                except SCMError:
                    scm = SCM(os.curdir, no_scm=True)

            if not fs or not root_dir:
                root_dir = scm.root_dir

        assert root_dir
        return root_dir, dvc_dir

    def __init__(  # noqa: PLR0915
        self,
        root_dir: Optional[str] = None,
        fs: Optional["FileSystem"] = None,
        rev: Optional[str] = None,
        subrepos: bool = False,
        uninitialized: bool = False,
        config: Optional["DictStrAny"] = None,
        url: Optional[str] = None,
        repo_factory: Optional[Callable] = None,
        scm: Optional[Union["Git", "NoSCM"]] = None,
    ):
        from dvc.cachemgr import CacheManager
        from dvc.config import Config
        from dvc.data_cloud import DataCloud
        from dvc.fs import GitFileSystem, LocalFileSystem, localfs
        from dvc.lock import LockNoop, make_lock
        from dvc.repo.artifacts import Artifacts
        from dvc.repo.metrics import Metrics
        from dvc.repo.params import Params
        from dvc.repo.plots import Plots
        from dvc.repo.stage import StageLoad
        from dvc.scm import SCM
        from dvc.stage.cache import StageCache
        from dvc_data.hashfile.state import State, StateNoop

        self.url = url
        self._fs_conf = {"repo_factory": repo_factory}
        self._fs = fs or localfs
        self._scm = scm
        self._data_index = None

        if rev and not fs:
            self._scm = scm = SCM(root_dir or os.curdir)
            root_dir = "/"
            self._fs = GitFileSystem(scm=self._scm, rev=rev)

        self.root_dir: str
        self.dvc_dir: Optional[str]
        (
            self.root_dir,
            self.dvc_dir,
        ) = self._get_repo_dirs(
            root_dir=root_dir,
            fs=self.fs,
            uninitialized=uninitialized,
            scm=scm,
        )

        self.config: Config = Config(self.dvc_dir, fs=self.fs, config=config)
        self._uninitialized = uninitialized

        # used by DVCFileSystem to determine if it should traverse subrepos
        self.subrepos = subrepos

        self.cloud: "DataCloud" = DataCloud(self)
        self.stage: "StageLoad" = StageLoad(self)

        self.lock: "LockBase"
        self.cache: CacheManager
        self.state: "StateBase"
        if isinstance(self.fs, GitFileSystem) or not self.dvc_dir:
            self.lock = LockNoop()
            self.state = StateNoop()
            self.cache = CacheManager(self)
        else:
            if isinstance(self.fs, LocalFileSystem):
                assert self.tmp_dir
                self.fs.makedirs(self.tmp_dir, exist_ok=True)

                self.lock = make_lock(
                    self.fs.path.join(self.tmp_dir, "lock"),
                    tmp_dir=self.tmp_dir,
                    hardlink_lock=self.config["core"].get("hardlink_lock", False),
                    friendly=True,
                )
                os.makedirs(self.site_cache_dir, exist_ok=True)
                self.state = State(self.root_dir, self.site_cache_dir, self.dvcignore)
            else:
                self.lock = LockNoop()
                self.state = StateNoop()

            self.cache = CacheManager(self)

            self.stage_cache = StageCache(self)

            self._ignore()

        self.metrics: Metrics = Metrics(self)
        self.plots: Plots = Plots(self)
        self.params: Params = Params(self)
        self.artifacts: Artifacts = Artifacts(self)

        self.stage_collection_error_handler: Optional[
            Callable[[str, Exception], None]
        ] = None
        self._lock_depth: int = 0

    def __str__(self):
        return self.url or self.root_dir

    @cached_property
    def local_dvc_dir(self):
        from dvc.fs import GitFileSystem, LocalFileSystem

        if not self.dvc_dir:
            return None

        if isinstance(self.fs, LocalFileSystem):
            return self.dvc_dir

        if not isinstance(self.fs, GitFileSystem):
            return None

        relparts = ()
        if self.root_dir != "/":
            # subrepo
            relparts = self.fs.path.relparts(self.root_dir, "/")

        dvc_dir = os.path.join(
            self.scm.root_dir,
            *relparts,
            self.DVC_DIR,
        )
        if os.path.exists(dvc_dir):
            return dvc_dir

        return None

    @cached_property
    def tmp_dir(self):
        if self.local_dvc_dir is None:
            return None

        return os.path.join(self.local_dvc_dir, "tmp")

    @cached_property
    def index(self) -> "Index":
        from dvc.repo.index import Index

        return Index.from_repo(self)

    def check_graph(
        self, stages: Iterable["Stage"], callback: Optional[Callable] = None
    ) -> None:
        if not getattr(self, "_skip_graph_checks", False):
            new = self.index.update(stages)
            if callable(callback):
                callback()
            new.check_graph()

    @staticmethod
    def open(url, *args, **kwargs):  # noqa: A003
        from .open_repo import open_repo

        return open_repo(url, *args, **kwargs)

    @cached_property
    def scm(self) -> Union["Git", "NoSCM"]:
        from dvc.scm import SCM, SCMError

        if self._scm:
            return self._scm

        no_scm = self.config["core"].get("no_scm", False)
        try:
            return SCM(self.root_dir, no_scm=no_scm)
        except SCMError:
            if self._uninitialized:
                # might not be a git/dvc repo at all
                # used in `params/metrics/plots` targets
                return SCM(self.root_dir, no_scm=True)
            raise

    @cached_property
    def scm_context(self) -> "SCMContext":
        from dvc.repo.scm_context import SCMContext

        return SCMContext(self.scm, self.config)

    @cached_property
    def dvcignore(self) -> DvcIgnoreFilter:
        return DvcIgnoreFilter(self.fs, self.root_dir)

    def get_rev(self):
        from dvc.fs import GitFileSystem, LocalFileSystem

        assert self.scm
        if isinstance(self.fs, LocalFileSystem):
            from dvc.scm import map_scm_exception

            with map_scm_exception():
                return self.scm.get_rev()
        assert isinstance(self.fs, GitFileSystem)
        return self.fs.rev

    @cached_property
    def experiments(self) -> "Experiments":
        from dvc.repo.experiments import Experiments

        return Experiments(self)

    @cached_property
    def machine(self) -> Optional["MachineManager"]:
        from dvc.machine import MachineManager

        if self.tmp_dir and (
            self.config["feature"].get("machine", False) or env2bool("DVC_TEST")
        ):
            return MachineManager(self)
        return None

    @property
    def fs(self) -> "FileSystem":
        return self._fs

    @fs.setter
    def fs(self, fs: "FileSystem"):
        self._fs = fs
        # Our graph cache is no longer valid, as it was based on the previous
        # fs.
        self._reset()

    @property
    def data_index(self) -> "DataIndex":
        from dvc_data.index import DataIndex

        if self._data_index is None:
            index_dir = os.path.join(self.site_cache_dir, "index", "data")
            os.makedirs(index_dir, exist_ok=True)
            self._data_index = DataIndex.open(os.path.join(index_dir, "db.db"))

        return self._data_index

    def __repr__(self):
        return f"{self.__class__.__name__}: '{self.root_dir}'"

    @classmethod
    def find_root(cls, root=None, fs=None) -> str:
        from dvc.fs import LocalFileSystem, localfs

        fs = fs or localfs
        root = root or os.curdir
        root_dir = fs.path.realpath(root)

        if not fs.isdir(root_dir):
            raise NotDvcRepoError(f"directory '{root}' does not exist")

        while True:
            dvc_dir = fs.path.join(root_dir, cls.DVC_DIR)
            if fs.isdir(dvc_dir):
                return root_dir
            if isinstance(fs, LocalFileSystem) and os.path.ismount(root_dir):
                break
            parent = fs.path.parent(root_dir)
            if parent == root_dir:
                break
            root_dir = parent

        msg = "you are not inside of a DVC repository"

        if isinstance(fs, LocalFileSystem):
            msg = f"{msg} (checked up to mount point '{root_dir}')"

        raise NotDvcRepoError(msg)

    @classmethod
    def find_dvc_dir(cls, root=None, fs=None) -> str:
        from dvc.fs import localfs

        fs = fs or localfs
        root_dir = cls.find_root(root, fs=fs)
        return fs.path.join(root_dir, cls.DVC_DIR)

    @staticmethod
    def init(root_dir=os.curdir, no_scm=False, force=False, subdir=False) -> "Repo":
        from dvc.repo.init import init

        return init(root_dir=root_dir, no_scm=no_scm, force=force, subdir=subdir)

    def unprotect(self, target):
        return self.cache.repo.unprotect(target)

    def _ignore(self):
        flist = [self.config.files["local"]]
        if tmp_dir := self.tmp_dir:
            flist.append(tmp_dir)
        if path_isin(self.cache.repo.path, self.root_dir):
            flist.append(self.cache.repo.path)

        for file in flist:
            self.scm_context.ignore(file)

    def brancher(self, *args, **kwargs):
        from dvc.repo.brancher import brancher

        return brancher(self, *args, **kwargs)

    def switch(self, rev: str) -> ContextManager[str]:
        from dvc.repo.brancher import switch

        return switch(self, rev)

    def used_objs(  # noqa: PLR0913
        self,
        targets=None,
        all_branches=False,
        with_deps=False,
        all_tags=False,
        all_commits=False,
        all_experiments=False,
        commit_date: Optional[str] = None,
        remote=None,
        force=False,
        jobs=None,
        recursive=False,
        used_run_cache=None,
        revs=None,
        num=1,
        push: bool = False,
    ):
        """Get the stages related to the given target and collect
        the `info` of its outputs.

        This is useful to know what files from the cache are _in use_
        (namely, a file described as an output on a stage).

        The scope is, by default, the working directory, but you can use
        `all_branches`/`all_tags`/`all_commits`/`all_experiments` to expand
        the scope.

        Returns:
            A dict mapping (remote) ODB instances to sets of objects that
            belong to each ODB. If the ODB instance is None, the objects
            are naive and do not belong to a specific remote ODB.
        """
        used = defaultdict(set)

        for _ in self.brancher(
            revs=revs,
            all_branches=all_branches,
            all_tags=all_tags,
            all_commits=all_commits,
            all_experiments=all_experiments,
            commit_date=commit_date,
            num=num,
        ):
            for odb, objs in self.index.used_objs(
                targets,
                remote=remote,
                force=force,
                jobs=jobs,
                recursive=recursive,
                with_deps=with_deps,
                push=push,
            ).items():
                used[odb].update(objs)

        if used_run_cache:
            for odb, objs in self.stage_cache.get_used_objs(
                used_run_cache, remote=remote, force=force, jobs=jobs
            ).items():
                used[odb].update(objs)

        return used

    def find_outs_by_path(self, path, outs=None, recursive=False, strict=True):
        # using `outs_graph` to ensure graph checks are run
        outs = outs or self.index.outs_graph

        abs_path = self.fs.path.abspath(path)
        fs_path = abs_path

        def func(out):
            def eq(one, two):
                return one == two

            match = eq if strict else out.fs.path.isin_or_eq

            if out.protocol == "local" and match(fs_path, out.fs_path):
                return True

            if recursive and out.fs.path.isin(out.fs_path, fs_path):
                return True

            return False

        matched = list(filter(func, outs))
        if not matched:
            raise OutputNotFoundError(path, self)

        return matched

    def is_dvc_internal(self, path):
        path_parts = self.fs.path.normpath(path).split(self.fs.sep)
        return self.DVC_DIR in path_parts

    @cached_property
    def datafs(self) -> "DataFileSystem":
        from dvc.fs.data import DataFileSystem

        return DataFileSystem(index=self.index.data["repo"])

    @cached_property
    def dvcfs(self) -> "DVCFileSystem":
        from dvc.fs.dvc import DVCFileSystem

        return DVCFileSystem(repo=self, subrepos=self.subrepos, **self._fs_conf)

    @cached_property
    def site_cache_dir(self) -> str:
        import getpass
        import hashlib

        from dvc.dirs import site_cache_dir
        from dvc.fs import GitFileSystem

        cache_dir = self.config["core"].get("site_cache_dir") or site_cache_dir()

        if isinstance(self.fs, GitFileSystem):
            relparts = ()
            if self.root_dir != "/":
                # subrepo
                relparts = self.fs.path.relparts(self.root_dir, "/")
            root_dir = os.path.join(self.scm.root_dir, *relparts)
        else:
            root_dir = self.root_dir

        repos_dir = os.path.join(cache_dir, "repo")

        umask = os.umask(0)
        try:
            os.makedirs(repos_dir, mode=0o777, exist_ok=True)
        finally:
            os.umask(umask)

        md5 = hashlib.md5(  # noqa: S324  # nosec B324, B303
            str((root_dir, getpass.getuser())).encode()
        )
        repo_token = md5.hexdigest()
        return os.path.join(repos_dir, repo_token)

    def close(self):
        self.scm.close()
        self.state.close()
        if self._data_index is not None:
            self._data_index.close()

    def _reset(self):
        self.scm._reset()  # pylint: disable=protected-access
        self.state.close()
        self.__dict__.pop("index", None)
        self.__dict__.pop("dvcignore", None)
        self.__dict__.pop("dvcfs", None)
        self.__dict__.pop("datafs", None)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()




dvc/repo/add.py
import os
from contextlib import contextmanager
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Iterator,
    List,
    NamedTuple,
    Optional,
    Tuple,
    Union,
)

from dvc.exceptions import (
    CacheLinkError,
    DvcException,
    InvalidArgumentError,
    OutputDuplicationError,
    OutputNotFoundError,
    OverlappingOutputPathsError,
)
from dvc.repo.scm_context import scm_context
from dvc.types import StrOrBytesPath
from dvc.ui import ui
from dvc.utils import glob_targets, resolve_output, resolve_paths

from . import locked

if TYPE_CHECKING:
    from dvc.repo import Repo
    from dvc.stage import Stage


class StageInfo(NamedTuple):
    stage: "Stage"
    output_exists: bool


def find_targets(
    targets: Union[StrOrBytesPath, Iterator[StrOrBytesPath]], glob: bool = False
) -> List[str]:
    if isinstance(targets, (str, bytes, os.PathLike)):
        targets_list = [os.fsdecode(targets)]
    else:
        targets_list = [os.fsdecode(target) for target in targets]
    return glob_targets(targets_list, glob=glob)


def validate_args(targets: List[str], **kwargs: Any) -> None:
    invalid_opt = None
    to_remote = kwargs.get("to_remote")

    if len(targets) > 1 and kwargs.get("file"):
        raise InvalidArgumentError("cannot use '--file' with multiple targets")

    if to_remote or kwargs.get("out"):
        message = "{option} can't be used with "
        message += "--to-remote" if to_remote else "-o"
        if len(targets) != 1:
            invalid_opt = "multiple targets"
        elif kwargs.get("no_commit"):
            invalid_opt = "--no-commit option"
        elif kwargs.get("external"):
            invalid_opt = "--external option"
    else:
        message = "{option} can't be used without --to-remote"
        if kwargs.get("remote"):
            invalid_opt = "--remote"
        elif kwargs.get("jobs"):
            invalid_opt = "--remote-jobs"

    if invalid_opt is not None:
        raise InvalidArgumentError(message.format(option=invalid_opt))


def get_or_create_stage(
    repo: "Repo",
    target: str,
    file: Optional[str] = None,
    transfer: bool = False,
    external: bool = False,
    force: bool = False,
    out: Optional[str] = None,
) -> StageInfo:
    if out:
        target = resolve_output(target, out, force=force)
    path, wdir, out = resolve_paths(repo, target, always_local=transfer and not out)

    try:
        (out_obj,) = repo.find_outs_by_path(target, strict=False)
        stage = out_obj.stage
        if not stage.is_data_source:
            raise DvcException(f"cannot update {out!r}: not a data source")
        return StageInfo(stage, output_exists=True)
    except OutputNotFoundError:
        stage = repo.stage.create(
            single_stage=True,
            validate=False,
            fname=file or path,
            wdir=wdir,
            outs=[out],
            external=external,
            force=force,
        )
        return StageInfo(stage, output_exists=False)


OVERLAPPING_CHILD_FMT = (
    "Cannot add '{out}', because it is overlapping with other "
    "DVC tracked output: '{parent}'.\n"
    "To include '{out}' in '{parent}', run "
    "'dvc commit {parent_stage}'"
)

OVERLAPPING_PARENT_FMT = (
    "Cannot add '{parent}', because it is overlapping with other "
    "DVC tracked output: '{out}'.\n"
    "To include '{out}' in '{parent}', run "
    "'dvc remove {out_stage}' and then 'dvc add {parent}'"
)


@contextmanager
def translate_graph_error(stages: List["Stage"]) -> Iterator[None]:
    try:
        yield
    except OverlappingOutputPathsError as exc:
        if exc.parent in [o for s in stages for o in s.outs]:
            msg = OVERLAPPING_PARENT_FMT.format(
                out=exc.overlapping_out,
                parent=exc.parent,
                out_stage=exc.overlapping_out.stage.addressing,
            )
        else:
            msg = OVERLAPPING_CHILD_FMT.format(
                out=exc.overlapping_out,
                parent=exc.parent,
                parent_stage=exc.parent.stage.addressing,
            )
        raise OverlappingOutputPathsError(  # noqa: B904
            exc.parent, exc.overlapping_out, msg
        )
    except OutputDuplicationError as exc:
        raise OutputDuplicationError(  # noqa: B904
            exc.output, list(set(exc.stages) - set(stages))
        )


def progress_iter(stages: Dict[str, StageInfo]) -> Iterator[Tuple[str, StageInfo]]:
    total = len(stages)
    desc = "Adding..."
    with ui.progress(
        stages.items(), total=total, desc=desc, unit="file", leave=True
    ) as pbar:
        if total == 1:
            pbar.bar_format = desc
            pbar.refresh()

        for item, stage_info in pbar:
            if total > 1:
                pbar.set_msg(str(stage_info.stage.outs[0]))
                pbar.refresh()
            yield item, stage_info
            if total == 1:  # restore bar format for stats
                # pylint: disable=no-member
                pbar.bar_format = pbar.BAR_FMT_DEFAULT


LINK_FAILURE_MESSAGE = (
    "\nSome targets could not be linked from cache to workspace.\n{}\n"
    "To re-link these targets, reconfigure cache types and then run:\n"
    "\n\tdvc checkout {}"
)


@contextmanager
def warn_link_failures() -> Iterator[List[str]]:
    link_failures: List[str] = []
    try:
        yield link_failures
    finally:
        if link_failures:
            msg = LINK_FAILURE_MESSAGE.format(
                CacheLinkError.SUPPORT_LINK,
                " ".join(link_failures),
            )
            ui.error_write(msg)


def _add_transfer(
    stage: "Stage",
    source: str,
    remote: Optional[str] = None,
    to_remote: bool = False,
    jobs: Optional[int] = None,
    force: bool = False,
) -> None:
    odb = None
    if to_remote:
        odb = stage.repo.cloud.get_remote_odb(remote, "add")
    stage.transfer(source, odb=odb, to_remote=to_remote, jobs=jobs, force=force)
    stage.dump()


def _add(stage: "Stage", source: Optional[str] = None, no_commit: bool = False) -> None:
    out = stage.outs[0]
    path = out.fs.path.abspath(source) if source else None
    try:
        stage.add_outs(path, no_commit=no_commit)
    except CacheLinkError:
        stage.dump()
        raise
    stage.dump()


@locked
@scm_context
def add(  # noqa: PLR0913
    repo: "Repo",
    targets: Union[StrOrBytesPath, Iterator[StrOrBytesPath]],
    no_commit: bool = False,
    file: Optional[str] = None,
    external: bool = False,
    glob: bool = False,
    out: Optional[str] = None,
    remote: Optional[str] = None,
    to_remote: bool = False,
    jobs: Optional[int] = None,
    force: bool = False,
) -> List["Stage"]:
    transfer = to_remote or bool(out)
    add_targets = find_targets(targets, glob=glob)
    if not add_targets:
        return []

    validate_args(
        add_targets,
        no_commit=no_commit,
        file=file,
        external=external,
        out=out,
        remote=remote,
        to_remote=to_remote,
        jobs=jobs,
    )
    stages_with_targets = {
        target: get_or_create_stage(
            repo,
            target,
            file=file,
            transfer=transfer,
            external=external,
            force=force,
            out=out,
        )
        for target in add_targets
    }

    stages = [stage for stage, _ in stages_with_targets.values()]
    msg = "Collecting stages from the workspace"
    with translate_graph_error(stages), ui.status(msg) as st:
        repo.check_graph(stages=stages, callback=lambda: st.update("Checking graph"))

    if transfer:
        assert stages_with_targets
        (source, (stage, _)) = next(iter(stages_with_targets.items()))
        _add_transfer(stage, source, remote, to_remote, jobs=jobs, force=force)
        return [stage]

    with warn_link_failures() as link_failures:
        for source, (stage, output_exists) in progress_iter(stages_with_targets):
            try:
                _add(stage, source if output_exists else None, no_commit=no_commit)
            except CacheLinkError:
                link_failures.append(stage.relpath)
    return stages




dvc/repo/artifacts.py
import logging
import re
from pathlib import Path
from typing import Dict, Optional

from dvc.annotations import Artifact
from dvc.dvcfile import PROJECT_FILE
from dvc.exceptions import InvalidArgumentError
from dvc.repo import Repo
from dvc.utils import relpath
from dvc.utils.serialize import modify_yaml

logger = logging.getLogger(__name__)


# Constants are taken from GTO.
# When we make it a dependency, we can import them instead
SEPARATOR_IN_NAME = ":"
DIRNAME = r"[a-z0-9-_./]+"
NAME = r"[a-z0-9]([a-z0-9-/]*[a-z0-9])?"
NAME_RE = re.compile(f"^{NAME}$")
FULLNAME = f"((?P<dirname>{DIRNAME}){SEPARATOR_IN_NAME})?(?P<name>{NAME})"
FULLNAME_RE = re.compile(f"^{FULLNAME}$")


def name_is_compatible(name: str) -> bool:
    return bool(NAME_RE.search(name))


def check_name_format(name: str) -> None:
    if not name_is_compatible(name):
        raise InvalidArgumentError(
            f"Can't use '{name}' as artifact name (ID)."
            " You can use letters and numbers, and use '-' as separator"
            " (but not at the start or end)."
        )


def check_for_nested_dvc_repo(dvcfile: Path):
    if dvcfile.is_absolute():
        raise InvalidArgumentError("Use relative path to dvc.yaml.")
    path = dvcfile.parent
    while path.name:
        if (path / Repo.DVC_DIR).is_dir():
            raise InvalidArgumentError(
                f"Nested DVC repos like {path} are not supported."
            )
        path = path.parent


class Artifacts:
    def __init__(self, repo: "Repo") -> None:
        self.repo = repo

    def read(self) -> Dict[str, Dict[str, Artifact]]:
        artifacts: Dict[str, Dict[str, Artifact]] = {}
        for (
            dvcfile,
            dvcfile_artifacts,
        ) in self.repo.index._artifacts.items():  # pylint: disable=protected-access
            dvcyaml = relpath(dvcfile, self.repo.root_dir)
            artifacts[dvcyaml] = {}
            for name, value in dvcfile_artifacts.items():
                try:
                    check_name_format(name)
                except InvalidArgumentError as e:
                    logger.warning(e.msg)
                artifacts[dvcyaml][name] = Artifact(**value)
        return artifacts

    def add(self, name: str, artifact: Artifact, dvcfile: Optional[str] = None):
        with self.repo.scm_context(quiet=True):
            check_name_format(name)
            dvcyaml = Path(dvcfile or PROJECT_FILE)
            check_for_nested_dvc_repo(
                dvcyaml.relative_to(self.repo.root_dir)
                if dvcyaml.is_absolute()
                else dvcyaml
            )

            with modify_yaml(dvcyaml) as data:
                artifacts = data.setdefault("artifacts", {})
                artifacts.update({name: artifact.to_dict()})

            self.repo.scm_context.track_file(dvcfile)

        return artifacts.get(name)




dvc/repo/brancher.py
import logging
from contextlib import contextmanager
from typing import TYPE_CHECKING, Iterator, Optional, Tuple

from scmrepo.git import Git

from dvc.exceptions import NotDvcRepoError
from dvc.scm import iter_revs

if TYPE_CHECKING:
    from dvc.repo import Repo

logger = logging.getLogger(__name__)


def brancher(  # noqa: E302
    self,
    revs=None,
    all_branches=False,
    all_tags=False,
    all_commits=False,
    all_experiments=False,
    commit_date: Optional[str] = None,
    sha_only=False,
    num=1,
):
    """Generator that iterates over specified revisions.

    Args:
        revs (list): a list of revisions to iterate over.
        all_branches (bool): iterate over all available branches.
        all_commits (bool): iterate over all commits.
        all_tags (bool): iterate over all available tags.
        commit_date (str): Keep experiments from the commits after(include)
                            a certain date. Date must match the extended
                            ISO 8601 format (YYYY-MM-DD).
        sha_only (bool): only return git SHA for a revision.

    Yields:
        str: the display name for the currently selected fs, it could be:
            - a git revision identifier
            - empty string it there is no branches to iterate over
            - "workspace" if there are uncommitted changes in the SCM repo
    """
    if not any(
        [
            revs,
            all_branches,
            all_tags,
            all_commits,
            all_experiments,
            commit_date,
        ]
    ):
        yield ""
        return

    from dvc.fs import LocalFileSystem

    repo_root_parts = ()
    if self.fs.path.isin(self.root_dir, self.scm.root_dir):
        repo_root_parts = self.fs.path.relparts(self.root_dir, self.scm.root_dir)

    cwd_parts = ()
    if self.fs.path.isin(self.fs.path.getcwd(), self.scm.root_dir):
        cwd_parts = self.fs.path.relparts(self.fs.path.getcwd(), self.scm.root_dir)

    saved_fs = self.fs
    saved_root = self.root_dir

    scm = self.scm

    logger.trace("switching fs to workspace")  # type: ignore[attr-defined]
    self.fs = LocalFileSystem(url=self.root_dir)
    yield "workspace"

    revs = revs.copy() if revs else []
    if "workspace" in revs:
        revs.remove("workspace")

    found_revs = iter_revs(
        scm,
        revs,
        all_branches=all_branches,
        all_tags=all_tags,
        all_commits=all_commits,
        all_experiments=all_experiments,
        commit_date=commit_date,
        num=num,
    )

    try:
        for sha, names in found_revs.items():
            try:
                _switch_fs(self, sha, repo_root_parts, cwd_parts)
                yield sha if sha_only else ",".join(names)
            except NotDvcRepoError:
                # ignore revs that don't contain repo root
                # (i.e. revs from before a subdir=True repo was init'ed)
                pass
    finally:
        self.fs = saved_fs
        self.root_dir = saved_root


def _switch_fs(
    repo: "Repo",
    rev: str,
    repo_root_parts: Tuple[str, ...],
    cwd_parts: Tuple[str, ...],
):
    from dvc.fs import GitFileSystem, LocalFileSystem

    if rev == "workspace":
        logger.trace("switching fs to workspace")  # type: ignore[attr-defined]
        repo.fs = LocalFileSystem(url=repo.root_dir)
        return

    logger.trace("switching fs to revision %s", rev[:7])  # type: ignore[attr-defined]
    assert isinstance(repo.scm, Git)
    fs = GitFileSystem(scm=repo.scm, rev=rev)
    root_dir = repo.fs.path.join("/", *repo_root_parts)
    if not fs.exists(root_dir):
        raise NotDvcRepoError(f"Commit '{rev[:7]}' does not contain a DVC repo")

    repo.fs = fs
    repo.root_dir = root_dir

    if cwd_parts:
        cwd = repo.fs.path.join("/", *cwd_parts)
        repo.fs.path.chdir(cwd)


@contextmanager
def switch(repo: "Repo", rev: str) -> Iterator[str]:
    """Switch to a specific revision."""
    from dvc.scm import resolve_rev

    if rev != "workspace":
        rev = resolve_rev(repo.scm, rev)

    repo_root_parts = ()
    if repo.fs.path.isin(repo.root_dir, repo.scm.root_dir):
        repo_root_parts = repo.fs.path.relparts(repo.root_dir, repo.scm.root_dir)

    cwd_parts = ()
    if repo.fs.path.isin(repo.fs.path.getcwd(), repo.scm.root_dir):
        cwd_parts = repo.fs.path.relparts(repo.fs.path.getcwd(), repo.scm.root_dir)

    saved_fs = repo.fs
    saved_root = repo.root_dir
    try:
        _switch_fs(repo, rev, repo_root_parts, cwd_parts)
        yield rev
    finally:
        repo.fs = saved_fs
        repo.root_dir = saved_root




dvc/repo/checkout.py
import logging
import os
from typing import TYPE_CHECKING, Dict, List, Set

from dvc.exceptions import CheckoutError, CheckoutErrorSuggestGit, NoOutputOrStageError
from dvc.utils import relpath

from . import locked

if TYPE_CHECKING:
    from . import Repo
    from .stage import StageInfo

logger = logging.getLogger(__name__)


def _fspath_dir(path):
    if not os.path.exists(str(path)):
        return str(path)

    path = relpath(path)
    return os.path.join(path, "") if os.path.isdir(path) else path


def _remove_unused_links(repo):
    used = [out.fspath for out in repo.index.outs if out.protocol == "local"]
    unused = repo.state.get_unused_links(used, repo.fs)
    ret = [_fspath_dir(u) for u in unused]
    repo.state.remove_links(unused, repo.fs)
    return ret


def get_all_files_numbers(pairs):
    return sum(stage.get_all_files_number(filter_info) for stage, filter_info in pairs)


def _collect_pairs(
    self: "Repo", targets, with_deps: bool, recursive: bool
) -> Set["StageInfo"]:
    from dvc.stage.exceptions import StageFileBadNameError, StageFileDoesNotExistError

    pairs: Set["StageInfo"] = set()
    for target in targets:
        try:
            pairs.update(
                self.stage.collect_granular(
                    target, with_deps=with_deps, recursive=recursive
                )
            )
        except (
            StageFileDoesNotExistError,
            StageFileBadNameError,
            NoOutputOrStageError,
        ) as exc:
            if not target:
                raise
            raise CheckoutErrorSuggestGit(target) from exc

    return pairs


@locked
def checkout(
    self,
    targets=None,
    with_deps=False,
    force=False,
    relink=False,
    recursive=False,
    allow_missing=False,
    **kwargs,
):
    from dvc.fs.callbacks import Callback

    stats: Dict[str, List[str]] = {
        "added": [],
        "deleted": [],
        "modified": [],
        "failed": [],
    }
    if not targets:
        targets = [None]
        stats["deleted"] = _remove_unused_links(self)

    if isinstance(targets, str):
        targets = [targets]

    pairs = _collect_pairs(self, targets, with_deps, recursive)
    total = get_all_files_numbers(pairs)
    with Callback.as_tqdm_callback(
        unit="file",
        desc="Checkout",
        disable=total == 0,
    ) as cb:
        cb.set_size(total)
        for stage, filter_info in pairs:
            result = stage.checkout(
                force=force,
                progress_callback=cb,
                relink=relink,
                filter_info=filter_info,
                allow_missing=allow_missing,
                **kwargs,
            )
            for key, items in result.items():
                stats[key].extend(_fspath_dir(path) for path in items)

    if stats.get("failed"):
        raise CheckoutError(stats["failed"], stats)

    del stats["failed"]
    return stats




dvc/repo/collect.py
import logging
from typing import TYPE_CHECKING, Callable, Iterable, List, Optional, Tuple

if TYPE_CHECKING:
    from dvc.output import Output
    from dvc.repo import Repo

logger = logging.getLogger(__name__)


FilterFn = Callable[["Output"], bool]
Outputs = List["Output"]
StrPaths = List[str]


def _collect_outs(
    repo: "Repo", output_filter: Optional[FilterFn] = None, deps: bool = False
) -> Outputs:
    index = repo.index
    index.check_graph()  # ensure graph is correct
    return list(filter(output_filter, index.deps if deps else index.outs))


def _collect_paths(
    repo: "Repo",
    targets: Iterable[str],
    recursive: bool = False,
) -> StrPaths:
    from dvc.fs.dvc import DVCFileSystem

    fs = DVCFileSystem(repo=repo)
    fs_paths = [fs.from_os_path(target) for target in targets]

    target_paths: StrPaths = []
    for fs_path in fs_paths:
        if recursive and fs.isdir(fs_path):
            target_paths.extend(fs.find(fs_path))
        target_paths.append(fs_path)

    return target_paths


def _filter_outs(
    outs: Outputs, fs_paths: StrPaths, duplicates=False
) -> Tuple[Outputs, StrPaths]:
    res_outs: Outputs = []
    fs_res_paths = fs_paths

    for out in outs:
        fs_path = out.repo.dvcfs.from_os_path(out.fs_path)
        if fs_path in fs_paths:
            res_outs.append(out)
            if not duplicates:
                # MUTATING THE SAME LIST!!
                fs_res_paths.remove(fs_path)

    return res_outs, fs_res_paths


def collect(
    repo: "Repo",
    deps: bool = False,
    targets: Optional[Iterable[str]] = None,
    output_filter: Optional[FilterFn] = None,
    recursive: bool = False,
    duplicates: bool = False,
) -> Tuple[Outputs, StrPaths]:
    assert targets or output_filter

    outs: Outputs = _collect_outs(repo, output_filter=output_filter, deps=deps)

    if not targets:
        fs_paths: StrPaths = []
        return outs, fs_paths

    target_paths = _collect_paths(repo, targets, recursive=recursive)

    return _filter_outs(outs, target_paths, duplicates=duplicates)




dvc/repo/commit.py
from dvc import prompt

from . import locked


def _prepare_message(stage, changes):
    changed_deps, changed_outs, changed_stage = changes
    if changed_deps and changed_outs:
        msg = "dependencies {deps} and outputs {outs} of {stage} changed."
    elif changed_deps:
        msg = "dependencies {deps} of {stage} changed."
    elif changed_outs:
        msg = "outputs {outs} of {stage} changed."
    else:
        msg = "{stage_changed}"

    msg += " Are you sure you want to commit it?"

    kw = {
        "stage": stage,
        "deps": changed_deps,
        "outs": changed_outs,
        "stage_changed": changed_stage,
    }
    return msg.format_map(kw)


def prompt_to_commit(stage, changes, force=False):
    from dvc.stage.exceptions import StageCommitError

    if not (force or prompt.confirm(_prepare_message(stage, changes))):
        raise StageCommitError(
            f"unable to commit changed {stage}. Use `-f|--force` to force."
        )


@locked
def commit(
    self,
    target,
    with_deps=False,
    recursive=False,
    force=False,
    allow_missing=False,
    data_only=False,
    relink=True,
):
    stages_info = [
        info
        for info in self.stage.collect_granular(
            target, with_deps=with_deps, recursive=recursive
        )
        if not data_only or info.stage.is_data_source
    ]
    for stage_info in stages_info:
        stage = stage_info.stage
        if force:
            stage.save(allow_missing=allow_missing)
        else:
            changes = stage.changed_entries()
            if any(changes):
                prompt_to_commit(stage, changes, force=force)
                stage.save(allow_missing=allow_missing)
        stage.commit(
            filter_info=stage_info.filter_info,
            allow_missing=allow_missing,
            relink=relink,
        )
        stage.dump(update_pipeline=False)
    return [s.stage for s in stages_info]




dvc/repo/data.py
import os
import posixpath
from typing import TYPE_CHECKING, Any, Dict, Iterable, List, TypedDict, Union

from dvc.ui import ui

if TYPE_CHECKING:
    from dvc.repo import Repo
    from dvc.scm import Git, NoSCM
    from dvc_data.index import DataIndex
    from dvc_data.index.diff import Change


def posixpath_to_os_path(path: str) -> str:
    return path.replace(posixpath.sep, os.path.sep)


def _adapt_typ(typ: str) -> str:
    from dvc_data.index.diff import ADD, DELETE, MODIFY

    if typ == MODIFY:
        return "modified"

    if typ == ADD:
        return "added"

    if typ == DELETE:
        return "deleted"

    return typ


def _adapt_path(change: "Change") -> str:
    isdir = False
    if change.new and change.new.meta:
        isdir = change.new.meta.isdir
    elif change.old and change.old.meta:
        isdir = change.old.meta.isdir
    key = change.key
    if isdir:
        key = (*key, "")
    return os.path.sep.join(key)


def _diff(
    old: "DataIndex",
    new: "DataIndex",
    *,
    granular: bool = False,
    not_in_cache: bool = False,
    not_in_remote: bool = False,
    remote_refresh: bool = False,
) -> Dict[str, List[str]]:
    from dvc_data.index import StorageError
    from dvc_data.index.diff import UNCHANGED, UNKNOWN, diff

    ret: Dict[str, List[str]] = {}

    def _add_change(typ, change):
        typ = _adapt_typ(typ)
        if typ not in ret:
            ret[typ] = []

        ret[typ].append(_adapt_path(change))

    for change in diff(
        old,
        new,
        with_unchanged=True,
        shallow=not granular,
        hash_only=True,
        with_unknown=True,
    ):
        if (
            change.typ == UNCHANGED
            and change.old
            and change.new
            and not change.old.hash_info
            and not change.new.hash_info
        ):
            # NOTE: emulating previous behaviour
            continue

        if change.typ == UNKNOWN and not change.new:
            # NOTE: emulating previous behaviour
            continue

        if (
            not_in_cache
            and change.old
            and change.old.hash_info
            and not old.storage_map.cache_exists(change.old)
        ):
            # NOTE: emulating previous behaviour
            _add_change("not_in_cache", change)

        try:
            if (
                not_in_remote
                and change.old
                and change.old.hash_info
                and not old.storage_map.remote_exists(
                    change.old, refresh=remote_refresh
                )
            ):
                _add_change("not_in_remote", change)
        except StorageError:
            pass

        _add_change(change.typ, change)

    return ret


class GitInfo(TypedDict, total=False):
    staged: Dict[str, List[str]]
    unstaged: Dict[str, List[str]]
    untracked: List[str]
    is_empty: bool
    is_dirty: bool


def _git_info(scm: Union["Git", "NoSCM"], untracked_files: str = "all") -> GitInfo:
    from scmrepo.exceptions import SCMError

    from dvc.scm import NoSCM

    if isinstance(scm, NoSCM):
        return {}

    try:
        scm.get_rev()
    except SCMError:
        empty_repo = True
    else:
        empty_repo = False

    staged, unstaged, untracked = scm.status(untracked_files=untracked_files)
    if os.name == "nt":
        untracked = [posixpath_to_os_path(path) for path in untracked]
    # NOTE: order is important here.
    return GitInfo(
        staged=staged,
        unstaged=unstaged,
        untracked=untracked,
        is_empty=empty_repo,
        is_dirty=any([staged, unstaged, untracked]),
    )


def _diff_index_to_wtree(repo: "Repo", **kwargs: Any) -> Dict[str, List[str]]:
    from .index import build_data_index

    with ui.status("Building workspace index"):
        workspace = build_data_index(
            repo.index, repo.root_dir, repo.fs, compute_hash=True
        )

    with ui.status("Calculating diff between index/workspace"):
        return _diff(
            repo.index.data["repo"],
            workspace,
            not_in_cache=True,
            **kwargs,
        )


def _diff_head_to_index(
    repo: "Repo", head: str = "HEAD", **kwargs: Any
) -> Dict[str, List[str]]:
    index = repo.index.data["repo"]

    with repo.switch(head):
        head_index = repo.index.data["repo"]

    with ui.status("Calculating diff between head/index"):
        return _diff(head_index, index, **kwargs)


class Status(TypedDict):
    not_in_cache: List[str]
    not_in_remote: List[str]
    committed: Dict[str, List[str]]
    uncommitted: Dict[str, List[str]]
    untracked: List[str]
    unchanged: List[str]
    git: GitInfo


def _transform_git_paths_to_dvc(repo: "Repo", files: Iterable[str]) -> List[str]:
    """Transform files rel. to Git root to DVC root, and drop outside files."""
    rel = repo.fs.path.relpath(repo.root_dir, repo.scm.root_dir).rstrip("/")

    # if we have repo root in a different location than scm's root,
    # i.e. subdir repo, all git_paths need to be transformed rel. to the DVC
    # repo root and anything outside need to be filtered out.
    if rel not in (os.curdir, ""):
        prefix = rel + os.sep
        length = len(prefix)
        files = (file[length:] for file in files if file.startswith(prefix))

    start = repo.fs.path.relpath(repo.fs.path.getcwd(), repo.root_dir)
    if start in (os.curdir, ""):
        return list(files)
    # we need to convert repo relative paths to curdir relative.
    return [repo.fs.path.relpath(file, start) for file in files]


def status(
    repo: "Repo",
    untracked_files: str = "no",
    **kwargs: Any,
) -> Status:
    from dvc.scm import NoSCMError, SCMError

    head = kwargs.pop("head", "HEAD")
    uncommitted_diff = _diff_index_to_wtree(
        repo,
        **kwargs,
    )
    unchanged = set(uncommitted_diff.pop("unchanged", []))

    try:
        committed_diff = _diff_head_to_index(repo, head=head, **kwargs)
    except (SCMError, NoSCMError):
        committed_diff = {}
    else:
        unchanged &= set(committed_diff.pop("unchanged", []))

    git_info = _git_info(repo.scm, untracked_files=untracked_files)
    untracked = git_info.get("untracked", [])
    untracked = _transform_git_paths_to_dvc(repo, untracked)
    # order matters here
    return Status(
        not_in_cache=uncommitted_diff.pop("not_in_cache", []),
        not_in_remote=uncommitted_diff.pop("not_in_remote", []),
        committed=committed_diff,
        uncommitted=uncommitted_diff,
        untracked=untracked,
        unchanged=list(unchanged),
        git=git_info,
    )




dvc/repo/destroy.py
from dvc.ignore import destroy as destroy_dvcignore
from dvc.utils.fs import remove

from . import locked


@locked
def _destroy_stages(repo):
    for stage in repo.index.stages:
        stage.unprotect_outs()
        stage.dvcfile.remove(force=True)


# NOTE: not locking `destroy`, as `remove` will need to delete `.dvc` dir,
# which will cause issues on Windows, as `.dvc/lock` will be busy.
def destroy(repo):
    _destroy_stages(repo)
    repo.close()
    destroy_dvcignore(repo.root_dir)
    remove(repo.dvc_dir)




dvc/repo/diff.py
import errno
import logging
import os
from collections import defaultdict
from typing import Dict, List, Optional

from dvc.repo import locked
from dvc.ui import ui

logger = logging.getLogger(__name__)


def _path(entry):
    if entry and entry.meta and entry.meta.isdir:
        return os.path.join(*entry.key, "")
    return os.path.join(*entry.key)


def _hash(entry):
    if entry and entry.hash_info:
        return entry.hash_info.value
    return None


def _diff(old, new, with_missing=False):
    from dvc_data.index.diff import ADD, DELETE, MODIFY, RENAME
    from dvc_data.index.diff import diff as idiff

    ret: "Dict[str, List[Dict]]" = {
        "added": [],
        "deleted": [],
        "modified": [],
        "renamed": [],
        "not in cache": [],
    }

    for change in idiff(
        old,
        new,
        with_renames=True,
        hash_only=True,
    ):
        if change.typ == ADD:
            ret["added"].append(
                {
                    "path": _path(change.new),
                    "hash": _hash(change.new),
                }
            )
        elif change.typ == DELETE:
            ret["deleted"].append(
                {
                    "path": _path(change.old),
                    "hash": _hash(change.old),
                }
            )
        elif change.typ == MODIFY:
            ret["modified"].append(
                {
                    "path": _path(change.old),
                    "hash": {
                        "old": _hash(change.old),
                        "new": _hash(change.new),
                    },
                }
            )
        elif change.typ == RENAME:
            ret["renamed"].append(
                {
                    "path": {
                        "old": _path(change.old),
                        "new": _path(change.new),
                    },
                    "hash": _hash(change.old),
                }
            )

        if (
            with_missing
            and change.old
            and change.old.hash_info
            and not old.storage_map.cache_exists(change.old)
        ):
            ret["not in cache"].append(
                {
                    "path": _path(change.old),
                    "hash": _hash(change.old),
                }
            )

    return ret if any(ret.values()) else {}


@locked
def diff(
    self,
    a_rev: str = "HEAD",
    b_rev: Optional[str] = None,
    targets: Optional[List[str]] = None,
    recursive: bool = False,
):
    """
    By default, it compares the workspace with the last commit's fs.

    This implementation differs from `git diff` since DVC doesn't have
    the concept of `index`, but it keeps the same interface, thus,
    `dvc diff` would be the same as `dvc diff HEAD`.
    """
    if self.scm.no_commits:
        return {}

    indexes = {}
    missing_targets = defaultdict(set)
    with_missing = False
    if not b_rev:
        b_rev = "workspace"
        with_missing = True

    for rev in self.brancher(revs=[a_rev, b_rev]):
        if rev == "workspace" and b_rev != "workspace":
            # brancher always returns workspace, but we only need to compute
            # workspace paths/checksums if b_rev was None
            continue

        def onerror(target, _exc):
            # pylint: disable-next=cell-var-from-loop
            missing_targets[rev].add(target)  # noqa: B023

        view = self.index.targets_view(
            targets,
            onerror=onerror,
            recursive=recursive,
        )

        if rev == "workspace":
            from .index import build_data_index

            with ui.status("Building workspace index"):
                data = build_data_index(
                    view,
                    self.root_dir,
                    self.fs,
                    compute_hash=True,
                )
        else:
            data = view.data["repo"]

        assert rev not in indexes
        indexes[rev] = data

    if targets:
        old_missing = missing_targets.get(a_rev, set())
        new_missing = missing_targets.get(b_rev, set())

        # check for overlapping missing targets between a_rev and b_rev
        for target in old_missing & new_missing:
            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), target)

    old = indexes[a_rev]
    new = indexes[b_rev]

    with ui.status("Calculating diff"):
        return _diff(old, new, with_missing=with_missing)




dvc/repo/fetch.py
import logging
from contextlib import suppress
from typing import TYPE_CHECKING, Optional, Sequence

from dvc.config import NoRemoteError
from dvc.exceptions import DownloadError
from dvc.fs import Schemes

from . import locked

if TYPE_CHECKING:
    from dvc.data_cloud import Remote
    from dvc.repo import Repo
    from dvc.types import TargetType
    from dvc_data.hashfile.db import HashFileDB
    from dvc_data.hashfile.transfer import TransferResult

logger = logging.getLogger(__name__)


@locked
def fetch(  # noqa: C901, PLR0913
    self,
    targets=None,
    jobs=None,
    remote=None,
    all_branches=False,
    with_deps=False,
    all_tags=False,
    recursive=False,
    all_commits=False,
    run_cache=False,
    revs=None,
    odb: Optional["HashFileDB"] = None,
) -> int:
    """Download data items from a cloud and imported repositories

    Returns:
        int: number of successfully downloaded files

    Raises:
        DownloadError: thrown when there are failed downloads, either
            during `cloud.pull` or trying to fetch imported files

        config.NoRemoteError: thrown when downloading only local files and no
            remote is configured
    """
    from dvc.repo.imports import save_imports
    from dvc_data.hashfile.transfer import TransferResult

    if isinstance(targets, str):
        targets = [targets]

    worktree_remote: Optional["Remote"] = None
    with suppress(NoRemoteError):
        _remote = self.cloud.get_remote(name=remote)
        if _remote.worktree or _remote.fs.version_aware:
            worktree_remote = _remote

    failed_count = 0
    transferred_count = 0

    try:
        if run_cache:
            self.stage_cache.pull(remote)
    except DownloadError as exc:
        failed_count += exc.amount

    no_remote_msg: Optional[str] = None
    result = TransferResult(set(), set())
    try:
        if worktree_remote is not None:
            transferred_count += _fetch_worktree(
                self,
                worktree_remote,
                revs=revs,
                all_branches=all_branches,
                all_tags=all_tags,
                all_commits=all_commits,
                targets=targets,
                jobs=jobs,
                with_deps=with_deps,
                recursive=recursive,
            )
        else:
            d, f = _fetch(
                self,
                targets,
                all_branches=all_branches,
                all_tags=all_tags,
                all_commits=all_commits,
                with_deps=with_deps,
                force=True,
                remote=remote,
                jobs=jobs,
                recursive=recursive,
                revs=revs,
                odb=odb,
            )
            result.transferred.update(d)
            result.failed.update(f)
    except NoRemoteError as exc:
        no_remote_msg = str(exc)

    for rev in self.brancher(
        revs=revs,
        all_branches=all_branches,
        all_tags=all_tags,
        all_commits=all_commits,
    ):
        imported = save_imports(
            self,
            targets,
            unpartial=not rev or rev == "workspace",
            recursive=recursive,
        )
        result.transferred.update(imported)
        result.failed.difference_update(imported)

    failed_count += len(result.failed)

    if failed_count:
        if no_remote_msg:
            logger.error(no_remote_msg)
        raise DownloadError(failed_count)

    transferred_count += len(result.transferred)
    return transferred_count


def _fetch(
    repo: "Repo",
    targets: "TargetType",
    remote: Optional[str] = None,
    jobs: Optional[int] = None,
    odb: Optional["HashFileDB"] = None,
    **kwargs,
) -> "TransferResult":
    from dvc_data.hashfile.transfer import TransferResult

    result = TransferResult(set(), set())
    used = repo.used_objs(
        targets,
        remote=remote,
        jobs=jobs,
        **kwargs,
    )
    if odb:
        all_ids = set()
        for _odb, obj_ids in used.items():
            all_ids.update(obj_ids)
        d, f = repo.cloud.pull(
            all_ids,
            jobs=jobs,
            remote=remote,
            odb=odb,
        )
        result.transferred.update(d)
        result.failed.update(f)
    else:
        for src_odb, obj_ids in sorted(
            used.items(),
            key=lambda item: item[0] is not None
            and item[0].fs.protocol == Schemes.MEMORY,
        ):
            d, f = repo.cloud.pull(
                obj_ids,
                jobs=jobs,
                remote=remote,
                odb=src_odb,
            )
            result.transferred.update(d)
            result.failed.update(f)
    return result


def _fetch_worktree(
    repo: "Repo",
    remote: "Remote",
    revs: Optional[Sequence[str]] = None,
    all_branches: bool = False,
    all_tags: bool = False,
    all_commits: bool = False,
    targets: Optional["TargetType"] = None,
    jobs: Optional[int] = None,
    **kwargs,
) -> int:
    from dvc.repo.worktree import fetch_worktree

    downloaded = 0
    for _ in repo.brancher(
        revs=revs,
        all_branches=all_branches,
        all_tags=all_tags,
        all_commits=all_commits,
    ):
        downloaded += fetch_worktree(repo, remote, targets=targets, jobs=jobs, **kwargs)
    return downloaded




dvc/repo/freeze.py
import typing

from . import locked

if typing.TYPE_CHECKING:
    from . import Repo


@locked
def _set(repo: "Repo", target, frozen):
    stage = repo.stage.get_target(target)
    stage.frozen = frozen
    stage.dump(update_lock=False)

    return stage


def freeze(repo, target):
    return _set(repo, target, True)


def unfreeze(repo, target):
    return _set(repo, target, False)




dvc/repo/gc.py
import logging
from typing import TYPE_CHECKING, List, Optional, Set, Tuple

from dvc.exceptions import InvalidArgumentError

from . import locked

if TYPE_CHECKING:
    from dvc.repo import Repo
    from dvc_data.hashfile.hash_info import HashInfo
    from dvc_objects.db import ObjectDB

logger = logging.getLogger(__name__)


def _validate_args(**kwargs):
    not_in_remote = kwargs.pop("not_in_remote", None)
    cloud = kwargs.pop("cloud", None)
    remote = kwargs.pop("remote", None)
    if remote and not (cloud or not_in_remote):
        raise InvalidArgumentError("`--remote` requires `--cloud` or `--not-in-remote`")
    if not_in_remote and cloud:
        raise InvalidArgumentError(
            "`--not-in-remote` and `--cloud` are mutually exclusive"
        )
    if not any(kwargs.values()):
        raise InvalidArgumentError(
            "Either of `-w|--workspace`, `-a|--all-branches`, `-T|--all-tags` "
            "`--all-experiments`, `--all-commits`, `--date` or `--rev` "
            "needs to be set."
        )
    if kwargs.get("num") and not kwargs.get("rev"):
        raise InvalidArgumentError("`--num` can only be used alongside `--rev`")


def _used_obj_ids_not_in_remote(repo, default_remote, jobs, remote_odb_to_obj_ids):
    used_obj_ids = set()
    remote_oids = set()
    for remote_odb, obj_ids in remote_odb_to_obj_ids:
        if remote_odb is None:
            remote_odb = repo.cloud.get_remote_odb(default_remote, "gc --not-in-remote")

        remote_oids.update(
            remote_odb.list_oids_exists({x.value for x in obj_ids}, jobs=jobs)
        )
        used_obj_ids.update(obj_ids)
    return {obj for obj in used_obj_ids if obj.value not in remote_oids}


@locked
def gc(  # noqa: PLR0912, PLR0913, C901
    self: "Repo",
    all_branches: bool = False,
    cloud: bool = False,
    remote: Optional[str] = None,
    with_deps: bool = False,
    all_tags: bool = False,
    all_commits: bool = False,
    all_experiments: bool = False,
    force: bool = False,
    jobs: Optional[int] = None,
    repos: Optional[List[str]] = None,
    workspace: bool = False,
    commit_date: Optional[str] = None,
    rev: Optional[str] = None,
    num: Optional[int] = None,
    not_in_remote: bool = False,
):
    # require `workspace` to be true to come into effect.
    # assume `workspace` to be enabled if any of `all_tags`, `all_commits`,
    # `all_experiments` or `all_branches` are enabled.
    _validate_args(
        workspace=workspace,
        all_tags=all_tags,
        all_commits=all_commits,
        all_branches=all_branches,
        all_experiments=all_experiments,
        commit_date=commit_date,
        rev=rev,
        num=num,
        cloud=cloud,
        not_in_remote=not_in_remote,
    )

    from contextlib import ExitStack

    from dvc.repo import Repo
    from dvc_data.hashfile.db import get_index
    from dvc_data.hashfile.gc import gc as ogc

    if not repos:
        repos = []
    all_repos = [Repo(path) for path in repos]

    odb_to_obj_ids: List[Tuple[Optional["ObjectDB"], Set["HashInfo"]]] = []

    with ExitStack() as stack:
        for repo in all_repos:
            stack.enter_context(repo.lock)

        for repo in [*all_repos, self]:
            for odb, obj_ids in repo.used_objs(
                all_branches=all_branches,
                with_deps=with_deps,
                all_tags=all_tags,
                all_commits=all_commits,
                all_experiments=all_experiments,
                commit_date=commit_date,
                remote=remote,
                force=force,
                jobs=jobs,
                revs=[rev] if rev else None,
                num=num or 1,
            ).items():
                odb_to_obj_ids.append((odb, obj_ids))

    if not odb_to_obj_ids:
        odb_to_obj_ids = [(None, set())]

    if not_in_remote:
        used_obj_ids = _used_obj_ids_not_in_remote(self, remote, jobs, odb_to_obj_ids)
    else:
        used_obj_ids = set()
        for _, obj_ids in odb_to_obj_ids:
            used_obj_ids.update(obj_ids)

    for scheme, odb in self.cache.by_scheme():
        if not odb:
            continue
        removed = ogc(odb, used_obj_ids, jobs=jobs)
        if not removed:
            logger.info("No unused '%s' cache to remove.", scheme)

    if not cloud:
        return

    for remote_odb, obj_ids in odb_to_obj_ids:
        if remote_odb is None:
            remote_odb = repo.cloud.get_remote_odb(remote, "gc -c")
        removed = ogc(remote_odb, obj_ids, jobs=jobs)
        if removed:
            get_index(remote_odb).clear()
        else:
            logger.info("No unused cache to remove from remote.")




dvc/repo/get.py
import logging
import os
from typing import TYPE_CHECKING, Union

from dvc.exceptions import DvcException
from dvc.utils import resolve_output

if TYPE_CHECKING:
    from dvc.fs.dvc import DVCFileSystem


logger = logging.getLogger(__name__)


class GetDVCFileError(DvcException):
    def __init__(self):
        super().__init__(
            "the given path is a DVC file, you must specify a data file or a directory"
        )


def get(url, path, out=None, rev=None, jobs=None, force=False):
    from dvc.dvcfile import is_valid_filename
    from dvc.fs.callbacks import Callback
    from dvc.repo import Repo

    out = resolve_output(path, out, force=force)

    if is_valid_filename(out):
        raise GetDVCFileError()

    with Repo.open(
        url=url,
        rev=rev,
        subrepos=True,
        uninitialized=True,
    ) as repo:
        from dvc.fs.data import DataFileSystem

        fs: Union[DataFileSystem, "DVCFileSystem"]
        if os.path.isabs(path):
            fs = DataFileSystem(index=repo.index.data["local"])
            fs_path = fs.from_os_path(path)
        else:
            fs = repo.dvcfs
            fs_path = fs.from_os_path(path)

        with Callback.as_tqdm_callback(
            desc=f"Downloading {fs.path.name(path)}",
            unit="files",
        ) as cb:
            fs.get(
                fs_path,
                os.path.abspath(out),
                batch_size=jobs,
                callback=cb,
            )




dvc/repo/get_url.py
import os

from dvc import output
from dvc.exceptions import URLMissingError
from dvc.fs import download, parse_external_url
from dvc.utils import resolve_output


def get_url(url, out=None, *, config=None, jobs=None, force=False):
    out = resolve_output(url, out, force=force)
    out = os.path.abspath(out)
    (out,) = output.loads_from(None, [out], use_cache=False)

    src_fs, src_path = parse_external_url(url, config)
    if not src_fs.exists(src_path):
        raise URLMissingError(url)
    download(src_fs, src_path, out, jobs=jobs)




dvc/repo/graph.py
from typing import TYPE_CHECKING, Iterator, List, Set

from dvc.fs import localfs
from dvc.utils.fs import path_isin

if TYPE_CHECKING:
    from networkx import DiGraph

    from dvc.stage import Stage


def check_acyclic(graph: "DiGraph") -> None:
    import networkx as nx

    from dvc.exceptions import CyclicGraphError

    try:
        edges = nx.find_cycle(graph, orientation="original")
    except nx.NetworkXNoCycle:
        return

    stages: Set["Stage"] = set()
    for from_node, to_node, _ in edges:
        stages.add(from_node)
        stages.add(to_node)

    raise CyclicGraphError(list(stages))


def get_pipeline(pipelines, node):
    found = [i for i in pipelines if i.has_node(node)]
    assert len(found) == 1
    return found[0]


def get_pipelines(graph: "DiGraph"):
    import networkx as nx

    return [graph.subgraph(c).copy() for c in nx.weakly_connected_components(graph)]


def collect_pipeline(stage: "Stage", graph: "DiGraph") -> Iterator["Stage"]:
    import networkx as nx

    pipeline = get_pipeline(get_pipelines(graph), stage)
    return nx.dfs_postorder_nodes(pipeline, stage)


def collect_inside_path(path: str, graph: "DiGraph") -> List["Stage"]:
    import networkx as nx

    stages = nx.dfs_postorder_nodes(graph)
    return [stage for stage in stages if path_isin(stage.path, path)]


def build_graph(stages, outs_trie=None):
    """Generate a graph by using the given stages on the given directory

    The nodes of the graph are the stage's path relative to the root.

    Edges are created when the output of one stage is used as a
    dependency in other stage.

    The direction of the edges goes from the stage to its dependency:

    For example, running the following:

        $ dvc run -o A "echo A > A"
        $ dvc run -d A -o B "echo B > B"
        $ dvc run -d B -o C "echo C > C"

    Will create the following graph:

           ancestors <--
                       |
            C.dvc -> B.dvc -> A.dvc
            |          |
            |          --> descendants
            |
            ------- pipeline ------>
                       |
                       v
          (weakly connected components)

    Args:
        stages (list): used to build a graph from

    Raises:
        OutputDuplicationError: two outputs with the same path
        StagePathAsOutputError: stage inside an output directory
        OverlappingOutputPathsError: output inside output directory
        CyclicGraphError: resulting graph has cycles
    """
    import networkx as nx

    from dvc.exceptions import StagePathAsOutputError

    from .trie import build_outs_trie

    graph = nx.DiGraph()

    # Use trie to efficiently find overlapping outs and deps
    outs_trie = outs_trie or build_outs_trie(stages)

    for stage in stages:
        out = outs_trie.shortest_prefix(localfs.path.parts(stage.path)).value
        if out:
            raise StagePathAsOutputError(stage, str(out))

    # Building graph
    graph.add_nodes_from(stages)
    for stage in stages:
        if stage.is_repo_import:
            continue

        for dep in stage.deps:
            dep_key = dep.fs.path.parts(dep.fs_path)
            overlapping = [n.value for n in outs_trie.prefixes(dep_key)]
            if outs_trie.has_subtrie(dep_key):
                overlapping.extend(outs_trie.values(prefix=dep_key))

            graph.add_edges_from((stage, out.stage) for out in overlapping)
    check_acyclic(graph)

    return graph


# NOTE: using stage graph instead of just list of stages to make sure that it
# has already passed all the sanity checks like cycles/overlapping outputs and
# so on.
def build_outs_graph(graph, outs_trie):
    import networkx as nx

    outs_graph = nx.DiGraph()

    outs_graph.add_nodes_from(outs_trie.values())
    for stage in graph.nodes():
        for dep in stage.deps:
            if dep.fs_path is None:
                # RepoDependency don't have a path
                continue
            dep_key = dep.fs.path.parts(dep.fs_path)
            overlapping = [n.value for n in outs_trie.prefixes(dep_key)]
            if outs_trie.has_subtrie(dep_key):
                overlapping.extend(outs_trie.values(prefix=dep_key))

            for from_out in stage.outs:
                outs_graph.add_edges_from((from_out, out) for out in overlapping)
    return outs_graph




dvc/repo/imp.py
def imp(self, url, path, out=None, fname=None, rev=None, **kwargs):
    erepo = {"url": url}
    if rev is not None:
        erepo["rev"] = rev

    return self.imp_url(path, out=out, fname=fname, erepo=erepo, frozen=True, **kwargs)




dvc/repo/imp_url.py
import os
from typing import TYPE_CHECKING

from dvc.exceptions import InvalidArgumentError, OutputDuplicationError
from dvc.repo.scm_context import scm_context
from dvc.utils import relpath, resolve_output, resolve_paths
from dvc.utils.fs import path_isin

if TYPE_CHECKING:
    from . import Repo

from . import locked


@locked
@scm_context
def imp_url(  # noqa: C901, PLR0913
    self: "Repo",
    url,
    out=None,
    fname=None,
    erepo=None,
    frozen=True,
    no_download=False,
    no_exec=False,
    remote=None,
    to_remote=False,
    jobs=None,
    force=False,
    fs_config=None,
    version_aware: bool = False,
):
    out = resolve_output(url, out, force=force)
    path, wdir, out = resolve_paths(self, out, always_local=to_remote and not out)

    if to_remote and (no_exec or no_download or version_aware):
        raise InvalidArgumentError(
            "--no-exec/--no-download/--version-aware cannot be combined with "
            "--to-remote"
        )

    if not to_remote and remote:
        raise InvalidArgumentError("--remote can't be used without --to-remote")

    # NOTE: when user is importing something from within their own repository
    if (
        erepo is None
        and os.path.exists(url)
        and path_isin(os.path.abspath(url), self.root_dir)
    ):
        url = relpath(url, wdir)

    if version_aware:
        if fs_config is None:
            fs_config = {}
        fs_config["version_aware"] = True

    stage = self.stage.create(
        single_stage=True,
        validate=False,
        fname=fname or path,
        wdir=wdir,
        deps=[url],
        outs=[out],
        erepo=erepo,
        fs_config=fs_config,
    )

    try:
        self.check_graph(stages={stage})
    except OutputDuplicationError as exc:
        raise OutputDuplicationError(  # noqa: B904
            exc.output, set(exc.stages) - {stage}
        )

    if no_exec:
        stage.ignore_outs()
    elif to_remote:
        remote_odb = self.cloud.get_remote_odb(remote, "import-url")
        stage.outs[0].transfer(url, odb=remote_odb, jobs=jobs)
        stage.save_deps()
        stage.md5 = stage.compute_md5()
    else:
        if stage.deps[0].fs.version_aware:
            stage.outs[0].can_push = False
        stage.run(jobs=jobs, no_download=no_download)

    stage.frozen = frozen
    stage.dump()
    return stage




dvc/repo/imports.py
import logging
import os
from tempfile import TemporaryDirectory
from typing import TYPE_CHECKING, List, Set, Tuple, Union

if TYPE_CHECKING:
    from dvc.dependency.base import Dependency
    from dvc.repo import Repo
    from dvc.repo.index import Index, IndexView
    from dvc.stage import Stage
    from dvc.types import TargetType
    from dvc_data.hashfile.hash_info import HashInfo


logger = logging.getLogger(__name__)


def unfetched_view(
    index: "Index", targets: "TargetType", unpartial: bool = False, **kwargs
) -> Tuple["IndexView", List["Dependency"]]:
    """Return index view of imports which have not been fetched.

    Returns:
        Tuple in the form (view, changed_deps) where changed_imports is a list
        of import dependencies that cannot be fetched due to changed data
        source.
    """
    changed_deps: List["Dependency"] = []

    def need_fetch(stage: "Stage") -> bool:
        if not stage.is_import or (stage.is_partial_import and not unpartial):
            return False

        out = stage.outs[0]
        if not out.changed_cache():
            return False

        dep = stage.deps[0]
        if dep.changed_checksum():
            changed_deps.append(dep)
            return False

        return True

    unfetched = index.targets_view(targets, stage_filter=need_fetch, **kwargs)
    return unfetched, changed_deps


def partial_view(index: "Index", targets: "TargetType", **kwargs) -> "IndexView":
    return index.targets_view(
        targets,
        stage_filter=lambda s: s.is_partial_import,
        **kwargs,
    )


def unpartial_imports(index: Union["Index", "IndexView"]) -> int:
    """Update any outs in the index which are no longer partial imports.

    Returns:
        Total number of files which were unpartialed.
    """
    from dvc_data.hashfile.hash_info import HashInfo
    from dvc_data.hashfile.meta import Meta

    updated = 0
    for out in index.outs:
        # we need to use view[key] here and since the out fields have not been
        # updated yet (out.get_entry() would return the partial-import state)
        workspace, key = out.index_key
        entry = index.data[workspace][key]
        if out.stage.is_partial_import:
            out.hash_info = entry.hash_info or HashInfo()
            out.meta = entry.meta or Meta()
            out.stage.md5 = out.stage.compute_md5()
            out.stage.dump()
            updated += out.meta.nfiles if out.meta.nfiles is not None else 1
    return updated


def save_imports(
    repo: "Repo", targets: "TargetType", unpartial: bool = False, **kwargs
) -> Set["HashInfo"]:
    """Save (download) imports from their original source location.

    Imports which are already cached will not be downloaded.

    Returns:
        Objects which were downloaded from source location.
    """
    from dvc.stage.exceptions import DataSourceChanged
    from dvc_data.index import checkout, md5, save
    from dvc_objects.fs.callbacks import Callback

    downloaded: Set["HashInfo"] = set()

    unfetched, changed = unfetched_view(
        repo.index, targets, unpartial=unpartial, **kwargs
    )
    for dep in changed:
        logger.warning(str(DataSourceChanged(f"{dep.stage} ({dep})")))

    data_view = unfetched.data["repo"]
    if len(data_view):
        cache = repo.cache.local
        if not cache.fs.exists(cache.path):
            os.makedirs(cache.path)
        with TemporaryDirectory(dir=cache.path) as tmpdir:
            with Callback.as_tqdm_callback(
                desc="Downloading imports from source",
                unit="files",
            ) as cb:
                checkout(data_view, tmpdir, cache.fs, callback=cb, storage="data")
            md5(data_view)
            save(data_view, odb=cache, hardlink=True)

        downloaded.update(
            entry.hash_info
            for _, entry in data_view.iteritems()
            if entry.meta is not None
            and not entry.meta.isdir
            and entry.hash_info is not None
        )

    if unpartial:
        unpartial_imports(partial_view(repo.index, targets, **kwargs))

    return downloaded




dvc/repo/index.py
import logging
import time
from functools import partial
from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    Dict,
    Iterable,
    Iterator,
    List,
    NamedTuple,
    Optional,
    Set,
    Tuple,
    Union,
)

from funcy.debug import format_time

from dvc.fs import LocalFileSystem
from dvc.utils.objects import cached_property

if TYPE_CHECKING:
    from networkx import DiGraph
    from pygtrie import Trie

    from dvc.dependency import Dependency, ParamsDependency
    from dvc.output import Output
    from dvc.repo import Repo
    from dvc.repo.stage import StageInfo
    from dvc.stage import Stage
    from dvc.types import TargetType
    from dvc_data.hashfile.hash_info import HashInfo
    from dvc_data.index import DataIndex, DataIndexKey, DataIndexView
    from dvc_objects.db import ObjectDB
    from dvc_objects.fs.base import FileSystem


logger = logging.getLogger(__name__)
ObjectContainer = Dict[Optional["ObjectDB"], Set["HashInfo"]]


def log_walk(seq):
    for root, dirs, files in seq:
        start = time.perf_counter()
        yield root, dirs, files
        duration = format_time(time.perf_counter() - start)
        logger.trace(  # type: ignore[attr-defined]
            "%s in collecting stages from %s", duration, root
        )


def collect_files(
    repo: "Repo", onerror: Optional[Callable[[str, Exception], None]] = None
):
    """Collects all of the stages present in the DVC repo.

    Args:
        onerror (optional): callable that will be called with two args:
            the filepath whose collection failed and the exc instance.
            It can report the error to continue with the collection
            (and, skip failed ones), or raise the exception to abort
            the collection.
    """
    from dvc.dvcfile import is_valid_filename
    from dvc.exceptions import DvcException
    from dvc.utils import relpath

    scm = repo.scm
    fs = repo.fs
    sep = fs.sep
    outs: Set[str] = set()

    is_local_fs = isinstance(fs, LocalFileSystem)

    def is_ignored(path):
        # apply only for the local fs
        return is_local_fs and scm.is_ignored(path)

    def is_dvcfile_and_not_ignored(root, file):
        return is_valid_filename(file) and not is_ignored(f"{root}{sep}{file}")

    def is_out_or_ignored(root, directory):
        dir_path = f"{root}{sep}{directory}"
        # trailing slash needed to check if a directory is gitignored
        return dir_path in outs or is_ignored(f"{dir_path}{sep}")

    walk_iter = repo.dvcignore.walk(fs, repo.root_dir, followlinks=False)
    if logger.isEnabledFor(logging.TRACE):  # type: ignore[attr-defined]
        walk_iter = log_walk(walk_iter)

    for root, dirs, files in walk_iter:
        dvcfile_filter = partial(is_dvcfile_and_not_ignored, root)
        for file in filter(dvcfile_filter, files):
            file_path = fs.path.join(root, file)
            try:
                index = Index.from_file(repo, file_path)
            except DvcException as exc:
                if onerror:
                    onerror(relpath(file_path), exc)
                    continue
                raise

            outs.update(
                out.fspath
                for stage in index.stages
                for out in stage.outs
                if out.protocol == "local"
            )
            yield file_path, index
        dirs[:] = [d for d in dirs if not is_out_or_ignored(root, d)]


def _load_data_from_outs(index, prefix, outs):
    from dvc_data.index import DataIndexEntry

    for out in outs:
        if not out.use_cache:
            continue

        ws, key = out.index_key

        entry = DataIndexEntry(
            key=key,
            meta=out.meta,
            hash_info=out.hash_info,
        )

        if out.stage.is_import and not out.stage.is_repo_import:
            dep = out.stage.deps[0]
            entry.meta = dep.meta
            entry.hash_info = dep.hash_info

        # FIXME PyGTrie-based DataIndex doesn't remove entry.key during
        # index.add, so we have to set the entry manually here to make
        # index.view() work correctly.
        index[(*prefix, ws, *key)] = entry


def _load_storage_from_out(storage_map, key, out):
    from dvc.config import NoRemoteError
    from dvc_data.index import FileStorage, ObjectStorage

    if out.odb:
        storage_map.add_data(ObjectStorage(key, out.odb))
    storage_map.add_cache(ObjectStorage(key, out.cache))
    try:
        remote = out.repo.cloud.get_remote(out.remote)
        if remote.fs.version_aware:
            storage_map.add_remote(
                FileStorage(
                    key=key,
                    fs=remote.fs,
                    path=remote.fs.path.join(remote.path, *key),
                    index=remote.index,
                )
            )
        else:
            storage_map.add_remote(ObjectStorage(key, remote.odb, index=remote.index))
    except NoRemoteError:
        pass

    if out.stage.is_import:
        dep = out.stage.deps[0]
        storage_map.add_data(FileStorage(key, dep.fs, dep.fs_path))


class Index:
    def __init__(
        self,
        repo: "Repo",
        stages: Optional[List["Stage"]] = None,
        metrics: Optional[Dict[str, List[str]]] = None,
        plots: Optional[Dict[str, List[str]]] = None,
        params: Optional[Dict[str, Any]] = None,
        artifacts: Optional[Dict[str, Any]] = None,
    ) -> None:
        self.repo = repo
        self.stages = stages or []
        self._metrics = metrics or {}
        self._plots = plots or {}
        self._params = params or {}
        self._artifacts = artifacts or {}
        self._collected_targets: Dict[int, List["StageInfo"]] = {}

    @cached_property
    def rev(self) -> Optional[str]:
        if not isinstance(self.repo.fs, LocalFileSystem):
            return self.repo.get_rev()[:7]
        return None

    def __repr__(self) -> str:
        rev = self.rev or "workspace"
        return f"Index({self.repo}, fs@{rev})"

    @classmethod
    def from_repo(
        cls,
        repo: "Repo",
        onerror: Optional[Callable[[str, Exception], None]] = None,
    ) -> "Index":
        stages = []
        metrics = {}
        plots = {}
        params = {}
        artifacts = {}

        onerror = onerror or repo.stage_collection_error_handler
        for _, idx in collect_files(repo, onerror=onerror):
            # pylint: disable=protected-access
            stages.extend(idx.stages)
            metrics.update(idx._metrics)
            plots.update(idx._plots)
            params.update(idx._params)
            artifacts.update(idx._artifacts)
        return cls(
            repo,
            stages=stages,
            metrics=metrics,
            plots=plots,
            params=params,
            artifacts=artifacts,
        )

    @classmethod
    def from_file(cls, repo: "Repo", path: str) -> "Index":
        from dvc.dvcfile import load_file

        dvcfile = load_file(repo, path)
        return cls(
            repo,
            stages=list(dvcfile.stages.values()),
            metrics={path: dvcfile.metrics} if dvcfile.metrics else {},
            plots={path: dvcfile.plots} if dvcfile.plots else {},
            params={path: dvcfile.params} if dvcfile.params else {},
            artifacts={path: dvcfile.artifacts} if dvcfile.artifacts else {},
        )

    def update(self, stages: Iterable["Stage"]) -> "Index":
        stages = set(stages)
        # we remove existing stages with same hashes at first
        # and then re-add the new ones later.
        stages_set = (set(self.stages) - stages) | stages
        return self.__class__(
            self.repo,
            stages=list(stages_set),
            metrics=self._metrics,
            plots=self._plots,
            params=self._params,
            artifacts=self._artifacts,
        )

    @cached_property
    def outs_trie(self) -> "Trie":
        from dvc.repo.trie import build_outs_trie

        return build_outs_trie(self.stages)

    @cached_property
    def outs_graph(self) -> "DiGraph":
        from dvc.repo.graph import build_outs_graph

        return build_outs_graph(self.graph, self.outs_trie)

    @cached_property
    def graph(self) -> "DiGraph":
        from dvc.repo.graph import build_graph

        return build_graph(self.stages, self.outs_trie)

    def check_graph(self) -> None:
        if not getattr(self.repo, "_skip_graph_checks", False):
            self.graph  # noqa: B018, pylint: disable=pointless-statement

    @property
    def params(self) -> Iterator["ParamsDependency"]:
        from dvc.dependency import ParamsDependency

        for dep in self.deps:
            if isinstance(dep, ParamsDependency):
                yield dep

    @property
    def outs(self) -> Iterator["Output"]:
        for stage in self.stages:
            yield from stage.outs

    @property
    def decorated_outs(self) -> Iterator["Output"]:
        for output in self.outs:
            if output.is_decorated:
                yield output

    @property
    def metrics(self) -> Iterator["Output"]:
        for output in self.outs:
            if output.is_metric:
                yield output

    @property
    def plots(self) -> Iterator["Output"]:
        for output in self.outs:
            if output.is_plot:
                yield output

    @property
    def deps(self) -> Iterator["Dependency"]:
        for stage in self.stages:
            yield from stage.deps

    @cached_property
    def _plot_sources(self) -> List[str]:
        from dvc.repo.plots import _collect_pipeline_files

        sources: List[str] = []
        for data in _collect_pipeline_files(self.repo, [], {}).values():
            for plot_id, props in data.get("data", {}).items():
                if isinstance(props.get("y"), dict):
                    sources.extend(props["y"])
                    if isinstance(props.get("x"), dict):
                        sources.extend(props["x"])
                else:
                    sources.append(plot_id)
        return sources

    @cached_property
    def data_keys(self) -> Dict[str, Set["DataIndexKey"]]:
        from collections import defaultdict

        by_workspace: Dict[str, Set["DataIndexKey"]] = defaultdict(set)

        by_workspace["repo"] = set()
        by_workspace["local"] = set()

        for out in self.outs:
            if not out.use_cache:
                continue

            workspace, key = out.index_key
            by_workspace[workspace].add(key)

        return dict(by_workspace)

    @cached_property
    def data_tree(self):
        from dvc_data.hashfile.tree import Tree

        tree = Tree()
        for out in self.outs:
            if not out.use_cache:
                continue

            ws, key = out.index_key

            tree.add((ws, *key), out.meta, out.hash_info)

        tree.digest()

        return tree

    @cached_property
    def data(self) -> "Dict[str, DataIndex]":
        prefix: "DataIndexKey"
        loaded = False

        index = self.repo.data_index
        prefix = ("tree", self.data_tree.hash_info.value)
        if index.has_node(prefix):
            loaded = True

        if not loaded:
            _load_data_from_outs(index, prefix, self.outs)
            index.commit()

        by_workspace = {}
        by_workspace["repo"] = index.view((*prefix, "repo"))
        by_workspace["local"] = index.view((*prefix, "local"))

        for out in self.outs:
            if not out.use_cache:
                continue

            ws, key = out.index_key
            if ws not in by_workspace:
                by_workspace[ws] = index.view((*prefix, ws))

            data_index = by_workspace[ws]
            _load_storage_from_out(data_index.storage_map, key, out)

        return by_workspace

    @staticmethod
    def _hash_targets(
        targets: Iterable[Optional[str]],
        **kwargs: Any,
    ) -> int:
        return hash(
            (
                frozenset(targets),
                kwargs.get("with_deps", False),
                kwargs.get("recursive", False),
            )
        )

    def collect_targets(
        self, targets: Optional["TargetType"], *, onerror=None, **kwargs: Any
    ) -> List["StageInfo"]:
        from dvc.exceptions import DvcException
        from dvc.repo.stage import StageInfo
        from dvc.utils.collections import ensure_list

        if not onerror:

            def onerror(_target, _exc):
                raise  # pylint: disable=misplaced-bare-raise

        targets = ensure_list(targets)
        if not targets:
            return [StageInfo(stage) for stage in self.stages]
        targets_hash = self._hash_targets(targets, **kwargs)
        if targets_hash not in self._collected_targets:
            collected = []
            for target in targets:
                try:
                    collected.extend(self.repo.stage.collect_granular(target, **kwargs))
                except DvcException as exc:
                    onerror(target, exc)
            self._collected_targets[targets_hash] = collected

        return self._collected_targets[targets_hash]

    def used_objs(
        self,
        targets: Optional["TargetType"] = None,
        with_deps: bool = False,
        remote: Optional[str] = None,
        force: bool = False,
        recursive: bool = False,
        jobs: Optional[int] = None,
        push: bool = False,
    ) -> "ObjectContainer":
        from collections import defaultdict

        used: "ObjectContainer" = defaultdict(set)
        pairs = self.collect_targets(targets, recursive=recursive, with_deps=with_deps)
        for stage, filter_info in pairs:
            for odb, objs in stage.get_used_objs(
                remote=remote,
                force=force,
                jobs=jobs,
                filter_info=filter_info,
                push=push,
            ).items():
                used[odb].update(objs)
        return used

    def targets_view(
        self,
        targets: Optional["TargetType"],
        stage_filter: Optional[Callable[["Stage"], bool]] = None,
        outs_filter: Optional[Callable[["Output"], bool]] = None,
        **kwargs: Any,
    ) -> "IndexView":
        """Return read-only view of index for the specified targets.
        Args:
            targets: Targets to collect
            stage_filter: Optional stage filter to be applied after collecting
                targets.
            outs_filter: Optional output filter to be applied after collecting
                targets.
        Additional kwargs will be passed into the stage collector.
        Note:
            If both stage_filter and outs_filter are provided, stage_filter
            will be applied first, and the resulting view will only contain
            outputs from stages that matched stage_filter. Outputs from stages
            that did not match will be excluded from the view (whether or not
            the output would have matched outs_filter).
        """
        stage_infos = [
            stage_info
            for stage_info in self.collect_targets(targets, **kwargs)
            if not stage_filter or stage_filter(stage_info.stage)
        ]
        return IndexView(self, stage_infos, outs_filter=outs_filter)


class _DataPrefixes(NamedTuple):
    explicit: Set["DataIndexKey"]
    recursive: Set["DataIndexKey"]


class IndexView:
    """Read-only view of Index.data using filtered stages."""

    def __init__(  # pylint: disable=redefined-outer-name
        self,
        index: Index,
        stage_infos: Iterable["StageInfo"],
        outs_filter: Optional[Callable[["Output"], bool]],
    ):
        self._index = index
        self._stage_infos = stage_infos
        # NOTE: stage_infos might have the same stage multiple times but with
        # different filter_info
        self.stages = list({stage for stage, _ in stage_infos})
        self._outs_filter = outs_filter

    @property
    def repo(self) -> "Repo":
        return self._index.repo

    @property
    def deps(self) -> Iterator["Dependency"]:
        for stage in self.stages:
            yield from stage.deps

    @property
    def _filtered_outs(self) -> Iterator[Tuple["Output", Optional[str]]]:
        for stage, filter_info in self._stage_infos:
            for out in stage.filter_outs(filter_info):
                if not self._outs_filter or self._outs_filter(out):
                    yield out, filter_info

    @property
    def outs(self) -> Iterator["Output"]:
        yield from {out for (out, _) in self._filtered_outs}

    @cached_property
    def _data_prefixes(self) -> Dict[str, "_DataPrefixes"]:
        from collections import defaultdict

        prefixes: Dict[str, "_DataPrefixes"] = defaultdict(
            lambda: _DataPrefixes(set(), set())
        )
        for out, filter_info in self._filtered_outs:
            workspace, key = out.index_key
            if filter_info and out.fs.path.isin(filter_info, out.fs_path):
                key = key + out.fs.path.relparts(filter_info, out.fs_path)
            entry = self._index.data[workspace][key]
            if entry and entry.meta and entry.meta.isdir:
                prefixes[workspace].recursive.add(key)
            prefixes[workspace].explicit.update(key[:i] for i in range(len(key), 0, -1))
        return prefixes

    @cached_property
    def data_keys(self) -> Dict[str, Set["DataIndexKey"]]:
        from collections import defaultdict

        ret: Dict[str, Set["DataIndexKey"]] = defaultdict(set)

        for out, filter_info in self._filtered_outs:
            workspace, key = out.index_key
            if filter_info and out.fs.path.isin(filter_info, out.fs_path):
                key = key + out.fs.path.relparts(filter_info, out.fs_path)
            ret[workspace].add(key)

        return dict(ret)

    @cached_property
    def data(self) -> Dict[str, Union["DataIndex", "DataIndexView"]]:
        from dvc_data.index import DataIndex, view

        def key_filter(workspace: str, key: "DataIndexKey"):
            try:
                prefixes = self._data_prefixes[workspace]
                return key in prefixes.explicit or any(
                    key[: len(prefix)] == prefix for prefix in prefixes.recursive
                )
            except KeyError:
                return False

        data: Dict[str, Union["DataIndex", "DataIndexView"]] = {}
        for workspace, data_index in self._index.data.items():
            if self.stages:
                data[workspace] = view(data_index, partial(key_filter, workspace))
            else:
                data[workspace] = DataIndex()
        return data


def build_data_index(
    index: Union["Index", "IndexView"],
    path: str,
    fs: "FileSystem",
    workspace: str = "repo",
    compute_hash: Optional[bool] = False,
) -> "DataIndex":
    from dvc_data.index import DataIndex, DataIndexEntry
    from dvc_data.index.build import build_entries, build_entry
    from dvc_data.index.save import build_tree

    ignore = None
    if workspace == "repo" and isinstance(fs, LocalFileSystem):
        ignore = index.repo.dvcignore

    data = DataIndex()
    for key in index.data_keys.get(workspace, set()):
        out_path = fs.path.join(path, *key)

        try:
            out_entry = build_entry(
                out_path,
                fs,
                compute_hash=compute_hash,
                state=index.repo.state,
            )
        except FileNotFoundError:
            out_entry = DataIndexEntry()

        out_entry.key = key
        data.add(out_entry)

        if not out_entry.meta or not out_entry.meta.isdir:
            continue

        for entry in build_entries(
            out_path,
            fs,
            compute_hash=compute_hash,
            state=index.repo.state,
            ignore=ignore,
        ):
            if not entry.key or entry.key == ("",):
                # NOTE: whether the root will be returned by build_entries
                # depends on the filesystem (e.g. local doesn't, but s3 does).
                continue

            entry.key = key + entry.key
            data.add(entry)

        if compute_hash:
            tree_meta, tree = build_tree(data, key)
            out_entry.meta = tree_meta
            out_entry.hash_info = tree.hash_info
            out_entry.loaded = True
            data.add(out_entry)

    return data




dvc/repo/init.py
import logging
import os

from dvc.config import Config
from dvc.exceptions import InitError, InvalidArgumentError
from dvc.ignore import init as init_dvcignore
from dvc.repo import Repo
from dvc.scm import SCM, SCMError
from dvc.utils import relpath
from dvc.utils.fs import remove

logger = logging.getLogger(__name__)


def init(root_dir=os.curdir, no_scm=False, force=False, subdir=False):  # noqa: C901
    """
    Creates an empty repo on the given directory -- basically a
    `.dvc` directory with subdirectories for configuration and cache.

    It should be tracked by a SCM or use the `--no-scm` flag.

    If the given directory is not empty, you must use the `--force`
    flag to override it.

    Args:
        root_dir: Path to repo's root directory.

    Returns:
        Repo instance.

    Raises:
        KeyError: Raises an exception.
    """

    if no_scm and subdir:
        raise InvalidArgumentError(
            "Cannot initialize repo with `--no-scm` and `--subdir`"
        )

    root_dir = os.path.realpath(root_dir)
    dvc_dir = os.path.join(root_dir, Repo.DVC_DIR)

    try:
        scm = SCM(root_dir, search_parent_directories=subdir, no_scm=no_scm)
    except SCMError:
        raise InitError(  # noqa: B904
            f"{root_dir} is not tracked by any supported SCM tool (e.g. Git). "
            "Use `--no-scm` if you don't want to use any SCM or "
            "`--subdir` if initializing inside a subdirectory of a parent SCM "
            "repository."
        )

    if scm.is_ignored(dvc_dir):
        raise InitError(
            f"{dvc_dir} is ignored by your SCM tool. \n"
            "Make sure that it's tracked, "
            "for example, by adding '!.dvc' to .gitignore."
        )

    if os.path.isdir(dvc_dir):
        if not force:
            raise InitError(f"'{relpath(dvc_dir)}' exists. Use `-f` to force.")

        remove(dvc_dir)

    os.mkdir(dvc_dir)

    config = Config.init(dvc_dir)

    if no_scm:
        with config.edit() as conf:
            conf["core"]["no_scm"] = True

    dvcignore = init_dvcignore(root_dir)

    proj = Repo(root_dir)

    if os.path.isdir(proj.site_cache_dir):
        proj.close()
        try:
            remove(proj.site_cache_dir)
        except OSError:
            logger.debug("failed to remove %s", dvc_dir, exc_info=True)
        proj = Repo(root_dir)

    with proj.scm_context(autostage=True) as context:
        files = [
            config.files["repo"],
            dvcignore,
        ]
        ignore_file = context.scm.ignore_file
        if ignore_file:
            files.extend([os.path.join(dvc_dir, ignore_file)])
        proj.scm_context.track_file(files)

    logger.info("Initialized DVC repository.\n")
    if not no_scm:
        logger.info("You can now commit the changes to git.\n")
    return proj




dvc/repo/install.py
from typing import TYPE_CHECKING

from dvc.exceptions import DvcException

if TYPE_CHECKING:
    from dvc.repo import Repo
    from dvc.scm import Git


def pre_commit_install(scm: "Git") -> None:
    import os

    from dvc.utils.serialize import modify_yaml

    config_path = os.path.join(scm.root_dir, ".pre-commit-config.yaml")
    with modify_yaml(config_path) as config:
        entry = {
            "repo": "https://github.com/iterative/dvc",
            "rev": "main",
            "hooks": [
                {
                    "id": "dvc-pre-commit",
                    "additional_dependencies": [".[all]"],
                    "language_version": "python3",
                    "stages": ["commit"],
                },
                {
                    "id": "dvc-pre-push",
                    "additional_dependencies": [".[all]"],
                    "language_version": "python3",
                    "stages": ["push"],
                },
                {
                    "id": "dvc-post-checkout",
                    "additional_dependencies": [".[all]"],
                    "language_version": "python3",
                    "stages": ["post-checkout"],
                    "always_run": True,
                },
            ],
        }

        config["repos"] = config.get("repos", [])
        if entry not in config["repos"]:
            config["repos"].append(entry)


def install_hooks(scm: "Git") -> None:
    from scmrepo.exceptions import GitHookAlreadyExists

    from dvc.utils import format_link

    hooks = ["post-checkout", "pre-commit", "pre-push"]
    for hook in hooks:
        try:
            scm.verify_hook(hook)
        except GitHookAlreadyExists as exc:
            link = format_link("https://man.dvc.org/install")
            raise DvcException(  # noqa: B904
                f"{exc}. Please refer to {link} for more info."
            )

    for hook in hooks:
        scm.install_hook(hook, f"exec dvc git-hook {hook} $@")


def install(self: "Repo", use_pre_commit_tool: bool = False) -> None:
    """Adds dvc commands to SCM hooks for the repo.

    If use_pre_commit_tool is set and pre-commit is installed it will be used
    to install the hooks.
    """
    from dvc.scm import Git

    scm = self.scm
    if not isinstance(scm, Git):
        return

    driver = "dvc git-hook merge-driver --ancestor %O --our %A --their %B "
    scm.install_merge_driver("dvc", "DVC merge driver", driver)

    if use_pre_commit_tool:
        return pre_commit_install(scm)

    return install_hooks(scm)




dvc/repo/ls.py
import os
from typing import TYPE_CHECKING, Optional

if TYPE_CHECKING:
    from dvc.fs.dvc import DVCFileSystem

    from . import Repo


def ls(
    url: str,
    path: Optional[str] = None,
    rev: Optional[str] = None,
    recursive: Optional[bool] = None,
    dvc_only: bool = False,
):
    """Methods for getting files and outputs for the repo.

    Args:
        url (str): the repo url
        path (str, optional): relative path into the repo
        rev (str, optional): SHA commit, branch or tag name
        recursive (bool, optional): recursively walk the repo
        dvc_only (bool, optional): show only DVC-artifacts

    Returns:
        list of `entry`

    Notes:
        `entry` is a dictionary with structure
        {
            "path": str,
            "isout": bool,
            "isdir": bool,
            "isexec": bool,
        }
    """
    from . import Repo

    with Repo.open(url, rev=rev, subrepos=True, uninitialized=True) as repo:
        path = path or ""

        ret = _ls(repo, path, recursive, dvc_only)

        ret_list = []
        for path, info in ret.items():
            info["path"] = path
            ret_list.append(info)
        ret_list.sort(key=lambda f: f["path"])
        return ret_list


def _ls(
    repo: "Repo",
    path: str,
    recursive: Optional[bool] = None,
    dvc_only: bool = False,
):
    fs: "DVCFileSystem" = repo.dvcfs
    fs_path = fs.from_os_path(path)

    fs_path = fs.info(fs_path)["name"]

    infos = {}
    for root, dirs, files in fs.walk(
        fs_path, dvcfiles=True, dvc_only=dvc_only, detail=True
    ):
        if not recursive:
            files.update(dirs)

        parts = fs.path.relparts(root, fs_path)
        if parts == (".",):
            parts = ()

        for name, entry in files.items():
            infos[os.path.join(*parts, name)] = entry

        if not recursive:
            break

    if not infos and fs.isfile(fs_path):
        infos[os.path.basename(path)] = fs.info(fs_path)

    ret = {}
    for name, info in infos.items():
        dvc_info = info.get("dvc_info", {})
        ret[name] = {
            "isout": dvc_info.get("isout", False),
            "isdir": info["type"] == "directory",
            "isexec": info.get("isexec", False),
        }

    return ret




dvc/repo/ls_url.py
from dvc.exceptions import URLMissingError
from dvc.fs import parse_external_url


def ls_url(url, *, config=None, recursive=False):
    fs, fs_path = parse_external_url(url, config=config)
    try:
        info = fs.info(fs_path)
    except FileNotFoundError as exc:
        raise URLMissingError(url) from exc
    if info["type"] != "directory":
        return [{"path": info["name"], "isdir": False}]

    ret = []
    for _, dirs, files in fs.walk(fs_path, detail=True):
        if not recursive:
            files.update(dirs)

        for info in files.values():
            ls_info = {
                "path": fs.path.relpath(info["name"], fs_path),
                "isdir": info["type"] == "directory",
            }
            ret.append(ls_info)

        if not recursive:
            break

    return ret




dvc/repo/move.py
import os

from dvc.exceptions import MoveNotDataSourceError
from dvc.repo.scm_context import scm_context

from . import locked


def _expand_target_path(from_path, to_path):
    if os.path.isdir(to_path):
        return os.path.join(to_path, os.path.basename(from_path))
    return to_path


@locked
@scm_context
def move(self, from_path, to_path):
    """
    Renames an output file and modifies the stage associated
    to reflect the change on the pipeline.

    If the output has the same name as its stage, it would
    also rename the corresponding .dvc file.

    E.g.
          Having: (hello, hello.dvc)

          $ dvc move hello greetings

          Result: (greeting, greeting.dvc)

    It only works with outputs generated by `add` or `import`,
    also known as data sources.
    """
    from dvc import output
    from dvc.dvcfile import DVC_FILE_SUFFIX
    from dvc.stage import Stage

    from_out = output.loads_from(Stage(self), [from_path])[0]
    assert from_out.protocol == "local"

    to_path = _expand_target_path(from_path, to_path)

    outs = self.find_outs_by_path(from_out.fspath)
    assert len(outs) == 1
    out = outs[0]
    stage = out.stage

    if not stage.is_data_source:
        raise MoveNotDataSourceError(stage.addressing)

    stage_name = os.path.splitext(os.path.basename(stage.path))[0]
    from_name = os.path.basename(from_out.fspath)
    if stage_name == from_name:
        new_fname = os.path.join(
            os.path.dirname(to_path),
            os.path.basename(to_path) + DVC_FILE_SUFFIX,
        )
        new_wdir = os.path.abspath(os.path.join(os.curdir, os.path.dirname(to_path)))
        to_path = os.path.relpath(to_path, new_wdir)
        new_stage = self.stage.create(
            single_stage=True,
            fname=new_fname,
            wdir=new_wdir,
            outs=[to_path],
            meta=stage.meta,
        )

        os.unlink(stage.path)
        stage = new_stage
    else:
        to_path = os.path.relpath(to_path, stage.wdir)

    to_out = output.loads_from(stage, [to_path], out.use_cache, out.metric)[0]

    out.move(to_out)
    stage.save()
    stage.dump()




dvc/repo/open_repo.py
import logging
import os
import tempfile
import threading
from contextlib import contextmanager
from typing import TYPE_CHECKING, Dict, Iterator, Optional, Tuple

from funcy import retry, wrap_with

from dvc.exceptions import NotDvcRepoError
from dvc.repo import Repo
from dvc.scm import CloneError, map_scm_exception
from dvc.utils import relpath

if TYPE_CHECKING:
    from dvc.scm import Git

logger = logging.getLogger(__name__)


@contextmanager
@map_scm_exception()
def _external_repo(
    url,
    rev: Optional[str] = None,
    for_write: bool = False,
    **kwargs,
) -> Iterator["Repo"]:
    logger.debug("Creating external repo %s@%s", url, rev)
    path = _cached_clone(url, rev, for_write=for_write)
    # Local HEAD points to the tip of whatever branch we first cloned from
    # (which may not be the default branch), use origin/HEAD here to get
    # the tip of the default branch
    rev = rev or "refs/remotes/origin/HEAD"

    config = _get_remote_config(url) if os.path.isdir(url) else {}
    config.update({"cache": {"dir": _get_cache_dir(url)}})
    config.update(kwargs.pop("config", None) or {})

    main_root = "/"
    if for_write:
        # we already checked out needed revision
        rev = None
        main_root = path

    repo_kwargs = dict(
        root_dir=path,
        url=url,
        config=config,
        repo_factory=erepo_factory(url, main_root, {"cache": config["cache"]}),
        rev=rev,
        **kwargs,
    )

    repo = Repo(**repo_kwargs)

    try:
        yield repo
    finally:
        repo.close()
        if for_write:
            _remove(path)


def open_repo(url, *args, **kwargs):
    if url is None:
        url = os.getcwd()

    if os.path.exists(url):
        try:
            config = _get_remote_config(url)
            config.update(kwargs.get("config") or {})
            kwargs["config"] = config
            return Repo(url, *args, **kwargs)
        except NotDvcRepoError:
            pass  # fallthrough to _external_repo

    return _external_repo(url, *args, **kwargs)


def erepo_factory(url, root_dir, cache_config):
    from dvc.fs import localfs

    def make_repo(path, fs=None, **_kwargs):
        _config = cache_config.copy()
        if os.path.isdir(url):
            fs = fs or localfs
            repo_path = os.path.join(url, *fs.path.relparts(path, root_dir))
            _config.update(_get_remote_config(repo_path))
        return Repo(path, fs=fs, config=_config, **_kwargs)

    return make_repo


CLONES: Dict[str, Tuple[str, bool]] = {}
CACHE_DIRS: Dict[str, str] = {}


@wrap_with(threading.Lock())
def _get_cache_dir(url):
    try:
        cache_dir = CACHE_DIRS[url]
    except KeyError:
        cache_dir = CACHE_DIRS[url] = tempfile.mkdtemp("dvc-cache")
    return cache_dir


def clean_repos():
    # Outside code should not see cache while we are removing
    paths = [path for path, _ in CLONES.values()] + list(CACHE_DIRS.values())
    CLONES.clear()
    CACHE_DIRS.clear()

    for path in paths:
        _remove(path)


def _get_remote_config(url):
    try:
        repo = Repo(url)
    except NotDvcRepoError:
        return {}

    try:
        name = repo.config["core"].get("remote")
        if not name:
            # Fill the empty upstream entry with a new remote pointing to the
            # original repo's cache location.
            name = "auto-generated-upstream"
            return {
                "core": {"remote": name},
                "remote": {name: {"url": repo.cache.local.path}},
            }

        # Use original remote to make sure that we are using correct url,
        # credential paths, etc if they are relative to the config location.
        return {"remote": {name: repo.config["remote"][name]}}
    finally:
        repo.close()


def _cached_clone(url, rev, for_write=False):
    """Clone an external git repo to a temporary directory.

    Returns the path to a local temporary directory with the specified
    revision checked out. If for_write is set prevents reusing this dir via
    cache.
    """
    from shutil import copytree

    # even if we have already cloned this repo, we may need to
    # fetch/fast-forward to get specified rev
    clone_path, shallow = _clone_default_branch(url, rev, for_write=for_write)

    if not for_write and (url) in CLONES:
        return CLONES[url][0]

    # Copy to a new dir to keep the clone clean
    repo_path = tempfile.mkdtemp("dvc-erepo")
    logger.debug("erepo: making a copy of %s clone", url)
    copytree(clone_path, repo_path)

    # Check out the specified revision
    if for_write:
        _git_checkout(repo_path, rev)
    else:
        CLONES[url] = (repo_path, shallow)
    return repo_path


@wrap_with(threading.Lock())
def _clone_default_branch(url, rev, for_write=False):  # noqa: C901, PLR0912
    """Get or create a clean clone of the url.

    The cloned is reactualized with git pull unless rev is a known sha.
    """
    from dvc.scm import Git

    clone_path, shallow = CLONES.get(url) or (None, False)

    git = None
    try:
        if clone_path:
            git = Git(clone_path)
            # Do not pull for known shas, branches and tags might move
            if not Git.is_sha(rev) or not git.has_rev(rev):
                if shallow:
                    # If we are missing a rev in a shallow clone, fallback to
                    # a full (unshallowed) clone. Since fetching specific rev
                    # SHAs is only available in certain git versions, if we
                    # have need to reference multiple specific revs for a
                    # given repo URL it is easier/safer for us to work with
                    # full clones in this case.
                    logger.debug("erepo: unshallowing clone for '%s'", url)
                    _pull(git, unshallow=True)
                    shallow = False
                    CLONES[url] = (clone_path, shallow)
                else:
                    logger.debug("erepo: git pull '%s'", url)
                    _pull(git)
        else:
            from dvc.scm import clone

            logger.debug("erepo: git clone '%s' to a temporary dir", url)
            clone_path = tempfile.mkdtemp("dvc-clone")
            if not for_write and rev and not Git.is_sha(rev):
                # If rev is a tag or branch name try shallow clone first

                try:
                    git = clone(url, clone_path, shallow_branch=rev)
                    shallow = os.path.exists(
                        os.path.join(clone_path, Git.GIT_DIR, "shallow")
                    )
                    if shallow:
                        logger.debug("erepo: using shallow clone for branch '%s'", rev)
                except CloneError:
                    git_dir = os.path.join(clone_path, ".git")
                    if os.path.exists(git_dir):
                        _remove(git_dir)
            if not git:
                git = clone(url, clone_path)
                shallow = False
            CLONES[url] = (clone_path, shallow)
    finally:
        if git:
            git.close()

    return clone_path, shallow


def _pull(git: "Git", unshallow: bool = False):
    from dvc.repo.experiments.utils import fetch_all_exps

    git.fetch(unshallow=unshallow)
    _merge_upstream(git)
    fetch_all_exps(git, "origin")


def _merge_upstream(git: "Git"):
    from scmrepo.exceptions import SCMError

    try:
        branch = git.active_branch()
        upstream = f"refs/remotes/origin/{branch}"
        if git.get_ref(upstream):
            git.merge(upstream)
    except SCMError:
        pass


def _git_checkout(repo_path, rev):
    from dvc.scm import Git

    logger.debug("erepo: git checkout %s@%s", repo_path, rev)
    git = Git(repo_path)
    try:
        git.checkout(rev)
    finally:
        git.close()


def _remove(path):
    from dvc.utils.fs import remove

    if os.name == "nt":
        # git.exe may hang for a while not permitting to remove temp dir
        os_retry = retry(5, errors=OSError, timeout=0.1)
        try:
            os_retry(remove)(path)
        except PermissionError:
            logger.warning("Failed to remove '%s'", relpath(path), exc_info=True)
    else:
        remove(path)




dvc/repo/pull.py
import logging
from typing import TYPE_CHECKING, Optional

from dvc.repo import locked
from dvc.utils import glob_targets

if TYPE_CHECKING:
    from dvc_objects.db import ObjectDB

logger = logging.getLogger(__name__)


@locked
def pull(  # noqa: PLR0913
    self,
    targets=None,
    jobs=None,
    remote=None,
    all_branches=False,
    with_deps=False,
    all_tags=False,
    force=False,
    recursive=False,
    all_commits=False,
    run_cache=False,
    glob=False,
    odb: Optional["ObjectDB"] = None,
    allow_missing=False,
):
    if isinstance(targets, str):
        targets = [targets]

    expanded_targets = glob_targets(targets, glob=glob)

    processed_files_count = self.fetch(
        expanded_targets,
        jobs,
        remote=remote,
        all_branches=all_branches,
        all_tags=all_tags,
        all_commits=all_commits,
        with_deps=with_deps,
        recursive=recursive,
        run_cache=run_cache,
        odb=odb,
    )
    stats = self.checkout(
        targets=expanded_targets,
        with_deps=with_deps,
        force=force,
        recursive=recursive,
        allow_missing=allow_missing,
    )

    stats["fetched"] = processed_files_count
    return stats




dvc/repo/push.py
from contextlib import suppress
from typing import TYPE_CHECKING, Optional, Sequence

from dvc.config import NoRemoteError
from dvc.exceptions import InvalidArgumentError, UploadError
from dvc.utils import glob_targets

from . import locked

if TYPE_CHECKING:
    from dvc.data_cloud import Remote
    from dvc.repo import Repo
    from dvc.types import TargetType
    from dvc_objects.db import ObjectDB


@locked
def push(  # noqa: C901, PLR0913
    self,
    targets=None,
    jobs=None,
    remote=None,
    all_branches=False,
    with_deps=False,
    all_tags=False,
    recursive=False,
    all_commits=False,
    run_cache=False,
    revs=None,
    glob=False,
    odb: Optional["ObjectDB"] = None,
    include_imports=False,
):
    worktree_remote: Optional["Remote"] = None
    with suppress(NoRemoteError):
        _remote = self.cloud.get_remote(name=remote)
        if _remote and (_remote.worktree or _remote.fs.version_aware):
            worktree_remote = _remote

    pushed = 0
    used_run_cache = self.stage_cache.push(remote, odb=odb) if run_cache else []
    pushed += len(used_run_cache)

    if isinstance(targets, str):
        targets = [targets]

    expanded_targets = glob_targets(targets, glob=glob)

    if worktree_remote is not None:
        pushed += _push_worktree(
            self,
            worktree_remote,
            revs=revs,
            all_branches=all_branches,
            all_tags=all_tags,
            all_commits=all_commits,
            targets=expanded_targets,
            jobs=jobs,
            with_deps=with_deps,
            recursive=recursive,
        )
    else:
        used = self.used_objs(
            expanded_targets,
            all_branches=all_branches,
            all_tags=all_tags,
            all_commits=all_commits,
            with_deps=with_deps,
            force=True,
            remote=remote,
            jobs=jobs,
            recursive=recursive,
            used_run_cache=used_run_cache,
            revs=revs,
            push=True,
        )

        if odb:
            all_ids = set()
            for dest_odb, obj_ids in used.items():
                if not include_imports and dest_odb and dest_odb.read_only:
                    continue
                all_ids.update(obj_ids)
            result = self.cloud.push(all_ids, jobs, remote=remote, odb=odb)
            if result.failed:
                raise UploadError(len(result.failed))
            pushed += len(result.transferred)
        else:
            for dest_odb, obj_ids in used.items():
                if dest_odb and dest_odb.read_only:
                    continue
                result = self.cloud.push(
                    obj_ids, jobs, remote=remote, odb=odb or dest_odb
                )
                if result.failed:
                    raise UploadError(len(result.failed))
                pushed += len(result.transferred)
    return pushed


def _push_worktree(
    repo: "Repo",
    remote: "Remote",
    revs: Optional[Sequence[str]] = None,
    all_branches: bool = False,
    all_tags: bool = False,
    all_commits: bool = False,
    targets: Optional["TargetType"] = None,
    jobs: Optional[int] = None,
    **kwargs,
) -> int:
    from dvc.repo.worktree import push_worktree

    if revs or all_branches or all_tags or all_commits:
        raise InvalidArgumentError(
            "Multiple rev push is unsupported for cloud versioned remotes"
        )

    return push_worktree(repo, remote, targets=targets, jobs=jobs, **kwargs)




dvc/repo/remove.py
import logging
import typing

from dvc.dvcfile import DVC_FILE_SUFFIX
from dvc.stage.exceptions import (
    StageFileDoesNotExistError,
    StageFileIsNotDvcFileError,
    StageNotFound,
)

from . import locked

if typing.TYPE_CHECKING:
    from dvc.repo import Repo

logger = logging.getLogger(__name__)


@locked
def remove(self: "Repo", target: str, outs: bool = False):
    try:
        stages = self.stage.from_target(target, accept_group=False)
    except (StageNotFound, StageFileDoesNotExistError) as e:
        # If the user specified a tracked file as a target instead of a stage,
        # e.g. `data.csv` instead of `data.csv.dvc`,
        # give a more helpful error message.
        if self.fs.exists(target + DVC_FILE_SUFFIX):
            raise StageFileIsNotDvcFileError(target) from e
        raise

    for stage in stages:
        stage.remove(remove_outs=outs, force=outs)

    return stages




dvc/repo/reproduce.py
import logging
from typing import TYPE_CHECKING, Iterator, List

from dvc.exceptions import ReproductionError
from dvc.repo.scm_context import scm_context
from dvc.stage.cache import RunCacheNotSupported

from . import locked

if TYPE_CHECKING:
    from dvc.stage import Stage

    from . import Repo

logger = logging.getLogger(__name__)


def _reproduce_stage(stage: "Stage", **kwargs) -> List["Stage"]:
    if stage.frozen and not stage.is_import:
        logger.warning(
            "%s is frozen. Its dependencies are not going to be reproduced.",
            stage,
        )

    stage = stage.reproduce(**kwargs)
    if not stage:
        return []

    if not kwargs.get("dry", False):
        stage.dump(update_pipeline=False)
        _track_stage(stage)

    return [stage]


def _get_stage_files(stage: "Stage") -> Iterator[str]:
    yield stage.dvcfile.relpath
    for dep in stage.deps:
        if (
            not dep.use_scm_ignore
            and dep.is_in_repo
            and not stage.repo.dvcfs.isdvc(stage.repo.dvcfs.from_os_path(str(dep)))
        ):
            yield dep.fs_path
    for out in stage.outs:
        if not out.use_scm_ignore and out.is_in_repo:
            yield out.fs_path


def _track_stage(stage: "Stage") -> None:
    from dvc.utils import relpath

    context = stage.repo.scm_context
    for path in _get_stage_files(stage):
        context.track_file(relpath(path))
    return context.track_changed_files()


@locked
@scm_context
def reproduce(  # noqa: C901, PLR0912
    self: "Repo",
    targets=None,
    recursive=False,
    pipeline=False,
    all_pipelines=False,
    **kwargs,
):
    from .graph import get_pipeline, get_pipelines

    glob = kwargs.pop("glob", False)

    if isinstance(targets, str):
        targets = [targets]

    if not all_pipelines and not targets:
        from dvc.dvcfile import PROJECT_FILE

        targets = [PROJECT_FILE]

    interactive = kwargs.get("interactive", False)
    if not interactive:
        kwargs["interactive"] = self.config["core"].get("interactive", False)

    stages = set()
    if pipeline or all_pipelines:
        pipelines = get_pipelines(self.index.graph)
        if all_pipelines:
            used_pipelines = pipelines
        else:
            used_pipelines = []
            for target in targets:
                stage = self.stage.get_target(target)
                used_pipelines.append(get_pipeline(pipelines, stage))

        for pline in used_pipelines:
            for stage in pline:
                if pline.in_degree(stage) == 0:
                    stages.add(stage)
    else:
        for target in targets:
            stages.update(
                self.stage.collect(
                    target,
                    recursive=recursive,
                    glob=glob,
                )
            )

    if kwargs.get("pull", False) and kwargs.get("run_cache", True):
        logger.debug("Pulling run cache")
        try:
            self.stage_cache.pull(None)
        except RunCacheNotSupported as e:
            logger.warning("Failed to pull run cache: %s", e)

    return _reproduce_stages(self.index.graph, list(stages), **kwargs)


def _reproduce_stages(  # noqa: C901
    graph,
    stages,
    downstream=False,
    single_item=False,
    on_unchanged=None,
    **kwargs,
):
    r"""Derive the evaluation of the given node for the given graph.

    When you _reproduce a stage_, you want to _evaluate the descendants_
    to know if it make sense to _recompute_ it. A post-ordered search
    will give us an order list of the nodes we want.

    For example, let's say that we have the following pipeline:

                               E
                              / \
                             D   F
                            / \   \
                           B   C   G
                            \ /
                             A

    The derived evaluation of D would be: [A, B, C, D]

    In case that `downstream` option is specified, the desired effect
    is to derive the evaluation starting from the given stage up to the
    ancestors. However, the `networkx.ancestors` returns a set, without
    any guarantee of any order, so we are going to reverse the graph and
    use a reverse post-ordered search using the given stage as a starting
    point.

                   E                                   A
                  / \                                 / \
                 D   F                               B   C   G
                / \   \        --- reverse -->        \ /   /
               B   C   G                               D   F
                \ /                                     \ /
                 A                                       E

    The derived evaluation of _downstream_ B would be: [B, D, E]
    """
    steps = _get_steps(graph, stages, downstream, single_item)

    force_downstream = kwargs.pop("force_downstream", False)
    result = []
    unchanged: List["Stage"] = []
    # `ret` is used to add a cosmetic newline.
    ret: List["Stage"] = []

    for stage in steps:
        if ret:
            logger.info("")

        try:
            ret = _reproduce_stage(stage, **kwargs)

            if len(ret) == 0:
                unchanged.extend([stage])
            elif force_downstream:
                # NOTE: we are walking our pipeline from the top to the
                # bottom. If one stage is changed, it will be reproduced,
                # which tells us that we should force reproducing all of
                # the other stages down below, even if their direct
                # dependencies didn't change.
                kwargs["force"] = True

            if ret:
                result.extend(ret)
        except Exception as exc:  # noqa: BLE001
            raise ReproductionError(stage.addressing) from exc

    if on_unchanged is not None:
        on_unchanged(unchanged)
    return result


def _get_steps(graph, stages, downstream, single_item):
    import networkx as nx

    active = graph.copy()
    if not single_item:
        # NOTE: frozen stages don't matter for single_item
        for stage in graph:
            if stage.frozen:
                # NOTE: disconnect frozen stage from its dependencies
                active.remove_edges_from(graph.out_edges(stage))

    all_pipelines: List["Stage"] = []
    for stage in stages:
        if downstream:
            # NOTE (py3 only):
            # Python's `deepcopy` defaults to pickle/unpickle the object.
            # Stages are complex objects (with references to `repo`,
            # `outs`, and `deps`) that cause struggles when you try
            # to serialize them. We need to create a copy of the graph
            # itself, and then reverse it, instead of using
            # graph.reverse() directly because it calls `deepcopy`
            # underneath -- unless copy=False is specified.
            nodes = nx.dfs_postorder_nodes(active.reverse(copy=False), stage)
            all_pipelines += reversed(list(nodes))
        else:
            all_pipelines += nx.dfs_postorder_nodes(active, stage)

    steps = []
    for stage in all_pipelines:
        if stage not in steps:
            # NOTE: order of steps still matters for single_item
            if single_item and stage not in stages:
                continue

            steps.append(stage)

    return steps


def _repro_callback(experiments_callback, unchanged, stages):
    experiments_callback(unchanged, stages)




dvc/repo/run.py
from typing import TYPE_CHECKING, Union

from dvc.utils.cli_parse import parse_params

from . import locked
from .scm_context import scm_context

if TYPE_CHECKING:
    from dvc.stage import PipelineStage, Stage

    from . import Repo


@locked
@scm_context
def run(
    self: "Repo",
    no_exec: bool = False,
    no_commit: bool = False,
    run_cache: bool = True,
    force: bool = True,
    **kwargs,
) -> Union["Stage", "PipelineStage"]:
    kwargs.update({"force": force, "params": parse_params(kwargs.get("params", []))})
    stage = self.stage.create(**kwargs)

    if no_exec:
        stage.ignore_outs()
    else:
        stage.run(no_commit=no_commit, run_cache=run_cache)

    stage.dump(update_lock=not no_exec)
    return stage




dvc/repo/scm_context.py
import logging
import shlex
from contextlib import contextmanager
from functools import wraps
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Iterable,
    Iterator,
    List,
    Optional,
    Set,
    Union,
)

from dvc.utils import relpath
from dvc.utils.collections import ensure_list

if TYPE_CHECKING:
    from dvc.repo import Repo
    from dvc.scm import Base


logger = logging.getLogger(__name__)


class SCMContext:
    def __init__(self, scm: "Base", config: Optional[Dict[str, Any]] = None) -> None:
        from funcy import get_in

        self.scm: "Base" = scm
        self.autostage: bool = get_in(
            config or {}, ["core", "autostage"], default=False
        )
        self.ignored_paths: List[str] = []
        self.files_to_track: Set[str] = set()
        self.quiet: bool = False

    def track_file(self, paths: Union[str, Iterable[str], None] = None) -> None:
        """Track file to remind user to track new files or autostage later."""
        return self.files_to_track.update(ensure_list(paths))

    @staticmethod
    def _make_git_add_cmd(paths: Union[str, Iterable[str]]) -> str:
        files = " ".join(map(shlex.quote, ensure_list(paths)))
        return f"\tgit add {files}"

    def add(self, paths: Union[str, Iterable[str]]) -> None:
        from scmrepo.exceptions import UnsupportedIndexFormat

        try:
            return self.scm.add(paths)
        except UnsupportedIndexFormat:
            link = "https://github.com/iterative/dvc/issues/610"
            add_cmd = self._make_git_add_cmd([relpath(path) for path in paths])
            logger.info("")
            msg = (
                f"failed to add, add manually using:\n\n{add_cmd}\n"
                f"\nSee {link} for more details.\n"
            )
            logger.warning(msg)

    def track_changed_files(self) -> None:
        """Stage files that have changed."""
        if not self.files_to_track:
            return
        logger.debug("Staging files: %s", self.files_to_track)
        return self.add(self.files_to_track)

    def ignore(self, path: str) -> None:
        from scmrepo.exceptions import FileNotInRepoError

        from dvc.scm import SCMError

        try:
            gitignore_file = self.scm.ignore(path)
        except FileNotInRepoError as exc:
            raise SCMError(str(exc))  # noqa: B904

        if gitignore_file:
            logger.debug("Added '%s' to gitignore file.", path)
            self.track_file(relpath(gitignore_file))
            return self.ignored_paths.append(path)

    def ignore_remove(self, path: str) -> None:
        from scmrepo.exceptions import FileNotInRepoError

        from dvc.scm import SCMError

        logger.debug("Removing '%s' from gitignore file.", path)
        try:
            gitignore_file = self.scm.ignore_remove(path)
        except FileNotInRepoError as exc:
            raise SCMError(str(exc))  # noqa: B904

        if gitignore_file:
            return self.track_file(relpath(gitignore_file))

    @contextmanager
    def __call__(
        self, autostage: Optional[bool] = None, quiet: Optional[bool] = None
    ) -> Iterator["SCMContext"]:
        try:
            yield self
        except Exception:
            for path in self.ignored_paths:
                self.ignore_remove(path)
            raise
        finally:
            self.ignored_paths = []

        if not self.files_to_track:
            return

        if autostage is None:
            autostage = self.autostage
        if quiet is None:
            quiet = self.quiet

        from dvc.scm import NoSCM

        if autostage:
            self.track_changed_files()
        elif (
            not quiet
            and not isinstance(self.scm, NoSCM)
            and logger.isEnabledFor(logging.INFO)
        ):
            add_cmd = self._make_git_add_cmd(self.files_to_track)
            logger.info("\nTo track the changes with git, run:\n\n%s", add_cmd)
            logger.info(
                "\nTo enable auto staging, run:\n\n\tdvc config core.autostage true"
            )

        self.files_to_track = set()

    def __enter__(self) -> "SCMContext":
        self._cm = self()  # pylint: disable=attribute-defined-outside-init
        return self._cm.__enter__()  # pylint: disable=no-member

    def __exit__(self, *exc_args) -> None:
        assert self._cm
        self._cm.__exit__(*exc_args)  # pylint: disable=no-member


def scm_context(method, autostage: Optional[bool] = None, quiet: Optional[bool] = None):
    @wraps(method)
    def run(repo: "Repo", *args, **kw):
        with repo.scm_context(autostage=autostage, quiet=quiet):
            return method(repo, *args, **kw)

    return run




dvc/repo/stage.py
import fnmatch
import logging
import typing
from contextlib import suppress
from functools import wraps
from typing import Iterable, List, NamedTuple, Optional, Set, Tuple, Union

from dvc.exceptions import (
    NoOutputOrStageError,
    OutputDuplicationError,
    OutputNotFoundError,
)
from dvc.repo import lock_repo
from dvc.ui import ui
from dvc.utils import as_posix, parse_target

logger = logging.getLogger(__name__)

if typing.TYPE_CHECKING:
    from networkx import DiGraph

    from dvc.repo import Repo
    from dvc.stage import PipelineStage, Stage
    from dvc.stage.loader import StageLoader

PROJECT_FILE = "dvc.yaml"


class StageInfo(NamedTuple):
    stage: "Stage"
    filter_info: Optional[str] = None


StageList = List["Stage"]
StageIter = Iterable["Stage"]
StageSet = Set["Stage"]


def _collect_with_deps(stages: StageList, graph: "DiGraph") -> StageSet:
    from dvc.repo.graph import collect_pipeline

    res: StageSet = set()
    for stage in stages:
        res.update(collect_pipeline(stage, graph=graph))
    return res


def _maybe_collect_from_dvc_yaml(
    loader: "StageLoad", target, with_deps: bool, **load_kwargs
) -> StageIter:
    from dvc.stage.exceptions import StageNotFound

    stages: StageList = []
    if loader.fs.exists(PROJECT_FILE):
        with suppress(StageNotFound):
            stages = loader.load_all(PROJECT_FILE, target, **load_kwargs)
    if with_deps:
        return _collect_with_deps(stages, loader.repo.index.graph)
    return stages


def _collect_specific_target(
    loader: "StageLoad",
    target: str,
    with_deps: bool,
    recursive: bool,
) -> Tuple[StageIter, Optional[str], Optional[str]]:
    from dvc.dvcfile import is_valid_filename

    # Optimization: do not collect the graph for a specific target
    file, name = parse_target(target)

    # if the target has a file, we can load directly from it.
    if not file:
        # but, if there's no file, parsing is ambiguous as it can be a
        # stage name in `dvc.yaml` file or an output. We prioritize
        # `dvc.yaml` stage name here. If it exists, then we move on.
        # else, we assume it's a output name in the `collect_granular()` below
        msg = "Checking if stage '%s' is in '%s'"
        logger.debug(msg, target, PROJECT_FILE)
        if not (recursive and loader.fs.isdir(target)):
            stages = _maybe_collect_from_dvc_yaml(loader, target, with_deps)
            if stages:
                return stages, file, name
    elif not with_deps and is_valid_filename(file):
        stages = loader.load_all(file, name)
        return stages, file, name
    return [], file, name


def locked(f):
    @wraps(f)
    def wrapper(loader: "StageLoad", *args, **kwargs):
        with lock_repo(loader.repo):
            return f(loader, *args, **kwargs)

    return wrapper


class StageLoad:
    def __init__(self, repo: "Repo") -> None:
        self.repo: "Repo" = repo

    @property
    def fs(self):
        return self.repo.fs

    @locked
    def add(
        self,
        single_stage: bool = False,
        fname: Optional[str] = None,
        validate: bool = True,
        force: bool = False,
        update_lock: bool = False,
        **stage_data,
    ):
        stage = self.create(
            single_stage=single_stage,
            fname=fname,
            validate=validate,
            force=force,
            **stage_data,
        )
        stage.dump(update_lock=update_lock)
        try:
            stage.ignore_outs()
        except FileNotFoundError as exc:
            ui.warn(
                f"Could not create .gitignore entry in {exc.filename}."
                " DVC will attempt to create .gitignore entry again when"
                " the stage is run."
            )

        return stage

    def create(
        self,
        single_stage: bool = False,
        validate: bool = True,
        fname: Optional[str] = None,
        force: bool = False,
        **stage_data,
    ) -> Union["Stage", "PipelineStage"]:
        """Creates a stage.

        Args:
            single_stage: if true, the .dvc file based stage is created,
                fname is required in that case
            fname: name of the file to use, not used for dvc.yaml files
            validate: if true, the new created stage is checked against the
                stages in the repo. Eg: graph correctness,
                potential overwrites in dvc.yaml file (unless `force=True`).
            force: ignores overwrites in dvc.yaml file
            stage_data: Stage data to create from
                (see create_stage and loads_from for more information)
        """
        from dvc.stage import PipelineStage, Stage, create_stage, restore_fields
        from dvc.stage.exceptions import InvalidStageName
        from dvc.stage.utils import is_valid_name, prepare_file_path, validate_kwargs

        stage_data = validate_kwargs(
            single_stage=single_stage, fname=fname, **stage_data
        )
        if single_stage:
            stage_cls = Stage
            path = fname or prepare_file_path(stage_data)
        else:
            path = PROJECT_FILE
            stage_cls = PipelineStage
            stage_name = stage_data["name"]
            if not (stage_name and is_valid_name(stage_name)):
                raise InvalidStageName

        stage = create_stage(stage_cls, repo=self.repo, path=path, **stage_data)
        if validate:
            if not force:
                from dvc.stage.utils import check_stage_exists

                check_stage_exists(self.repo, stage, stage.path)

            try:
                self.repo.check_graph(stages={stage})
            except OutputDuplicationError as exc:
                # Don't include the stage currently being added.
                exc.stages.remove(stage)
                raise OutputDuplicationError(exc.output, exc.stages) from None

        restore_fields(stage)
        return stage

    def from_target(
        self, target: str, accept_group: bool = True, glob: bool = False
    ) -> StageList:
        """
        Returns a list of stage from the provided target.
        (see load method below for further details)
        """
        path, name = parse_target(target, isa_glob=glob)
        return self.load_all(path=path, name=name, accept_group=accept_group, glob=glob)

    def get_target(self, target: str) -> "Stage":
        """
        Returns a stage from the provided target.
        (see load_one method for further details)
        """
        path, name = parse_target(target)
        return self.load_one(path=path, name=name)

    def _get_filepath(
        self, path: Optional[str] = None, name: Optional[str] = None
    ) -> str:
        if path:
            return self.repo.fs.path.realpath(path)

        path = PROJECT_FILE
        logger.debug("Assuming '%s' to be a stage inside '%s'", name, path)
        return path

    @staticmethod
    def _get_group_keys(stages: "StageLoader", group: str) -> Iterable[str]:
        from dvc.parsing import JOIN

        for key in stages:
            assert isinstance(key, str)
            if key.startswith(f"{group}{JOIN}"):
                yield key

    def _get_keys(
        self,
        stages: "StageLoader",
        name: Optional[str] = None,
        accept_group: bool = True,
        glob: bool = False,
    ) -> Iterable[str]:
        if not name:
            return stages.keys()
        if accept_group and stages.is_foreach_generated(name):
            return self._get_group_keys(stages, name)
        if glob:
            return fnmatch.filter(stages.keys(), name)
        return [name]

    def load_all(
        self,
        path: Optional[str] = None,
        name: Optional[str] = None,
        accept_group: bool = True,
        glob: bool = False,
    ) -> StageList:
        """Load a list of stages from a file.

        Args:
            path: if not provided, default `dvc.yaml` is assumed.
            name: required for `dvc.yaml` files, ignored for `.dvc` files.
            accept_group: if true, all of the the stages generated from `name`
                foreach are returned.
            glob: if true, `name` is considered as a glob, which is
                used to filter list of stages from the given `path`.
        """
        from dvc.dvcfile import load_file
        from dvc.stage.loader import SingleStageLoader, StageLoader

        path = self._get_filepath(path, name)
        dvcfile = load_file(self.repo, path)
        # `dvcfile.stages` is not cached
        stages = dvcfile.stages  # type: ignore[attr-defined]

        if isinstance(stages, SingleStageLoader):
            stage = stages[name]
            return [stage]

        assert isinstance(stages, StageLoader)
        keys = self._get_keys(stages, name, accept_group, glob)
        return [stages[key] for key in keys]

    def load_one(
        self, path: Optional[str] = None, name: Optional[str] = None
    ) -> "Stage":
        """Load a single stage from a file.

        Args:
            path: if not provided, default `dvc.yaml` is assumed.
            name: required for `dvc.yaml` files, ignored for `.dvc` files.
        """
        from dvc.dvcfile import load_file

        path = self._get_filepath(path, name)
        dvcfile = load_file(self.repo, path)
        stages = dvcfile.stages  # type: ignore[attr-defined]

        return stages[name]

    def load_file(self, path: Optional[str] = None) -> StageList:
        """Load all of the stages from a file."""
        return self.load_all(path)

    def load_glob(self, path: str, expr: Optional[str] = None):
        """Load stages from `path`, filtered with `expr` provided."""
        return self.load_all(path, expr, glob=True)

    def collect(
        self,
        target: Optional[str] = None,
        with_deps: bool = False,
        recursive: bool = False,
        graph: Optional["DiGraph"] = None,
        glob: bool = False,
    ) -> StageIter:
        """Collect list of stages from the provided target.

        Args:
            target: if not provided, all of the stages in the graph are
                returned.
                Target can be:
                - a foreach group name or a stage name in the `dvc.yaml` file.
                - a generated stage name from a foreach group.
                - a path to `dvc.yaml` or `.dvc` file.
                - in case of a stage to a dvc.yaml file in a different
                  directory than current working directory, it can be a path
                  to dvc.yaml file, followed by a colon `:`, followed by stage
                  name (eg: `../dvc.yaml:build`).
                - in case of `recursive`, it can be a path to a directory.
                - in case of `glob`, it can be a wildcard pattern to match
                  stages. Example: `build*` for stages in `dvc.yaml` file, or
                  `../dvc.yaml:build*` for stages in dvc.yaml in a different
                  directory.
                  Note that, glob only applies for the stage name, not to the
                  file, so `**/dvc.yaml:build*` is not possible.
            with_deps: if true, the stages including their dependencies are
                returned.
            recursive: if true and if `target` is a directory, all of the
                stages inside that directory is returned.
            graph: graph to use. Defaults to `repo.graph`.
            glob: Use `target` as a pattern to match stages in a file.
        """
        if not target:
            return list(graph) if graph else self.repo.index.stages

        if recursive and self.fs.isdir(target):
            from dvc.repo.graph import collect_inside_path

            path = self.fs.path.abspath(target)
            return collect_inside_path(path, graph or self.repo.index.graph)

        stages = self.from_target(target, glob=glob)
        if not with_deps:
            return stages

        return _collect_with_deps(stages, graph or self.repo.index.graph)

    def collect_granular(
        self,
        target: Optional[str] = None,
        with_deps: bool = False,
        recursive: bool = False,
        graph: Optional["DiGraph"] = None,
    ) -> List[StageInfo]:
        """Collects a list of (stage, filter_info) from the given target.

        Priority is in the order of following in case of ambiguity:
        - .dvc file or .yaml file
        - dir if recursive and directory exists
        - foreach_group_name or stage_name
        - generated stage name from a foreach group
        - output file

        Args:
            target: if not provided, all of the stages without any filters are
                returned.
                If `target` is a path to a dvc-tracked output,
                a (stage, output_path) is returned.
                Otherwise, the details above for `target` in `collect()`
                applies.

            (see `collect()` for other arguments)
        """
        if not target:
            return [StageInfo(stage) for stage in self.repo.index.stages]

        target = as_posix(target)

        stages, file, _ = _collect_specific_target(self, target, with_deps, recursive)
        if not stages:
            if not (recursive and self.fs.isdir(target)):
                try:
                    (out,) = self.repo.find_outs_by_path(target, strict=False)
                    return [StageInfo(out.stage, self.fs.path.abspath(target))]
                except OutputNotFoundError:
                    pass

            from dvc.dvcfile import is_valid_filename
            from dvc.stage.exceptions import StageFileDoesNotExistError, StageNotFound

            try:
                stages = self.collect(
                    target,
                    with_deps,
                    recursive,
                    graph,
                )
            except StageFileDoesNotExistError as exc:
                # collect() might try to use `target` as a stage name
                # and throw error that dvc.yaml does not exist, whereas it
                # should say that both stage name and file does not exist.
                if file and is_valid_filename(file):
                    raise
                raise NoOutputOrStageError(target, exc.file) from exc
            except StageNotFound as exc:
                raise NoOutputOrStageError(target, exc.file) from exc

        return [StageInfo(stage) for stage in stages]




dvc/repo/status.py
import logging
from itertools import chain, compress

from dvc.exceptions import InvalidArgumentError

from . import locked

logger = logging.getLogger(__name__)


def _joint_status(pairs):
    status_info = {}

    for stage, filter_info in pairs:
        if stage.frozen and not (stage.is_repo_import or stage.is_versioned_import):
            logger.warning(
                (
                    "%s is frozen. Its dependencies are"
                    " not going to be shown in the status output."
                ),
                stage,
            )
        status_info.update(stage.status(check_updates=True, filter_info=filter_info))

    return status_info


def _local_status(self, targets=None, with_deps=False, recursive=False):
    targets = targets or [None]
    pairs = chain.from_iterable(
        self.stage.collect_granular(t, with_deps=with_deps, recursive=recursive)
        for t in targets
    )

    return _joint_status(pairs)


def _cloud_status(
    self,
    targets=None,
    jobs=None,
    remote=None,
    all_branches=False,
    with_deps=False,
    all_tags=False,
    recursive=False,
    all_commits=False,
):
    """Returns a dictionary with the files that are new or deleted.

    - new: Remote doesn't have the file
    - deleted: File is no longer in the local cache
    - missing: File doesn't exist neither in the cache, neither in remote

    Example:
            Given the following commands:

            $ echo "foo" > foo
            $ echo "bar" > bar
            $ dvc add foo bar
            $ dvc status -c

            It will return something like:

            { "foo": "new", "bar": "new" }

            Now, after pushing and removing "bar" from the local cache:

            $ dvc push
            $ rm .dvc/cache/c1/57a79031e1c40f85931829bc5fc552

            The result would be:

            { "bar": "deleted" }
    """
    used = self.used_objs(
        targets,
        all_branches=all_branches,
        all_tags=all_tags,
        all_commits=all_commits,
        with_deps=with_deps,
        force=True,
        remote=remote,
        jobs=jobs,
        recursive=recursive,
        push=True,
    )

    ret = {}
    for odb, obj_ids in used.items():
        if odb is not None:
            # ignore imported objects
            continue
        status_info = self.cloud.status(obj_ids, jobs, remote=remote)
        for status_ in ("deleted", "new", "missing"):
            for hash_info in getattr(status_info, status_, []):
                ret[hash_info.obj_name] = status_

    return ret


@locked
def status(
    self,
    targets=None,
    jobs=None,
    cloud=False,
    remote=None,
    all_branches=False,
    with_deps=False,
    all_tags=False,
    all_commits=False,
    recursive=False,
):
    if isinstance(targets, str):
        targets = [targets]

    if cloud or remote:
        return _cloud_status(
            self,
            targets,
            jobs,
            all_branches=all_branches,
            with_deps=with_deps,
            remote=remote,
            all_tags=all_tags,
            all_commits=all_commits,
            recursive=True,
        )

    ignored = list(
        compress(
            ["--all-branches", "--all-tags", "--all-commits", "--jobs"],
            [all_branches, all_tags, all_commits, jobs],
        )
    )
    if ignored:
        msg = "The following options are meaningless for local status: {}"
        raise InvalidArgumentError(msg.format(", ".join(ignored)))

    return _local_status(self, targets, with_deps=with_deps, recursive=recursive)




dvc/repo/trie.py
from funcy import first
from pygtrie import Trie

from dvc.exceptions import OutputDuplicationError, OverlappingOutputPathsError


def build_outs_trie(stages):
    outs = Trie()

    for stage in stages:
        for out in stage.outs:
            out_key = out.fs.path.parts(out.fs_path)

            # Check for dup outs
            if out_key in outs:
                dup_stages = [stage, outs[out_key].stage]
                raise OutputDuplicationError(str(out), dup_stages)

            # Check for overlapping outs
            if outs.has_subtrie(out_key):
                parent = out
                overlapping = first(outs.values(prefix=out_key))
            else:
                parent = outs.shortest_prefix(out_key).value
                overlapping = out
            if parent and overlapping:
                msg = (
                    "The output paths:\n'{}'('{}')\n'{}'('{}')\n"
                    "overlap and are thus in the same tracked directory.\n"
                    "To keep reproducibility, outputs should be in separate "
                    "tracked directories or tracked individually."
                ).format(
                    str(parent),
                    parent.stage.addressing,
                    str(overlapping),
                    overlapping.stage.addressing,
                )
                raise OverlappingOutputPathsError(parent, overlapping, msg)

            outs[out_key] = out

    return outs




dvc/repo/update.py
from typing import TYPE_CHECKING, List

from dvc.exceptions import InvalidArgumentError

from . import locked

if TYPE_CHECKING:
    from dvc.repo.stage import StageInfo


@locked
def update(  # noqa: C901
    self,
    targets=None,
    rev=None,
    recursive=False,
    to_remote=False,
    no_download=False,
    remote=None,
    jobs=None,
):
    from .worktree import update_worktree_stages

    if not targets:
        targets = [None]

    if isinstance(targets, str):
        targets = [targets]

    if to_remote and no_download:
        raise InvalidArgumentError("--to-remote can't be used with --no-download")

    if not to_remote and remote:
        raise InvalidArgumentError("--remote can't be used without --to-remote")

    import_stages = set()
    other_stage_infos: List["StageInfo"] = []

    for stage_info in self.index.collect_targets(targets, recursive=recursive):
        if stage_info.stage.is_import:
            import_stages.add(stage_info.stage)
        else:
            other_stage_infos.append(stage_info)

    for stage in import_stages:
        stage.update(
            rev,
            to_remote=to_remote,
            remote=remote,
            no_download=no_download,
            jobs=jobs,
        )
        stage.dump()

    if other_stage_infos:
        if rev:
            raise InvalidArgumentError("--rev can't be used with worktree update")
        if no_download:
            raise InvalidArgumentError(
                "--no-download can't be used with worktree update"
            )
        if to_remote:
            raise InvalidArgumentError("--to-remote can't be used with worktree update")
        update_worktree_stages(
            self,
            other_stage_infos,
        )

    stages = import_stages | {stage_info.stage for stage_info in other_stage_infos}
    return list(stages)




dvc/repo/worktree.py
import logging
from functools import partial
from typing import TYPE_CHECKING, Any, Dict, Iterable, Optional, Set, Tuple, Union

from funcy import first

from dvc.exceptions import DvcException
from dvc.fs.callbacks import Callback
from dvc.stage.exceptions import StageUpdateError

if TYPE_CHECKING:
    from dvc.data_cloud import Remote
    from dvc.output import Output
    from dvc.repo import Repo
    from dvc.repo.index import Index, IndexView
    from dvc.repo.stage import StageInfo
    from dvc.stage import Stage
    from dvc.types import TargetType
    from dvc_data.hashfile.meta import Meta
    from dvc_data.index import DataIndex, DataIndexView
    from dvc_objects.fs.base import FileSystem

logger = logging.getLogger(__name__)


# for files, if our version's checksum (etag) matches the latest remote
# checksum, we do not need to push, even if the version IDs don't match
def _meta_checksum(fs: "FileSystem", meta: "Meta") -> Any:
    if not meta or meta.isdir:
        return meta
    assert fs.PARAM_CHECKSUM
    return getattr(meta, fs.PARAM_CHECKSUM)


def worktree_view_by_remotes(
    index: "Index",
    targets: Optional["TargetType"] = None,
    push: bool = False,
    **kwargs: Any,
) -> Iterable[Tuple[Optional[str], "IndexView"]]:
    # pylint: disable=protected-access

    from dvc.repo.index import IndexView

    def outs_filter(view: "IndexView", remote: Optional[str]):
        def _filter(out: "Output") -> bool:
            if out.remote != remote:
                return False
            if view._outs_filter:
                return view._outs_filter(out)
            return True

        return _filter

    view = worktree_view(index, targets=targets, push=push, **kwargs)
    remotes = {out.remote for out in view.outs}

    if len(remotes) <= 1:
        yield first(remotes), view
        return

    for remote in remotes:
        yield remote, IndexView(index, view._stage_infos, outs_filter(view, remote))


def worktree_view(
    index: "Index",
    targets: Optional["TargetType"] = None,
    push: bool = False,
    **kwargs: Any,
) -> "IndexView":
    """Return view of data that can be stored in worktree remotes.

    Args:
        targets: Optional targets.
        push: Whether the view should be restricted to pushable data only.

    Additional kwargs will be passed into target collection.
    """

    def stage_filter(stage: "Stage") -> bool:
        if push and stage.is_repo_import:
            return False
        return True

    def outs_filter(out: "Output") -> bool:
        if not out.is_in_repo or not out.use_cache or (push and not out.can_push):
            return False
        return True

    return index.targets_view(
        targets,
        stage_filter=stage_filter,
        outs_filter=outs_filter,
        **kwargs,
    )


def _get_remote(
    repo: "Repo", name: Optional[str], default: "Remote", command: str
) -> "Remote":
    if name in (None, default.name):
        return default
    return repo.cloud.get_remote(name, command)


def fetch_worktree(
    repo: "Repo",
    remote: "Remote",
    targets: Optional["TargetType"] = None,
    jobs: Optional[int] = None,
    **kwargs: Any,
) -> int:
    from dvc_data.index import save

    transferred = 0
    for remote_name, view in worktree_view_by_remotes(
        repo.index, push=True, targets=targets, **kwargs
    ):
        remote_obj = _get_remote(repo, remote_name, remote, "fetch")
        index = view.data["repo"]
        total = len(index)
        with Callback.as_tqdm_callback(
            unit="file",
            desc=f"Fetching from remote {remote_obj.name!r}",
            disable=total == 0,
        ) as cb:
            cb.set_size(total)
            transferred += save(index, callback=cb, jobs=jobs, storage="remote")
    return transferred


def push_worktree(
    repo: "Repo",
    remote: "Remote",
    targets: Optional["TargetType"] = None,
    jobs: Optional[int] = None,
    **kwargs: Any,
) -> int:
    from dvc.repo.index import build_data_index
    from dvc_data.index import checkout
    from dvc_data.index.checkout import VersioningNotSupported

    pushed = 0
    stages: Set["Stage"] = set()

    for remote_name, view in worktree_view_by_remotes(
        repo.index, push=True, targets=targets, **kwargs
    ):
        remote_obj = _get_remote(repo, remote_name, remote, "push")
        new_index = view.data["repo"]
        if remote_obj.worktree:
            logger.debug("indexing latest worktree for '%s'", remote_obj.path)
            old_index = build_data_index(view, remote_obj.path, remote_obj.fs)
            logger.debug("Pushing worktree changes to '%s'", remote_obj.path)
        else:
            old_index = None
            logger.debug("Pushing version-aware files to '%s'", remote_obj.path)

        if remote_obj.worktree:
            diff_kwargs: Dict[str, Any] = {
                "meta_only": True,
                "meta_cmp_key": partial(_meta_checksum, remote_obj.fs),
            }
        else:
            diff_kwargs = {}

        total = len(new_index)
        with Callback.as_tqdm_callback(
            unit="file",
            desc=f"Pushing to remote {remote_obj.name!r}",
            disable=total == 0,
        ) as cb:
            cb.set_size(total)
            try:
                stats = checkout(
                    new_index,
                    remote_obj.path,
                    remote_obj.fs,
                    old=old_index,
                    delete=remote_obj.worktree,
                    callback=cb,
                    latest_only=remote_obj.worktree,
                    jobs=jobs,
                    **diff_kwargs,
                )
                pushed += sum(len(changes) for changes in stats.values())
            except VersioningNotSupported:
                logger.exception("")
                raise DvcException(
                    f"remote {remote_obj.name!r} does not support versioning"
                ) from None

        if remote_obj.index is not None:
            for key, entry in new_index.iteritems():
                remote_obj.index[key] = entry
            remote_obj.index.commit()

        for out in view.outs:
            workspace, _key = out.index_key
            _merge_push_meta(out, repo.index.data[workspace], remote_obj.name)
            stages.add(out.stage)

    for stage in stages:
        stage.dump(with_files=True, update_pipeline=False)
    return pushed


def _merge_push_meta(
    out: "Output",
    index: Union["DataIndex", "DataIndexView"],
    remote: Optional[str] = None,
):
    """Merge existing output meta with newly pushed meta.

    Existing version IDs for unchanged files will be preserved to reduce merge
    conflicts (i.e. the DVC output's version ID may not match the pushed/latest
    version ID as long when the file content of both versions is the same).
    """
    from dvc_data.hashfile.tree import Tree
    from dvc_data.index.save import build_tree

    _, key = out.index_key
    entry = index[key]
    repo = out.stage.repo
    if out.isdir():
        old_tree = out.get_obj()
        assert isinstance(old_tree, Tree)
        entry.hash_info = old_tree.hash_info
        entry.meta = out.meta
        for subkey, entry in index.iteritems(key):
            if entry.meta is not None and entry.meta.isdir:
                continue
            fs_path = repo.fs.path.join(repo.root_dir, *subkey)
            meta, hash_info = old_tree.get(
                repo.fs.path.relparts(fs_path, out.fs_path)
            ) or (None, None)
            entry.hash_info = hash_info
            if entry.meta:
                entry.meta.remote = remote
            if meta is not None and meta.version_id is not None:
                # preserve existing version IDs for unchanged files in
                # this dir (entry will have the latest remote version
                # ID after checkout)
                entry.meta = meta
        tree_meta, new_tree = build_tree(index, key)
        out.obj = new_tree
        out.hash_info = new_tree.hash_info
        out.meta = tree_meta
    else:
        if entry.hash_info:
            out.hash_info = entry.hash_info
        if out.meta.version_id is None:
            out.meta = entry.meta
    if out.meta:
        out.meta.remote = remote


def update_worktree_stages(
    repo: "Repo",
    stage_infos: Iterable["StageInfo"],
):
    from dvc.repo.index import IndexView

    def outs_filter(out: "Output") -> bool:
        return out.is_in_repo and out.use_cache and out.can_push

    view = IndexView(
        repo.index,
        stage_infos,
        outs_filter=outs_filter,
    )
    local_index = view.data["repo"]
    remote_indexes: Dict[str, Tuple["Remote", "DataIndex"]] = {}
    for stage in view.stages:
        for out in stage.outs:
            _update_worktree_out(repo, out, local_index, remote_indexes)
        stage.dump(with_files=True, update_pipeline=False)


def _update_worktree_out(
    repo: "Repo",
    out: "Output",
    local_index: Union["DataIndex", "DataIndexView"],
    remote_indexes: Dict[str, Tuple["Remote", "DataIndex"]],
):
    from dvc_data.index import build

    remote_name = out.remote or out.meta.remote
    if not remote_name:
        logger.warning(
            "Could not update '%s', it was never pushed to a remote",
            out,
        )
        return

    if remote_name in remote_indexes:
        remote, remote_index = remote_indexes[remote_name]
    else:
        remote = repo.cloud.get_remote(remote_name, "update")
        if not remote.worktree:
            raise StageUpdateError(out.stage.relpath)
        logger.debug("indexing latest worktree for '%s'", remote.path)
        remote_index = build(remote.path, remote.fs)
        remote_indexes[remote_name] = remote, remote_index
    _workspace, key = out.index_key
    if key not in remote_index:
        logger.warning(
            "Could not update '%s', it does not exist in the remote",
            out,
        )
        return

    entry = remote_index[key]
    if (
        entry.meta
        and entry.meta.isdir
        and not any(
            subkey != key and subentry.meta and not subentry.meta.isdir
            for subkey, subentry in remote_index.iteritems(key)
        )
    ):
        logger.warning(
            "Could not update '%s', directory is empty in the remote",
            out,
        )
        return

    _fetch_out_changes(out, local_index, remote_index, remote)
    _update_out_meta(
        repo,
        out,
        local_index,
        remote_index,
        remote,
    )


def _fetch_out_changes(
    out: "Output",
    local_index: Union["DataIndex", "DataIndexView"],
    remote_index: Union["DataIndex", "DataIndexView"],
    remote: "Remote",
):
    from dvc_data.index import checkout

    old, new = _get_diff_indexes(out, local_index, remote_index)
    total = len(new)
    with Callback.as_tqdm_callback(
        unit="file", desc=f"Updating '{out}'", disable=total == 0
    ) as cb:
        cb.set_size(total)
        checkout(
            new,
            out.repo.root_dir,
            out.fs,
            old=old,
            delete=True,
            update_meta=False,
            meta_only=True,
            meta_cmp_key=partial(_meta_checksum, remote.fs),
            storage="data",
            callback=cb,
        )
        out.save()


def _get_diff_indexes(
    out: "Output",
    local_index: Union["DataIndex", "DataIndexView"],
    remote_index: Union["DataIndex", "DataIndexView"],
) -> Tuple["DataIndex", "DataIndex"]:
    from dvc_data.index import DataIndex

    _, key = out.index_key
    old = DataIndex()
    new = DataIndex()
    for _, entry in local_index.iteritems(key):
        old.add(entry)
    for _, entry in remote_index.iteritems(key):
        new.add(entry)

    for prefix, storage in local_index.storage_map.items():
        old.storage_map[prefix] = storage

    for prefix, storage in remote_index.storage_map.items():
        new.storage_map[prefix] = storage

    return old, new


def _update_out_meta(
    repo: "Repo",
    out: "Output",
    local_index: Union["DataIndex", "DataIndexView"],
    remote_index: Union["DataIndex", "DataIndexView"],
    remote: "Remote",
):
    from dvc_data.index.save import build_tree

    index = _get_update_diff_index(repo, out, local_index, remote_index, remote)

    _, key = out.index_key
    entry = index[key]
    if out.isdir():
        tree_meta, new_tree = build_tree(index, key)
        out.obj = new_tree
        out.hash_info = new_tree.hash_info
        out.meta = tree_meta
    else:
        if entry.hash_info:
            out.hash_info = entry.hash_info
        out.meta = entry.meta
    if out.meta:
        out.meta.remote = remote.name


def _get_update_diff_index(
    repo: "Repo",
    out: "Output",
    local_index: Union["DataIndex", "DataIndexView"],
    remote_index: Union["DataIndex", "DataIndexView"],
    remote: "Remote",
) -> "DataIndex":
    from dvc_data.hashfile.tree import Tree
    from dvc_data.index import DataIndex
    from dvc_data.index.diff import ADD, MODIFY, UNCHANGED, diff

    old, new = _get_diff_indexes(out, local_index, remote_index)
    index = DataIndex()
    for change in diff(
        old,
        new,
        meta_only=True,
        meta_cmp_key=partial(_meta_checksum, remote.fs),
        with_unchanged=True,
    ):
        if change.typ == ADD or change.typ == MODIFY:
            entry = change.new
            # preserve md5's which were calculated in out.save() after
            # downloading
            if out.isdir():
                if not entry.meta.isdir:
                    fs_path = repo.fs.path.join(repo.root_dir, *entry.key)
                    tree = out.obj
                    assert isinstance(tree, Tree)
                    _, entry.hash_info = tree.get(  # type: ignore[misc]
                        repo.fs.path.relparts(fs_path, out.fs_path)
                    )
            else:
                entry.hash_info = out.hash_info
            index[change.new.key] = change.new
        elif change.typ == UNCHANGED:
            index[change.old.key] = change.old
    return index




dvc/repo/experiments/__init__.py
import logging
import os
import re
from typing import TYPE_CHECKING, Dict, Iterable, List, Optional

from funcy import chain, first

from dvc.ui import ui
from dvc.utils import relpath
from dvc.utils.objects import cached_property

from .cache import ExpCache
from .exceptions import (
    BaselineMismatchError,
    ExperimentExistsError,
    InvalidExpRefError,
    MultipleBranchError,
)
from .refs import (
    APPLY_STASH,
    CELERY_FAILED_STASH,
    CELERY_STASH,
    EXEC_APPLY,
    EXEC_NAMESPACE,
    EXPS_NAMESPACE,
    WORKSPACE_STASH,
    ExpRefInfo,
)
from .stash import ApplyStash
from .utils import check_ref_format, exp_refs_by_rev, unlocked_repo

if TYPE_CHECKING:
    from .queue.base import BaseStashQueue, QueueEntry
    from .queue.celery import LocalCeleryQueue
    from .queue.tempdir import TempDirQueue
    from .queue.workspace import WorkspaceQueue
    from .stash import ExpStashEntry

logger = logging.getLogger(__name__)


class Experiments:
    """Class that manages experiments in a DVC repo.

    Args:
        repo (dvc.repo.Repo): repo instance that these experiments belong to.
    """

    BRANCH_RE = re.compile(r"^(?P<baseline_rev>[a-f0-9]{7})-(?P<exp_sha>[a-f0-9]+)")

    def __init__(self, repo):
        from dvc.scm import NoSCMError

        if repo.config["core"].get("no_scm", False):
            raise NoSCMError

        self.repo = repo

    @property
    def scm(self):
        from dvc.scm import SCMError

        if self.repo.scm.no_commits:
            raise SCMError("Empty Git repo. Add a commit to use experiments.")

        return self.repo.scm

    @cached_property
    def dvc_dir(self) -> str:
        return relpath(self.repo.dvc_dir, self.repo.scm.root_dir)

    @cached_property
    def args_file(self) -> str:
        from .executor.base import BaseExecutor

        return os.path.join(self.repo.tmp_dir, BaseExecutor.PACKED_ARGS_FILE)

    @cached_property
    def workspace_queue(self) -> "WorkspaceQueue":
        from .queue.workspace import WorkspaceQueue

        return WorkspaceQueue(self.repo, WORKSPACE_STASH)

    @cached_property
    def tempdir_queue(self) -> "TempDirQueue":
        from .queue.tempdir import TempDirQueue

        # NOTE: tempdir and workspace stash is shared since both
        # implementations immediately push -> pop (queue length is only 0 or 1)
        return TempDirQueue(self.repo, WORKSPACE_STASH)

    @cached_property
    def celery_queue(self) -> "LocalCeleryQueue":
        from .queue.celery import LocalCeleryQueue

        return LocalCeleryQueue(self.repo, CELERY_STASH, CELERY_FAILED_STASH)

    @cached_property
    def apply_stash(self) -> ApplyStash:
        return ApplyStash(self.scm, APPLY_STASH)

    @cached_property
    def cache(self) -> ExpCache:
        return ExpCache(self.repo)

    @property
    def stash_revs(self) -> Dict[str, "ExpStashEntry"]:
        revs = {}
        for queue in (self.workspace_queue, self.celery_queue):
            revs.update(queue.stash.stash_revs)
        return revs

    def reproduce_one(
        self,
        tmp_dir: bool = False,
        copy_paths: Optional[List[str]] = None,
        message: Optional[str] = None,
        **kwargs,
    ):
        """Reproduce and checkout a single (standalone) experiment."""
        exp_queue: "BaseStashQueue" = (
            self.tempdir_queue if tmp_dir else self.workspace_queue
        )
        self.queue_one(exp_queue, **kwargs)
        results = self._reproduce_queue(
            exp_queue, copy_paths=copy_paths, message=message
        )
        exp_rev = first(results)
        if exp_rev is not None:
            self._log_reproduced(results, tmp_dir=tmp_dir)
        return results

    def queue_one(
        self,
        queue: "BaseStashQueue",
        **kwargs,
    ) -> "QueueEntry":
        """Queue a single experiment."""
        if kwargs.pop("machine", None) is not None:
            # TODO: decide how to handle queued remote execution
            raise NotImplementedError

        return self.new(
            queue,
            **kwargs,
        )

    def reproduce_celery(  # noqa: C901
        self, entries: Optional[Iterable["QueueEntry"]] = None, **kwargs
    ) -> Dict[str, str]:
        results: Dict[str, str] = {}
        if entries is None:
            entries = list(
                chain(
                    self.celery_queue.iter_active(),
                    self.celery_queue.iter_queued(),
                )
            )

        logger.debug(
            "reproduce all these entries '%s'",
            entries,
        )

        if not entries:
            return results

        self.celery_queue.start_workers(count=kwargs.get("jobs", 1))
        failed = []
        try:
            ui.write(
                "Following logs for all queued experiments. Use Ctrl+C to "
                "stop following logs (experiment execution will continue).\n"
            )
            for entry in entries:
                # wait for task execution to start
                self.celery_queue.wait_for_start(entry, sleep_interval=1)
                self.celery_queue.follow(entry)
                # wait for task collection to complete
                try:
                    result = self.celery_queue.get_result(entry)
                except FileNotFoundError:
                    result = None
                if result is None or result.exp_hash is None:
                    name = entry.name or entry.stash_rev[:7]
                    failed.append(name)
                elif result.ref_info:
                    exp_rev = self.scm.get_ref(str(result.ref_info))
                    results[exp_rev] = result.exp_hash
        except KeyboardInterrupt:
            ui.write(
                "Experiment(s) are still executing in the background. To "
                "abort execution use 'dvc queue kill' or 'dvc queue stop'."
            )
        if failed:
            names = ", ".join(name for name in failed)
            ui.error(f"Failed to reproduce experiment(s) '{names}'")
        if results:
            self._log_reproduced((rev for rev in results), True)
        return results

    def _log_reproduced(self, revs: Iterable[str], tmp_dir: bool = False):
        names = []
        rev_names = self.get_exact_name(revs)
        for rev in revs:
            name = rev_names[rev]
            names.append(name if name else rev[:7])
        ui.write("\nRan experiment(s): {}".format(", ".join(names)))
        if tmp_dir:
            ui.write(
                "To apply the results of an experiment to your workspace "
                "run:\n\n"
                "\tdvc exp apply <exp>"
            )
        else:
            ui.write("Experiment results have been applied to your workspace.")
        ui.write(
            "\nTo promote an experiment to a Git branch run:\n\n"
            "\tdvc exp branch <exp> <branch>\n"
        )

    def new(
        self,
        queue: "BaseStashQueue",
        *args,
        **kwargs,
    ) -> "QueueEntry":
        """Create and enqueue a new experiment.

        Experiment will be derived from the current workspace.
        """

        name = kwargs.get("name", None)
        baseline_sha = kwargs.get("baseline_rev") or self.repo.scm.get_rev()

        if name:
            exp_ref = ExpRefInfo(baseline_sha=baseline_sha, name=name)
            check_ref_format(self.scm, exp_ref)
            force = kwargs.get("force", False)
            if self.scm.get_ref(str(exp_ref)) and not force:
                raise ExperimentExistsError(exp_ref.name)

        return queue.put(*args, **kwargs)

    def _get_last_applied(self) -> Optional[str]:
        try:
            last_applied = self.scm.get_ref(EXEC_APPLY)
            if last_applied:
                self.check_baseline(last_applied)
            return last_applied
        except BaselineMismatchError:
            # If HEAD has moved since the the last applied experiment,
            # the applied experiment is no longer relevant
            self.scm.remove_ref(EXEC_APPLY)
        return None

    @unlocked_repo
    def _reproduce_queue(
        self,
        queue: "BaseStashQueue",
        copy_paths: Optional[List[str]] = None,
        message: Optional[str] = None,
        **kwargs,
    ) -> Dict[str, str]:
        """Reproduce queued experiments.

        Arguments:
            queue: Experiment queue.

        Returns:
            dict mapping successfully reproduced experiment revs to their
            results.
        """
        exec_results = queue.reproduce(copy_paths=copy_paths, message=message)

        results: Dict[str, str] = {}
        for _, exp_result in exec_results.items():
            results.update(exp_result)
        return results

    def check_baseline(self, exp_rev):
        baseline_sha = self.repo.scm.get_rev()
        if exp_rev == baseline_sha:
            return exp_rev

        exp_baseline = self._get_baseline(exp_rev)
        if exp_baseline is None:
            # if we can't tell from branch name, fall back to parent commit
            exp_commit = self.scm.resolve_commit(exp_rev)
            if exp_commit:
                exp_baseline = first(exp_commit.parents)
        if exp_baseline == baseline_sha:
            return exp_baseline
        raise BaselineMismatchError(exp_baseline, baseline_sha)

    def get_baseline(self, rev):
        """Return the baseline rev for an experiment rev."""
        return self._get_baseline(rev)

    def _get_baseline(self, rev):
        from dvc.scm import resolve_rev

        rev = resolve_rev(self.scm, rev)

        if rev in self.stash_revs:
            entry = self.stash_revs.get(rev)
            if entry:
                return entry.baseline_rev
            return None

        ref_info = first(exp_refs_by_rev(self.scm, rev))
        if ref_info:
            return ref_info.baseline_sha
        return None

    def get_branch_by_rev(
        self, rev: str, allow_multiple: bool = False
    ) -> Optional[str]:
        """Returns full refname for the experiment branch containing rev."""
        ref_infos = list(exp_refs_by_rev(self.scm, rev))
        if not ref_infos:
            return None
        if len(ref_infos) > 1 and not allow_multiple:
            for ref_info in ref_infos:
                if self.scm.get_ref(str(ref_info)) == rev:
                    return str(ref_info)
            raise MultipleBranchError(rev, ref_infos)
        return str(ref_infos[0])

    def get_exact_name(self, revs: Iterable[str]) -> Dict[str, Optional[str]]:
        """Returns preferred name for the specified revision.

        Prefers tags, branches (heads), experiments in that order.
        """
        result: Dict[str, Optional[str]] = {}
        exclude = f"{EXEC_NAMESPACE}/*"
        ref_dict = self.scm.describe(revs, base=EXPS_NAMESPACE, exclude=exclude)
        for rev in revs:
            name: Optional[str] = None
            ref = ref_dict[rev]
            if ref:
                try:
                    name = ExpRefInfo.from_ref(ref).name
                except InvalidExpRefError:
                    pass
            if not name:
                if rev in self.stash_revs:
                    name = self.stash_revs[rev].name
                else:
                    failed_stash = self.celery_queue.failed_stash
                    if failed_stash and rev in failed_stash.stash_revs:
                        name = failed_stash.stash_revs[rev].name
            result[rev] = name
        return result

    def apply(self, *args, **kwargs):
        from dvc.repo.experiments.apply import apply

        return apply(self.repo, *args, **kwargs)

    def branch(self, *args, **kwargs):
        from dvc.repo.experiments.branch import branch

        return branch(self.repo, *args, **kwargs)

    def diff(self, *args, **kwargs):
        from dvc.repo.experiments.diff import diff

        return diff(self.repo, *args, **kwargs)

    def show(self, *args, **kwargs):
        from dvc.repo.experiments.show import show

        return show(self.repo, *args, **kwargs)

    def run(self, *args, **kwargs):
        from dvc.repo.experiments.run import run

        return run(self.repo, *args, **kwargs)

    def save(self, *args, **kwargs):
        from dvc.repo.experiments.save import save

        return save(self.repo, *args, **kwargs)

    def push(self, *args, **kwargs):
        from dvc.repo.experiments.push import push

        return push(self.repo, *args, **kwargs)

    def pull(self, *args, **kwargs):
        from dvc.repo.experiments.pull import pull

        return pull(self.repo, *args, **kwargs)

    def ls(self, *args, **kwargs):
        from dvc.repo.experiments.ls import ls

        return ls(self.repo, *args, **kwargs)

    def remove(self, *args, **kwargs):
        from dvc.repo.experiments.remove import remove

        return remove(self.repo, *args, **kwargs)

    def clean(self, *args, **kwargs):
        from dvc.repo.experiments.clean import clean

        return clean(self.repo, *args, **kwargs)




dvc/repo/experiments/apply.py
import logging
import os
from typing import TYPE_CHECKING, Optional

from dvc.repo import locked
from dvc.repo.scm_context import scm_context
from dvc.scm import Git
from dvc.ui import ui
from dvc.utils.fs import remove

from .exceptions import BaselineMismatchError, InvalidExpRevError
from .executor.base import BaseExecutor
from .refs import EXEC_APPLY

if TYPE_CHECKING:
    from dvc.repo import Repo
    from dvc.repo.experiments import Experiments

logger = logging.getLogger(__name__)


@locked
@scm_context
def apply(repo: "Repo", rev: str, **kwargs):  # noqa: C901
    from dvc.repo.checkout import checkout as dvc_checkout
    from dvc.scm import RevError, resolve_rev

    exps: "Experiments" = repo.experiments

    is_stash: bool = False

    assert isinstance(repo.scm, Git)
    try:
        exp_rev = resolve_rev(repo.scm, rev)
    except RevError as exc:
        (
            exp_ref_info,
            queue_entry,
        ) = exps.celery_queue.get_ref_and_entry_by_names(
            rev
        )[rev]
        if exp_ref_info:
            exp_rev = repo.scm.get_ref(str(exp_ref_info))
        elif queue_entry:
            exp_rev = queue_entry.stash_rev
            is_stash = True
        else:
            raise InvalidExpRevError(rev) from exc
    except BaselineMismatchError as exc:
        raise InvalidExpRevError(rev) from exc

    _apply(repo, exp_rev, name=rev, is_stash=is_stash)
    dvc_checkout(repo, **kwargs)

    repo.scm.set_ref(EXEC_APPLY, exp_rev)
    ui.write(
        f"Changes for experiment '{rev}' have been applied to your current workspace.",
    )


def _apply(repo: "Repo", rev: str, name: Optional[str] = None, is_stash: bool = False):
    exps: "Experiments" = repo.experiments

    with exps.apply_stash.preserve_workspace(rev, name=name):
        with repo.scm.detach_head(rev, force=True):
            if is_stash:
                assert repo.tmp_dir is not None
                args_path = os.path.join(repo.tmp_dir, BaseExecutor.PACKED_ARGS_FILE)
                if os.path.exists(args_path):
                    remove(args_path)




dvc/repo/experiments/branch.py
import logging

from dvc.exceptions import InvalidArgumentError
from dvc.repo import locked
from dvc.repo.scm_context import scm_context
from dvc.scm import RevError

from .exceptions import InvalidExpRevError
from .utils import exp_refs_by_rev

logger = logging.getLogger(__name__)


@locked
@scm_context
def branch(repo, exp_rev, branch_name=None, **kwargs):
    from dvc.scm import resolve_rev

    try:
        rev = resolve_rev(repo.scm, exp_rev)
    except RevError:
        raise InvalidArgumentError(exp_rev)  # noqa: B904
    ref_info = None

    ref_infos = list(exp_refs_by_rev(repo.scm, rev))
    if len(ref_infos) == 1:
        ref_info = ref_infos[0]
    elif len(ref_infos) > 1:
        current_rev = repo.scm.get_rev()
        for info in ref_infos:
            if info.baseline_sha == current_rev:
                ref_info = info
                break
        if not ref_info:
            msg = [
                (
                    f"Ambiguous experiment name '{exp_rev}' can refer to "
                    "multiple experiments. To create a branch use a full "
                    "experiment ref:"
                ),
                "",
            ]
            msg.extend([str(info) for info in ref_infos])
            raise InvalidArgumentError("\n".join(msg))

    if not ref_info:
        raise InvalidExpRevError(exp_rev)

    branch_name = branch_name or f"{ref_info.name}-branch"

    branch_ref = f"refs/heads/{branch_name}"
    if repo.scm.get_ref(branch_ref):
        raise InvalidArgumentError(f"Git branch '{branch_name}' already exists.")

    target = repo.scm.get_ref(str(ref_info))
    repo.scm.set_ref(
        branch_ref,
        target,
        message=f"dvc: Created from experiment '{ref_info.name}'",
    )
    fmt = (
        "Git branch '%s' has been created from experiment '%s'.\n"
        "To switch to the new branch run:\n\n"
        "\tgit checkout %s"
    )
    logger.info(fmt, branch_name, ref_info.name, branch_name)




dvc/repo/experiments/brancher.py
from contextlib import ExitStack, contextmanager
from typing import TYPE_CHECKING, Iterator, Tuple

from dvc.repo.experiments.exceptions import InvalidExpRevError
from dvc.scm import RevError

if TYPE_CHECKING:
    from dvc.repo import Repo


@contextmanager
def switch_repo(
    repo: "Repo",
    rev: str,
) -> Iterator[Tuple["Repo", str]]:
    """Return a repo instance (brancher) switched to rev.

    If rev is the name of a running experiment, the returned instance will be
    the live repo wherever the experiment is running.

    NOTE: This will not resolve git SHA's that only exist in queued exp workspaces
    (it will only match queued exp names).
    """
    try:
        with repo.switch(rev):
            yield repo, rev
        return
    except RevError as exc:
        orig_exc = exc
    exps = repo.experiments

    if rev == exps.workspace_queue.get_running_exp():
        yield repo, "workspace"
        return

    for queue in (exps.tempdir_queue, exps.celery_queue):
        try:
            active_repo = queue.active_repo(rev)
        except InvalidExpRevError:
            continue
        stack = ExitStack()
        stack.enter_context(active_repo)
        stack.enter_context(active_repo.switch("workspace"))
        with stack:
            yield active_repo, rev
        return
    raise orig_exc




dvc/repo/experiments/cache.py
import logging
import os
from typing import TYPE_CHECKING, Optional, Union

from dvc.fs import localfs
from dvc_objects.db import ObjectDB

from .serialize import DeserializeError, SerializableError, SerializableExp
from .utils import EXEC_TMP_DIR

if TYPE_CHECKING:
    from dvc.repo import Repo

logger = logging.getLogger(__name__)


class ExpCache:
    """Serialized experiment state cache.

    ODB with git SHAs as keys. Objects can be either SerializableExp or
    SerializableError.
    """

    CACHE_DIR = os.path.join(EXEC_TMP_DIR, "cache")

    def __init__(self, repo: "Repo"):
        path = os.path.join(repo.tmp_dir, self.CACHE_DIR)
        self.odb = ObjectDB(localfs, path)

    def delete(self, rev: str):
        self.odb.delete(rev)

    def put(
        self,
        exp: Union[SerializableExp, SerializableError],
        rev: Optional[str] = None,
        force: bool = False,
    ):
        rev = rev or getattr(exp, "rev", None)
        assert rev
        assert rev != "workspace"
        if force or not self.odb.exists(rev):
            try:
                self.delete(rev)
            except FileNotFoundError:
                pass
            self.odb.add_bytes(rev, exp.as_bytes())
            logger.trace(  # type: ignore[attr-defined]
                "ExpCache: cache put '%s'", rev[:7]
            )

    def get(self, rev: str) -> Optional[Union[SerializableExp, SerializableError]]:
        obj = self.odb.get(rev)
        try:
            with obj.fs.open(obj.path, "rb") as fobj:
                data = fobj.read()
        except FileNotFoundError:
            logger.trace(  # type: ignore[attr-defined]
                "ExpCache: cache miss '%s'", rev[:7]
            )
            return None
        for typ in (SerializableExp, SerializableError):
            try:
                exp = typ.from_bytes(data)  # type: ignore[attr-defined]
                logger.trace(  # type: ignore[attr-defined]
                    "ExpCache: cache load '%s'", rev[:7]
                )
                return exp
            except DeserializeError:
                continue
        logger.debug("ExpCache: unknown object type for '%s'", rev)
        return None




dvc/repo/experiments/clean.py
from typing import TYPE_CHECKING

from dvc.ui import ui

if TYPE_CHECKING:
    from dvc.repo import Repo


def clean(repo: "Repo"):
    ui.write("Cleaning up dvc-task messages...")
    repo.experiments.celery_queue.celery.clean()
    ui.write("Done!")




dvc/repo/experiments/collect.py
import itertools
import logging
import os
from datetime import datetime
from typing import (
    TYPE_CHECKING,
    Collection,
    Dict,
    Iterable,
    Iterator,
    List,
    Optional,
    Union,
)

from funcy import first
from scmrepo.exceptions import SCMError as InnerSCMError

from dvc.scm import Git, SCMError, iter_revs

from .exceptions import InvalidExpRefError
from .refs import EXEC_BRANCH, ExpRefInfo
from .serialize import ExpRange, ExpState, SerializableError, SerializableExp

if TYPE_CHECKING:
    from dvc.repo import Repo

    from .cache import ExpCache

logger = logging.getLogger(__name__)


def collect_rev(
    repo: "Repo",
    rev: str,
    param_deps: bool = False,
    force: bool = False,
    cache: Optional["ExpCache"] = None,
    **kwargs,
) -> ExpState:
    """Collect experiment state for the given revision.

    Exp will be loaded from cache when available unless rev is 'workspace' or
    force is set.
    """
    from dvc.fs import LocalFileSystem

    cache = cache or repo.experiments.cache
    assert cache
    # TODO: support filtering serialized exp when param_deps is set
    if rev != "workspace" and not (force or param_deps):
        cached_exp = cache.get(rev)
        if cached_exp:
            if isinstance(cached_exp, SerializableError):
                return ExpState(rev=rev, error=cached_exp)
            return ExpState(rev=rev, data=cached_exp)
    if rev == "workspace" and isinstance(repo.fs, LocalFileSystem):
        orig_cwd: Optional[str] = os.getcwd()
        os.chdir(repo.root_dir)
    else:
        orig_cwd = None
    try:
        data = _collect_rev(
            repo,
            rev,
            param_deps=param_deps,
            force=force,
            **kwargs,
        )
        if not (rev == "workspace" or param_deps or data.contains_error):
            cache.put(data, force=True)
        return ExpState(rev=rev, data=data)
    except Exception as exc:  # noqa: BLE001, pylint: disable=broad-except
        logger.debug("", exc_info=True)
        error = SerializableError(str(exc), type(exc).__name__)
        return ExpState(rev=rev, error=error)
    finally:
        if orig_cwd:
            os.chdir(orig_cwd)


def _collect_rev(
    repo: "Repo",
    rev: str,
    param_deps: bool = False,
    **kwargs,
) -> SerializableExp:
    with repo.switch(rev) as rev:
        if rev == "workspace":
            timestamp: Optional[datetime] = None
        else:
            commit = repo.scm.resolve_commit(rev)
            timestamp = datetime.fromtimestamp(commit.commit_time)

        return SerializableExp.from_repo(
            repo,
            rev=rev,
            param_deps=param_deps,
            timestamp=timestamp,
        )


def collect_branch(
    repo: "Repo",
    rev: str,
    end_rev: Optional[str] = None,
    **kwargs,
) -> Iterator["ExpState"]:
    """Iterate over exp states in a Git branch.

    Git branch will be traversed in reverse, starting from rev.

    Args:
        rev: Branch tip (head).
        end_rev: If specified, traversal will stop when end_rev is reached
            (exclusive, end_rev will not be collected).
    """
    try:
        for branch_rev in repo.scm.branch_revs(rev, end_rev):
            yield collect_rev(repo, branch_rev, **kwargs)
    except (SCMError, InnerSCMError):
        pass


def collect_exec_branch(
    repo: "Repo",
    baseline_rev: str,
    **kwargs,
) -> Iterator["ExpState"]:
    """Iterate over active experiment branch for the current executor."""
    last_rev = repo.scm.get_ref(EXEC_BRANCH) or repo.scm.get_rev()
    last_rev = repo.scm.get_rev()
    yield collect_rev(repo, "workspace", **kwargs)
    if last_rev != baseline_rev:
        yield from collect_branch(repo, last_rev, baseline_rev, **kwargs)


def collect_queued(
    repo: "Repo",
    baseline_revs: Collection[str],
    **kwargs,
) -> Dict[str, List["ExpRange"]]:
    """Collect queued experiments derived from the specified revisions.

    Args:
        repo: Repo.
        baseline_revs: Resolved baseline Git SHAs.

    Returns:
        Dict mapping baseline revision to list of queued experiments.
    """
    if not baseline_revs:
        return {}
    return repo.experiments.celery_queue.collect_queued_data(baseline_revs, **kwargs)


def collect_active(
    repo: "Repo",
    baseline_revs: Collection[str],
    **kwargs,
) -> Dict[str, List["ExpRange"]]:
    """Collect active (running) experiments derived from the specified revisions.

    Args:
        repo: Repo.
        baseline_revs: Resolved baseline Git SHAs.

    Returns:
        Dict mapping baseline revision to list of active experiments.
    """
    if not baseline_revs:
        return {}
    result: Dict[str, List["ExpRange"]] = {}
    exps = repo.experiments
    for queue in (exps.workspace_queue, exps.tempdir_queue, exps.celery_queue):
        for baseline, active_exps in queue.collect_active_data(
            baseline_revs, **kwargs
        ).items():
            if baseline in result:
                result[baseline].extend(active_exps)
            else:
                result[baseline] = list(active_exps)
    return result


def collect_failed(
    repo: "Repo",
    baseline_revs: Collection[str],
    **kwargs,
) -> Dict[str, List["ExpRange"]]:
    """Collect failed experiments derived from the specified revisions.

    Args:
        repo: Repo.
        baseline_revs: Resolved baseline Git SHAs.

    Returns:
        Dict mapping baseline revision to list of active experiments.
    """
    if not baseline_revs:
        return {}
    return repo.experiments.celery_queue.collect_failed_data(baseline_revs, **kwargs)


def collect_successful(
    repo: "Repo",
    baseline_revs: Collection[str],
    **kwargs,
) -> Dict[str, List["ExpRange"]]:
    """Collect successful experiments derived from the specified revisions.

    Args:
        repo: Repo.
        baseline_revs: Resolved baseline Git SHAs.

    Returns:
        Dict mapping baseline revision to successful experiments.
    """
    result: Dict[str, List["ExpRange"]] = {}
    for baseline_rev in baseline_revs:
        result[baseline_rev] = list(_collect_baseline(repo, baseline_rev, **kwargs))
    return result


def _collect_baseline(
    repo: "Repo",
    baseline_rev: str,
    **kwargs,
) -> Iterator["ExpRange"]:
    """Iterate over experiments derived from a baseline revision.

    Args:
        repo: Repo.
        baseline_revs: Resolved baseline Git SHAs.

    Yields:
        Tuple of (timestamp, exp_range).
    """
    ref_info = ExpRefInfo(baseline_sha=baseline_rev)
    refs: Optional[Iterable[str]] = kwargs.get("refs")
    if refs:
        ref_it = (ref for ref in iter(refs) if ref.startswith(str(ref_info)))
    else:
        ref_it = repo.scm.iter_refs(base=str(ref_info))
    for ref in ref_it:
        try:
            ref_info = ExpRefInfo.from_ref(ref)
            exp_rev = repo.scm.get_ref(ref)
            if not exp_rev:
                continue
        except (InvalidExpRefError, SCMError, InnerSCMError):
            continue
        exps = list(collect_branch(repo, exp_rev, baseline_rev, **kwargs))
        if exps:
            exps[0].name = ref_info.name
            yield ExpRange(exps, name=ref_info.name)


def collect(
    repo: "Repo",
    revs: Union[List[str], str, None] = None,
    all_branches: bool = False,
    all_tags: bool = False,
    all_commits: bool = False,
    num: int = 1,
    hide_queued: bool = False,
    hide_failed: bool = False,
    sha_only: bool = False,
    **kwargs,
) -> List["ExpState"]:
    """Collect baseline revisions and derived experiments."""
    assert isinstance(repo.scm, Git)
    if repo.scm.no_commits:
        return []
    if not any([revs, all_branches, all_tags, all_commits]):
        revs = ["HEAD"]
    if isinstance(revs, str):
        revs = [revs]
    cached_refs = list(repo.scm.iter_refs())
    baseline_revs = list(
        iter_revs(
            repo.scm,
            revs=revs,
            num=num,
            all_branches=all_branches,
            all_tags=all_tags,
            all_commits=all_commits,
        )
    )
    if sha_only:
        baseline_names: Dict[str, Optional[str]] = {}
    else:
        baseline_names = _describe(repo.scm, baseline_revs, refs=cached_refs)

    workspace_data = collect_rev(repo, "workspace", **kwargs)
    result: List["ExpState"] = [workspace_data]
    queued = collect_queued(repo, baseline_revs, **kwargs) if not hide_queued else {}
    active = collect_active(repo, baseline_revs, **kwargs)
    failed = collect_failed(repo, baseline_revs, **kwargs) if not hide_failed else {}
    successful = collect_successful(repo, baseline_revs, **kwargs)

    for baseline_rev in baseline_revs:
        baseline_data = collect_rev(repo, baseline_rev)
        experiments = list(
            itertools.chain.from_iterable(
                _sorted_ranges(collected.get(baseline_rev, []))
                for collected in (active, successful, queued, failed)
            )
        )
        result.append(
            ExpState(
                rev=baseline_rev,
                name=baseline_names.get(baseline_rev),
                data=baseline_data.data,
                error=baseline_data.error,
                experiments=experiments if experiments else None,
            )
        )
    return result


def _describe(
    scm: "Git",
    revs: Iterable[str],
    refs: Optional[Iterable[str]] = None,
) -> Dict[str, Optional[str]]:
    """Describe revisions using a tag, branch.

    The first matching name will be returned for each rev. Names are preferred in this
    order:
        - current branch (if rev matches HEAD and HEAD is a branch)
        - tags
        - branches

    Returns:
        Dict mapping revisions from revs to a name.
    """

    head_rev = scm.get_rev()
    head_ref = scm.get_ref("HEAD", follow=False)
    if head_ref and head_ref.startswith("refs/heads/"):
        head_branch = head_ref[len("refs/heads/") :]
    else:
        head_branch = None

    tags = {}
    branches = {}
    ref_it = iter(refs) if refs else scm.iter_refs()
    for ref in ref_it:
        is_tag = ref.startswith("refs/tags/")
        is_branch = ref.startswith("refs/heads/")
        if not (is_tag or is_branch):
            continue
        rev = scm.get_ref(ref)
        if not rev:
            logger.debug("unresolved ref %s", ref)
            continue
        if is_tag and rev not in tags:
            tags[rev] = ref[len("refs/tags/") :]
        if is_branch and rev not in branches:
            branches[rev] = ref[len("refs/heads/") :]
    names: Dict[str, Optional[str]] = {}
    for rev in revs:
        if rev == head_rev and head_branch:
            names[rev] = head_branch
        else:
            names[rev] = tags.get(rev) or branches.get(rev)
    return names


def _sorted_ranges(exp_ranges: Iterable["ExpRange"]) -> List["ExpRange"]:
    """Return list of ExpRange sorted by timestamp."""

    def _head_timestamp(exp_range: "ExpRange") -> datetime:
        head_exp = first(exp_range.revs)
        if head_exp and head_exp.data and head_exp.data.timestamp:
            return head_exp.data.timestamp
        return datetime.fromtimestamp(0)

    return sorted(exp_ranges, key=_head_timestamp, reverse=True)




dvc/repo/experiments/diff.py
import logging

from dvc.utils.diff import diff as _diff
from dvc.utils.diff import format_dict

logger = logging.getLogger(__name__)


def diff(repo, *args, a_rev=None, b_rev=None, param_deps=False, **kwargs):
    from dvc.repo.experiments.collect import collect_rev
    from dvc.scm import resolve_rev

    if repo.scm.no_commits:
        return {}

    if a_rev:
        rev = resolve_rev(repo.scm, a_rev)
    else:
        rev = resolve_rev(repo.scm, "HEAD")
    old = collect_rev(repo, rev, param_deps=param_deps)

    if b_rev:
        rev = resolve_rev(repo.scm, b_rev)
    else:
        rev = "workspace"
    new = collect_rev(repo, rev, param_deps=param_deps)

    with_unchanged = kwargs.pop("all", False)
    return {
        key: _diff(
            format_dict(getattr(old.data, key, {})),
            format_dict(getattr(new.data, key, {})),
            with_unchanged=with_unchanged,
        )
        for key in ["metrics", "params"]
    }




dvc/repo/experiments/exceptions.py
from typing import TYPE_CHECKING, Collection, Iterable, Optional

from dvc.exceptions import DvcException, InvalidArgumentError

if TYPE_CHECKING:
    from .refs import ExpRefInfo


class BaselineMismatchError(DvcException):
    def __init__(self, rev, expected):
        if hasattr(rev, "hexsha"):
            rev = rev.hexsha
        rev_str = f"{rev[:7]}" if rev is not None else "invalid commit"
        super().__init__(
            f"Experiment derived from '{rev_str}', expected '{expected[:7]}'."
        )
        self.rev = rev
        self.expected_rev = expected


class ExperimentExistsError(DvcException):
    def __init__(self, name: str, command: str = "run"):
        msg = (
            "Experiment conflicts with existing experiment "
            f"'{name}'. To overwrite the existing experiment run:\n\n"
            f"\tdvc exp {command} -f ...\n\n"
            f"To {command} this experiment with a different name run:\n\n"
            f"\tdvc exp {command} -n <new_name> ...\n"
        )
        super().__init__(msg)
        self.name = name


class InvalidExpRefError(DvcException):
    def __init__(self, ref):
        super().__init__(f"'{ref}' is not a valid experiment refname.")
        self.ref = ref


class InvalidExpRevError(InvalidArgumentError):
    def __init__(self, rev):
        super().__init__(f"'{rev}' does not appear to be an experiment commit.")


class MultipleBranchError(DvcException):
    def __init__(self, rev, ref_infos):
        super().__init__(
            f"Ambiguous commit '{rev[:7]}' belongs to multiple experiment branches."
        )
        self.rev = rev
        self.ref_infos = ref_infos


class AmbiguousExpRefInfo(InvalidArgumentError):
    def __init__(
        self,
        exp_name: str,
        exp_ref_list: Iterable["ExpRefInfo"],
    ):
        msg = [
            (
                f"Ambiguous name '{exp_name}' refers to multiple experiments."
                " Use one of the following full refnames instead:"
            ),
            "",
        ]
        msg.extend([f"\t{info}" for info in exp_ref_list])
        super().__init__("\n".join(msg))


class UnresolvedExpNamesError(InvalidArgumentError):
    NAME = "experiment name"

    def __init__(
        self,
        unresolved_list: Collection[str],
        *args,
        git_remote: Optional[str] = None,
    ):
        unresolved_names = "; ".join(unresolved_list)
        if not git_remote:
            if len(unresolved_list) > 1:
                super().__init__(f"'{unresolved_names}' are not valid {self.NAME}s")
            else:
                super().__init__(f"'{unresolved_names}' is not a valid {self.NAME}")
        else:
            super().__init__(
                f"Experiment '{unresolved_names}' does not exist in '{git_remote}'"
            )


class UnresolvedQueueExpNamesError(UnresolvedExpNamesError):
    NAME = "queued experiment name"


class UnresolvedRunningExpNamesError(UnresolvedExpNamesError):
    NAME = "running experiment name"


class ExpQueueEmptyError(DvcException):
    pass


class ExpNotStartedError(DvcException):
    def __init__(self, name: str):
        super().__init__(
            f"Queued experiment '{name}' exists but has not started running yet"
        )




dvc/repo/experiments/ls.py
import logging
from collections import defaultdict
from typing import Dict, List, Optional, Tuple, Union

from dvc.repo import locked
from dvc.repo.scm_context import scm_context
from dvc.scm import iter_revs

from .utils import exp_refs_by_baseline

logger = logging.getLogger(__name__)


@locked
@scm_context
def ls(
    repo,
    rev: Optional[Union[List[str], str]] = None,
    all_commits: bool = False,
    num: int = 1,
    git_remote: Optional[str] = None,
) -> Dict[str, List[Tuple[str, Optional[str]]]]:
    """List experiments.

    Returns a dict mapping baseline revs to a list of (exp_name, exp_sha) tuples.
    """
    rev_set = None
    if not all_commits:
        rev = rev or "HEAD"
        if isinstance(rev, str):
            rev = [rev]
        revs = iter_revs(repo.scm, rev, num)
        rev_set = set(revs.keys())
    ref_info_dict = exp_refs_by_baseline(repo.scm, rev_set, git_remote)

    tags = repo.scm.describe(ref_info_dict.keys())
    remained = {baseline for baseline, tag in tags.items() if tag is None}
    base = "refs/heads"
    ref_heads = repo.scm.describe(remained, base=base)

    results = defaultdict(list)
    for baseline in ref_info_dict:
        name = baseline
        if tags[baseline] or ref_heads[baseline]:
            name = tags[baseline] or ref_heads[baseline][len(base) + 1 :]
        for info in ref_info_dict[baseline]:
            if git_remote:
                exp_rev = None
            else:
                exp_rev = repo.scm.get_ref(str(info))
            results[name].append((info.name, exp_rev))

    return results




dvc/repo/experiments/pull.py
import logging
from typing import Iterable, List, Mapping, Optional, Set, Union

from funcy import group_by
from scmrepo.git.backend.base import SyncStatus

from dvc.repo import locked
from dvc.repo.scm_context import scm_context
from dvc.scm import TqdmGit, iter_revs
from dvc.ui import ui

from .exceptions import UnresolvedExpNamesError
from .refs import ExpRefInfo
from .utils import exp_commits, exp_refs, exp_refs_by_baseline, resolve_name

logger = logging.getLogger(__name__)


@locked
@scm_context
def pull(  # noqa: C901
    repo,
    git_remote: str,
    exp_names: Union[Iterable[str], str],
    all_commits=False,
    rev: Optional[Union[List[str], str]] = None,
    num=1,
    force: bool = False,
    pull_cache: bool = False,
    **kwargs,
) -> Iterable[str]:
    exp_ref_set: Set["ExpRefInfo"] = set()
    if all_commits:
        exp_ref_set.update(exp_refs(repo.scm, git_remote))
    else:
        if exp_names:
            if isinstance(exp_names, str):
                exp_names = [exp_names]
            exp_ref_dict = resolve_name(repo.scm, exp_names, git_remote)

            unresolved_exp_names = []
            for exp_name, exp_ref in exp_ref_dict.items():
                if exp_ref is None:
                    unresolved_exp_names.append(exp_name)
                else:
                    exp_ref_set.add(exp_ref)

            if unresolved_exp_names:
                raise UnresolvedExpNamesError(unresolved_exp_names)

        if rev:
            if isinstance(rev, str):
                rev = [rev]
            rev_dict = iter_revs(repo.scm, rev, num)
            rev_set = set(rev_dict.keys())
            ref_info_dict = exp_refs_by_baseline(repo.scm, rev_set, git_remote)
            for _, ref_info_list in ref_info_dict.items():
                exp_ref_set.update(ref_info_list)

    pull_result = _pull(repo, git_remote, exp_ref_set, force)

    if pull_result[SyncStatus.DIVERGED]:
        diverged_refs = [ref.name for ref in pull_result[SyncStatus.DIVERGED]]
        ui.warn(
            f"Local experiment '{diverged_refs}' has diverged from remote "
            "experiment with the same name. To override the local experiment "
            "re-run with '--force'."
        )

    if pull_cache:
        pull_cache_ref = (
            pull_result[SyncStatus.UP_TO_DATE] + pull_result[SyncStatus.SUCCESS]
        )
        _pull_cache(repo, pull_cache_ref, **kwargs)

    return [ref.name for ref in pull_result[SyncStatus.SUCCESS]]


def _pull(
    repo,
    git_remote: str,
    refs: Iterable["ExpRefInfo"],
    force: bool,
) -> Mapping[SyncStatus, List["ExpRefInfo"]]:
    refspec_list = [f"{exp_ref}:{exp_ref}" for exp_ref in refs]
    logger.debug("git pull experiment '%s' -> '%s'", git_remote, refspec_list)

    with TqdmGit(desc="Fetching git refs") as pbar:
        results: Mapping[str, SyncStatus] = repo.scm.fetch_refspecs(
            git_remote,
            refspec_list,
            force=force,
            progress=pbar.update_git,
        )

    def group_result(refspec):
        return results[str(refspec)]

    pull_result: Mapping[SyncStatus, List["ExpRefInfo"]] = group_by(group_result, refs)

    return pull_result


def _pull_cache(
    repo,
    refs: Union[ExpRefInfo, Iterable["ExpRefInfo"]],
    dvc_remote=None,
    jobs=None,
    run_cache=False,
    odb=None,
):
    if isinstance(refs, ExpRefInfo):
        refs = [refs]
    revs = list(exp_commits(repo.scm, refs))
    logger.debug("dvc fetch experiment '%s'", refs)
    repo.fetch(jobs=jobs, remote=dvc_remote, run_cache=run_cache, revs=revs, odb=odb)




dvc/repo/experiments/push.py
import logging
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Iterable,
    List,
    Mapping,
    Optional,
    Set,
    Union,
)

from funcy import compact, group_by
from scmrepo.git.backend.base import SyncStatus

from dvc.env import DVC_STUDIO_TOKEN, DVC_STUDIO_URL
from dvc.exceptions import DvcException
from dvc.repo import locked
from dvc.repo.scm_context import scm_context
from dvc.scm import Git, TqdmGit, iter_revs
from dvc.utils import env2bool
from dvc.utils.collections import ensure_list

from .exceptions import UnresolvedExpNamesError
from .refs import ExpRefInfo
from .utils import exp_commits, exp_refs, exp_refs_by_baseline, resolve_name

if TYPE_CHECKING:
    from dvc.repo import Repo

logger = logging.getLogger(__name__)


class UploadError(DvcException):
    def __init__(self, msg, result):
        self.result = result
        super().__init__(msg)


def notify_refs_to_studio(
    repo: "Repo", git_remote: str, **refs: List[str]
) -> Optional[str]:
    import os

    config = repo.config["studio"]
    refs = compact(refs)
    if not refs or env2bool("DVC_TEST"):
        return None

    token = (
        os.environ.get(DVC_STUDIO_TOKEN)
        or os.environ.get("STUDIO_TOKEN")
        or config.get("token")
    )
    if not token:
        logger.debug("Studio token not found.")
        return None

    from dulwich.porcelain import get_remote_repo

    from dvc.utils import studio

    _, repo_url = get_remote_repo(repo.scm.dulwich.repo, git_remote)
    studio_url = os.environ.get(DVC_STUDIO_URL) or config.get("url")
    d = studio.notify_refs(repo_url, token, base_url=studio_url, **refs)
    return d.get("url")


def exp_refs_from_names(scm: "Git", exp_names: List[str]) -> Set["ExpRefInfo"]:
    exp_ref_set = set()
    exp_ref_dict = resolve_name(scm, exp_names)
    unresolved_exp_names = []
    for exp_name, exp_ref in exp_ref_dict.items():
        if exp_ref is None:
            unresolved_exp_names.append(exp_name)
        else:
            exp_ref_set.add(exp_ref)

    if unresolved_exp_names:
        raise UnresolvedExpNamesError(unresolved_exp_names)
    return exp_ref_set


def exp_refs_from_rev(scm: "Git", rev: List[str], num: int = 1) -> Set["ExpRefInfo"]:
    exp_ref_set = set()
    rev_dict = iter_revs(scm, rev, num)
    rev_set = set(rev_dict.keys())
    ref_info_dict = exp_refs_by_baseline(scm, rev_set)
    for _, ref_info_list in ref_info_dict.items():
        exp_ref_set.update(ref_info_list)
    return exp_ref_set


@locked
@scm_context
def push(
    repo: "Repo",
    git_remote: str,
    exp_names: Union[List[str], str],
    all_commits: bool = False,
    rev: Optional[Union[List[str], str]] = None,
    num: int = 1,
    force: bool = False,
    push_cache: bool = False,
    **kwargs: Any,
) -> Dict[str, Any]:
    exp_ref_set: Set["ExpRefInfo"] = set()
    assert isinstance(repo.scm, Git)
    if all_commits:
        exp_ref_set.update(exp_refs(repo.scm))
    if exp_names:
        exp_ref_set.update(exp_refs_from_names(repo.scm, ensure_list(exp_names)))
    if rev:
        if isinstance(rev, str):
            rev = [rev]
        exp_ref_set.update(exp_refs_from_rev(repo.scm, rev, num=num))

    push_result = _push(repo, git_remote, exp_ref_set, force)

    refs = {
        status.name.lower(): [ref.name for ref in ref_list]
        for status, ref_list in push_result.items()
    }
    result: Dict[str, Any] = {**refs, "uploaded": 0}

    pushed_refs_info = (
        push_result[SyncStatus.UP_TO_DATE] + push_result[SyncStatus.SUCCESS]
    )

    e = None
    if push_cache:
        try:
            result["uploaded"] = _push_cache(repo, pushed_refs_info, **kwargs)
        except Exception as exc:  # noqa: BLE001, pylint: disable=broad-except
            e = exc

    pushed_refs = [str(r) for r in pushed_refs_info]
    result["url"] = notify_refs_to_studio(repo, git_remote, pushed=pushed_refs)

    if e:
        raise UploadError("failed to push cache", result) from e
    return result


def _push(
    repo: "Repo",
    git_remote: str,
    refs: Iterable["ExpRefInfo"],
    force: bool,
) -> Mapping[SyncStatus, List["ExpRefInfo"]]:
    from scmrepo.exceptions import AuthError

    from dvc.scm import GitAuthError

    refspec_list = [f"{exp_ref}:{exp_ref}" for exp_ref in refs]
    logger.debug("git push experiment %s -> '%s'", refspec_list, git_remote)

    with TqdmGit(desc="Pushing git refs") as pbar:
        try:
            results: Mapping[str, SyncStatus] = repo.scm.push_refspecs(
                git_remote,
                refspec_list,
                force=force,
                progress=pbar.update_git,
            )
        except AuthError as exc:
            raise GitAuthError(str(exc))  # noqa: B904

    def group_result(refspec):
        return results[str(refspec)]

    pull_result: Mapping[SyncStatus, List["ExpRefInfo"]] = group_by(group_result, refs)

    return pull_result


def _push_cache(
    repo: "Repo",
    refs: Union[ExpRefInfo, Iterable["ExpRefInfo"]],
    dvc_remote: Optional[str] = None,
    jobs: Optional[int] = None,
    run_cache: bool = False,
) -> int:
    if isinstance(refs, ExpRefInfo):
        refs = [refs]
    assert isinstance(repo.scm, Git)
    revs = list(exp_commits(repo.scm, refs))
    logger.debug("dvc push experiment '%s'", refs)
    return repo.push(jobs=jobs, remote=dvc_remote, run_cache=run_cache, revs=revs)




dvc/repo/experiments/refs.py
from typing import Optional

from .exceptions import InvalidExpRefError

# Experiment refs are stored according baseline git SHA:
#   refs/exps/01/234abcd.../<exp_name>
EXPS_NAMESPACE = "refs/exps"
EXPS_STASH = f"{EXPS_NAMESPACE}/stash"
WORKSPACE_STASH = EXPS_STASH
APPLY_NAMESPACE = f"{EXPS_NAMESPACE}/apply"
APPLY_HEAD = f"{APPLY_NAMESPACE}/ORIG_HEAD"
APPLY_STASH = f"{APPLY_NAMESPACE}/stash"
CELERY_NAMESPACE = f"{EXPS_NAMESPACE}/celery"
CELERY_STASH = f"{CELERY_NAMESPACE}/stash"
CELERY_FAILED_STASH = f"{CELERY_NAMESPACE}/failed"
EXEC_NAMESPACE = f"{EXPS_NAMESPACE}/exec"
EXEC_APPLY = f"{EXEC_NAMESPACE}/EXEC_APPLY"
EXEC_BRANCH = f"{EXEC_NAMESPACE}/EXEC_BRANCH"
EXEC_BASELINE = f"{EXEC_NAMESPACE}/EXEC_BASELINE"
EXEC_HEAD = f"{EXEC_NAMESPACE}/EXEC_HEAD"
EXEC_MERGE = f"{EXEC_NAMESPACE}/EXEC_MERGE"
TEMP_NAMESPACE = f"{EXPS_NAMESPACE}/temp"
STASHES = {WORKSPACE_STASH, CELERY_STASH}
ITER_SKIP_NAMESPACES = {
    APPLY_NAMESPACE,
    CELERY_NAMESPACE,
    EXEC_NAMESPACE,
    TEMP_NAMESPACE,
}


class ExpRefInfo:
    namespace = EXPS_NAMESPACE

    def __init__(self, baseline_sha: str, name: Optional[str] = None):
        self.baseline_sha = baseline_sha
        self.name: str = name if name else ""

    def __str__(self):
        return "/".join(self.parts)

    def __repr__(self):
        baseline = f"'{self.baseline_sha}'"
        name = f"'{self.name}'" if self.name else "None"
        return f"ExpRefInfo(baseline_sha={baseline}, name={name})"

    @property
    def parts(self):
        return (
            (self.namespace,)
            + ((self.baseline_sha[:2], self.baseline_sha[2:]))
            + ((self.name,) if self.name else ())
        )

    @classmethod
    def from_ref(cls, ref: str):
        try:
            parts = ref.split("/")
            if (
                len(parts) < 4
                or len(parts) > 5
                or "/".join(parts[:2]) != EXPS_NAMESPACE
            ):
                raise InvalidExpRefError(ref)
        except ValueError:
            raise InvalidExpRefError(ref)  # noqa: B904
        baseline_sha = parts[2] + parts[3]
        name = parts[4] if len(parts) == 5 else None
        return cls(baseline_sha, name)




dvc/repo/experiments/remove.py
import logging
from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Union

from dvc.repo import locked
from dvc.repo.scm_context import scm_context
from dvc.scm import Git, iter_revs

from .exceptions import UnresolvedExpNamesError
from .utils import exp_refs, exp_refs_by_baseline, push_refspec

if TYPE_CHECKING:
    from dvc.repo import Repo
    from dvc.repo.experiments.queue.celery import LocalCeleryQueue

    from .queue.base import ExpRefAndQueueEntry, QueueEntry
    from .refs import ExpRefInfo


logger = logging.getLogger(__name__)


@locked
@scm_context
def remove(  # noqa: C901, PLR0912
    repo: "Repo",
    exp_names: Union[None, str, List[str]] = None,
    rev: Optional[Union[List[str], str]] = None,
    all_commits: bool = False,
    num: int = 1,
    queue: bool = False,
    git_remote: Optional[str] = None,
) -> List[str]:
    removed: List[str] = []
    if not any([exp_names, queue, all_commits, rev]):
        return removed
    celery_queue: "LocalCeleryQueue" = repo.experiments.celery_queue

    if queue:
        removed.extend(celery_queue.clear(queued=True))

    assert isinstance(repo.scm, Git)

    exp_ref_list: List["ExpRefInfo"] = []
    queue_entry_list: List["QueueEntry"] = []
    if exp_names:
        results: Dict[
            str, "ExpRefAndQueueEntry"
        ] = celery_queue.get_ref_and_entry_by_names(exp_names, git_remote)
        remained: List[str] = []
        for name, result in results.items():
            if not result.exp_ref_info and not result.queue_entry:
                remained.append(name)
                continue
            removed.append(name)
            if result.exp_ref_info:
                exp_ref_list.append(result.exp_ref_info)
            if result.queue_entry:
                queue_entry_list.append(result.queue_entry)

        if remained:
            raise UnresolvedExpNamesError(remained, git_remote=git_remote)
    elif rev:
        if isinstance(rev, str):
            rev = [rev]
        exp_ref_dict = _resolve_exp_by_baseline(repo, rev, num, git_remote)
        removed.extend(exp_ref_dict.keys())
        exp_ref_list.extend(exp_ref_dict.values())
    elif all_commits:
        exp_ref_list.extend(exp_refs(repo.scm, git_remote))
        removed = [ref.name for ref in exp_ref_list]

    if exp_ref_list:
        _remove_commited_exps(repo.scm, exp_ref_list, git_remote)

    if queue_entry_list:
        from .queue.remove import remove_tasks

        remove_tasks(celery_queue, queue_entry_list)

    if git_remote:
        from .push import notify_refs_to_studio

        removed_refs = [str(r) for r in exp_ref_list]
        notify_refs_to_studio(repo, git_remote, removed=removed_refs)
    return removed


def _resolve_exp_by_baseline(
    repo: "Repo",
    rev: List[str],
    num: int,
    git_remote: Optional[str] = None,
) -> Dict[str, "ExpRefInfo"]:
    assert isinstance(repo.scm, Git)

    commit_ref_dict: Dict[str, "ExpRefInfo"] = {}
    rev_dict = iter_revs(repo.scm, rev, num)
    rev_set = set(rev_dict.keys())
    ref_info_dict = exp_refs_by_baseline(repo.scm, rev_set, git_remote)
    for _, ref_info_list in ref_info_dict.items():
        for ref_info in ref_info_list:
            commit_ref_dict[ref_info.name] = ref_info
    return commit_ref_dict


def _remove_commited_exps(
    scm: "Git", exp_refs_list: Iterable["ExpRefInfo"], remote: Optional[str]
) -> List[str]:
    if remote:
        from dvc.scm import TqdmGit

        for ref_info in exp_refs_list:
            with TqdmGit(desc="Pushing git refs") as pbar:
                push_refspec(
                    scm,
                    remote,
                    [(None, str(ref_info))],
                    progress=pbar.update_git,
                )
    else:
        from .utils import remove_exp_refs

        remove_exp_refs(scm, exp_refs_list)
    return [exp_ref.name for exp_ref in exp_refs_list]




dvc/repo/experiments/run.py
import logging
from typing import Dict, Iterable, Optional

from dvc.dependency.param import ParamsDependency
from dvc.exceptions import InvalidArgumentError
from dvc.repo import locked
from dvc.ui import ui
from dvc.utils.cli_parse import to_path_overrides

logger = logging.getLogger(__name__)


@locked
def run(  # noqa: C901, PLR0912
    repo,
    targets: Optional[Iterable[str]] = None,
    params: Optional[Iterable[str]] = None,
    run_all: bool = False,
    jobs: int = 1,
    tmp_dir: bool = False,
    queue: bool = False,
    copy_paths: Optional[Iterable[str]] = None,
    message: Optional[str] = None,
    **kwargs,
) -> Dict[str, str]:
    """Reproduce the specified targets as an experiment.

    Accepts the same additional kwargs as Repo.reproduce.

    Returns a dict mapping new experiment SHAs to the results
    of `repro` for that experiment.
    """
    if run_all:
        return repo.experiments.reproduce_celery(jobs=jobs)

    hydra_sweep = None
    if params:
        from dvc.utils.hydra import to_hydra_overrides

        path_overrides = to_path_overrides(params)

        if tmp_dir or queue:
            untracked = repo.scm.untracked_files()
            for path in path_overrides:
                if path in untracked:
                    logger.debug(
                        "'%s' is currently untracked but will be modified by DVC. "
                        "Adding it to git.",
                        path,
                    )
                    repo.scm.add([path])

        hydra_sweep = any(
            x.is_sweep_override()
            for param_file in path_overrides
            for x in to_hydra_overrides(path_overrides[param_file])
        )

        if hydra_sweep and not queue:
            raise InvalidArgumentError(
                "Sweep overrides can't be used without `--queue`"
            )
    else:
        path_overrides = {}

    hydra_enabled = repo.config.get("hydra", {}).get("enabled", False)
    hydra_output_file = ParamsDependency.DEFAULT_PARAMS_FILE
    if hydra_enabled and hydra_output_file not in path_overrides:
        # Force `_update_params` even if `--set-param` was not used
        path_overrides[hydra_output_file] = []

    if not queue:
        return repo.experiments.reproduce_one(
            targets=targets,
            params=path_overrides,
            tmp_dir=tmp_dir,
            copy_paths=copy_paths,
            message=message,
            **kwargs,
        )

    if hydra_sweep:
        from dvc.utils.hydra import get_hydra_sweeps

        sweeps = get_hydra_sweeps(path_overrides)
        name_prefix = kwargs.get("name")
    else:
        sweeps = [path_overrides]

    for idx, sweep_overrides in enumerate(sweeps):
        if hydra_sweep and name_prefix is not None:
            kwargs["name"] = f"{name_prefix}-{idx+1}"
        queue_entry = repo.experiments.queue_one(
            repo.experiments.celery_queue,
            targets=targets,
            params=sweep_overrides,
            copy_paths=copy_paths,
            message=message,
            **kwargs,
        )
        if sweep_overrides:
            ui.write(f"Queueing with overrides '{sweep_overrides}'.")
        name = queue_entry.name or queue_entry.stash_rev[:7]
        ui.write(f"Queued experiment '{name}' for future execution.")

    return {}




dvc/repo/experiments/save.py
import logging
import os
from typing import TYPE_CHECKING, List, Optional

from funcy import first

if TYPE_CHECKING:
    from dvc.repo import Repo


logger = logging.getLogger(__name__)


def save(
    repo: "Repo",
    name: Optional[str] = None,
    force: bool = False,
    include_untracked: Optional[List[str]] = None,
    message: Optional[str] = None,
) -> Optional[str]:
    """Save the current workspace status as an experiment.

    Returns the saved experiment's SHAs.
    """
    logger.debug("Saving workspace in %s", os.getcwd())

    queue = repo.experiments.workspace_queue
    entry = repo.experiments.new(queue=queue, name=name, force=force)
    executor = queue.init_executor(repo.experiments, entry)

    try:
        save_result = executor.save(
            executor.info,
            force=force,
            include_untracked=include_untracked,
            message=message,
        )
        result = queue.collect_executor(repo.experiments, executor, save_result)
    finally:
        executor.cleanup()

    return first(result)




dvc/repo/experiments/serialize.py
import json
from dataclasses import asdict, dataclass, field
from datetime import datetime
from typing import TYPE_CHECKING, Any, Callable, Dict, Iterator, List, Literal, Optional

from dvc.exceptions import DvcException
from dvc.repo.metrics.show import _gather_metrics
from dvc.repo.params.show import _gather_params
from dvc.utils import onerror_collect, relpath

if TYPE_CHECKING:
    from dvc.repo import Repo


class DeserializeError(DvcException):
    pass


class _ISOEncoder(json.JSONEncoder):
    def default(self, o: object) -> Any:
        if isinstance(o, datetime):
            return o.isoformat()
        return super().default(o)


@dataclass(frozen=True)
class SerializableExp:
    """Serializable experiment data."""

    rev: str
    timestamp: Optional[datetime] = None
    params: Dict[str, Any] = field(default_factory=dict)
    metrics: Dict[str, Any] = field(default_factory=dict)
    deps: Dict[str, "ExpDep"] = field(default_factory=dict)
    outs: Dict[str, "ExpOut"] = field(default_factory=dict)
    meta: Dict[str, Any] = field(default_factory=dict)

    @classmethod
    def from_repo(
        cls,
        repo: "Repo",
        rev: Optional[str] = None,
        onerror: Optional[Callable] = None,
        param_deps: bool = False,
        **kwargs,
    ) -> "SerializableExp":
        """Returns a SerializableExp from the current repo state.

        Params, metrics, deps, outs are filled via repo fs/index, all other fields
        should be passed via kwargs.
        """
        from dvc.dependency import ParamsDependency, RepoDependency

        if not onerror:
            onerror = onerror_collect

        rev = rev or repo.get_rev()
        assert rev
        # NOTE: _gather_params/_gather_metrics return defaultdict which is not
        # supported in dataclasses.asdict() on all python releases
        # see https://bugs.python.org/issue35540
        params = dict(_gather_params(repo, deps=param_deps, onerror=onerror))
        metrics = dict(
            _gather_metrics(
                repo,
                targets=None,
                rev=rev[:7],
                recursive=False,
                onerror=onerror_collect,
            )
        )
        return cls(
            rev=rev,
            params=params,
            metrics=metrics,
            deps={
                relpath(dep.fs_path, repo.root_dir): ExpDep(
                    hash=dep.hash_info.value if dep.hash_info else None,
                    size=dep.meta.size if dep.meta else None,
                    nfiles=dep.meta.nfiles if dep.meta else None,
                )
                for dep in repo.index.deps
                if not isinstance(dep, (ParamsDependency, RepoDependency))
            },
            outs={
                relpath(out.fs_path, repo.root_dir): ExpOut(
                    hash=out.hash_info.value if out.hash_info else None,
                    size=out.meta.size if out.meta else None,
                    nfiles=out.meta.nfiles if out.meta else None,
                    use_cache=out.use_cache,
                    is_data_source=out.stage.is_data_source,
                )
                for out in repo.index.outs
                if not (out.is_metric or out.is_plot)
            },
            **kwargs,
        )

    def dumpd(self) -> Dict[str, Any]:
        return asdict(self)

    def as_bytes(self) -> bytes:
        return _ISOEncoder().encode(self.dumpd()).encode("utf-8")

    @classmethod
    def from_bytes(cls, data: bytes):
        try:
            parsed = json.loads(data)
            if "timestamp" in parsed:
                parsed["timestamp"] = datetime.fromisoformat(parsed["timestamp"])
            if "deps" in parsed:
                parsed["deps"] = {k: ExpDep(**v) for k, v in parsed["deps"].items()}
            if "outs" in parsed:
                parsed["outs"] = {k: ExpOut(**v) for k, v in parsed["outs"].items()}
            return cls(**parsed)
        except (TypeError, json.JSONDecodeError) as exc:
            raise DeserializeError("failed to load SerializableExp") from exc

    @property
    def contains_error(self) -> bool:
        return (
            self.params.get("error")
            or any(value.get("error") for value in self.params.values())
            or self.metrics.get("error")
            or any(value.get("error") for value in self.metrics.values())
        )


@dataclass(frozen=True)
class ExpDep:
    hash: Optional[str]  # noqa: A003
    size: Optional[int]
    nfiles: Optional[int]


@dataclass(frozen=True)
class ExpOut:
    hash: Optional[str]  # noqa: A003
    size: Optional[int]
    nfiles: Optional[int]
    use_cache: bool
    is_data_source: bool


@dataclass(frozen=True)
class SerializableError:
    msg: str
    type: str = ""  # noqa: A003

    def dumpd(self) -> Dict[str, Any]:
        return asdict(self)

    def as_bytes(self) -> bytes:
        return json.dumps(self.dumpd()).encode("utf-8")

    @classmethod
    def from_bytes(cls, data: bytes):
        try:
            parsed = json.loads(data)
            return cls(**parsed)
        except (TypeError, json.JSONDecodeError) as exc:
            raise DeserializeError("failed to load SerializableError") from exc


@dataclass
class ExpState:
    """Git/DVC experiment state."""

    rev: str
    name: Optional[str] = None
    data: Optional[SerializableExp] = None
    error: Optional[SerializableError] = None
    experiments: Optional[List["ExpRange"]] = None

    def dumpd(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class ExpRange:
    revs: List["ExpState"]
    executor: Optional["ExpExecutor"] = None
    name: Optional[str] = None

    def __len__(self) -> int:
        return len(self.revs)

    def __iter__(self) -> Iterator["ExpState"]:
        return iter(self.revs)

    def __getitem__(self, index: int) -> "ExpState":
        return self.revs[index]

    def dumpd(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class LocalExpExecutor:
    root: Optional[str] = None
    log: Optional[str] = None
    pid: Optional[int] = None
    returncode: Optional[int] = None
    task_id: Optional[str] = None


@dataclass
class ExpExecutor:
    state: Literal["success", "queued", "running", "failed"]
    name: Optional[str] = None
    local: Optional[LocalExpExecutor] = None



dvc/repo/experiments/show.py
import logging
from collections import Counter, defaultdict
from datetime import date, datetime
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Iterable,
    Iterator,
    List,
    Literal,
    Mapping,
    NamedTuple,
    Optional,
    Set,
    Tuple,
    Union,
)

from dvc.exceptions import InvalidArgumentError
from dvc.scm import Git
from dvc.ui import ui
from dvc.utils.flatten import flatten

from .collect import collect

if TYPE_CHECKING:
    from dvc.compare import TabularData
    from dvc.repo import Repo
    from dvc.ui.table import CellT

    from .serialize import ExpRange, ExpState

logger = logging.getLogger(__name__)


def show(
    repo: "Repo",
    revs: Union[List[str], str, None] = None,
    all_branches: bool = False,
    all_tags: bool = False,
    all_commits: bool = False,
    num: int = 1,
    hide_queued: bool = False,
    hide_failed: bool = False,
    sha_only: bool = False,
    **kwargs,
) -> List["ExpState"]:
    return collect(
        repo,
        revs=revs,
        all_branches=all_branches,
        all_tags=all_tags,
        all_commits=all_commits,
        num=num,
        hide_queued=hide_queued,
        hide_failed=hide_failed,
        sha_only=sha_only,
        **kwargs,
    )


def tabulate(
    baseline_states: Iterable["ExpState"],
    fill_value: Optional[str] = "-",
    error_value: str = "!",
    **kwargs,
) -> Tuple["TabularData", Dict[str, Iterable[str]]]:
    """Return table data for experiments.

    Returns:
        Tuple of (table_data, data_headers)
    """
    from funcy import lconcat
    from funcy.seqs import flatten as flatten_list

    from dvc.compare import TabularData

    data_names = _collect_names(baseline_states)
    metrics_names = data_names.metrics
    params_names = data_names.params
    deps_names = data_names.sorted_deps

    headers = [
        "Experiment",
        "rev",
        "typ",
        "Created",
        "parent",
        "State",
        "Executor",
    ]
    names = {**metrics_names, **params_names}
    counter = Counter(flatten_list([list(a.keys()) for a in names.values()]))
    counter.update(headers)
    metrics_headers = _normalize_headers(metrics_names, counter)
    params_headers = _normalize_headers(params_names, counter)

    all_headers = lconcat(headers, metrics_headers, params_headers, deps_names)
    td = TabularData(all_headers, fill_value=fill_value)
    td.extend(
        _build_rows(
            baseline_states,
            all_headers=all_headers,
            metrics_headers=metrics_headers,
            params_headers=params_headers,
            metrics_names=metrics_names,
            params_names=params_names,
            deps_names=deps_names,
            fill_value=fill_value,
            error_value=error_value,
            **kwargs,
        )
    )
    data_headers: Dict[str, Iterable[str]] = {
        "metrics": metrics_headers,
        "params": params_headers,
        "deps": deps_names,
    }
    return td, data_headers


def _build_rows(
    baseline_states: Iterable["ExpState"],
    *,
    all_headers: Iterable[str],
    fill_value: Optional[str],
    sort_by: Optional[str] = None,
    sort_order: Optional[Literal["asc", "desc"]] = None,
    **kwargs,
) -> Iterator[Tuple["CellT", ...]]:
    for baseline in baseline_states:
        row: Dict[str, "CellT"] = {k: fill_value for k in all_headers}
        row["Experiment"] = ""
        if baseline.name:
            row["rev"] = baseline.name
        elif Git.is_sha(baseline.rev):
            row["rev"] = baseline.rev[:7]
        else:
            row["rev"] = baseline.rev
        row["typ"] = "baseline"
        row["parent"] = ""
        if baseline.data:
            row["Created"] = format_time(
                baseline.data.timestamp, fill_value=fill_value, **kwargs
            )
            row.update(
                _data_cells(  # pylint: disable=missing-kwoa
                    baseline, fill_value=fill_value, **kwargs
                )
            )
        yield tuple(row.values())
        if baseline.experiments:
            if sort_by:
                metrics_names: Mapping[str, Iterable[str]] = kwargs.get(
                    "metrics_names", {}
                )
                params_names: Mapping[str, Iterable[str]] = kwargs.get(
                    "params_names", {}
                )
                sort_path, sort_name, sort_type = _sort_column(
                    sort_by, metrics_names, params_names
                )
                reverse = sort_order == "desc"
                experiments = _sort_exp(
                    baseline.experiments, sort_path, sort_name, sort_type, reverse
                )
            else:
                experiments = baseline.experiments
            for i, child in enumerate(experiments):
                yield from _exp_range_rows(
                    child,
                    all_headers=all_headers,
                    fill_value=fill_value,
                    is_base=i == len(baseline.experiments) - 1,
                    **kwargs,
                )


def _sort_column(
    sort_by: str,
    metric_names: Mapping[str, Iterable[str]],
    param_names: Mapping[str, Iterable[str]],
) -> Tuple[str, str, str]:
    path, _, sort_name = sort_by.rpartition(":")
    matches: Set[Tuple[str, str, str]] = set()

    if path:
        if path in metric_names and sort_name in metric_names[path]:
            matches.add((path, sort_name, "metrics"))
        if path in param_names and sort_name in param_names[path]:
            matches.add((path, sort_name, "params"))
    else:
        for path in metric_names:
            if sort_name in metric_names[path]:
                matches.add((path, sort_name, "metrics"))
        for path in param_names:
            if sort_name in param_names[path]:
                matches.add((path, sort_name, "params"))

    if len(matches) == 1:
        return matches.pop()
    if len(matches) > 1:
        raise InvalidArgumentError(
            "Ambiguous sort column '{}' matched '{}'".format(
                sort_by,
                ", ".join([f"{path}:{name}" for path, name, _ in matches]),
            )
        )
    raise InvalidArgumentError(f"Unknown sort column '{sort_by}'")


def _sort_exp(
    experiments: Iterable["ExpRange"],
    sort_path: str,
    sort_name: str,
    typ: str,
    reverse: bool,
) -> List["ExpRange"]:
    from funcy import first

    def _sort(exp_range: "ExpRange"):
        exp = first(exp_range.revs)
        if not exp:
            return True
        data = exp.data.dumpd().get(typ, {}).get(sort_path, {}).get("data", {})
        val = flatten(data).get(sort_name)
        return val is None, val

    return sorted(experiments, key=_sort, reverse=reverse)


def _exp_range_rows(
    exp_range: "ExpRange",
    *,
    all_headers: Iterable[str],
    fill_value: Optional[str],
    is_base: bool = False,
    **kwargs,
) -> Iterator[Tuple["CellT", ...]]:
    for i, exp in enumerate(exp_range.revs):
        row: Dict[str, "CellT"] = {k: fill_value for k in all_headers}
        row["Experiment"] = exp.name or ""
        row["rev"] = exp.rev[:7] if Git.is_sha(exp.rev) else exp.rev
        if len(exp_range.revs) > 1:
            if i == 0:
                row["typ"] = "checkpoint_tip"
            elif i == len(exp_range.revs) - 1:
                row["typ"] = "checkpoint_base"
            else:
                row["typ"] = "checkpoint_commit"
        else:
            row["typ"] = "branch_base" if is_base else "branch_commit"
        row["parent"] = ""
        if exp_range.executor:
            row["State"] = exp_range.executor.state.capitalize()
            if exp_range.executor.name:
                row["Executor"] = exp_range.executor.name.capitalize()
        if exp.data:
            row["Created"] = format_time(
                exp.data.timestamp, fill_value=fill_value, **kwargs
            )
            row.update(
                _data_cells(  # pylint: disable=missing-kwoa
                    exp, fill_value=fill_value, **kwargs
                )
            )
        yield tuple(row.values())


def _data_cells(
    exp: "ExpState",
    *,
    metrics_headers: Iterable[str],
    params_headers: Iterable[str],
    metrics_names: Mapping[str, Iterable[str]],
    params_names: Mapping[str, Iterable[str]],
    deps_names: Iterable[str],
    fill_value: Optional[str] = "-",
    error_value: str = "!",
    precision: Optional[int] = None,
    **kwargs,
) -> Iterator[Tuple[str, "CellT"]]:
    def _d_cells(
        d: Mapping[str, Any],
        names: Mapping[str, Iterable[str]],
        headers: Iterable[str],
    ) -> Iterator[Tuple[str, "CellT"]]:
        from dvc.compare import _format_field, with_value

        for fname, data in d.items():
            item = data.get("data", {})
            item = flatten(item) if isinstance(item, dict) else {fname: item}
            for name in names[fname]:
                value = with_value(
                    item.get(name),
                    error_value if data.get("error") else fill_value,
                )
                # wrap field data in ui.rich_text, otherwise rich may
                # interpret unescaped braces from list/dict types as rich
                # markup tags
                value = ui.rich_text(str(_format_field(value, precision)))
                if name in headers:
                    yield name, value
                else:
                    yield f"{fname}:{name}", value

    if not exp.data:
        return
    yield from _d_cells(exp.data.metrics, metrics_names, metrics_headers)
    yield from _d_cells(exp.data.params, params_names, params_headers)
    for name in deps_names:
        dep = exp.data.deps.get(name)
        if dep:
            yield name, dep.hash or fill_value


def format_time(
    timestamp: Optional[datetime],
    fill_value: Optional[str] = "-",
    iso: bool = False,
    **kwargs,
) -> Optional[str]:
    if not timestamp:
        return fill_value
    if iso:
        return timestamp.isoformat()
    if timestamp.date() == date.today():
        fmt = "%I:%M %p"
    else:
        fmt = "%b %d, %Y"
    return timestamp.strftime(fmt)


class _DataNames(NamedTuple):
    # NOTE: we use nested dict instead of set for metrics/params names to
    # preserve key ordering
    metrics: Dict[str, Dict[str, Any]]
    params: Dict[str, Dict[str, Any]]
    deps: Set[str]

    @property
    def sorted_deps(self):
        return sorted(self.deps)

    def update(self, other: "_DataNames"):
        def _update_d(
            d: Dict[str, Dict[str, Any]], other_d: Mapping[str, Mapping[str, Any]]
        ):
            for k, v in other_d.items():
                if k in d:
                    d[k].update(v)
                else:
                    d[k] = dict(v)

        _update_d(self.metrics, other.metrics)
        _update_d(self.params, other.params)
        self.deps.update(other.deps)


def _collect_names(exp_states: Iterable["ExpState"]) -> _DataNames:
    result = _DataNames(defaultdict(dict), defaultdict(dict), set())

    def _collect_d(result_d: Dict[str, Dict[str, Any]], data_d: Dict[str, Any]):
        for path, item in data_d.items():
            item = item.get("data", {})
            if isinstance(item, dict):
                item = flatten(item)
                result_d[path].update((key, None) for key in item)

    for exp in exp_states:
        if exp.data:
            _collect_d(result.metrics, exp.data.metrics)
            _collect_d(result.params, exp.data.params)
            result.deps.update(exp.data.deps)
        if exp.experiments:
            for child in exp.experiments:
                result.update(_collect_names(child.revs))

    return result


def _normalize_headers(
    names: Mapping[str, Mapping[str, Any]], count: Mapping[str, int]
) -> List[str]:
    return [
        name if count[name] == 1 else f"{path}:{name}"
        for path in names
        for name in names[path]
    ]




dvc/repo/experiments/stash.py
import logging
import re
from contextlib import contextmanager
from typing import Dict, Iterable, Iterator, NamedTuple, Optional

from scmrepo.git import Stash

from dvc.exceptions import DvcException
from dvc_objects.fs.local import localfs
from dvc_objects.fs.utils import as_atomic

from .refs import APPLY_HEAD, APPLY_STASH

logger = logging.getLogger(__name__)


class ExpStashEntry(NamedTuple):
    """Experiment stash entry.

    stash_index: Stash index for this entry. Can be None if this commit
        is not pushed onto the stash ref.
    head_rev: HEAD Git commit to be checked out for this experiment.
    baseline_rev: Experiment baseline commit.
    branch: Optional branch name for this experiment.
    name: Optional exp name.
    """

    stash_index: Optional[int]
    head_rev: str
    baseline_rev: str
    branch: Optional[str]
    name: Optional[str]


class ExpStash(Stash):
    MESSAGE_FORMAT = "dvc-exp:{rev}:{baseline_rev}:{name}"
    MESSAGE_RE = re.compile(
        r"(?:commit: )"
        r"dvc-exp:(?P<rev>[0-9a-f]+):(?P<baseline_rev>[0-9a-f]+)"
        r":(?P<name>[^~^:\\?\[\]*]*)"
        r"(:(?P<branch>.+))?$"
    )

    @property
    def stash_revs(self) -> Dict[str, ExpStashEntry]:
        revs = {}
        for i, entry in enumerate(self):
            msg = entry.message.decode("utf-8").strip()
            m = self.MESSAGE_RE.match(msg)
            if m:
                revs[entry.new_sha.decode("utf-8")] = ExpStashEntry(
                    i,
                    m.group("rev"),
                    m.group("baseline_rev"),
                    m.group("branch"),
                    m.group("name"),
                )
        return revs

    @classmethod
    def format_message(
        cls,
        rev: str,
        baseline_rev: str,
        name: Optional[str] = None,
        branch: Optional[str] = None,
    ) -> str:
        msg = cls.MESSAGE_FORMAT.format(
            rev=rev, baseline_rev=baseline_rev, name=name if name else ""
        )
        branch_msg = f":{branch}" if branch else ""
        return f"{msg}{branch_msg}"

    def remove_revs(self, stash_revs: Iterable[ExpStashEntry]):
        """Remove the specified entries from the queue by stash revision."""
        for index in sorted(
            (
                entry.stash_index
                for entry in stash_revs
                if entry.stash_index is not None
            ),
            reverse=True,
        ):
            self.drop(index)


class ApplyStashEntry(NamedTuple):
    """Apply stash entry.

    stash_index: Stash index for this entry. Can be None if this commit
        is not pushed onto the stash ref.
    head_rev: HEAD Git commit prior to exp apply.
    rev: Applied experiment commit.
    name: Optional applied exp name.
    """

    stash_index: Optional[int]
    head_rev: str
    rev: str
    name: Optional[str]


class ApplyStash(Stash):
    DEFAULT_STASH = APPLY_STASH
    MESSAGE_FORMAT = "dvc-exp-apply:{head_rev}:{rev}:{name}"
    MESSAGE_RE = re.compile(
        r"(?:commit: )"
        r"dvc-exp-apply:(?P<head_rev>[0-9a-f]+):(?P<rev>[0-9a-f]+)"
        r":(?P<name>[^~^:\\?\[\]*]*)"
    )

    @property
    def stash_revs(self) -> Dict[str, ApplyStashEntry]:
        revs = {}
        for i, entry in enumerate(self):
            msg = entry.message.decode("utf-8").strip()
            m = self.MESSAGE_RE.match(msg)
            if m:
                revs[entry.new_sha.decode("utf-8")] = ApplyStashEntry(
                    i,
                    m.group("head_rev"),
                    m.group("rev"),
                    m.group("name"),
                )
        return revs

    @classmethod
    def format_message(
        cls,
        head_rev: str,
        rev: str,
        name: Optional[str] = None,
    ) -> str:
        return cls.MESSAGE_FORMAT.format(
            head_rev=head_rev, rev=rev, name=name if name else ""
        )

    @contextmanager
    def preserve_workspace(
        self, rev: str, name: Optional[str] = None
    ) -> Iterator[Optional[str]]:
        if len(self):
            logger.debug("Clearing existing exp-apply stash")
            self.clear()
        head = self.scm.get_rev()
        self.scm.set_ref(APPLY_HEAD, head)
        message = self.format_message(head, rev, name=name)
        stash_rev = self.push(message=message, include_untracked=True)
        try:
            yield stash_rev
            if stash_rev:
                self._apply_difference(stash_rev, rev)
        except Exception:  # pylint: disable=broad-except
            self.revert_workspace()
            raise

    def _apply_difference(self, stash_rev: str, rev: str):
        """Selectively apply changes from stash_rev.

        Only changes to files from left which do not exist in right will be applied.
        """
        self._copy_difference(stash_rev, rev)
        commit = self.scm.resolve_commit(stash_rev)
        for parent_rev in commit.parents:
            parent_commit = self.scm.resolve_commit(parent_rev)
            if parent_commit.message.startswith("untracked files on "):
                self._copy_difference(parent_rev, rev)

    def _copy_difference(self, left_rev: str, right_rev: str):
        left_fs = self.scm.get_fs(left_rev)
        right_fs = self.scm.get_fs(right_rev)
        paths = [path for path in left_fs.find("/") if not right_fs.exists(path)]
        dest_paths = [
            localfs.path.join(self.scm.root_dir, left_fs.path.relpath(path, "/"))
            for path in paths
        ]
        for src, dest in zip(paths, dest_paths):
            with as_atomic(localfs, dest, create_parents=True) as tmp_file:
                left_fs.get_file(src, tmp_file)

    def revert_workspace(self):
        apply_head = self.scm.get_ref(self.ref)
        head = self.scm.get_rev()
        if apply_head != head:
            raise DvcException(
                f"Cannot revert workspace, current HEAD '{head[:7]}' does not match the"
                f" pre-apply HEAD '{apply_head[:7]}'"
            )
        self.scm.reset(hard=True)
        if len(self):
            # In the event that the apply-stash and current workspace contain
            # conflicting untracked files, we do:
            #   1. stash the current untracked files
            #   2. restore/pop the apply-stash (with untracked files)
            #   3. restore/pop the untracked files from (1) and ignore any conflicts
            #      (forcefully reverting to the apply-stash version)
            workspace_rev = self.scm.stash.push(include_untracked=True)
            try:
                self.pop()
            finally:
                if workspace_rev:
                    self.scm.stash.pop(skip_conflicts=True)
        self.scm.remove_ref(self.ref)




dvc/repo/experiments/utils.py
import os
import random
import sys
from collections import defaultdict
from functools import wraps
from typing import (
    TYPE_CHECKING,
    Callable,
    Dict,
    Generator,
    Iterable,
    List,
    Mapping,
    Optional,
    Set,
    Tuple,
    Union,
)

from dvc.exceptions import InvalidArgumentError
from dvc.repo.experiments.exceptions import AmbiguousExpRefInfo
from dvc.rwlock import rwlock
from dvc.scm import Git

from .refs import (
    EXEC_APPLY,
    EXEC_BASELINE,
    EXEC_BRANCH,
    EXPS_NAMESPACE,
    ITER_SKIP_NAMESPACES,
    STASHES,
    ExpRefInfo,
)

if TYPE_CHECKING:
    from dvc.repo import Repo
    from dvc.scm import NoSCM


EXEC_TMP_DIR = "exps"
EXEC_PID_DIR = "run"


def get_exp_rwlock(
    repo: "Repo",
    reads: Optional[List[str]] = None,
    writes: Optional[List[str]] = None,
):
    reads = reads or []
    writes = writes or []

    cmd = " ".join(sys.argv)
    assert repo.tmp_dir is not None
    path = os.path.join(repo.tmp_dir, EXEC_TMP_DIR)
    repo.fs.makedirs(path, exist_ok=True)

    return rwlock(
        path,
        repo.fs,
        cmd,
        reads,
        writes,
        repo.config["core"].get("hardlink_lock", False),
    )


def unlocked_repo(f):
    @wraps(f)
    def wrapper(exp, *args, **kwargs):
        exp.repo.lock.unlock()
        exp.repo._reset()  # pylint: disable=protected-access
        try:
            ret = f(exp, *args, **kwargs)
        finally:
            exp.repo.lock.lock()
        return ret

    return wrapper


def _ignore_ref(ref: str) -> bool:
    return (
        any(ref.startswith(namespace) for namespace in ITER_SKIP_NAMESPACES)
        or ref in STASHES
    )


def exp_refs(
    scm: "Git", url: Optional[str] = None
) -> Generator["ExpRefInfo", None, None]:
    """Iterate over all experiment refs."""
    ref_gen = (
        iter_remote_refs(scm, url, base=EXPS_NAMESPACE)
        if url
        else scm.iter_refs(base=EXPS_NAMESPACE)
    )
    for ref in ref_gen:
        if _ignore_ref(ref):
            continue
        yield ExpRefInfo.from_ref(ref)


def exp_refs_by_rev(scm: "Git", rev: str) -> Generator[ExpRefInfo, None, None]:
    """Iterate over all experiment refs pointing to the specified revision."""
    for ref in scm.get_refs_containing(rev, EXPS_NAMESPACE):
        if not _ignore_ref(ref):
            yield ExpRefInfo.from_ref(ref)


def exp_refs_by_baseline(
    scm: "Git",
    revs: Optional[Set[str]] = None,
    url: Optional[str] = None,
) -> Mapping[str, List[ExpRefInfo]]:
    """Iterate over all experiment refs with the specified baseline."""
    all_exp_refs = exp_refs(scm, url)
    result = defaultdict(list)
    for ref in all_exp_refs:
        if revs is None or ref.baseline_sha in revs:
            result[ref.baseline_sha].append(ref)
    return result


def iter_remote_refs(scm: "Git", url: str, base: Optional[str] = None, **kwargs):
    from scmrepo.exceptions import AuthError, InvalidRemote

    from dvc.scm import GitAuthError, InvalidRemoteSCMRepo

    try:
        yield from scm.iter_remote_refs(url, base=base, **kwargs)
    except InvalidRemote as exc:
        raise InvalidRemoteSCMRepo(str(exc))  # noqa: B904
    except AuthError as exc:
        raise GitAuthError(str(exc))  # noqa: B904


def push_refspec(
    scm: "Git",
    url: str,
    push_list=List[Tuple[Optional[str], str]],
    force: bool = False,
    on_diverged: Optional[Callable[[str, str], bool]] = None,
    **kwargs,
):
    from scmrepo.exceptions import AuthError
    from scmrepo.git.backend.base import SyncStatus

    from dvc.scm import GitAuthError, SCMError

    refspecs = []
    for src, dest in push_list:
        if not src:
            refspecs.append(f":{dest}")
        elif src.endswith("/"):
            dest = dest.rstrip("/") + "/"
            for ref in scm.iter_refs(base=src):
                refname = ref.split("/")[-1]
                refspecs.append(f"{ref}:{dest}{refname}")
        elif dest.endswith("/"):
            refname = src.split("/")[-1]
            refspecs.append(f"{src}:{dest}/{refname}")
        else:
            refspecs.append(f"{src}:{dest}")

    try:
        results = scm.push_refspecs(
            url, refspecs, force=force, on_diverged=on_diverged, **kwargs
        )
        diverged = [ref for ref in results if results[ref] == SyncStatus.DIVERGED]

        if diverged:
            raise SCMError(f"local ref '{diverged}' diverged from remote '{url}'")
    except AuthError as exc:
        raise GitAuthError(str(exc))  # noqa: B904


def remote_exp_refs(scm: "Git", url: str) -> Generator[ExpRefInfo, None, None]:
    """Iterate over all remote experiment refs."""
    for ref in iter_remote_refs(scm, url, base=EXPS_NAMESPACE):
        if _ignore_ref(ref):
            continue
        yield ExpRefInfo.from_ref(ref)


def exp_refs_by_names(
    scm: "Git", names: Set[str], url: Optional[str] = None
) -> Dict[str, List[ExpRefInfo]]:
    """Iterate over all experiment refs matching the specified names."""
    resolve_results = defaultdict(list)
    ref_info_gen = exp_refs(scm, url)
    for ref_info in ref_info_gen:
        if ref_info.name in names:
            resolve_results[ref_info.name].append(ref_info)

    return resolve_results


def remote_exp_refs_by_baseline(
    scm: "Git", url: str, rev: str
) -> Generator[ExpRefInfo, None, None]:
    """Iterate over all remote experiment refs with the specified baseline."""
    ref_info = ExpRefInfo(baseline_sha=rev)
    for ref in iter_remote_refs(scm, url, base=str(ref_info)):
        if _ignore_ref(ref):
            continue
        yield ExpRefInfo.from_ref(ref)


def exp_commits(
    scm: "Git", ref_infos: Optional[Iterable[ExpRefInfo]] = None
) -> Iterable[str]:
    """Iterate over all experiment commits."""
    shas: Set["str"] = set()
    refs = ref_infos if ref_infos else exp_refs(scm)
    for ref_info in refs:
        shas.update(scm.branch_revs(str(ref_info), ref_info.baseline_sha))
    yield from shas


def remove_exp_refs(scm: "Git", ref_infos: Iterable[ExpRefInfo]):
    exec_branch = scm.get_ref(EXEC_BRANCH, follow=False)
    exec_apply = scm.get_ref(EXEC_APPLY)

    for ref_info in ref_infos:
        ref = scm.get_ref(str(ref_info))
        if exec_branch and str(ref_info):
            scm.remove_ref(EXEC_BRANCH)
        if exec_apply and exec_apply == ref:
            scm.remove_ref(EXEC_APPLY)
        scm.remove_ref(str(ref_info))


def fix_exp_head(scm: Union["Git", "NoSCM"], ref: Optional[str]) -> Optional[str]:
    if ref:
        name, tail = Git.split_ref_pattern(ref)
        if name == "HEAD" and scm.get_ref(EXEC_BASELINE):
            return "".join((EXEC_BASELINE, tail))
    return ref


def resolve_name(
    scm: "Git",
    exp_names: Union[Iterable[str], str],
    git_remote: Optional[str] = None,
) -> Dict[str, Optional[ExpRefInfo]]:
    """find the ref_info of specified names."""
    if isinstance(exp_names, str):
        exp_names = [exp_names]

    result = {}
    unresolved = set()
    for exp_name in exp_names:
        if exp_name.startswith("refs/"):
            result[exp_name] = ExpRefInfo.from_ref(exp_name)
        else:
            unresolved.add(exp_name)

    unresolved_result = exp_refs_by_names(scm, unresolved, git_remote)
    cur_rev = scm.get_rev()
    for name in unresolved:
        ref_info_list = unresolved_result[name]
        if not ref_info_list:
            result[name] = None
        elif len(ref_info_list) == 1:
            result[name] = ref_info_list[0]
        else:
            for ref_info in ref_info_list:
                if ref_info.baseline_sha == cur_rev:
                    result[name] = ref_info
                    break
            else:
                raise AmbiguousExpRefInfo(name, ref_info_list)
    return result


def check_ref_format(scm: "Git", ref: ExpRefInfo):
    # "/" forbidden, only in dvc exp as we didn't support it for now.
    if not scm.check_ref_format(str(ref)) or "/" in ref.name:
        raise InvalidArgumentError(
            f"Invalid exp name {ref.name}, the exp name must follow rules in "
            "https://git-scm.com/docs/git-check-ref-format"
        )


def fetch_all_exps(scm: "Git", url: str, progress: Optional[Callable] = None, **kwargs):
    refspecs = [
        f"{ref}:{ref}"
        for ref in iter_remote_refs(scm, url, base=EXPS_NAMESPACE)
        if not _ignore_ref(ref)
    ]
    scm.fetch_refspecs(
        url,
        refspecs,
        progress=progress,
        **kwargs,
    )


def get_random_exp_name(scm, baseline_rev):
    # fmt: off
    NOUNS = ('abac', 'abbs', 'aces', 'acid', 'acne', 'acre', 'acts', 'ados', 'adze', 'afro', 'agas', 'aged', 'ages', 'agio', 'agma', 'airs', 'airt', 'aits', 'akes', 'alap', 'albs', 'alga', 'ally', 'alto', 'amah', 'ambo', 'amie', 'amyl', 'ankh', 'apex', 'aqua', 'arcs', 'areg', 'aria', 'aril', 'arks', 'army', 'auks', 'aune', 'aura', 'awls', 'awns', 'axon', 'azan', 'baby', 'bade', 'bael', 'bags', 'bait', 'ball', 'banc', 'bang', 'bani', 'barb', 'bark', 'bate', 'bats', 'bawl', 'beak', 'bean', 'beep', 'belt', 'berk', 'beth', 'bias', 'bice', 'bids', 'bind', 'bise', 'bish', 'bite', 'boar', 'boat', 'body', 'boff', 'bold', 'boll', 'bolo', 'bomb', 'bond', 'book', 'boor', 'boot', 'bort', 'bosk', 'bots', 'bott', 'bout', 'bras', 'bree', 'brig', 'brio', 'buck', 'buhl', 'bump', 'bunk', 'bunt', 'buoy', 'byes', 'byte', 'cane', 'cant', 'caps', 'care', 'cart', 'cats', 'cedi', 'ceps', 'cere', 'chad', 'cham', 'chat', 'chay', 'chic', 'chin', 'chis', 'chiv', 'choc', 'chow', 'chum', 'ciao', 'cigs', 'clay', 'clip', 'clog', 'coal', 'coat', 'code', 'coed', 'cogs', 'coho', 'cole', 'cols', 'colt', 'conk', 'cons', 'cony', 'coof', 'cook', 'cool', 'coos', 'corm', 'cors', 'coth', 'cows', 'coze', 'crag', 'craw', 'cree', 'crib', 'cuds', 'cull', 'cult', 'curb', 'curn', 'curs', 'cusp', 'cuss', 'cwms', 'cyma', 'cyst', 'dabs', 'dado', 'daff', 'dais', 'daks', 'damn', 'dams', 'darg', 'dart', 'data', 'dawk', 'dawn', 'daws', 'daze', 'dean', 'debs', 'debt', 'deep', 'dees', 'dele', 'delf', 'dent', 'deys', 'dhow', 'digs', 'dirk', 'dita', 'diva', 'divs', 'doek', 'doge', 'dogs', 'dogy', 'dohs', 'doit', 'dole', 'doll', 'dolt', 'dona', 'dook', 'door', 'dops', 'doss', 'doxy', 'drab', 'drop', 'drum', 'duad', 'duct', 'duff', 'duke', 'dunk', 'dunt', 'ears', 'ease', 'eggs', 'eild', 'emeu', 'emus', 'envy', 'epha', 'eric', 'erns', 'esne', 'esse', 'ewes', 'expo', 'eyas', 'eyot', 'eyry', 'fare', 'farl', 'farm', 'feds', 'feel', 'fees', 'feme', 'fess', 'fibs', 'fids', 'fils', 'firm', 'fish', 'flab', 'flap', 'flea', 'flew', 'flex', 'flip', 'flit', 'flus', 'flux', 'foil', 'fond', 'food', 'fool', 'ford', 'fore', 'frit', 'friz', 'froe', 'funs', 'furl', 'fuss', 'fuzz', 'gaby', 'gaff', 'gale', 'gang', 'gaol', 'gape', 'gash', 'gaur', 'gaze', 'gear', 'genu', 'gest', 'geum', 'ghat', 'gigs', 'gimp', 'gird', 'girl', 'glee', 'glen', 'glia', 'glop', 'gnat', 'goad', 'goaf', 'gobs', 'gonk', 'good', 'goos', 'gore', 'gram', 'gray', 'grig', 'grip', 'grot', 'grub', 'gude', 'gula', 'gulf', 'guns', 'gust', 'gyms', 'gyro', 'hack', 'haet', 'hajj', 'hake', 'half', 'halm', 'hard', 'harl', 'hask', 'hate', "he'd", 'heck', 'heel', 'heir', 'help', 'hems', 'here', 'hill', 'hips', 'hits', 'hobo', 'hock', 'hogs', 'hold', 'holy', 'hood', 'hoot', 'hope', 'horn', 'hose', 'hour', 'hows', 'huck', 'hugs', 'huia', 'hulk', 'hull', 'hunk', 'hunt', 'huts', 'hymn', 'ibex', 'ices', 'iglu', 'impi', 'inks', 'inti', 'ions', 'iota', 'iron', 'jabs', 'jags', 'jake', 'jass', 'jato', 'jaws', 'jean', 'jeer', 'jerk', 'jest', 'jiao', 'jigs', 'jill', 'jinn', 'jird', 'jive', 'jock', 'joey', 'jogs', 'joss', 'jota', 'jots', 'juba', 'jube', 'judo', 'jump', 'junk', 'jura', 'juts', 'jynx', 'kago', 'kail', 'kaka', 'kale', 'kana', 'keek', 'keep', 'kefs', 'kegs', 'kerf', 'kern', 'keys', 'kibe', 'kick', 'kids', 'kifs', 'kill', 'kina', 'kind', 'kine', 'kite', 'kiwi', 'knap', 'knit', 'koas', 'kobs', 'kyat', 'lack', 'lahs', 'lair', 'lama', 'lamb', 'lame', 'lats', 'lava', 'lays', 'leaf', 'leak', 'leas', 'lees', 'leks', 'leno', 'libs', 'lich', 'lick', 'lien', 'lier', 'lieu', 'life', 'lift', 'limb', 'line', 'link', 'linn', 'lira', 'loft', 'loge', 'loir', 'long', 'loof', 'look', 'loot', 'lore', 'loss', 'lots', 'loup', 'love', 'luce', 'ludo', 'luke', 'lulu', 'lure', 'lush', 'magi', 'maid', 'main', 'mako', 'male', 'mana', 'many', 'mart', 'mash', 'mast', 'mate', 'math', 'mats', 'matt', 'maul', 'maya', 'mays', 'meal', 'mean', 'meed', 'mela', 'mene', 'mere', 'merk', 'mesh', 'mete', 'mice', 'milo', 'mime', 'mina', 'mine', 'mirk', 'miss', 'mobs', 'moit', 'mold', 'molt', 'mome', 'moms', 'monk', 'moot', 'mope', 'more', 'morn', 'mows', 'moxa', 'much', 'mung', 'mush', 'muss', 'myth', 'name', 'nard', 'nark', 'nave', 'navy', 'neck', 'newt', 'nibs', 'nims', 'nine', 'nock', 'noil', 'noma', 'nosh', 'nowt', 'nuke', 'oafs', 'oast', 'oats', 'obit', 'odor', 'okra', 'omer', 'oner', 'ones', 'orcs', 'ords', 'orfe', 'orle', 'ossa', 'outs', 'over', 'owls', 'pail', 'pall', 'palp', 'pams', 'pang', 'pans', 'pant', 'paps', 'pate', 'pats', 'paws', 'pear', 'peba', 'pech', 'pecs', 'peel', 'peer', 'pees', 'pein', 'peri', 'phon', 'pice', 'pita', 'pith', 'play', 'plop', 'plot', 'plow', 'plug', 'plum', 'polo', 'pomp', 'pond', 'pons', 'pony', 'poof', 'pope', 'poss', 'pots', 'pour', 'prad', 'prat', 'prep', 'prob', 'prof', 'prow', 'puck', 'puds', 'puke', 'puku', 'pump', 'puns', 'pupa', 'purl', 'pyre', 'quad', 'quay', 'quey', 'quiz', 'raid', 'rail', 'rain', 'raja', 'rale', 'rams', 'rand', 'rant', 'raps', 'rasp', 'razz', 'rede', 'reef', 'reif', 'rein', 'repp', 'rial', 'ribs', 'rick', 'rift', 'rill', 'rime', 'rims', 'ring', 'rins', 'rise', 'rite', 'rits', 'roam', 'robe', 'rods', 'roma', 'rook', 'rort', 'rotl', 'roup', 'roux', 'rube', 'rubs', 'ruby', 'rues', 'rugs', 'ruin', 'runs', 'ryas', 'sack', 'sacs', 'saga', 'sail', 'sale', 'salp', 'salt', 'sand', 'sang', 'sash', 'saut', 'says', 'scab', 'scow', 'scud', 'scup', 'scut', 'seal', 'seam', 'sech', 'seed', 'seep', 'seer', 'self', 'sena', 'send', 'sera', 'sere', 'shad', 'shah', 'sham', 'shay', 'shes', 'ship', 'shoe', 'sick', 'sida', 'sign', 'sike', 'sima', 'sine', 'sing', 'sinh', 'sink', 'sins', 'site', 'size', 'skat', 'skin', 'skip', 'skis', 'slaw', 'sled', 'slew', 'sley', 'slob', 'slue', 'slug', 'smut', 'snap', 'snib', 'snip', 'snob', 'snog', 'snot', 'snow', 'snub', 'snug', 'soft', 'soja', 'soke', 'song', 'sons', 'sook', 'sorb', 'sori', 'souk', 'soul', 'sous', 'soya', 'spit', 'stay', 'stew', 'stir', 'stob', 'stud', 'suds', 'suer', 'suit', 'sumo', 'sums', 'sups', 'suqs', 'suss', 'sway', 'syce', 'synd', 'taal', 'tach', 'taco', 'tads', 'taka', 'tale', 'tamp', 'tams', 'tang', 'tans', 'tape', 'tare', 'taro', 'tarp', 'tart', 'tass', 'taus', 'teat', 'teds', 'teff', 'tegu', 'tell', 'term', 'thar', 'thaw', 'tics', 'tier', 'tiff', 'tils', 'tilt', 'tint', 'tipi', 'tire', 'tirl', 'toby', 'tods', 'toea', 'toff', 'toga', 'toil', 'toke', 'tola', 'tole', 'tomb', 'toms', 'torc', 'tors', 'tort', 'tosh', 'tote', 'tret', 'trey', 'trio', 'trug', 'tuck', 'tugs', 'tule', 'tune', 'tuns', 'tuts', 'tyke', 'tyne', 'typo', 'ulna', 'umbo', 'unau', 'unit', 'upas', 'user', 'uvea', 'vacs', 'vane', 'vang', 'vans', 'vara', 'vase', 'veep', 'veer', 'vega', 'veil', 'vela', 'vent', 'vies', 'view', 'vina', 'vine', 'vise', 'vlei', 'volt', 'vows', 'wads', 'waft', 'wage', 'wain', 'walk', 'want', 'wart', 'wave', 'waws', 'weal', 'wean', 'weds', 'weep', 'weft', 'weir', 'weka', 'weld', 'wens', 'weys', 'whap', 'whey', 'whin', 'whit', 'whop', 'wide', 'wife', 'wind', 'wine', 'wino', 'wins', 'wire', 'wise', 'woes', 'wont', 'wool', 'work', 'worm', 'wort', 'yack', 'yank', 'yapp', 'yard', 'yate', 'yawl', 'yegg', 'yell', 'yeuk', 'yews', 'yips', 'yobs', 'yogi', 'yoke', 'yolk', 'yoni', 'zack', 'zags', 'zest', 'zhos', 'zigs', 'zila', 'zips', 'ziti', 'zoea', 'zone', 'zoon')  # noqa: E501, Q000, N806
    ADJECTIVES = ('about', 'above', 'abuzz', 'acerb', 'acold', 'acred', 'added', 'addle', 'adept', 'adult', 'adunc', 'adust', 'afoul', 'after', 'agape', 'agaze', 'agile', 'aging', 'agley', 'aglow', 'ahead', 'ahull', 'aided', 'alary', 'algal', 'alike', 'alive', 'alone', 'aloof', 'alpha', 'amber', 'amiss', 'amort', 'ample', 'amuck', 'angry', 'anile', 'apeak', 'apish', 'arced', 'areal', 'armed', 'aroid', 'ashen', 'aspen', 'astir', 'atilt', 'atrip', 'aulic', 'aural', 'awash', 'awful', 'awing', 'awned', 'axile', 'azoic', 'azure', 'baggy', 'baked', 'balky', 'bally', 'balmy', 'banal', 'bandy', 'bardy', 'bared', 'barer', 'barky', 'basal', 'based', 'baser', 'basic', 'batty', 'bawdy', 'beady', 'beaky', 'beamy', 'beaut', 'beefy', 'beery', 'beige', 'bendy', 'bifid', 'bijou', 'biped', 'birch', 'bitty', 'blame', 'bland', 'blank', 'blear', 'blest', 'blind', 'blond', 'blown', 'blowy', 'bluer', 'bluff', 'blunt', 'boned', 'bonny', 'boozy', 'bored', 'boric', 'bosky', 'bosom', 'bound', 'bovid', 'bowed', 'boxed', 'braky', 'brash', 'brief', 'briny', 'brisk', 'broad', 'broch', 'brood', 'brown', 'brute', 'buggy', 'bulgy', 'bumpy', 'burly', 'burnt', 'burry', 'bushy', 'busty', 'butch', 'buxom', 'cadgy', 'cagey', 'calmy', 'campy', 'canny', 'caped', 'cased', 'catty', 'cauld', 'cedar', 'cered', 'ceric', 'chary', 'cheap', 'cheek', 'chewy', 'chief', 'chill', 'chirk', 'choky', 'cissy', 'civil', 'cleft', 'coaly', 'color', 'comfy', 'comic', 'compo', 'conic', 'couth', 'coxal', 'crack', 'crank', 'crash', 'crass', 'crisp', 'cronk', 'cross', 'crude', 'cruel', 'crumb', 'cured', 'curly', 'curst', 'cushy', 'cutty', 'cynic', 'dated', 'dazed', 'dedal', 'deism', 'diazo', 'dicey', 'dingy', 'direr', 'dirty', 'dishy', 'dizzy', 'dolce', 'doped', 'dopey', 'dormy', 'dorty', 'dosed', 'dotal', 'dotty', 'dowdy', 'dowie', 'downy', 'dozen', 'drawn', 'dread', 'drear', 'dress', 'dried', 'ducky', 'duddy', 'dummy', 'dumpy', 'duple', 'dural', 'dusky', 'dusty', 'dutch', 'dying', 'eager', 'eaten', 'ebony', 'edged', 'eerie', 'eight', 'elder', 'elect', 'elfin', 'elite', 'empty', 'enate', 'enemy', 'epoxy', 'erect', 'ethic', 'every', 'extra', 'faced', 'faery', 'faint', 'famed', 'fancy', 'farci', 'fatal', 'fated', 'fatty', 'fazed', 'felon', 'fenny', 'ferny', 'fetal', 'fetid', 'fewer', 'fiery', 'fifty', 'filar', 'filmy', 'final', 'fined', 'finer', 'finny', 'fired', 'first', 'fishy', 'fixed', 'fizzy', 'flaky', 'flamy', 'flash', 'flawy', 'fleet', 'flory', 'flown', 'fluid', 'fluky', 'flush', 'focal', 'foggy', 'folio', 'forky', 'forte', 'forty', 'found', 'frail', 'frank', 'freed', 'freer', 'fresh', 'fried', 'front', 'frore', 'fuggy', 'funky', 'funny', 'furry', 'fusil', 'fussy', 'fuzzy', 'gabby', 'gamer', 'gamey', 'gamic', 'gammy', 'garni', 'gauge', 'gaunt', 'gauzy', 'gawky', 'gawsy', 'gemmy', 'genal', 'genic', 'ghast', 'gimpy', 'girly', 'glare', 'glary', 'glial', 'glued', 'gluey', 'godly', 'gooey', 'goofy', 'goosy', 'gouty', 'grade', 'grand', 'grapy', 'grave', 'gross', 'group', 'gruff', 'guest', 'gules', 'gulfy', 'gummy', 'gushy', 'gusty', 'gutsy', 'gutta', 'gypsy', 'gyral', 'hadal', 'hammy', 'handy', 'hardy', 'hasty', 'hated', 'hazel', 'heady', 'heapy', 'hefty', 'heigh', 'hempy', 'herby', 'hexed', 'hi-fi', 'hilly', 'hired', 'holey', 'honey', 'hooly', 'hoven', 'huger', 'hulky', 'humid', 'hunky', 'hyoid', 'idled', 'iliac', 'inane', 'incog', 'inert', 'inner', 'inter', 'iodic', 'ionic', 'irate', 'irony', 'itchy', 'jaggy', 'jammy', 'japan', 'jazzy', 'jerky', 'jetty', 'joint', 'jowly', 'juicy', 'jumpy', 'jural', 'kacha', 'kaput', 'kempt', 'keyed', 'kinky', 'known', 'kooky', 'kraal', 'laced', 'laigh', 'lairy', 'lamer', 'lardy', 'larky', 'lated', 'later', 'lathy', 'leady', 'leafy', 'leaky', 'leary', 'least', 'ledgy', 'leery', 'legal', 'leggy', 'lento', 'level', 'licht', 'licit', 'liege', 'light', 'liked', 'liney', 'lippy', 'lived', 'livid', 'loamy', 'loath', 'lobar', 'local', 'loony', 'loose', 'loral', 'losel', 'lousy', 'loved', 'lower', 'lowly', 'lowse', 'loyal', 'lucid', 'lucky', 'lumpy', 'lunar', 'lurid', 'lushy', 'lying', 'lyric', 'macho', 'macro', 'magic', 'major', 'malar', 'mangy', 'manky', 'manly', 'mardy', 'massy', 'mated', 'matte', 'mauve', 'mazed', 'mealy', 'meaty', 'medal', 'melic', 'mesic', 'mesne', 'messy', 'metal', 'miffy', 'milky', 'mined', 'minim', 'minor', 'minus', 'mired', 'mirky', 'misty', 'mixed', 'modal', 'model', 'moire', 'molar', 'moldy', 'moody', 'moony', 'mopey', 'moral', 'mossy', 'mothy', 'motor', 'mousy', 'moved', 'mucid', 'mucky', 'muddy', 'muggy', 'muley', 'mural', 'murky', 'mushy', 'muted', 'muzzy', 'myoid', 'naggy', 'naive', 'naked', 'named', 'nasty', 'natal', 'naval', 'nervy', 'newsy', 'nicer', 'niffy', 'nifty', 'ninth', 'nitty', 'nival', 'noble', 'nodal', 'noisy', 'non-U', 'north', 'nosed', 'noted', 'nowed', 'nubby', 'oaken', 'oared', 'oaten', 'obese', 'ocher', 'ochre', 'often', 'ohmic', 'oiled', 'olden', 'older', 'oleic', 'olive', 'optic', 'ortho', 'osmic', 'other', 'outer', 'ovoid', 'owing', 'owned', 'paced', 'pagan', 'paled', 'paler', 'pally', 'paper', 'pappy', 'parky', 'party', 'pasty', 'pavid', 'pawky', 'peaky', 'pearl', 'peart', 'peaty', 'pedal', 'peppy', 'perdu', 'perky', 'pesky', 'phony', 'piano', 'picky', 'piled', 'piney', 'pious', 'pique', 'pithy', 'platy', 'plump', 'plush', 'podgy', 'potty', 'power', 'prest', 'pricy', 'prima', 'prime', 'print', 'privy', 'prize', 'prone', 'proof', 'prosy', 'proud', 'proxy', 'pseud', 'pucka', 'pudgy', 'puffy', 'pukka', 'pupal', 'purer', 'pursy', 'pushy', 'pyoid', 'quack', 'quare', 'quasi', 'quiet', 'quits', 'rabic', 'rabid', 'radio', 'raked', 'randy', 'rapid', 'rarer', 'raspy', 'rathe', 'ratty', 'ready', 'reedy', 'reeky', 'refer', 'regal', 'riant', 'ridgy', 'right', 'riled', 'rimed', 'rindy', 'risen', 'risky', 'ritzy', 'rival', 'riven', 'robed', 'rocky', 'roily', 'roman', 'rooky', 'ropey', 'round', 'rowdy', 'ruddy', 'ruled', 'rummy', 'runic', 'runny', 'runty', 'rural', 'rusty', 'rutty', 'sable', 'salic', 'sandy', 'sappy', 'sarky', 'sassy', 'sated', 'saved', 'savvy', 'scald', 'scaly', 'scary', 'score', 'scrap', 'sedgy', 'seely', 'seral', 'sewed', 'shaky', 'sharp', 'sheen', 'shier', 'shill', 'shoal', 'shock', 'shoed', 'shore', 'short', 'shyer', 'silky', 'silly', 'silty', 'sixth', 'sixty', 'skint', 'slack', 'slant', 'sleek', 'slier', 'slimy', 'slung', 'small', 'smart', 'smoky', 'snaky', 'sneak', 'snide', 'snowy', 'snuff', 'so-so', 'soapy', 'sober', 'socko', 'solar', 'soled', 'solid', 'sonic', 'sooth', 'sooty', 'soppy', 'sorer', 'sound', 'soupy', 'spent', 'spicy', 'spiky', 'spiny', 'spiry', 'splay', 'split', 'sport', 'spumy', 'squat', 'staid', 'stiff', 'still', 'stoic', 'stone', 'stony', 'store', 'stout', 'straw', 'stray', 'strip', 'stung', 'suave', 'sudsy', 'sulfa', 'sulky', 'sunny', 'super', 'sural', 'surer', 'surfy', 'surgy', 'surly', 'swell', 'swept', 'swish', 'sworn', 'tabby', 'taboo', 'tacit', 'tacky', 'tamed', 'tamer', 'tangy', 'taped', 'tarot', 'tarry', 'tasty', 'tatty', 'taunt', 'tawie', 'teary', 'techy', 'telic', 'tenor', 'tense', 'tenth', 'tenty', 'tepid', 'terse', 'testy', 'third', 'tidal', 'tight', 'tiled', 'timid', 'tinct', 'tined', 'tippy', 'tipsy', 'tonal', 'toned', 'tonic', 'toric', 'total', 'tough', 'toxic', 'trade', 'treed', 'treen', 'trial', 'truer', 'tubal', 'tubby', 'tumid', 'tuned', 'tutti', 'twill', 'typal', 'typed', 'typic', 'umber', 'unapt', 'unbid', 'uncut', 'undue', 'undug', 'unfed', 'unfit', 'union', 'unlet', 'unmet', 'unwed', 'unwet', 'upper', 'upset', 'urban', 'utile', 'uveal', 'vagal', 'valid', 'vapid', 'varus', 'vatic', 'veiny', 'vital', 'vivid', 'vocal', 'vogie', 'volar', 'vying', 'wacky', 'wally', 'waney', 'warty', 'washy', 'waspy', 'waste', 'waugh', 'waxen', 'webby', 'wedgy', 'weeny', 'weepy', 'weest', 'weird', 'welsh', 'wersh', 'whist', 'white', 'whity', 'whole', 'wider', 'wight', 'winey', 'wired', 'wised', 'wiser', 'withy', 'wonky', 'woods', 'woozy', 'world', 'wormy', 'worse', 'worst', 'woven', 'wrath', 'wrier', 'wrong', 'wroth', 'xeric', 'yarer', 'yolky', 'young', 'yucky', 'yummy', 'zesty', 'zingy', 'zinky', 'zippy', 'zonal')  # noqa: E501, Q000, N806
    # fmt: on
    # Use custom random generator to make sure that names are random even if
    # global random seed is set (common for ML pipelines).
    random_generator = random.Random()
    while True:
        adjective = random_generator.choice(ADJECTIVES)
        noun = random_generator.choice(NOUNS)
        name = f"{adjective}-{noun}"
        exp_ref = ExpRefInfo(baseline_sha=baseline_rev, name=name)
        if not scm.get_ref(str(exp_ref)):
            return name


def to_studio_params(dvc_params):
    """Convert from internal DVC format to Studio format.

    From:

    {
        "workspace": {
            "data": {
                "params.yaml": {
                    "data": {"foo": 1}
                }
            }
        }
    }

    To:

    {
        "params.yaml": {"foo": 1}
    }
    """
    result: Dict = {}
    if not dvc_params:
        return result
    for rev_data in dvc_params.values():
        for file_name, file_data in rev_data.get("data", {}).items():
            result[file_name] = file_data.get("data", {})

    return result




dvc/repo/experiments/executor/__init__.py




dvc/repo/experiments/executor/base.py
import logging
import os
import pickle  # nosec B403
import shutil
from abc import ABC, abstractmethod
from contextlib import contextmanager
from dataclasses import asdict, dataclass
from enum import IntEnum
from itertools import chain
from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    Dict,
    Iterable,
    Iterator,
    List,
    NamedTuple,
    Optional,
    Tuple,
    Type,
    TypeVar,
    Union,
)

from funcy import get_in
from scmrepo.exceptions import SCMError

from dvc.env import DVC_EXP_AUTO_PUSH, DVC_EXP_GIT_REMOTE
from dvc.exceptions import DvcException
from dvc.repo.experiments.exceptions import ExperimentExistsError
from dvc.repo.experiments.refs import EXEC_BASELINE, EXEC_BRANCH, ExpRefInfo
from dvc.repo.experiments.utils import to_studio_params
from dvc.repo.metrics.show import _collect_top_level_metrics
from dvc.repo.params.show import _collect_top_level_params
from dvc.stage.serialize import to_lockfile
from dvc.utils import dict_sha256, env2bool, relpath
from dvc.utils.fs import remove
from dvc.utils.studio import env_to_config

if TYPE_CHECKING:
    from queue import Queue

    from dvc.repo import Repo
    from dvc.repo.experiments.stash import ExpStashEntry
    from dvc.scm import Git
    from dvc.stage import PipelineStage

logger = logging.getLogger(__name__)


class ExecutorResult(NamedTuple):
    exp_hash: Optional[str]
    ref_info: Optional["ExpRefInfo"]
    force: bool


class TaskStatus(IntEnum):
    PENDING = 0
    PREPARING = 1
    RUNNING = 2
    SUCCESS = 3
    FAILED = 4
    CANCELED = 5
    FINISHED = 6


@dataclass
class ExecutorInfo:
    git_url: str
    baseline_rev: str
    location: str
    root_dir: str
    dvc_dir: str
    name: Optional[str] = None
    wdir: Optional[str] = None
    result_hash: Optional[str] = None
    result_ref: Optional[str] = None
    result_force: bool = False
    status: TaskStatus = TaskStatus.PENDING

    @classmethod
    def from_dict(cls, d):
        if d.pop("collected", None):
            d["status"] = TaskStatus.FINISHED
        return cls(**d)

    def asdict(self):
        return asdict(self)

    @property
    def result(self) -> Optional["ExecutorResult"]:
        if self.result_hash is None:
            return None
        return ExecutorResult(
            self.result_hash,
            ExpRefInfo.from_ref(self.result_ref) if self.result_ref else None,
            self.result_force,
        )

    def dump_json(self, filename: str):
        from dvc.utils.serialize import modify_json

        os.makedirs(os.path.dirname(filename), exist_ok=True)
        with modify_json(filename) as d:
            d.update(self.asdict())

    @classmethod
    def load_json(cls, filename: str) -> "ExecutorInfo":
        from dvc.utils.serialize import load_json

        return cls.from_dict(load_json(filename))


_T = TypeVar("_T", bound="BaseExecutor")


class BaseExecutor(ABC):
    """Base class for executing experiments in parallel.

    Parameters:
        root_dir: Path to SCM root.
        dvc_dir: Path to .dvc dir relative to SCM root.
        baseline_rev: Experiment baseline revision.
        wdir: Path to exec working directory relative to SCM root.
        name: Executor (experiment) name.
        result: Completed executor result.
    """

    PACKED_ARGS_FILE = "repro.dat"
    WARN_UNTRACKED = False
    QUIET = False
    INFOFILE_EXT = ".run"
    DEFAULT_LOCATION: str = "workspace"

    def __init__(
        self,
        root_dir: str,
        dvc_dir: str,
        baseline_rev: str,
        status: TaskStatus,
        wdir: Optional[str] = None,
        name: Optional[str] = None,
        location: Optional[str] = None,
        result: Optional["ExecutorResult"] = None,
        **kwargs,
    ):
        self.dvc_dir = dvc_dir
        self.root_dir = root_dir
        self.wdir = wdir
        self.name = name
        self.baseline_rev = baseline_rev
        self.location: str = location or self.DEFAULT_LOCATION
        self.result = result
        self.status = status

    @abstractmethod
    def init_git(
        self,
        repo: "Repo",
        scm: "Git",
        stash_rev: str,
        entry: "ExpStashEntry",
        infofile: Optional[str],
        branch: Optional[str] = None,
    ):
        """Init git repo and populate it using exp refs from the specified
        SCM instance.
        """

    @property
    @abstractmethod
    def git_url(self) -> str:
        pass

    @abstractmethod
    def init_cache(self, repo: "Repo", rev: str, run_cache: bool = True):
        """Initialize DVC cache."""

    @abstractmethod
    def collect_cache(
        self, repo: "Repo", exp_ref: "ExpRefInfo", run_cache: bool = True
    ):
        """Collect DVC cache."""

    @property
    def info(self) -> "ExecutorInfo":
        if self.result is not None:
            result_dict: Dict[str, Any] = {
                "result_hash": self.result.exp_hash,
                "result_ref": (
                    str(self.result.ref_info) if self.result.ref_info else None
                ),
                "result_force": self.result.force,
            }
        else:
            result_dict = {}
        return ExecutorInfo(
            git_url=self.git_url,
            baseline_rev=self.baseline_rev,
            location=self.location,
            root_dir=self.root_dir,
            dvc_dir=self.dvc_dir,
            name=self.name,
            wdir=self.wdir,
            status=self.status,
            **result_dict,
        )

    @classmethod
    def from_info(cls: Type[_T], info: "ExecutorInfo") -> _T:
        if info.result_hash:
            result: Optional["ExecutorResult"] = ExecutorResult(
                info.result_hash,
                (ExpRefInfo.from_ref(info.result_ref) if info.result_ref else None),
                info.result_force,
            )
        else:
            result = None
        return cls(
            root_dir=info.root_dir,
            dvc_dir=info.dvc_dir,
            baseline_rev=info.baseline_rev,
            status=info.status,
            name=info.name,
            wdir=info.wdir,
            result=result,
        )

    @classmethod
    @abstractmethod
    def from_stash_entry(
        cls: Type[_T],
        repo: "Repo",
        entry: "ExpStashEntry",
        **kwargs,
    ) -> _T:
        pass

    @classmethod
    def _from_stash_entry(
        cls: Type[_T],
        repo: "Repo",
        entry: "ExpStashEntry",
        root_dir: str,
        **kwargs,
    ) -> _T:
        return cls(
            root_dir=root_dir,
            dvc_dir=relpath(repo.dvc_dir, repo.scm.root_dir),
            baseline_rev=entry.baseline_rev,
            status=TaskStatus.PREPARING,
            name=entry.name,
            wdir=relpath(os.getcwd(), repo.scm.root_dir),
            **kwargs,
        )

    @classmethod
    def _get_top_level_paths(cls, repo: "Repo") -> List["str"]:
        return list(
            chain(
                _collect_top_level_metrics(repo),
                _collect_top_level_params(repo),
                repo.index._plot_sources,  # pylint: disable=protected-access
            )
        )

    @classmethod
    def save(
        cls,
        info: "ExecutorInfo",
        force: bool = False,
        include_untracked: Optional[List[str]] = None,
        message: Optional[str] = None,
    ) -> ExecutorResult:
        from dvc.dvcfile import LOCK_FILE
        from dvc.repo import Repo

        exp_hash: Optional[str] = None
        exp_ref: Optional[ExpRefInfo] = None

        dvc = Repo(os.path.join(info.root_dir, info.dvc_dir))
        old_cwd = os.getcwd()
        if info.wdir:
            os.chdir(os.path.join(dvc.scm.root_dir, info.wdir))
        else:
            os.chdir(dvc.root_dir)

        include_untracked = include_untracked or []
        include_untracked.extend(cls._get_top_level_paths(dvc))
        # dvc repro automatically stages dvc.lock. Running redundant `git add`
        # on it causes an error when exiting the detached head context.
        if LOCK_FILE in dvc.scm.untracked_files():
            include_untracked.append(LOCK_FILE)

        try:
            stages = dvc.commit([], force=True, relink=False)
            exp_hash = cls.hash_exp(stages)
            if include_untracked:
                dvc.scm.add(include_untracked)
            cls.commit(
                dvc.scm,  # type: ignore[arg-type]
                exp_hash,
                exp_name=info.name,
                force=force,
                message=message,
            )
            ref: Optional[str] = dvc.scm.get_ref(EXEC_BRANCH, follow=False)
            exp_ref = ExpRefInfo.from_ref(ref) if ref else None
            untracked = dvc.scm.untracked_files()
            if untracked:
                logger.warning(
                    "The following untracked files were present in "
                    "the workspace before saving but "
                    "will not be included in the experiment commit:\n"
                    "\t%s",
                    ", ".join(untracked),
                )
            info.result_hash = exp_hash
            info.result_ref = ref
            info.result_force = False
            info.status = TaskStatus.SUCCESS
        except DvcException:
            info.status = TaskStatus.FAILED
            raise
        finally:
            dvc.close()
            os.chdir(old_cwd)

        return ExecutorResult(ref, exp_ref, info.result_force)

    @staticmethod
    def hash_exp(stages: Iterable["PipelineStage"]) -> str:
        from dvc.stage import PipelineStage

        exp_data = {}
        for stage in stages:
            if isinstance(stage, PipelineStage):
                exp_data.update(to_lockfile(stage))
        return dict_sha256(exp_data)

    def cleanup(self, infofile: Optional[str] = None):
        if infofile is not None:
            info = ExecutorInfo.load_json(infofile)
            if info.status < TaskStatus.FAILED:
                info.status = TaskStatus.FINISHED
            info.dump_json(infofile)

    # TODO: come up with better way to stash repro arguments
    @staticmethod
    def pack_repro_args(path, *args, fs=None, extra=None, **kwargs):
        dpath = os.path.dirname(path)
        if fs:
            open_func = fs.open
            fs.makedirs(dpath)
        else:
            open_func = open
            os.makedirs(dpath, exist_ok=True)

        data = {"args": args, "kwargs": kwargs}
        if extra is not None:
            data["extra"] = extra
        with open_func(path, "wb") as fobj:
            pickle.dump(data, fobj)

    @staticmethod
    def unpack_repro_args(path):
        with open(path, "rb") as fobj:
            data = pickle.load(fobj)  # noqa: S301 # nosec B301
        return data["args"], data["kwargs"]

    def fetch_exps(
        self,
        dest_scm: "Git",
        refs: List[str],
        force: bool = False,
        on_diverged: Optional[Callable[[str], None]] = None,
        **kwargs,
    ) -> Iterable[str]:
        """Fetch reproduced experiment refs into the specified SCM.

        Args:
            dest_scm: Destination Git instance.
            refs: reference names to be fetched from the remotes.
            force: If True, diverged refs will be overwritten
            on_diverged: Callback in the form on_diverged(ref)
                to be called when an experiment ref has diverged.

        Extra kwargs will be passed into the remote git client.
        """

        def on_diverged_ref(orig_ref: str, new_rev: str):
            if force:
                logger.debug("Replacing existing experiment '%s'", orig_ref)
                return True

            if on_diverged:
                return on_diverged(orig_ref)

            self._raise_ref_conflict(dest_scm, orig_ref, new_rev)
            logger.debug("Reproduced existing experiment '%s'", orig_ref)
            return False

        # fetch experiments
        try:
            refspecs = [f"{ref}:{ref}" for ref in refs]
            dest_scm.fetch_refspecs(
                self.git_url,
                refspecs,
                on_diverged=on_diverged_ref,
                force=force,
                **kwargs,
            )
        except SCMError:
            pass

        return refs

    @classmethod
    def _validate_remotes(cls, dvc: "Repo", git_remote: Optional[str]):
        from scmrepo.exceptions import InvalidRemote

        from dvc.scm import InvalidRemoteSCMRepo

        if git_remote == dvc.root_dir:
            logger.warning(
                (
                    "'%s' points to the current Git repo, experiment "
                    "Git refs will not be pushed. But DVC cache and run cache "
                    "will automatically be pushed to the default DVC remote "
                    "(if any) on each experiment commit."
                ),
                git_remote,
            )
        try:
            dvc.scm.validate_git_remote(git_remote)
        except InvalidRemote as exc:
            raise InvalidRemoteSCMRepo(str(exc))  # noqa: B904
        dvc.cloud.get_remote_odb()

    @classmethod
    def reproduce(
        cls,
        info: "ExecutorInfo",
        rev: str,
        queue: Optional["Queue"] = None,
        infofile: Optional[str] = None,
        log_errors: bool = True,
        log_level: Optional[int] = None,
        copy_paths: Optional[List[str]] = None,
        message: Optional[str] = None,
        **kwargs,
    ) -> "ExecutorResult":
        """Run dvc repro and return the result.

        Returns tuple of (exp_hash, exp_ref, force) where exp_hash is the
            experiment hash (or None on error), exp_ref is the experiment ref,
            and force is a bool specifying whether or not this experiment
            should force overwrite any existing duplicates.
        """
        from dvc.repo.checkout import checkout as dvc_checkout
        from dvc.repo.reproduce import reproduce as dvc_reproduce
        from dvc.stage import PipelineStage
        from dvc.ui import ui

        auto_push = env2bool(DVC_EXP_AUTO_PUSH)
        git_remote = os.getenv(DVC_EXP_GIT_REMOTE, None)

        unchanged = []

        if queue is not None:
            queue.put((rev, os.getpid()))
        if log_errors and log_level is not None:
            cls._set_log_level(log_level)

        def filter_pipeline(stages):
            unchanged.extend(
                [stage for stage in stages if isinstance(stage, PipelineStage)]
            )

        exp_hash: Optional[str] = None
        exp_ref: Optional["ExpRefInfo"] = None
        repro_force: bool = False

        if info.name:
            ui.write(f"Reproducing experiment '{info.name}'")

        with cls._repro_dvc(
            info,
            infofile,
            log_errors=log_errors,
            copy_paths=copy_paths,
            message=message,
            **kwargs,
        ) as dvc:
            if auto_push:
                cls._validate_remotes(dvc, git_remote)

            args, kwargs = cls._repro_args(dvc)
            if args:
                targets: Optional[Union[list, str]] = args[0]
            else:
                targets = kwargs.get("targets")

            repro_force = kwargs.get("force", False)
            logger.trace(  # type: ignore[attr-defined]
                "Executor repro with force = '%s'", str(repro_force)
            )

            repro_dry = kwargs.get("dry")

            if not repro_dry:
                dvc_checkout(
                    dvc,
                    targets=targets,
                    with_deps=targets is not None,
                    force=True,
                    quiet=True,
                    allow_missing=True,
                    recursive=kwargs.get("recursive", False),
                )

            stages = dvc_reproduce(
                dvc,
                *args,
                on_unchanged=filter_pipeline,
                **kwargs,
            )
            if paths := cls._get_top_level_paths(dvc):
                logger.debug("Staging top-level files: %s", paths)
                dvc.scm_context.add(paths)

            exp_hash = cls.hash_exp(stages)
            if not repro_dry:
                ref, exp_ref, repro_force = cls._repro_commit(
                    dvc,
                    info,
                    exp_hash,
                    auto_push,
                    git_remote,
                    repro_force,
                    message=message,
                )
                info.result_hash = exp_hash
                info.result_ref = ref
                info.result_force = repro_force

        # ideally we would return stages here like a normal repro() call, but
        # stages is not currently picklable and cannot be returned across
        # multiprocessing calls
        return ExecutorResult(exp_hash, exp_ref, repro_force)

    @classmethod
    def _repro_commit(
        cls,
        dvc,
        info,
        exp_hash,
        auto_push,
        git_remote,
        repro_force,
        message: Optional[str] = None,
    ) -> Tuple[Optional[str], Optional["ExpRefInfo"], bool]:
        cls.commit(
            dvc.scm,
            exp_hash,
            exp_name=info.name,
            force=repro_force,
            message=message,
        )
        if auto_push:
            cls._auto_push(dvc, dvc.scm, git_remote)
        ref: Optional[str] = dvc.scm.get_ref(EXEC_BRANCH, follow=False)
        exp_ref: Optional["ExpRefInfo"] = ExpRefInfo.from_ref(ref) if ref else None
        if cls.WARN_UNTRACKED:
            untracked = dvc.scm.untracked_files()
            if untracked:
                logger.warning(
                    (
                        "The following untracked files were present in "
                        "the experiment directory after reproduction but "
                        "will not be included in experiment commits:\n"
                        "\t%s"
                    ),
                    ", ".join(untracked),
                )
        return ref, exp_ref, repro_force

    @classmethod
    @contextmanager
    def _repro_dvc(  # noqa: C901
        cls,
        info: "ExecutorInfo",
        infofile: Optional[str] = None,
        log_errors: bool = True,
        copy_paths: Optional[List[str]] = None,
        message: Optional[str] = None,
        **kwargs,
    ) -> Iterator["Repo"]:
        from dvc_studio_client.post_live_metrics import post_live_metrics

        from dvc.repo import Repo

        with Repo(os.path.join(info.root_dir, info.dvc_dir)) as dvc:
            info.status = TaskStatus.RUNNING
            if infofile is not None:
                info.dump_json(infofile)
            if cls.QUIET:
                dvc.scm_context.quiet = cls.QUIET
            old_cwd = os.getcwd()

            for path in copy_paths or []:
                cls._copy_path(os.path.realpath(path), os.path.join(dvc.root_dir, path))

            if info.wdir:
                os.chdir(os.path.join(dvc.scm.root_dir, info.wdir))
            else:
                os.chdir(dvc.root_dir)

            args_path = os.path.join(dvc.tmp_dir, cls.PACKED_ARGS_FILE)
            if os.path.exists(args_path):
                _, kwargs = cls.unpack_repro_args(args_path)
            dvc_studio_config = dvc.config.get("studio")
            # set missing config options using saved config
            # inferring repo url will fail if not set here
            run_env_config = env_to_config(kwargs.get("run_env", {}))
            dvc_studio_config = {**run_env_config, **dvc_studio_config}
            try:
                post_live_metrics(
                    "start",
                    info.baseline_rev,
                    info.name,  # type: ignore[arg-type]
                    "dvc",
                    params=to_studio_params(dvc.params.show()),
                    dvc_studio_config=dvc_studio_config,
                    message=message,
                )
                logger.debug("Running repro in '%s'", os.getcwd())
                yield dvc
                info.status = TaskStatus.SUCCESS
            except DvcException:
                if log_errors:
                    logger.exception("")
                info.status = TaskStatus.FAILED
                raise
            except Exception:
                if log_errors:
                    logger.exception("unexpected error")
                info.status = TaskStatus.FAILED
                raise
            finally:
                post_live_metrics(
                    "done",
                    info.baseline_rev,
                    info.name,  # type: ignore[arg-type]
                    "dvc",
                    experiment_rev=dvc.experiments.scm.get_ref(EXEC_BRANCH),
                    metrics=get_in(dvc.metrics.show(), ["", "data"]),
                    dvc_studio_config=dvc_studio_config,
                )

                if infofile is not None:
                    info.dump_json(infofile)
                os.chdir(old_cwd)

    @classmethod
    def _repro_args(cls, dvc):
        args_path = os.path.join(dvc.tmp_dir, cls.PACKED_ARGS_FILE)
        if os.path.exists(args_path):
            args, kwargs = cls.unpack_repro_args(args_path)
            remove(args_path)
            # explicitly git rm/unstage the args file
            dvc.scm.add([args_path], force=True)
        else:
            args = []
            kwargs = {}
        return args, kwargs

    @staticmethod
    def _auto_push(
        dvc: "Repo",
        scm: "Git",
        git_remote: Optional[str],
        push_cache=True,
        run_cache=True,
    ):
        branch = scm.get_ref(EXEC_BRANCH, follow=False)
        try:
            dvc.experiments.push(
                git_remote,
                branch,
                push_cache=push_cache,
                run_cache=run_cache,
            )
        except BaseException as exc:  # noqa: BLE001, pylint: disable=W0703
            logger.warning(
                (
                    "Something went wrong while auto pushing experiment "
                    "to the remote '%s': %s"
                ),
                git_remote,
                exc,
            )

    @classmethod
    def commit(
        cls,
        scm: "Git",
        exp_hash: str,
        exp_name: Optional[str] = None,
        force: bool = False,
        message: Optional[str] = None,
    ):
        """Commit stages as an experiment and return the commit SHA."""
        rev = scm.get_rev()
        if not scm.is_dirty(untracked_files=False):
            logger.debug("No changes to commit")

        check_conflict = False
        branch = scm.get_ref(EXEC_BRANCH, follow=False)
        if branch:
            old_ref = rev
            logger.debug("Commit to current experiment branch '%s'", branch)
        else:
            baseline_rev = scm.get_ref(EXEC_BASELINE)
            name = exp_name if exp_name else f"exp-{exp_hash[:5]}"
            ref_info = ExpRefInfo(baseline_rev, name)
            branch = str(ref_info)
            old_ref = None
            if scm.get_ref(branch):
                if not force:
                    check_conflict = True
                logger.debug(
                    "%s existing experiment branch '%s'",
                    "Replace" if force else "Reuse",
                    branch,
                )
            else:
                logger.debug("Commit to new experiment branch '%s'", branch)

        scm.add([], update=True)
        message = message or f"dvc: commit experiment {exp_hash}"
        scm.commit(message, no_verify=True)
        new_rev = scm.get_rev()
        if check_conflict:
            new_rev = cls._raise_ref_conflict(scm, branch, new_rev)
        else:
            scm.set_ref(branch, new_rev, old_ref=old_ref)
        scm.set_ref(EXEC_BRANCH, branch, symbolic=True)

        return new_rev

    @staticmethod
    def _raise_ref_conflict(scm, ref, new_rev):
        # If this commit is a duplicate of the existing commit at 'ref', return
        # the existing commit. Otherwise, error out and require user to re-run
        # with --force as needed
        orig_rev = scm.get_ref(ref)
        if scm.diff(orig_rev, new_rev):
            raise ExperimentExistsError(ref)
        return orig_rev

    @staticmethod
    def _set_log_level(level):
        # When executor.reproduce is run in a multiprocessing child process,
        # dvc.cli.main will not be called for that child process so we need to
        # setup logging ourselves
        dvc_logger = logging.getLogger("dvc")
        if level is not None:
            dvc_logger.setLevel(level)

    @staticmethod
    def _copy_path(src, dst):
        try:
            if os.path.isfile(src):
                shutil.copy(src, dst)
            elif os.path.isdir(src):
                shutil.copytree(src, dst)
            else:
                raise DvcException(
                    f"Unable to copy '{src}'. It is not a file or directory."
                )
        except OSError as exc:
            raise DvcException(f"Unable to copy '{src}' to '{dst}'.") from exc

    @contextmanager
    def set_temp_refs(self, scm: "Git", temp_dict: Dict[str, str]):
        try:
            for ref, rev in temp_dict.items():
                scm.set_ref(ref, rev)
            yield
        finally:
            for ref in temp_dict:
                if scm.get_ref(ref):
                    scm.remove_ref(ref)




dvc/repo/experiments/executor/local.py
import logging
import os
from contextlib import ExitStack
from tempfile import mkdtemp
from typing import TYPE_CHECKING, Optional, Union

from configobj import ConfigObj
from funcy import retry
from shortuuid import uuid

from dvc.lock import LockError
from dvc.repo.experiments.refs import (
    EXEC_BASELINE,
    EXEC_BRANCH,
    EXEC_HEAD,
    EXEC_MERGE,
    EXEC_NAMESPACE,
    TEMP_NAMESPACE,
)
from dvc.repo.experiments.utils import EXEC_TMP_DIR, get_exp_rwlock
from dvc.scm import SCM, Git
from dvc.utils.fs import remove
from dvc.utils.objects import cached_property

from .base import BaseExecutor, TaskStatus

if TYPE_CHECKING:
    from dvc.repo import Repo
    from dvc.repo.experiments.refs import ExpRefInfo
    from dvc.repo.experiments.stash import ExpStashEntry
    from dvc.scm import NoSCM

logger = logging.getLogger(__name__)


class BaseLocalExecutor(BaseExecutor):
    """Base local machine executor."""

    @property
    def git_url(self) -> str:
        root_dir = os.path.abspath(self.root_dir)
        if os.name == "nt":
            root_dir = root_dir.replace(os.sep, "/")
        return f"file://{root_dir}"

    @cached_property
    def scm(self) -> Union["Git", "NoSCM"]:
        return SCM(self.root_dir)

    def cleanup(self, infofile: Optional[str] = None):
        self.scm.close()
        del self.scm
        super().cleanup(infofile)

    def collect_cache(
        self, repo: "Repo", exp_ref: "ExpRefInfo", run_cache: bool = True
    ):
        """Collect DVC cache."""


class TempDirExecutor(BaseLocalExecutor):
    """Temp directory experiment executor."""

    # Temp dir executors should warn if untracked files exist (to help with
    # debugging user code), and suppress other DVC hints (like `git add`
    # suggestions) that are not applicable outside of workspace runs
    WARN_UNTRACKED = True
    QUIET = True
    DEFAULT_LOCATION = "tempdir"

    @retry(180, errors=LockError, timeout=1)
    def init_git(
        self,
        repo: "Repo",
        scm: "Git",
        stash_rev: str,
        entry: "ExpStashEntry",
        infofile: Optional[str],
        branch: Optional[str] = None,
    ):
        from dulwich.repo import Repo as DulwichRepo

        from dvc.repo.experiments.utils import push_refspec

        DulwichRepo.init(os.fspath(self.root_dir))

        self.status = TaskStatus.PREPARING
        if infofile:
            self.info.dump_json(infofile)

        temp_head = f"{TEMP_NAMESPACE}/head-{uuid()}"
        temp_merge = f"{TEMP_NAMESPACE}/merge-{uuid()}"
        temp_baseline = f"{TEMP_NAMESPACE}/baseline-{uuid()}"

        temp_ref_dict = {
            temp_head: entry.head_rev,
            temp_merge: stash_rev,
            temp_baseline: entry.baseline_rev,
        }
        with get_exp_rwlock(
            repo, writes=[temp_head, temp_merge, temp_baseline]
        ), self.set_temp_refs(scm, temp_ref_dict):
            # Executor will be initialized with an empty git repo that
            # we populate by pushing:
            #   EXEC_HEAD - the base commit for this experiment
            #   EXEC_MERGE - the unmerged changes (from our stash)
            #       to be reproduced
            #   EXEC_BASELINE - the baseline commit for this experiment
            refspec = [
                (temp_head, EXEC_HEAD),
                (temp_merge, EXEC_MERGE),
                (temp_baseline, EXEC_BASELINE),
            ]

            if branch:
                refspec.append((branch, branch))
                with get_exp_rwlock(repo, reads=[branch]):
                    push_refspec(scm, self.git_url, refspec)
                self.scm.set_ref(EXEC_BRANCH, branch, symbolic=True)
            else:
                push_refspec(scm, self.git_url, refspec)
                if self.scm.get_ref(EXEC_BRANCH):
                    self.scm.remove_ref(EXEC_BRANCH)

        # checkout EXEC_HEAD and apply EXEC_MERGE on top of it without
        # committing
        assert isinstance(self.scm, Git)
        head = EXEC_BRANCH if branch else EXEC_HEAD
        self.scm.checkout(head, detach=True)
        merge_rev = self.scm.get_ref(EXEC_MERGE)

        self.scm.stash.apply(merge_rev)
        self._update_config(repo.config.read("local"))

    def _update_config(self, update):
        local_config = os.path.join(
            self.root_dir,
            self.dvc_dir,
            "config.local",
        )
        logger.debug("Writing experiments local config '%s'", local_config)
        if os.path.exists(local_config):
            conf_obj = ConfigObj(local_config)
            conf_obj.merge(update)
        else:
            conf_obj = ConfigObj(update)
        if conf_obj:
            with open(local_config, "wb") as fobj:
                conf_obj.write(fobj)

    def init_cache(
        self, repo: "Repo", rev: str, run_cache: bool = True  # noqa: ARG002
    ):
        """Initialize DVC cache."""
        self._update_config({"cache": {"dir": repo.cache.repo.path}})

    def cleanup(self, infofile: Optional[str] = None):
        super().cleanup(infofile)
        logger.debug("Removing tmpdir '%s'", self.root_dir)
        remove(self.root_dir)

    @classmethod
    def from_stash_entry(
        cls,
        repo: "Repo",
        entry: "ExpStashEntry",
        wdir: Optional[str] = None,
        **kwargs,
    ):
        assert repo.tmp_dir
        parent_dir: str = wdir or os.path.join(repo.tmp_dir, EXEC_TMP_DIR)
        os.makedirs(parent_dir, exist_ok=True)
        tmp_dir = mkdtemp(dir=parent_dir)
        try:
            executor = cls._from_stash_entry(repo, entry, tmp_dir, **kwargs)
            logger.debug("Init temp dir executor in '%s'", tmp_dir)
            return executor
        except Exception:
            remove(tmp_dir)
            raise


class WorkspaceExecutor(BaseLocalExecutor):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._detach_stack = ExitStack()

    @classmethod
    def from_stash_entry(
        cls,
        repo: "Repo",
        entry: "ExpStashEntry",
        **kwargs,
    ):
        root_dir = repo.scm.root_dir
        executor: "WorkspaceExecutor" = cls._from_stash_entry(
            repo, entry, root_dir, **kwargs
        )
        logger.debug("Init workspace executor in '%s'", root_dir)
        return executor

    @retry(180, errors=LockError, timeout=1)
    def init_git(
        self,
        repo: "Repo",
        scm: "Git",
        stash_rev: str,
        entry: "ExpStashEntry",
        infofile: Optional[str],
        branch: Optional[str] = None,
    ):
        self.status = TaskStatus.PREPARING
        if infofile:
            self.info.dump_json(infofile)

        assert isinstance(self.scm, Git)

        with get_exp_rwlock(repo, writes=[EXEC_NAMESPACE]):
            scm.set_ref(EXEC_HEAD, entry.head_rev)
            scm.set_ref(EXEC_MERGE, stash_rev)
            scm.set_ref(EXEC_BASELINE, entry.baseline_rev)
            self._detach_stack.enter_context(
                self.scm.detach_head(
                    self.scm.get_ref(EXEC_HEAD),
                    force=True,
                    client="dvc",
                )
            )
            merge_rev = self.scm.get_ref(EXEC_MERGE)
            self.scm.stash.apply(merge_rev)
            if branch:
                self.scm.set_ref(EXEC_BRANCH, branch, symbolic=True)
            elif scm.get_ref(EXEC_BRANCH):
                self.scm.remove_ref(EXEC_BRANCH)

    def init_cache(self, repo: "Repo", rev: str, run_cache: bool = True):
        pass

    def cleanup(self, infofile: Optional[str] = None):
        super().cleanup(infofile)
        if infofile:
            remove(os.path.dirname(infofile))
        with self._detach_stack:
            self.scm.remove_ref(EXEC_BASELINE)
            self.scm.remove_ref(EXEC_MERGE)
            if self.scm.get_ref(EXEC_BRANCH):
                self.scm.remove_ref(EXEC_BRANCH)




dvc/repo/experiments/executor/ssh.py
import logging
import os
import posixpath
import sys
from contextlib import contextmanager
from typing import TYPE_CHECKING, Callable, Iterable, List, Optional

from dvc_ssh import SSHFileSystem
from funcy import first

from dvc.repo.experiments.refs import (
    EXEC_BASELINE,
    EXEC_BRANCH,
    EXEC_HEAD,
    EXEC_MERGE,
    EXEC_NAMESPACE,
)

from .base import BaseExecutor, ExecutorInfo, ExecutorResult, TaskStatus

if TYPE_CHECKING:
    from queue import Queue

    from dvc.repo import Repo
    from dvc.repo.experiments.refs import ExpRefInfo
    from dvc.repo.experiments.stash import ExpStashEntry
    from dvc.scm import Git

logger = logging.getLogger(__name__)


@contextmanager
def _sshfs(fs_factory, **kwargs):
    if fs_factory:
        with fs_factory() as fs:
            yield fs
        return
    yield SSHFileSystem(**kwargs)


class SSHExecutor(BaseExecutor):
    """SSH experiment executor."""

    WARN_UNTRACKED = True
    QUIET = True
    SETUP_SCRIPT_FILENAME = "exec-setup.sh"

    def __init__(
        self,
        *args,
        host: Optional[str] = None,
        port: Optional[int] = None,
        username: Optional[str] = None,
        fs_factory: Optional[Callable] = None,
        setup_script: Optional[str] = None,
        **kwargs,
    ):
        assert host

        super().__init__(*args, **kwargs)
        self.host: str = host
        self.port = port
        self.username = username
        self._fs_factory = fs_factory
        self._repo_abspath = ""
        self._setup_script = setup_script

    @classmethod
    def gen_dirname(cls, name: Optional[str] = None):
        from shortuuid import uuid

        return "-".join([name or "dvc-exp", "executor", uuid()])

    @classmethod
    def from_stash_entry(
        cls,
        repo: "Repo",
        entry: "ExpStashEntry",
        **kwargs,
    ):
        machine_name: Optional[str] = kwargs.pop("machine_name", None)
        assert repo.machine
        executor = cls._from_stash_entry(
            repo,
            entry,
            cls.gen_dirname(entry.name),
            location=machine_name,
            **repo.machine.get_executor_kwargs(machine_name),
            setup_script=repo.machine.get_setup_script(machine_name),
        )
        logger.debug("Init SSH executor for host '%s'", executor.host)
        return executor

    def sshfs(self):
        return _sshfs(self._fs_factory, host=self.host, port=self.port)

    @property
    def git_url(self) -> str:
        user = f"{self.username}@" if self.username else ""
        port = f":{self.port}" if self.port is not None else ""
        path = f"{self.root_dir}" if self.root_dir else ""
        if path and not posixpath.isabs(path):
            path = f"/~/{path}"
        return f"ssh://{user}{self.host}{port}{path}"

    @property
    def abs_url(self) -> str:
        assert self._repo_abspath
        user = f"{self.username}@" if self.username else ""
        port = f":{self.port}" if self.port is not None else ""
        return f"ssh://{user}{self.host}{port}{self._repo_abspath}"

    @staticmethod
    def _git_client_args(fs):
        return {
            "password": fs.fs_args.get("password"),
            "key_filename": first(fs.fs_args.get("client_keys", [])),
        }

    def init_git(
        self,
        repo: "Repo",  # noqa: ARG002
        scm: "Git",
        stash_rev: str,
        entry: "ExpStashEntry",
        infofile: Optional[str],
        branch: Optional[str] = None,
    ):
        from dvc.repo.experiments.utils import push_refspec

        self.status = TaskStatus.PREPARING
        if infofile:
            self.info.dump_json(infofile)

        with self.sshfs() as fs:
            fs.makedirs(self.root_dir)
            self._ssh_cmd(fs, "git init .")
            self._ssh_cmd(fs, "git config user.name dvc-exp")
            self._ssh_cmd(fs, "git config user.email dvc-exp@noreply.localhost")

            result = self._ssh_cmd(fs, "pwd")
            path = result.stdout.strip()
            self._repo_abspath = path

            # TODO: support multiple client key retries in git backends
            # (see https://github.com/iterative/dvc/issues/6508)
            kwargs = self._git_client_args(fs)

            ref_dict = {
                EXEC_HEAD: entry.head_rev,
                EXEC_MERGE: stash_rev,
                EXEC_BASELINE: entry.baseline_rev,
            }
            with self.set_temp_refs(scm, ref_dict):
                exec_namespace = f"{EXEC_NAMESPACE}/"
                refspec = [(exec_namespace, exec_namespace)]
                push_refspec(scm, self.git_url, refspec, **kwargs)

            if branch:
                push_refspec(scm, self.git_url, [(branch, branch)], **kwargs)
                self._ssh_cmd(fs, f"git symbolic-ref {EXEC_BRANCH} {branch}")
            else:
                self._ssh_cmd(fs, f"git symbolic-ref -d {EXEC_BRANCH}", check=False)

            # checkout EXEC_HEAD and apply EXEC_MERGE on top of it without
            # committing
            head = EXEC_BRANCH if branch else EXEC_HEAD
            self._ssh_cmd(fs, f"git checkout {head}")
            merge_rev = scm.get_ref(EXEC_MERGE)
            self._ssh_cmd(fs, f"git stash apply {merge_rev}")

            if self._setup_script:
                self._init_setup_script(fs)

    @classmethod
    def _setup_script_path(cls, dvc_dir: str):
        return posixpath.join(
            dvc_dir,
            "tmp",
            cls.SETUP_SCRIPT_FILENAME,
        )

    def _init_setup_script(self, fs: "SSHFileSystem"):
        assert self._repo_abspath
        script_path = self._setup_script_path(
            posixpath.join(self._repo_abspath, self.dvc_dir)
        )
        assert self._setup_script
        fs.put_file(self._setup_script, script_path)

    def _ssh_cmd(self, sshfs, cmd, chdir=None, **kwargs):
        working_dir = chdir or self.root_dir
        return sshfs.fs.execute(f"cd {working_dir};{cmd}", **kwargs)

    def init_cache(self, repo: "Repo", rev: str, run_cache: bool = True):
        from dvc.repo.push import push

        with self.get_odb() as odb:
            push(
                repo,
                revs=[rev],
                run_cache=run_cache,
                odb=odb,
                include_imports=True,
            )

    def collect_cache(
        self, repo: "Repo", exp_ref: "ExpRefInfo", run_cache: bool = True
    ):
        """Collect DVC cache."""
        from dvc.repo.experiments.pull import _pull_cache

        with self.get_odb() as odb:
            _pull_cache(repo, exp_ref, run_cache=run_cache, odb=odb)

    @contextmanager
    def get_odb(self):
        from dvc.cachemgr import CacheManager, get_odb

        cache_path = posixpath.join(
            self._repo_abspath,
            self.dvc_dir,
            CacheManager.CACHE_DIR,
        )

        with self.sshfs() as fs:
            yield get_odb(fs, cache_path, **fs.config)

    def fetch_exps(self, *args, **kwargs) -> Iterable[str]:
        with self.sshfs() as fs:
            kwargs.update(self._git_client_args(fs))
            return super().fetch_exps(*args, **kwargs)

    @classmethod
    def reproduce(
        cls,
        info: "ExecutorInfo",
        rev: str,  # noqa: ARG003
        queue: Optional["Queue"] = None,  # noqa: ARG003
        infofile: Optional[str] = None,
        log_errors: bool = True,
        log_level: Optional[int] = None,
        copy_paths: Optional[List[str]] = None,  # noqa: ARG003
        message: Optional[str] = None,  # noqa: ARG003
        **kwargs,
    ) -> "ExecutorResult":
        """Reproduce an experiment on a remote machine over SSH.

        Internally uses 'dvc exp exec-run' over SSH.
        """
        import json
        import time
        from tempfile import TemporaryFile

        from asyncssh import ProcessError

        fs_factory: Optional[Callable] = kwargs.pop("fs_factory", None)
        if log_errors and log_level is not None:
            cls._set_log_level(log_level)

        with _sshfs(fs_factory) as fs:
            while not fs.exists("/var/log/dvc-machine-init.log"):
                logger.info("Waiting for dvc-machine startup script to complete...")
                time.sleep(5)
            logger.info("Reproducing experiment on '%s'", fs.fs_args.get("host"))
            with TemporaryFile(mode="w+", encoding="utf-8") as fobj:
                json.dump(info.asdict(), fobj)
                fobj.seek(0)
                fs.put_file(fobj, infofile)
            cmd = ["source ~/.profile"]
            script_path = cls._setup_script_path(info.dvc_dir)
            if fs.exists(posixpath.join(info.root_dir, script_path)):
                cmd.extend([f"pushd {info.root_dir}", f"source {script_path}", "popd"])
            exec_cmd = f"dvc exp exec-run --infofile {infofile}"
            if log_level is not None:
                if log_level <= logging.TRACE:  # type: ignore[attr-defined]
                    exec_cmd += " -vv"
                elif log_level <= logging.DEBUG:
                    exec_cmd += " -v"
            cmd.append(exec_cmd)
            try:
                sys.stdout.flush()
                sys.stderr.flush()
                stdout = os.dup(sys.stdout.fileno())
                stderr = os.dup(sys.stderr.fileno())
                fs.fs.execute("; ".join(cmd), stdout=stdout, stderr=stderr)
                with fs.open(infofile) as fobj:
                    result_info = ExecutorInfo.from_dict(json.load(fobj))
                if result_info.result_hash:
                    return result_info.result
            except ProcessError:
                pass
            return ExecutorResult(None, None, False)




dvc/repo/experiments/queue/__init__.py




dvc/repo/experiments/queue/base.py
import logging
import os
from abc import ABC, abstractmethod
from dataclasses import asdict, dataclass
from typing import (
    TYPE_CHECKING,
    Any,
    Collection,
    Dict,
    Generator,
    Iterable,
    List,
    Mapping,
    NamedTuple,
    Optional,
    Type,
    Union,
)

from dvc_studio_client.post_live_metrics import get_studio_config
from funcy import retry

from dvc.dependency import ParamsDependency
from dvc.env import DVC_EXP_BASELINE_REV, DVC_EXP_NAME
from dvc.lock import LockError
from dvc.repo.experiments.exceptions import ExperimentExistsError
from dvc.repo.experiments.executor.base import BaseExecutor
from dvc.repo.experiments.executor.local import WorkspaceExecutor
from dvc.repo.experiments.refs import ExpRefInfo
from dvc.repo.experiments.stash import ExpStash, ExpStashEntry
from dvc.repo.experiments.utils import (
    EXEC_PID_DIR,
    EXEC_TMP_DIR,
    get_exp_rwlock,
    get_random_exp_name,
)
from dvc.utils.objects import cached_property
from dvc.utils.studio import config_to_env

from .utils import get_remote_executor_refs

if TYPE_CHECKING:
    from dvc.repo import Repo
    from dvc.repo.experiments import Experiments
    from dvc.repo.experiments.executor.base import ExecutorResult
    from dvc.repo.experiments.serialize import ExpRange
    from dvc.scm import Git

logger = logging.getLogger(__name__)


@dataclass(frozen=True)
class QueueEntry:
    dvc_root: str
    scm_root: str
    stash_ref: str
    stash_rev: str
    baseline_rev: str
    branch: Optional[str]
    name: Optional[str]
    head_rev: Optional[str] = None

    def __eq__(self, other: object):
        return (
            isinstance(other, QueueEntry)
            and self.dvc_root == other.dvc_root
            and self.scm_root == other.scm_root
            and self.stash_ref == other.stash_ref
            and self.stash_rev == other.stash_rev
        )

    def asdict(self) -> Dict[str, Any]:
        return asdict(self)

    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> "QueueEntry":
        return cls(**d)


class QueueGetResult(NamedTuple):
    entry: QueueEntry
    executor: BaseExecutor


class QueueDoneResult(NamedTuple):
    entry: QueueEntry
    result: Optional["ExecutorResult"]


class ExpRefAndQueueEntry(NamedTuple):
    exp_ref_info: Optional["ExpRefInfo"]
    queue_entry: Optional["QueueEntry"]


class BaseStashQueue(ABC):
    """Naive Git-stash based experiment queue.

    Maps queued experiments to (Git) stash reflog entries.
    """

    def __init__(self, repo: "Repo", ref: str, failed_ref: Optional[str] = None):
        """Construct a queue.

        Arguments:
            scm: Git SCM instance for this queue.
            ref: Git stash ref for this queue.
            failed_ref: Failed run Git stash ref for this queue.
        """
        self.repo = repo
        assert self.repo.tmp_dir
        self.ref = ref
        self.failed_ref = failed_ref

    @property
    def scm(self) -> "Git":
        from dvc.scm import Git

        assert isinstance(self.repo.scm, Git)
        return self.repo.scm

    @cached_property
    def stash(self) -> ExpStash:
        return ExpStash(self.scm, self.ref)

    @cached_property
    def failed_stash(self) -> Optional[ExpStash]:
        return ExpStash(self.scm, self.failed_ref) if self.failed_ref else None

    @cached_property
    def pid_dir(self) -> str:
        assert self.repo.tmp_dir is not None
        return os.path.join(self.repo.tmp_dir, EXEC_TMP_DIR, EXEC_PID_DIR)

    @cached_property
    def args_file(self) -> str:
        assert self.repo.tmp_dir is not None
        return os.path.join(self.repo.tmp_dir, BaseExecutor.PACKED_ARGS_FILE)

    @abstractmethod
    def put(self, *args, **kwargs) -> QueueEntry:
        """Stash an experiment and add it to the queue."""

    @abstractmethod
    def get(self) -> QueueGetResult:
        """Pop and return the first item in the queue."""

    def remove(
        self,
        revs: Collection[str],
        all_: bool = False,
        queued: bool = False,
        **kwargs,
    ) -> List[str]:
        """Remove the specified entries from the queue.

        Arguments:
            revs: Stash revisions or queued exp names to be removed.
            queued: Remove all queued tasks.
            all: Remove all tasks.

        Returns:
            Revisions (or names) which were removed.
        """

        if all_ or queued:
            return self.clear()

        name_to_remove: List[str] = []
        entry_to_remove: List[ExpStashEntry] = []
        queue_entries = self.match_queue_entry_by_name(revs, self.iter_queued())
        for name, entry in queue_entries.items():
            if entry:
                entry_to_remove.append(self.stash.stash_revs[entry.stash_rev])
                name_to_remove.append(name)

        self.stash.remove_revs(entry_to_remove)
        return name_to_remove

    def clear(self, **kwargs) -> List[str]:
        """Remove all entries from the queue."""
        stash_revs = self.stash.stash_revs
        name_to_remove = list(stash_revs)
        self.stash.remove_revs(list(stash_revs.values()))

        return name_to_remove

    def status(self) -> List[Dict[str, Any]]:
        """Show the status of exp tasks in queue"""
        from datetime import datetime

        result: List[Dict[str, Optional[str]]] = []

        def _get_timestamp(rev: str) -> datetime:
            commit = self.scm.resolve_commit(rev)
            return datetime.fromtimestamp(commit.commit_time)

        def _format_entry(
            entry: QueueEntry,
            exp_result: Optional["ExecutorResult"] = None,
            status: str = "Unknown",
        ) -> Dict[str, Any]:
            name = entry.name
            if not name and exp_result and exp_result.ref_info:
                name = exp_result.ref_info.name
            # NOTE: We fallback to Unknown status for experiments
            # generated in prior (incompatible) DVC versions
            return {
                "rev": entry.stash_rev,
                "name": name,
                "timestamp": _get_timestamp(entry.stash_rev),
                "status": status,
            }

        result.extend(
            _format_entry(queue_entry, status="Running")
            for queue_entry in self.iter_active()
        )
        result.extend(
            _format_entry(queue_entry, status="Queued")
            for queue_entry in self.iter_queued()
        )
        result.extend(
            _format_entry(queue_entry, status="Failed")
            for queue_entry, _ in self.iter_failed()
        )
        result.extend(
            _format_entry(queue_entry, exp_result=exp_result, status="Success")
            for queue_entry, exp_result in self.iter_success()
        )
        return result

    @abstractmethod
    def iter_queued(self) -> Generator[QueueEntry, None, None]:
        """Iterate over items in the queue."""

    @abstractmethod
    def iter_active(self) -> Generator[QueueEntry, None, None]:
        """Iterate over items which are being actively processed."""

    @abstractmethod
    def iter_done(self) -> Generator[QueueDoneResult, None, None]:
        """Iterate over items which been processed."""

    @abstractmethod
    def iter_success(self) -> Generator[QueueDoneResult, None, None]:
        """Iterate over items which been success."""

    @abstractmethod
    def iter_failed(self) -> Generator[QueueDoneResult, None, None]:
        """Iterate over items which been failed."""

    @abstractmethod
    def reproduce(
        self, copy_paths: Optional[List[str]] = None, message: Optional[str] = None
    ) -> Mapping[str, Mapping[str, str]]:
        """Reproduce queued experiments sequentially."""

    @abstractmethod
    def get_result(self, entry: QueueEntry) -> Optional["ExecutorResult"]:
        """Return result of the specified item.

        This method blocks until the specified item has been collected.
        """

    @abstractmethod
    def kill(self, revs: str) -> None:
        """Kill the specified running entries in the queue.

        Arguments:
            revs: Stash revs or running exp name to be killed.
        """

    @abstractmethod
    def shutdown(self, kill: bool = False):
        """Shutdown the queue worker.

        Arguments:
            kill: If True, the any active experiments will be killed and the
                worker will shutdown immediately. If False, the worker will
                finish any active experiments before shutting down.
        """

    @abstractmethod
    def logs(
        self,
        rev: str,
        encoding: Optional[str] = None,
        follow: bool = False,
    ):
        """Print redirected output logs for an exp process.

        Args:
            rev: Stash rev or exp name.
            encoding: Text encoding for redirected output. Defaults to
                `locale.getpreferredencoding()`.
            follow: Attach to running exp process and follow additional
                output.
        """

    def _stash_exp(  # noqa: C901
        self,
        *args,
        params: Optional[Dict[str, List[str]]] = None,
        baseline_rev: Optional[str] = None,
        branch: Optional[str] = None,
        name: Optional[str] = None,
        **kwargs,
    ) -> QueueEntry:
        """Stash changes from the workspace as an experiment.

        Args:
            params: Dict mapping paths to `Hydra Override`_ patterns,
                provided via `exp run --set-param`.
            baseline_rev: Optional baseline rev for this experiment, defaults
                to the current SCM rev.
            branch: Optional experiment branch name. If specified, the
                experiment will be added to `branch` instead of creating
                a new branch.
            name: Optional experiment name. If specified this will be used as
                the human-readable name in the experiment branch ref. Has no
                effect of branch is specified.

        .. _Hydra Override:
            https://hydra.cc/docs/next/advanced/override_grammar/basic/
        """
        with self.scm.stash_workspace(reinstate_index=True) as workspace:
            with self.scm.detach_head(client="dvc") as orig_head:
                stash_head = orig_head
                if baseline_rev is None:
                    baseline_rev = orig_head

                try:
                    if workspace:
                        self.stash.apply(workspace)

                    # update experiment params from command line
                    if params:
                        self._update_params(params)

                    # DVC commit data deps to preserve state across workspace
                    # & tempdir runs
                    self._stash_commit_deps(*args, **kwargs)

                    # save additional repro command line arguments
                    run_env = {
                        DVC_EXP_BASELINE_REV: baseline_rev,
                    }
                    if not name:
                        name = get_random_exp_name(self.scm, baseline_rev)
                    run_env[DVC_EXP_NAME] = name

                    # save studio config to read later by dvc and dvclive
                    studio_config = get_studio_config(
                        dvc_studio_config=self.repo.config.get("studio")
                    )
                    run_env = {**config_to_env(studio_config), **run_env}
                    self._pack_args(*args, run_env=run_env, **kwargs)
                    # save experiment as a stash commit
                    msg = self._stash_msg(
                        stash_head,
                        baseline_rev=baseline_rev,
                        branch=branch,
                        name=name,
                    )
                    stash_rev = self.stash.push(message=msg)
                    assert stash_rev
                    logger.debug(
                        (
                            "Stashed experiment '%s' with baseline '%s' "
                            "for future execution."
                        ),
                        stash_rev[:7],
                        baseline_rev[:7],
                    )
                finally:
                    # Revert any of our changes before prior unstashing
                    self.scm.reset(hard=True)

        return QueueEntry(
            self.repo.root_dir,
            self.scm.root_dir,
            self.ref,
            stash_rev,
            baseline_rev,
            branch,
            name,
            stash_head,
        )

    def _stash_commit_deps(self, *args, **kwargs):
        if len(args):
            targets = args[0]
        else:
            targets = kwargs.get("targets")
        if isinstance(targets, str):
            targets = [targets]
        elif not targets:
            targets = [None]
        for target in targets:
            self.repo.commit(
                target,
                with_deps=True,
                recursive=kwargs.get("recursive", False),
                force=True,
                allow_missing=True,
                data_only=True,
                relink=False,
            )

    @staticmethod
    def _stash_msg(
        rev: str,
        baseline_rev: str,
        branch: Optional[str] = None,
        name: Optional[str] = None,
    ) -> str:
        if not baseline_rev:
            baseline_rev = rev
        msg = ExpStash.format_message(rev, baseline_rev, name)
        if branch:
            return f"{msg}:{branch}"
        return msg

    def _pack_args(self, *args, **kwargs) -> None:
        import pickle  # nosec B403

        if os.path.exists(self.args_file) and self.scm.is_tracked(self.args_file):
            logger.warning(
                (
                    "Temporary DVC file '.dvc/tmp/%s' exists and was "
                    "likely committed to Git by mistake. It should be removed "
                    "with:\n"
                    "\tgit rm .dvc/tmp/%s"
                ),
                BaseExecutor.PACKED_ARGS_FILE,
                BaseExecutor.PACKED_ARGS_FILE,
            )
            with open(self.args_file, "rb") as fobj:
                try:
                    data = pickle.load(fobj)  # noqa: S301  # nosec B301
                except Exception:  # noqa: BLE001, pylint: disable=broad-except
                    data = {}
            extra = int(data.get("extra", 0)) + 1
        else:
            extra = None
        BaseExecutor.pack_repro_args(self.args_file, *args, extra=extra, **kwargs)
        self.scm.add(self.args_file, force=True)

    @staticmethod
    def _format_new_params_msg(new_params, config_path):
        """Format an error message for when new parameters are identified"""
        new_param_count = len(new_params)
        pluralise = "s are" if new_param_count > 1 else " is"
        param_list = ", ".join(new_params)
        return (
            f"{new_param_count} parameter{pluralise} missing "
            f"from '{config_path}': {param_list}"
        )

    def _update_params(self, params: Dict[str, List[str]]):
        """Update param files with the provided `Hydra Override`_ patterns.

        Args:
            params: Dict mapping paths to `Hydra Override`_ patterns,
                provided via `exp run --set-param`.

        .. _Hydra Override:
            https://hydra.cc/docs/advanced/override_grammar/basic/
        """
        from dvc.utils.hydra import apply_overrides, compose_and_dump

        logger.debug("Using experiment params '%s'", params)

        hydra_config = self.repo.config.get("hydra", {})
        hydra_enabled = hydra_config.get("enabled", False)
        hydra_output_file = ParamsDependency.DEFAULT_PARAMS_FILE
        for path, overrides in params.items():
            if hydra_enabled and path == hydra_output_file:
                config_dir = os.path.join(
                    self.repo.root_dir, hydra_config.get("config_dir", "conf")
                )
                config_name = hydra_config.get("config_name", "config")
                compose_and_dump(
                    path,
                    config_dir,
                    config_name,
                    overrides,
                )
            else:
                apply_overrides(path, overrides)

        # Force params file changes to be staged in git
        # Otherwise in certain situations the changes to params file may be
        # ignored when we `git stash` them since mtime is used to determine
        # whether the file is dirty
        self.scm.add(list(params.keys()))

    @staticmethod
    @retry(180, errors=LockError, timeout=1)
    def get_stash_entry(
        exp: "Experiments",
        queue_entry: QueueEntry,
    ) -> "ExpStashEntry":
        stash = ExpStash(exp.scm, queue_entry.stash_ref)
        stash_rev = queue_entry.stash_rev
        with get_exp_rwlock(exp.repo, writes=[queue_entry.stash_ref]):
            stash_entry = stash.stash_revs.get(
                stash_rev,
                ExpStashEntry(None, stash_rev, stash_rev, None, None),
            )
            if stash_entry.stash_index is not None:
                stash.drop(stash_entry.stash_index)
        return stash_entry

    @classmethod
    def init_executor(
        cls,
        exp: "Experiments",
        queue_entry: QueueEntry,
        executor_cls: Type[BaseExecutor] = WorkspaceExecutor,
        **kwargs,
    ) -> BaseExecutor:
        stash_entry = cls.get_stash_entry(exp, queue_entry)

        executor = executor_cls.from_stash_entry(exp.repo, stash_entry, **kwargs)

        stash_rev = queue_entry.stash_rev
        infofile = exp.celery_queue.get_infofile_path(stash_rev)
        executor.init_git(
            exp.repo,
            exp.repo.scm,
            stash_rev,
            stash_entry,
            infofile,
            branch=stash_entry.branch,
        )

        executor.init_cache(exp.repo, stash_rev)

        return executor

    def get_infofile_path(self, name: str) -> str:
        return os.path.join(
            self.pid_dir,
            name,
            f"{name}{BaseExecutor.INFOFILE_EXT}",
        )

    @staticmethod
    @retry(180, errors=LockError, timeout=1)
    def collect_git(
        exp: "Experiments",
        executor: BaseExecutor,
        exec_result: "ExecutorResult",
    ) -> Dict[str, str]:
        results = {}

        def on_diverged(ref: str):
            ref_info = ExpRefInfo.from_ref(ref)
            raise ExperimentExistsError(ref_info.name)

        refs = get_remote_executor_refs(exp.scm, executor.git_url)

        with get_exp_rwlock(exp.repo, writes=refs):
            for ref in executor.fetch_exps(
                exp.scm,
                refs,
                force=exec_result.force,
                on_diverged=on_diverged,
            ):
                exp_rev = exp.scm.get_ref(ref)
                if exp_rev:
                    assert exec_result.exp_hash
                    logger.debug("Collected experiment '%s'.", exp_rev[:7])
                    results[exp_rev] = exec_result.exp_hash

        return results

    @classmethod
    def collect_executor(
        cls,
        exp: "Experiments",
        executor: BaseExecutor,
        exec_result: "ExecutorResult",
    ) -> Dict[str, str]:
        results = cls.collect_git(exp, executor, exec_result)

        if exec_result.ref_info is not None:
            executor.collect_cache(exp.repo, exec_result.ref_info)

        return results

    def match_queue_entry_by_name(
        self,
        exp_names: Collection[str],
        *entries: Iterable[Union[QueueEntry, QueueDoneResult]],
    ) -> Dict[str, Optional[QueueEntry]]:
        from funcy import concat

        entry_name_dict: Dict[str, QueueEntry] = {}
        entry_rev_dict: Dict[str, QueueEntry] = {}
        for entry in concat(*entries):
            if isinstance(entry, QueueDoneResult):
                queue_entry: QueueEntry = entry.entry
                if entry.result is not None and entry.result.ref_info is not None:
                    name: Optional[str] = entry.result.ref_info.name
                else:
                    name = queue_entry.name
            else:
                queue_entry = entry
                name = queue_entry.name
            if name:
                entry_name_dict[name] = queue_entry
            entry_rev_dict[queue_entry.stash_rev] = queue_entry

        result: Dict[str, Optional[QueueEntry]] = {}
        for exp_name in exp_names:
            result[exp_name] = None
            if exp_name in entry_name_dict:
                result[exp_name] = entry_name_dict[exp_name]
                continue
            if self.scm.is_sha(exp_name):
                for rev in entry_rev_dict:
                    if rev.startswith(exp_name.lower()):
                        result[exp_name] = entry_rev_dict[rev]
                        break

        return result

    def stash_failed(self, entry: QueueEntry) -> None:
        """Add an entry to the failed exp stash.

        Arguments:
            entry: Failed queue entry to add. ``entry.stash_rev`` must be a
                valid Git stash commit.
        """
        if self.failed_stash is not None:
            assert entry.head_rev
            logger.debug("Stashing failed exp '%s'", entry.stash_rev[:7])
            msg = self.failed_stash.format_message(
                entry.head_rev,
                baseline_rev=entry.baseline_rev,
                name=entry.name,
                branch=entry.branch,
            )
            self.scm.set_ref(
                self.failed_stash.ref,
                entry.stash_rev,
                message=f"commit: {msg}",
            )

    @abstractmethod
    def collect_active_data(
        self,
        baseline_revs: Optional[Collection[str]],
        fetch_refs: bool = False,
        **kwargs,
    ) -> Dict[str, List["ExpRange"]]:
        """Collect data for active (running) experiments.

        Args:
            baseline_revs: Optional resolved baseline Git SHAs. If set, only experiments
                derived from the specified revisions will be collected. Defaults to
                collecting all experiments.
            fetch_refs: Whether or not to fetch completed checkpoint commits from Git
                remote.

        Returns:
            Dict mapping baseline revision to list of active experiments.
        """

    @abstractmethod
    def collect_queued_data(
        self,
        baseline_revs: Optional[Collection[str]],
        **kwargs,
    ) -> Dict[str, List["ExpRange"]]:
        """Collect data for queued experiments.

        Args:
            baseline_revs: Optional resolved baseline Git SHAs. If set, only experiments
                derived from the specified revisions will be collected. Defaults to
                collecting all experiments.

        Returns:
            Dict mapping baseline revision to list of queued experiments.
        """

    @abstractmethod
    def collect_failed_data(
        self,
        baseline_revs: Optional[Collection[str]],
        **kwargs,
    ) -> Dict[str, List["ExpRange"]]:
        """Collect data for failed experiments.

        Args:
            baseline_revs: Optional resolved baseline Git SHAs. If set, only experiments
                derived from the specified revisions will be collected. Defaults to
                collecting all experiments.

        Returns:
            Dict mapping baseline revision to list of queued experiments.
        """

    def active_repo(self, name: str) -> "Repo":
        """Return a Repo for the specified active experiment if it exists."""
        from dvc.exceptions import DvcException
        from dvc.repo import Repo
        from dvc.repo.experiments.exceptions import (
            ExpNotStartedError,
            InvalidExpRevError,
        )
        from dvc.repo.experiments.executor.base import ExecutorInfo, TaskStatus

        for entry in self.iter_active():
            if entry.name != name:
                continue
            infofile = self.get_infofile_path(entry.stash_rev)
            executor_info = ExecutorInfo.load_json(infofile)
            if executor_info.status < TaskStatus.RUNNING:
                raise ExpNotStartedError(name)
            dvc_root = os.path.join(executor_info.root_dir, executor_info.dvc_dir)
            try:
                return Repo(dvc_root)
            except (FileNotFoundError, DvcException) as exc:
                raise InvalidExpRevError(name) from exc
        raise InvalidExpRevError(name)




dvc/repo/experiments/queue/celery.py
import hashlib
import locale
import logging
import os
from collections import defaultdict
from typing import (
    TYPE_CHECKING,
    Collection,
    Dict,
    Generator,
    List,
    Mapping,
    NamedTuple,
    Optional,
    Set,
    Union,
)

from celery.result import AsyncResult
from funcy import first

from dvc.daemon import daemonize
from dvc.exceptions import DvcException
from dvc.repo.experiments.exceptions import (
    UnresolvedQueueExpNamesError,
    UnresolvedRunningExpNamesError,
)
from dvc.repo.experiments.executor.base import ExecutorInfo
from dvc.repo.experiments.refs import CELERY_STASH
from dvc.repo.experiments.utils import EXEC_TMP_DIR, get_exp_rwlock
from dvc.ui import ui
from dvc.utils.objects import cached_property

from .base import BaseStashQueue, ExpRefAndQueueEntry, QueueDoneResult, QueueEntry
from .exceptions import CannotKillTasksError
from .tasks import run_exp
from .utils import fetch_running_exp_from_temp_dir

if TYPE_CHECKING:
    from kombu.message import Message

    from dvc.repo.experiments.executor.base import ExecutorResult
    from dvc.repo.experiments.refs import ExpRefInfo
    from dvc.repo.experiments.serialize import ExpRange
    from dvc_task.app import FSApp
    from dvc_task.proc.manager import ProcessManager
    from dvc_task.worker import TemporaryWorker

    from .base import QueueGetResult

logger = logging.getLogger(__name__)


class _MessageEntry(NamedTuple):
    msg: "Message"
    entry: QueueEntry


class _TaskEntry(NamedTuple):
    async_result: AsyncResult
    entry: QueueEntry


class LocalCeleryQueue(BaseStashQueue):
    """DVC experiment queue.

    Maps queued experiments to (Git) stash reflog entries.
    """

    CELERY_DIR = "celery"

    @cached_property
    def wdir(self) -> str:
        assert self.repo.tmp_dir is not None
        return os.path.join(self.repo.tmp_dir, EXEC_TMP_DIR, self.CELERY_DIR)

    @cached_property
    def celery(self) -> "FSApp":
        from kombu.transport.filesystem import Channel

        # related to https://github.com/iterative/dvc-task/issues/61
        Channel.QoS.restore_at_shutdown = False

        from dvc_task.app import FSApp

        app = FSApp(
            "dvc-exp-local",
            wdir=self.wdir,
            mkdir=True,
            include=[
                "dvc.repo.experiments.queue.tasks",
                "dvc_task.proc.tasks",
            ],
        )
        app.conf.update({"task_acks_late": True, "result_expires": None})
        return app

    @cached_property
    def proc(self) -> "ProcessManager":
        from dvc_task.proc.manager import ProcessManager

        return ProcessManager(self.pid_dir)

    @cached_property
    def worker(self) -> "TemporaryWorker":
        from dvc_task.worker import TemporaryWorker

        # NOTE: Use thread pool with concurrency 1 and disabled prefetch.
        # Worker scaling should be handled by running additional workers,
        # rather than increasing pool concurrency.
        #
        # We use "threads" over "solo" (inline single-threaded) execution so
        # that we still have access to the control/broadcast API (which
        # requires a separate message handling thread in the worker).
        #
        # Disabled prefetch ensures that each worker will can only schedule and
        # execute up to one experiment at a time (and a worker cannot prefetch
        # additional experiments from the queue).
        return TemporaryWorker(
            self.celery,
            pool="threads",
            concurrency=1,
            prefetch_multiplier=1,
            without_heartbeat=True,
            without_mingle=True,
            without_gossip=True,
            timeout=10,
            loglevel="debug" if logger.getEffectiveLevel() <= logging.DEBUG else "info",
        )

    def _spawn_worker(self, num: int = 1):
        """spawn one single worker to process to queued tasks.

        Argument:
            num: serial number of the worker.

        """
        from dvc_task.proc.process import ManagedProcess

        logger.debug("Spawning exp queue worker")
        wdir_hash = hashlib.sha256(self.wdir.encode("utf-8")).hexdigest()[:6]
        node_name = f"dvc-exp-{wdir_hash}-{num}@localhost"
        cmd = ["exp", "queue-worker", node_name]
        if num == 1:
            # automatically run celery cleanup when primary worker shuts down
            cmd.append("--clean")
        if logger.getEffectiveLevel() <= logging.DEBUG:
            cmd.append("-v")
        name = f"dvc-exp-worker-{num}"

        logger.debug("start a new worker: %s, node: %s", name, node_name)
        if os.name == "nt":
            daemonize(cmd)
        else:
            ManagedProcess.spawn(["dvc", *cmd], wdir=self.wdir, name=name)

    def start_workers(self, count: int) -> int:
        """start some workers to process the queued tasks.

        Argument:
            count: worker number to be started.

        Returns:
            newly spawned worker number.
        """

        logger.debug("Spawning %s exp queue workers", count)
        active_worker: Dict = self.worker_status()

        started = 0
        for num in range(1, 1 + count):
            wdir_hash = hashlib.sha256(self.wdir.encode("utf-8")).hexdigest()[:6]
            node_name = f"dvc-exp-{wdir_hash}-{num}@localhost"
            if node_name in active_worker:
                logger.debug("Exp queue worker %s already exist", node_name)
                continue
            self._spawn_worker(num)
            started += 1

        return started

    def put(
        self,
        *args,
        copy_paths: Optional[List[str]] = None,
        message: Optional[str] = None,
        **kwargs,
    ) -> QueueEntry:
        """Stash an experiment and add it to the queue."""
        with get_exp_rwlock(self.repo, writes=["workspace", CELERY_STASH]):
            entry = self._stash_exp(*args, **kwargs)
        self.celery.signature(
            run_exp.s(entry.asdict(), copy_paths=copy_paths, message=message)
        ).delay()
        return entry

    # NOTE: Queue consumption should not be done directly. Celery worker(s)
    # will automatically consume available experiments.
    def get(self) -> "QueueGetResult":
        raise NotImplementedError

    def iter_queued(self) -> Generator[QueueEntry, None, None]:
        for _, entry in self._iter_queued():
            yield entry

    def _iter_queued(self) -> Generator[_MessageEntry, None, None]:
        for msg in self.celery.iter_queued():
            if msg.headers.get("task") != run_exp.name:
                continue
            args, kwargs, _embed = msg.decode()
            entry_dict = kwargs.get("entry_dict", args[0])
            logger.trace(  # type: ignore[attr-defined]
                "Found queued task %s", entry_dict["stash_rev"]
            )
            yield _MessageEntry(msg, QueueEntry.from_dict(entry_dict))

    def _iter_processed(self) -> Generator[_MessageEntry, None, None]:
        for msg in self.celery.iter_processed():
            if msg.headers.get("task") != run_exp.name:
                continue
            args, kwargs, _embed = msg.decode()
            entry_dict = kwargs.get("entry_dict", args[0])
            yield _MessageEntry(msg, QueueEntry.from_dict(entry_dict))

    def _iter_active_tasks(self) -> Generator[_TaskEntry, None, None]:
        for msg, entry in self._iter_processed():
            task_id = msg.headers["id"]
            result: AsyncResult = AsyncResult(task_id)
            if not result.ready():
                logger.trace(  # type: ignore[attr-defined]
                    "Found active task %s", entry.stash_rev
                )
                yield _TaskEntry(result, entry)

    def _iter_done_tasks(self) -> Generator[_TaskEntry, None, None]:
        for msg, entry in self._iter_processed():
            task_id = msg.headers["id"]
            result: AsyncResult = AsyncResult(task_id)
            if result.ready():
                logger.trace(  # type: ignore[attr-defined]
                    "Found done task %s", entry.stash_rev
                )
                yield _TaskEntry(result, entry)

    def iter_active(self) -> Generator[QueueEntry, None, None]:
        for _, entry in self._iter_active_tasks():
            yield entry

    def iter_done(self) -> Generator[QueueDoneResult, None, None]:
        for result, entry in self._iter_done_tasks():
            try:
                exp_result = self.get_result(entry)
            except FileNotFoundError:
                if result.status == "SUCCESS":
                    raise DvcException(  # noqa: B904
                        f"Invalid experiment '{entry.stash_rev[:7]}'."
                    )
                if result.status == "FAILURE":
                    exp_result = None
            yield QueueDoneResult(entry, exp_result)

    def iter_success(self) -> Generator[QueueDoneResult, None, None]:
        for queue_entry, exp_result in self.iter_done():
            if exp_result and exp_result.exp_hash and exp_result.ref_info:
                yield QueueDoneResult(queue_entry, exp_result)

    def iter_failed(self) -> Generator[QueueDoneResult, None, None]:
        for queue_entry, exp_result in self.iter_done():
            if exp_result is None:
                yield QueueDoneResult(queue_entry, exp_result)

    def reproduce(
        self, copy_paths: Optional[List[str]] = None, message: Optional[str] = None
    ) -> Mapping[str, Mapping[str, str]]:
        raise NotImplementedError

    def _load_info(self, rev: str) -> ExecutorInfo:
        infofile = self.get_infofile_path(rev)
        return ExecutorInfo.load_json(infofile)

    def _get_done_result(
        self, entry: QueueEntry, timeout: Optional[float] = None
    ) -> Optional["ExecutorResult"]:
        from celery.exceptions import TimeoutError as _CeleryTimeout

        for msg, processed_entry in self._iter_processed():
            if entry.stash_rev == processed_entry.stash_rev:
                task_id = msg.headers["id"]
                result: AsyncResult = AsyncResult(task_id)
                if not result.ready():
                    logger.debug("Waiting for exp task '%s' to complete", result.id)
                    try:
                        result.get(timeout=timeout)
                    except _CeleryTimeout as exc:
                        raise DvcException(
                            "Timed out waiting for exp to finish."
                        ) from exc
                executor_info = self._load_info(entry.stash_rev)
                return executor_info.result
        raise FileNotFoundError

    def get_result(
        self, entry: QueueEntry, timeout: Optional[float] = None
    ) -> Optional["ExecutorResult"]:
        try:
            return self._get_done_result(entry, timeout)
        except FileNotFoundError:
            pass

        for queue_entry in self.iter_queued():
            if entry.stash_rev == queue_entry.stash_rev:
                raise DvcException("Experiment has not been started.")

        # NOTE: It's possible for an exp to complete while iterating through
        # other queued and active tasks, in which case the exp will get moved
        # out of the active task list, and needs to be loaded here.
        return self._get_done_result(entry, timeout)

    def wait(self, revs: Collection[str], **kwargs) -> None:
        """Block until the specified tasks have completed."""
        revs = [revs] if isinstance(revs, str) else revs
        results = self.match_queue_entry_by_name(
            revs, self.iter_queued(), self.iter_done(), self.iter_failed()
        )
        for entry in results.values():
            if not entry:
                continue
            self.wait_for_start(entry, **kwargs)
            try:
                self.get_result(entry)
            except FileNotFoundError:
                pass

    def wait_for_start(self, entry: QueueEntry, sleep_interval: float = 0.001) -> None:
        """Block until the specified task has been started."""
        import time

        while not self.proc.get(entry.stash_rev):
            time.sleep(sleep_interval)

    def _get_running_task_ids(self) -> Set[str]:
        running_task_ids: Set[str] = set()
        active_workers = self.worker_status()
        for _, tasks in active_workers.items():
            task = first(tasks)
            if task:
                running_task_ids.add(task["id"])
        return running_task_ids

    def _try_to_kill_tasks(
        self, to_kill: Dict[QueueEntry, str], force: bool
    ) -> Dict[QueueEntry, str]:
        fail_to_kill_entries: Dict[QueueEntry, str] = {}
        for queue_entry, rev in to_kill.items():
            try:
                if force:
                    self.proc.kill(queue_entry.stash_rev)
                else:
                    self.proc.interrupt(queue_entry.stash_rev)
                ui.write(f"{rev} has been killed.")
            except ProcessLookupError:
                fail_to_kill_entries[queue_entry] = rev
        return fail_to_kill_entries

    def _mark_inactive_tasks_failure(
        self, remained_entries: Dict[QueueEntry, str]
    ) -> None:
        remained_revs: List[str] = []
        running_ids = self._get_running_task_ids()
        logger.debug("Current running tasks ids: %s.", running_ids)
        for msg, entry in self._iter_processed():
            if entry not in remained_entries:
                continue
            task_id = msg.headers["id"]
            if task_id in running_ids:
                remained_revs.append(remained_entries[entry])
            else:
                result: AsyncResult = AsyncResult(task_id)
                if not result.ready():
                    logger.debug(
                        "Task id %s rev %s marked as failure.",
                        task_id,
                        remained_entries[entry],
                    )
                    backend = self.celery.backend
                    backend.mark_as_failure(task_id, None)  # type: ignore[attr-defined]

        if remained_revs:
            raise CannotKillTasksError(remained_revs)

    def _kill_entries(self, entries: Dict[QueueEntry, str], force: bool) -> None:
        logger.debug(
            "Found active tasks: '%s' to kill",
            list(entries.values()),
        )
        inactive_entries: Dict[QueueEntry, str] = self._try_to_kill_tasks(
            entries, force
        )

        if inactive_entries:
            self._mark_inactive_tasks_failure(inactive_entries)

    def kill(self, revs: Collection[str], force: bool = False) -> None:
        name_dict: Dict[str, Optional[QueueEntry]] = self.match_queue_entry_by_name(
            set(revs), self.iter_active()
        )

        missing_revs: List[str] = []
        to_kill: Dict[QueueEntry, str] = {}
        for rev, queue_entry in name_dict.items():
            if queue_entry is None:
                missing_revs.append(rev)
            else:
                to_kill[queue_entry] = rev

        if to_kill:
            self._kill_entries(to_kill, force)

        if missing_revs:
            raise UnresolvedRunningExpNamesError(missing_revs)

    def shutdown(self, kill: bool = False):
        self.celery.control.shutdown()
        if kill:
            to_kill: Dict[QueueEntry, str] = {}
            for entry in self.iter_active():
                to_kill[entry] = entry.name or entry.stash_rev
            if to_kill:
                self._kill_entries(to_kill, True)

    def follow(
        self,
        entry: QueueEntry,
        encoding: Optional[str] = None,
    ):
        for line in self.proc.follow(entry.stash_rev, encoding):
            ui.write(line, end="")

    def logs(
        self,
        rev: str,
        encoding: Optional[str] = None,
        follow: bool = False,
    ):
        queue_entry: Optional[QueueEntry] = self.match_queue_entry_by_name(
            {rev}, self.iter_active(), self.iter_done()
        ).get(rev)
        if queue_entry is None:
            if self.match_queue_entry_by_name({rev}, self.iter_queued()).get(rev):
                raise DvcException(
                    f"Experiment '{rev}' is in queue but has not been started"
                )
            raise UnresolvedQueueExpNamesError([rev])
        if follow:
            ui.write(
                f"Following logs for experiment '{rev}'. Use Ctrl+C to stop "
                "following logs (experiment execution will continue).\n"
            )
            try:
                self.follow(queue_entry)
            except KeyboardInterrupt:
                pass
            return
        try:
            proc_info = self.proc[queue_entry.stash_rev]
        except KeyError:
            raise DvcException(  # noqa: B904
                f"No output logs found for experiment '{rev}'"
            )
        with open(
            proc_info.stdout,
            encoding=encoding or locale.getpreferredencoding(),
        ) as fobj:
            ui.write(fobj.read())

    def worker_status(self) -> Dict[str, List[Dict]]:
        """Return the current active celery worker"""
        status = self.celery.control.inspect().active() or {}
        logger.debug("Worker status: %s", status)
        return status

    def clear(self, *args, **kwargs):
        from .remove import celery_clear

        return celery_clear(self, *args, **kwargs)

    def remove(self, *args, **kwargs):
        from .remove import celery_remove

        return celery_remove(self, *args, **kwargs)

    def get_ref_and_entry_by_names(
        self,
        exp_names: Union[str, List[str]],
        git_remote: Optional[str] = None,
    ) -> Dict[str, ExpRefAndQueueEntry]:
        """Find finished ExpRefInfo or queued or failed QueueEntry by name"""
        from dvc.repo.experiments.utils import resolve_name

        if isinstance(exp_names, str):
            exp_names = [exp_names]
        results: Dict[str, ExpRefAndQueueEntry] = {}

        exp_ref_match: Dict[str, Optional["ExpRefInfo"]] = resolve_name(
            self.scm, exp_names, git_remote
        )
        if not git_remote:
            queue_entry_match: Dict[
                str, Optional["QueueEntry"]
            ] = self.match_queue_entry_by_name(
                exp_names, self.iter_queued(), self.iter_done()
            )

        for exp_name in exp_names:
            exp_ref = exp_ref_match[exp_name]
            queue_entry = None if git_remote else queue_entry_match[exp_name]
            results[exp_name] = ExpRefAndQueueEntry(exp_ref, queue_entry)
        return results

    def collect_active_data(
        self,
        baseline_revs: Optional[Collection[str]],
        fetch_refs: bool = False,
        **kwargs,
    ) -> Dict[str, List["ExpRange"]]:
        from dvc.repo import Repo
        from dvc.repo.experiments.collect import collect_exec_branch
        from dvc.repo.experiments.serialize import (
            ExpExecutor,
            ExpRange,
            LocalExpExecutor,
        )

        result: Dict[str, List[ExpRange]] = defaultdict(list)
        for entry in self.iter_active():
            if baseline_revs and entry.baseline_rev not in baseline_revs:
                continue
            if fetch_refs:
                fetch_running_exp_from_temp_dir(self, entry.stash_rev, fetch_refs)
            proc_info = self.proc.get(entry.stash_rev)
            executor_info = self._load_info(entry.stash_rev)
            if proc_info:
                local_exec: Optional[LocalExpExecutor] = LocalExpExecutor(
                    root=executor_info.root_dir,
                    log=proc_info.stdout,
                    pid=proc_info.pid,
                    task_id=entry.stash_rev,
                )
            else:
                local_exec = None
            dvc_root = os.path.join(executor_info.root_dir, executor_info.dvc_dir)
            with Repo(dvc_root) as exec_repo:
                kwargs["cache"] = self.repo.experiments.cache
                exps = list(
                    collect_exec_branch(exec_repo, executor_info.baseline_rev, **kwargs)
                )
            exps[0].rev = entry.stash_rev
            exps[0].name = entry.name
            result[entry.baseline_rev].append(
                ExpRange(
                    exps,
                    executor=ExpExecutor(
                        "running",
                        name=executor_info.location,
                        local=local_exec,
                    ),
                    name=entry.name,
                )
            )
        return result

    def collect_queued_data(
        self,
        baseline_revs: Optional[Collection[str]],
        **kwargs,
    ) -> Dict[str, List["ExpRange"]]:
        from dvc.repo.experiments.collect import collect_rev
        from dvc.repo.experiments.serialize import (
            ExpExecutor,
            ExpRange,
            LocalExpExecutor,
        )

        result: Dict[str, List[ExpRange]] = defaultdict(list)
        for entry in self.iter_queued():
            if baseline_revs and entry.baseline_rev not in baseline_revs:
                continue
            exp = collect_rev(self.repo, entry.stash_rev, **kwargs)
            exp.name = entry.name
            local_exec: Optional[LocalExpExecutor] = LocalExpExecutor(
                task_id=entry.stash_rev,
            )
            result[entry.baseline_rev].append(
                ExpRange(
                    [exp],
                    executor=ExpExecutor("queued", name="dvc-task", local=local_exec),
                    name=entry.name,
                )
            )
        return result

    def collect_failed_data(
        self,
        baseline_revs: Optional[Collection[str]],
        **kwargs,
    ) -> Dict[str, List["ExpRange"]]:
        from dvc.repo.experiments.collect import collect_rev
        from dvc.repo.experiments.serialize import (
            ExpExecutor,
            ExpRange,
            LocalExpExecutor,
            SerializableError,
        )

        result: Dict[str, List[ExpRange]] = defaultdict(list)
        for entry, _ in self.iter_failed():
            if baseline_revs and entry.baseline_rev not in baseline_revs:
                continue
            proc_info = self.proc.get(entry.stash_rev)
            if proc_info:
                local_exec: Optional[LocalExpExecutor] = LocalExpExecutor(
                    log=proc_info.stdout,
                    pid=proc_info.pid,
                    returncode=proc_info.returncode,
                    task_id=entry.stash_rev,
                )
            else:
                local_exec = None
            exp = collect_rev(self.repo, entry.stash_rev, **kwargs)
            exp.name = entry.name
            exp.error = SerializableError("Experiment run failed")
            result[entry.baseline_rev].append(
                ExpRange(
                    [exp],
                    executor=ExpExecutor("failed", local=local_exec),
                    name=entry.name,
                )
            )
        return result




dvc/repo/experiments/queue/exceptions.py
from typing import Collection

from dvc.exceptions import DvcException


class CannotKillTasksError(DvcException):
    def __init__(self, revs: Collection[str]):
        rev_str = ",".join(revs)
        super().__init__(
            f"Task '{rev_str}' is initializing, please wait a few seconds "
            "until the experiments start running to retry the kill operation."
        )




dvc/repo/experiments/queue/remove.py
from typing import TYPE_CHECKING, Collection, Dict, Iterable, List, Set, Union

from dvc.repo.experiments.exceptions import UnresolvedExpNamesError
from dvc.repo.experiments.queue.base import QueueDoneResult

if TYPE_CHECKING:
    from dvc.repo.experiments.queue.base import QueueEntry
    from dvc.repo.experiments.queue.celery import LocalCeleryQueue
    from dvc.repo.experiments.stash import ExpStashEntry


def remove_tasks(  # noqa: C901, PLR0912
    celery_queue: "LocalCeleryQueue",
    queue_entries: Iterable["QueueEntry"],
):
    """Remove tasks from task queue.

    Arguments:
        queue_entries: An iterable list of task to remove
    """
    from celery.result import AsyncResult

    stash_revs: Dict[str, "ExpStashEntry"] = {}
    failed_stash_revs: List["ExpStashEntry"] = []
    done_entry_set: Set["QueueEntry"] = set()
    stash_rev_all = celery_queue.stash.stash_revs
    failed_rev_all: Dict[str, "ExpStashEntry"] = {}
    if celery_queue.failed_stash:
        failed_rev_all = celery_queue.failed_stash.stash_revs
    for entry in queue_entries:
        if entry.stash_rev in stash_rev_all:
            stash_revs[entry.stash_rev] = stash_rev_all[entry.stash_rev]
        else:
            done_entry_set.add(entry)
            if entry.stash_rev in failed_rev_all:
                failed_stash_revs.append(failed_rev_all[entry.stash_rev])

    try:
        for (
            msg,
            queue_entry,
        ) in celery_queue._iter_queued():  # pylint: disable=protected-access
            if queue_entry.stash_rev in stash_revs and msg.delivery_tag:
                celery_queue.celery.reject(msg.delivery_tag)
    finally:
        celery_queue.stash.remove_revs(list(stash_revs.values()))

    try:
        for (
            msg,
            queue_entry,
        ) in celery_queue._iter_processed():  # pylint: disable=protected-access
            if queue_entry not in done_entry_set:
                continue
            task_id = msg.headers["id"]
            result: AsyncResult = AsyncResult(task_id)
            if result is not None:
                result.forget()
            if msg.delivery_tag:
                celery_queue.celery.purge(msg.delivery_tag)
    finally:
        if celery_queue.failed_stash:
            celery_queue.failed_stash.remove_revs(failed_stash_revs)


def _get_names(entries: Iterable[Union["QueueEntry", "QueueDoneResult"]]):
    names: List[str] = []
    for entry in entries:
        if isinstance(entry, QueueDoneResult):
            if entry.result and entry.result.ref_info:
                names.append(entry.result.ref_info.name)
                continue
            entry = entry.entry
        name = entry.name
        name = name or entry.stash_rev[:7]
        names.append(name)
    return names


def celery_clear(
    self: "LocalCeleryQueue",
    queued: bool = False,
    failed: bool = False,
    success: bool = False,
) -> List[str]:
    """Remove entries from the queue.

    Arguments:
        queued: Remove all queued tasks.
        failed: Remove all failed tasks.
        success: Remove all success tasks.

    Returns:
        Revisions which were removed.
    """

    removed: List[str] = []
    entry_list: List["QueueEntry"] = []
    if queued:
        queue_entries: List["QueueEntry"] = list(self.iter_queued())
        entry_list.extend(queue_entries)
        removed.extend(_get_names(queue_entries))
    if failed:
        failed_tasks: List["QueueDoneResult"] = list(self.iter_failed())
        entry_list.extend([result.entry for result in failed_tasks])
        removed.extend(_get_names(failed_tasks))
    if success:
        success_tasks: List["QueueDoneResult"] = list(self.iter_success())
        entry_list.extend([result.entry for result in success_tasks])
        removed.extend(_get_names(success_tasks))

    remove_tasks(self, entry_list)

    return removed


def celery_remove(
    self: "LocalCeleryQueue",
    revs: Collection[str],
) -> List[str]:
    """Remove the specified entries from the queue.

    Arguments:
        revs: Stash revisions or queued exp names to be removed.

    Returns:
        Revisions (or names) which were removed.
    """

    match_results = self.match_queue_entry_by_name(
        revs, self.iter_queued(), self.iter_done()
    )

    remained: List[str] = []
    removed: List[str] = []
    entry_to_remove: List["QueueEntry"] = []
    for name, entry in match_results.items():
        if entry:
            entry_to_remove.append(entry)
            removed.append(name)
        else:
            remained.append(name)

    if remained:
        raise UnresolvedExpNamesError(remained)

    if entry_to_remove:
        remove_tasks(self, entry_to_remove)

    return removed




dvc/repo/experiments/queue/tasks.py
from typing import TYPE_CHECKING, Any, Dict, List, Optional

from celery import shared_task
from celery.utils.log import get_task_logger

from dvc.repo.experiments.executor.base import ExecutorInfo
from dvc.repo.experiments.executor.local import TempDirExecutor

from .base import BaseStashQueue, QueueEntry

if TYPE_CHECKING:
    from dvc.repo.experiments.executor.base import BaseExecutor


logger = get_task_logger(__name__)


@shared_task
def setup_exp(entry_dict: Dict[str, Any]) -> "BaseExecutor":
    """Setup an experiment.

    Arguments:
        entry_dict: Serialized QueueEntry for this experiment.

    Returns:
        Root executor (temp) directory for this experiment.
    """
    from dvc.repo import Repo

    entry = QueueEntry.from_dict(entry_dict)
    with Repo(entry.dvc_root) as repo:
        # TODO: split executor.init_cache into separate subtask - we can release
        # exp.scm_lock before DVC push
        executor = BaseStashQueue.init_executor(
            repo.experiments,
            entry,
            TempDirExecutor,
            location="dvc-task",
        )
        infofile = repo.experiments.celery_queue.get_infofile_path(entry.stash_rev)
        executor.info.dump_json(infofile)
    return executor


@shared_task
def collect_exp(
    proc_dict: Dict[str, Any],  # noqa: ARG001, pylint: disable=unused-argument
    entry_dict: Dict[str, Any],
) -> str:
    """Collect results for an experiment.

    Arguments:
        proc_dict: Serialized ProcessInfo for experiment executor process.
        entry_dict: Serialized QueueEntry for this experiment.

    Returns:
        Directory to be cleaned up after this experiment.
    """
    from dvc.repo import Repo

    entry = QueueEntry.from_dict(entry_dict)
    with Repo(entry.dvc_root) as repo:
        celery_queue = repo.experiments.celery_queue
        infofile = celery_queue.get_infofile_path(entry.stash_rev)
        executor_info = ExecutorInfo.load_json(infofile)
        logger.debug("Collecting experiment info '%s'", str(executor_info))
        executor = TempDirExecutor.from_info(executor_info)
        exec_result = executor_info.result
        try:
            if exec_result is not None:
                BaseStashQueue.collect_executor(repo.experiments, executor, exec_result)
            else:
                logger.debug("Experiment failed (Exec result was None)")
                celery_queue.stash_failed(entry)
        except Exception:  # pylint: disable=broad-except
            # Log exceptions but do not re-raise so that task chain execution
            # continues
            logger.exception("Failed to collect experiment")
    return executor.root_dir


@shared_task
def cleanup_exp(executor: TempDirExecutor, infofile: str) -> None:
    """Cleanup after an experiment.

    Arguments:
        tmp_dir: Temp directory to be removed.
        entry_dict: Serialized QueueEntry for this experiment.
    """
    executor.cleanup(infofile)


@shared_task
def run_exp(
    entry_dict: Dict[str, Any],
    copy_paths: Optional[List[str]] = None,
    message: Optional[str] = None,
) -> None:
    """Run a full experiment.

    Experiment subtasks are executed inline as one atomic operation.

    Arguments:
        entry_dict: Serialized QueueEntry for this experiment.
    """
    from dvc.repo import Repo

    entry = QueueEntry.from_dict(entry_dict)
    with Repo(entry.dvc_root) as repo:
        queue = repo.experiments.celery_queue
        infofile = queue.get_infofile_path(entry.stash_rev)
    executor = setup_exp.s(entry_dict)()
    try:
        cmd = ["dvc", "exp", "exec-run", "--infofile", infofile]
        if copy_paths:
            for path in copy_paths:
                cmd.extend(["--copy-paths", path])
        if message:
            cmd.extend(["--message", message])
        proc_dict = queue.proc.run_signature(cmd, name=entry.stash_rev)()
        collect_exp.s(proc_dict, entry_dict)()
    finally:
        cleanup_exp.s(executor, infofile)()




dvc/repo/experiments/queue/tempdir.py
import logging
import os
from collections import defaultdict
from typing import TYPE_CHECKING, Collection, Dict, Generator, List, Optional

from funcy import first

from dvc.exceptions import DvcException
from dvc.repo.experiments.exceptions import ExpQueueEmptyError
from dvc.repo.experiments.executor.base import ExecutorInfo, TaskStatus
from dvc.repo.experiments.executor.local import TempDirExecutor
from dvc.repo.experiments.utils import EXEC_PID_DIR, EXEC_TMP_DIR
from dvc.utils.objects import cached_property

from .base import BaseStashQueue, QueueEntry, QueueGetResult
from .utils import fetch_running_exp_from_temp_dir
from .workspace import WorkspaceQueue

if TYPE_CHECKING:
    from dvc.repo.experiments import Experiments
    from dvc.repo.experiments.executor.base import BaseExecutor, ExecutorResult
    from dvc.repo.experiments.serialize import ExpRange
    from dvc_task.proc.manager import ProcessManager

logger = logging.getLogger(__name__)


_STANDALONE_TMP_DIR = os.path.join(EXEC_TMP_DIR, "standalone")


class TempDirQueue(WorkspaceQueue):
    """Standalone/tempdir exp queue implementation."""

    _EXEC_NAME: Optional[str] = None

    @cached_property
    def _standalone_tmp_dir(self) -> str:
        assert self.repo.tmp_dir is not None
        return os.path.join(self.repo.tmp_dir, _STANDALONE_TMP_DIR)

    @cached_property
    def pid_dir(self) -> str:
        return os.path.join(self._standalone_tmp_dir, EXEC_PID_DIR)

    @cached_property
    def proc(self) -> "ProcessManager":
        from dvc_task.proc.manager import ProcessManager

        return ProcessManager(self.pid_dir)

    def get(self) -> QueueGetResult:
        revs = self.stash.stash_revs
        if not revs:
            raise ExpQueueEmptyError("No stashed standalone experiments.")
        stash_rev, stash_entry = first(revs.items())
        entry = QueueEntry(
            self.repo.root_dir,
            self.scm.root_dir,
            self.ref,
            stash_rev,
            stash_entry.baseline_rev,
            stash_entry.branch,
            stash_entry.name,
            stash_entry.head_rev,
        )
        executor = self.init_executor(
            self.repo.experiments,
            entry,
            TempDirExecutor,
            wdir=self._standalone_tmp_dir,
        )
        return QueueGetResult(entry, executor)

    def iter_active(self) -> Generator[QueueEntry, None, None]:
        # NOTE: Yielded queue entries are not complete for performance reasons.
        # Retrieving exec ref information is unavailable without doing a
        # git-fetch, and is unneeded in the common use cases for iter_active.
        for stash_rev in self.proc:
            infofile = self.get_infofile_path(stash_rev)
            executor_info = ExecutorInfo.load_json(infofile)
            if executor_info.status <= TaskStatus.SUCCESS and os.path.exists(
                executor_info.root_dir
            ):
                yield QueueEntry(
                    self.repo.root_dir,
                    self.scm.root_dir,
                    self.ref,
                    stash_rev,
                    executor_info.baseline_rev,
                    None,  # branch unavailable without doing a git-fetch
                    executor_info.name,
                    None,
                )

    def _reproduce_entry(
        self,
        entry: QueueEntry,
        executor: "BaseExecutor",
        copy_paths: Optional[List[str]] = None,
        message: Optional[str] = None,
        **kwargs,
    ) -> Dict[str, Dict[str, str]]:
        results: Dict[str, Dict[str, str]] = defaultdict(dict)
        exec_name = self._EXEC_NAME or entry.stash_rev
        infofile = self.get_infofile_path(exec_name)
        try:
            rev = entry.stash_rev
            exec_result = executor.reproduce(
                info=executor.info,
                rev=rev,
                infofile=infofile,
                log_level=logger.getEffectiveLevel(),
                log_errors=True,
                copy_paths=copy_paths,
                message=message,
            )
            if not exec_result.exp_hash:
                raise DvcException(f"Failed to reproduce experiment '{rev[:7]}'")
            if exec_result.ref_info:
                results[rev].update(
                    self.collect_executor(self.repo.experiments, executor, exec_result)
                )
        except DvcException:
            raise
        except Exception as exc:  # noqa: BLE001
            raise DvcException(f"Failed to reproduce experiment '{rev[:7]}'") from exc
        finally:
            executor.cleanup(infofile)
        return results

    @staticmethod
    def collect_executor(
        exp: "Experiments",
        executor: "BaseExecutor",
        exec_result: "ExecutorResult",
    ) -> Dict[str, str]:
        return BaseStashQueue.collect_executor(exp, executor, exec_result)

    def collect_active_data(
        self,
        baseline_revs: Optional[Collection[str]],
        fetch_refs: bool = False,
        **kwargs,
    ) -> Dict[str, List["ExpRange"]]:
        from dvc.repo import Repo
        from dvc.repo.experiments.collect import collect_exec_branch
        from dvc.repo.experiments.serialize import (
            ExpExecutor,
            ExpRange,
            LocalExpExecutor,
        )

        result: Dict[str, List[ExpRange]] = defaultdict(list)
        for entry in self.iter_active():
            if baseline_revs and entry.baseline_rev not in baseline_revs:
                continue
            if fetch_refs:
                fetch_running_exp_from_temp_dir(self, entry.stash_rev, fetch_refs)
            proc_info = self.proc.get(entry.stash_rev)
            infofile = self.get_infofile_path(entry.stash_rev)
            executor_info = ExecutorInfo.load_json(infofile)
            if proc_info:
                local_exec: Optional[LocalExpExecutor] = LocalExpExecutor(
                    root=executor_info.root_dir,
                    log=proc_info.stdout,
                    pid=proc_info.pid,
                )
            else:
                local_exec = None
            dvc_root = os.path.join(executor_info.root_dir, executor_info.dvc_dir)
            with Repo(dvc_root) as repo:
                exps = list(
                    collect_exec_branch(repo, executor_info.baseline_rev, **kwargs)
                )
            exps[0].rev = entry.stash_rev
            exps[0].name = entry.name
            result[entry.baseline_rev].append(
                ExpRange(
                    exps,
                    executor=ExpExecutor(
                        "running",
                        name=executor_info.location,
                        local=local_exec,
                    ),
                    name=entry.name,
                )
            )
        return result




dvc/repo/experiments/queue/utils.py
import logging
from typing import TYPE_CHECKING, Dict, List

from scmrepo.exceptions import SCMError

from dvc.repo.experiments.executor.base import ExecutorInfo, TaskStatus
from dvc.repo.experiments.refs import EXEC_NAMESPACE, EXPS_NAMESPACE, EXPS_STASH
from dvc.repo.experiments.utils import get_exp_rwlock, iter_remote_refs

logger = logging.getLogger(__name__)


if TYPE_CHECKING:
    from dvc.scm import Git

    from .base import BaseStashQueue


def get_remote_executor_refs(scm: "Git", remote_url: str) -> List[str]:
    """Get result list refs from a remote repository

    Args:
        remote_url : remote executor's url
    """
    refs = []
    for ref in iter_remote_refs(
        scm,
        remote_url,
        base=EXPS_NAMESPACE,
    ):
        if not ref.startswith(EXEC_NAMESPACE) and ref != EXPS_STASH:
            refs.append(ref)
    return refs


def fetch_running_exp_from_temp_dir(
    queue: "BaseStashQueue", rev: str, fetch_refs: bool
) -> Dict[str, Dict]:
    """Fetch status of running exps out of current working directory

    Args:
        queue (BaseStashQueue):
        rev (str): stash revision of the experiment
        fetch_refs (bool): fetch running checkpoint results to local or not.

    Returns:
        Dict[str, Dict]: _description_
    """
    from dvc.repo.experiments.executor.local import TempDirExecutor
    from dvc.scm import InvalidRemoteSCMRepo
    from dvc.utils.serialize import load_json

    result: Dict[str, Dict] = {}
    infofile = queue.get_infofile_path(rev)
    try:
        info = ExecutorInfo.from_dict(load_json(infofile))
    except OSError:
        return result
    if info.status <= TaskStatus.RUNNING:
        result[rev] = info.asdict()
        if info.git_url and fetch_refs and info.status > TaskStatus.PREPARING:

            def on_diverged(_ref: str):
                return True

            executor = TempDirExecutor.from_info(info)
            try:
                refs = get_remote_executor_refs(queue.scm, executor.git_url)
                with get_exp_rwlock(queue.repo, writes=refs):
                    for ref in executor.fetch_exps(
                        queue.scm,
                        refs,
                        on_diverged=on_diverged,
                    ):
                        logger.debug("Updated running experiment '%s'.", ref)
                        last_rev = queue.scm.get_ref(ref)
                        result[rev]["last"] = last_rev
                        if last_rev:
                            result[last_rev] = info.asdict()
            except (InvalidRemoteSCMRepo, SCMError):
                # ignore stale info files
                del result[rev]
    return result




dvc/repo/experiments/queue/workspace.py
import json
import logging
import os
from collections import defaultdict
from typing import TYPE_CHECKING, Collection, Dict, Generator, List, Optional

import psutil
from funcy import first

from dvc.exceptions import DvcException
from dvc.repo.experiments.exceptions import ExpQueueEmptyError
from dvc.repo.experiments.executor.base import ExecutorInfo, TaskStatus
from dvc.repo.experiments.executor.local import WorkspaceExecutor
from dvc.repo.experiments.refs import EXEC_BRANCH, WORKSPACE_STASH
from dvc.repo.experiments.utils import get_exp_rwlock
from dvc.utils.fs import remove
from dvc.utils.serialize import load_json

from .base import BaseStashQueue, QueueEntry, QueueGetResult

if TYPE_CHECKING:
    from dvc.repo.experiments import Experiments
    from dvc.repo.experiments.executor.base import BaseExecutor, ExecutorResult
    from dvc.repo.experiments.serialize import ExpRange

    from .base import QueueDoneResult

logger = logging.getLogger(__name__)


class WorkspaceQueue(BaseStashQueue):
    _EXEC_NAME: Optional[str] = "workspace"

    def put(self, *args, **kwargs) -> QueueEntry:
        kwargs.pop("copy_paths", None)
        with get_exp_rwlock(self.repo, writes=["workspace", WORKSPACE_STASH]):
            return self._stash_exp(*args, **kwargs)

    def get(self) -> QueueGetResult:
        revs = self.stash.stash_revs
        if not revs:
            raise ExpQueueEmptyError("No experiments in the queue.")
        stash_rev, stash_entry = first(revs.items())
        entry = QueueEntry(
            self.repo.root_dir,
            self.scm.root_dir,
            self.ref,
            stash_rev,
            stash_entry.baseline_rev,
            stash_entry.branch,
            stash_entry.name,
            stash_entry.head_rev,
        )
        executor = self.init_executor(self.repo.experiments, entry)
        return QueueGetResult(entry, executor)

    def iter_queued(self) -> Generator[QueueEntry, None, None]:
        for rev, entry in self.stash.stash_revs.items():
            yield QueueEntry(
                self.repo.root_dir,
                self.scm.root_dir,
                self.ref,
                rev,
                entry.baseline_rev,
                entry.branch,
                entry.name,
                entry.head_rev,
            )

    def iter_active(self) -> Generator[QueueEntry, None, None]:
        # Workspace run state is reflected in the workspace itself and does not
        # need to be handled via the queue
        raise NotImplementedError

    def iter_done(self) -> Generator["QueueDoneResult", None, None]:
        raise NotImplementedError

    def iter_failed(self) -> Generator["QueueDoneResult", None, None]:
        raise NotImplementedError

    def iter_success(self) -> Generator["QueueDoneResult", None, None]:
        raise NotImplementedError

    def reproduce(
        self, copy_paths: Optional[List[str]] = None, message: Optional[str] = None
    ) -> Dict[str, Dict[str, str]]:
        results: Dict[str, Dict[str, str]] = defaultdict(dict)
        try:
            while True:
                entry, executor = self.get()
                results.update(
                    self._reproduce_entry(
                        entry, executor, copy_paths=copy_paths, message=message
                    )
                )
        except ExpQueueEmptyError:
            pass
        return results

    def _reproduce_entry(
        self, entry: QueueEntry, executor: "BaseExecutor", **kwargs
    ) -> Dict[str, Dict[str, str]]:
        kwargs.pop("copy_paths", None)
        from dvc_task.proc.process import ProcessInfo

        results: Dict[str, Dict[str, str]] = defaultdict(dict)
        exec_name = self._EXEC_NAME or entry.stash_rev
        proc_info = ProcessInfo(os.getpid(), None, None, None, None)
        proc_info_path = self._proc_info_path(exec_name)
        os.makedirs(os.path.dirname(proc_info_path), exist_ok=True)
        proc_info.dump(proc_info_path)
        infofile = self.get_infofile_path(exec_name)
        try:
            rev = entry.stash_rev
            exec_result = executor.reproduce(
                info=executor.info,
                rev=rev,
                infofile=infofile,
                log_level=logger.getEffectiveLevel(),
                log_errors=not isinstance(executor, WorkspaceExecutor),
                message=kwargs.get("message"),
            )
            if not exec_result.exp_hash:
                raise DvcException(f"Failed to reproduce experiment '{rev[:7]}'")
            if exec_result.ref_info:
                results[rev].update(
                    self.collect_executor(self.repo.experiments, executor, exec_result)
                )
        except DvcException:
            raise
        except Exception as exc:  # noqa: BLE001
            raise DvcException(f"Failed to reproduce experiment '{rev[:7]}'") from exc
        finally:
            executor.cleanup(infofile)
            remove(self._proc_info_path(exec_name))
        return results

    def _proc_info_path(self, name: str) -> str:
        return os.path.join(self.pid_dir, name, f"{name}.json")

    @property
    def _active_pid(self) -> Optional[int]:
        from dvc_task.proc.process import ProcessInfo

        assert self._EXEC_NAME
        name = self._EXEC_NAME
        try:
            proc_info = ProcessInfo.load(self._proc_info_path(name))
            pid = proc_info.pid
            if psutil.pid_exists(pid):
                return pid
            logger.debug("Workspace exec PID '%d' no longer exists, removing.", pid)
            remove(self._proc_info_path(name))
        except (FileNotFoundError, json.JSONDecodeError):
            pass
        return None

    @staticmethod
    def collect_executor(  # pylint: disable=unused-argument
        exp: "Experiments",
        executor: "BaseExecutor",  # noqa: ARG004
        exec_result: "ExecutorResult",
    ) -> Dict[str, str]:
        results: Dict[str, str] = {}
        exp_rev = exp.scm.get_ref(EXEC_BRANCH)
        if exp_rev:
            assert exec_result.exp_hash
            logger.debug("Collected experiment '%s'.", exp_rev[:7])
            results[exp_rev] = exec_result.exp_hash

        return results

    def get_result(self, entry: QueueEntry) -> Optional["ExecutorResult"]:
        raise NotImplementedError

    def kill(self, revs: Collection[str]) -> None:
        raise NotImplementedError

    def shutdown(self, kill: bool = False):
        raise NotImplementedError

    def logs(
        self,
        rev: str,
        encoding: Optional[str] = None,
        follow: bool = False,
    ):
        raise NotImplementedError

    def get_running_exp(self) -> Optional[str]:
        """Return the name of the exp running in workspace (if it exists)."""
        assert self._EXEC_NAME
        if self._active_pid is None:
            return None

        infofile = self.get_infofile_path(self._EXEC_NAME)
        try:
            info = ExecutorInfo.from_dict(load_json(infofile))
        except OSError:
            return None
        return info.name

    def collect_active_data(
        self,
        baseline_revs: Optional[Collection[str]],
        fetch_refs: bool = False,  # noqa: ARG002
        **kwargs,
    ) -> Dict[str, List["ExpRange"]]:
        from dvc.repo.experiments.collect import collect_exec_branch
        from dvc.repo.experiments.serialize import (
            ExpExecutor,
            ExpRange,
            LocalExpExecutor,
        )

        result: Dict[str, List[ExpRange]] = defaultdict(list)
        pid = self._active_pid
        if pid is None:
            return result

        assert self._EXEC_NAME
        infofile = self.get_infofile_path(self._EXEC_NAME)
        try:
            info = ExecutorInfo.from_dict(load_json(infofile))
        except OSError:
            return result

        if (
            (not baseline_revs or info.baseline_rev in baseline_revs)
            and info.status < TaskStatus.FAILED
            and info.status != TaskStatus.SUCCESS
        ):
            local_exec = LocalExpExecutor(root=info.root_dir, pid=pid)
            exps = list(collect_exec_branch(self.repo, info.baseline_rev, **kwargs))
            exps[0].name = info.name
            result[info.baseline_rev] = [
                ExpRange(
                    exps,
                    executor=ExpExecutor("running", name="workspace", local=local_exec),
                    name=info.name,
                )
            ]
        return result

    def collect_queued_data(
        self,
        baseline_revs: Optional[Collection[str]],
        **kwargs,
    ) -> Dict[str, List["ExpRange"]]:
        raise NotImplementedError

    def collect_failed_data(
        self,
        baseline_revs: Optional[Collection[str]],
        **kwargs,
    ) -> Dict[str, List["ExpRange"]]:
        raise NotImplementedError




dvc/repo/metrics/__init__.py
class Metrics:
    def __init__(self, repo):
        self.repo = repo

    def show(self, *args, **kwargs):
        from dvc.repo.metrics.show import show

        return show(self.repo, *args, **kwargs)

    def diff(self, *args, **kwargs):
        from .diff import diff

        return diff(self.repo, *args, **kwargs)




dvc/repo/metrics/diff.py
from dvc.utils.diff import diff as _diff
from dvc.utils.diff import format_dict


def diff(repo, *args, a_rev=None, b_rev=None, **kwargs):
    if repo.scm.no_commits:
        return {}

    with_unchanged = kwargs.pop("all", False)

    a_rev = a_rev or "HEAD"
    b_rev = b_rev or "workspace"

    metrics = repo.metrics.show(
        *args, **kwargs, revs=[a_rev, b_rev], hide_workspace=False
    )

    old = metrics.get(a_rev, {}).get("data", {})
    new = metrics.get(b_rev, {}).get("data", {})

    return _diff(format_dict(old), format_dict(new), with_unchanged=with_unchanged)




dvc/repo/metrics/show.py
import logging
import os
from typing import TYPE_CHECKING, List

from scmrepo.exceptions import SCMError

from dvc.fs.dvc import DVCFileSystem
from dvc.repo import locked
from dvc.repo.collect import collect
from dvc.scm import NoSCMError
from dvc.utils import as_posix, error_handler, errored_revisions, onerror_collect
from dvc.utils.collections import ensure_list
from dvc.utils.serialize import load_path

if TYPE_CHECKING:
    from dvc.output import Output

logger = logging.getLogger(__name__)


def _is_metric(out: "Output") -> bool:
    return bool(out.metric)


def _to_fs_paths(metrics: List["Output"]) -> List["str"]:
    result = []
    for out in metrics:
        if out.metric:
            result.append(out.repo.dvcfs.from_os_path(out.fs_path))
    return result


def _collect_top_level_metrics(repo):
    top_metrics = repo.index._metrics  # pylint: disable=protected-access
    for dvcfile, metrics in top_metrics.items():
        wdir = repo.fs.path.relpath(repo.fs.path.parent(dvcfile), repo.root_dir)
        for file in metrics:
            path = repo.fs.path.join(wdir, as_posix(file))
            yield repo.fs.path.normpath(path)


def _collect_metrics(repo, targets, recursive):
    metrics, fs_paths = collect(
        repo, targets=targets, output_filter=_is_metric, recursive=recursive
    )
    return _to_fs_paths(metrics) + list(fs_paths)


def _extract_metrics(metrics, path, rev):
    if isinstance(metrics, (int, float, str)):
        return metrics

    if not isinstance(metrics, dict):
        return None

    ret = {}
    for key, val in metrics.items():
        m = _extract_metrics(val, path, rev)
        if m not in (None, {}):
            ret[key] = m
        else:
            logger.debug(
                (
                    "Could not parse '%s' metric from '%s' at '%s' "
                    "due to its unsupported type: '%s'"
                ),
                key,
                path,
                rev,
                type(val).__name__,
            )

    return ret


@error_handler
def _read_metric(path, fs, rev, **kwargs):
    val = load_path(path, fs)
    val = _extract_metrics(val, path, rev)
    return val or {}


def _read_metrics(repo, metrics, rev, onerror=None):
    fs = DVCFileSystem(repo=repo)

    relpath = ""
    if repo.root_dir != repo.fs.path.getcwd():
        relpath = repo.fs.path.relpath(repo.root_dir, repo.fs.path.getcwd())

    res = {}
    for metric in metrics:
        rel_metric_path = os.path.join(relpath, *fs.path.parts(metric))
        if not fs.isfile(metric):
            if fs.isfile(rel_metric_path):
                metric = rel_metric_path
            else:
                continue

        res[rel_metric_path] = _read_metric(metric, fs, rev, onerror=onerror)

    return res


def _gather_metrics(repo, targets, rev, recursive, onerror=None):
    metrics = _collect_metrics(repo, targets, recursive)
    metrics.extend(_collect_top_level_metrics(repo))
    return _read_metrics(repo, metrics, rev, onerror=onerror)


@locked
def show(
    repo,
    targets=None,
    all_branches=False,
    all_tags=False,
    recursive=False,
    revs=None,
    all_commits=False,
    onerror=None,
    hide_workspace=True,
):
    if onerror is None:
        onerror = onerror_collect

    targets = ensure_list(targets)
    targets = [repo.dvcfs.from_os_path(target) for target in targets]

    res = {}
    for rev in repo.brancher(
        revs=revs,
        all_branches=all_branches,
        all_tags=all_tags,
        all_commits=all_commits,
    ):
        res[rev] = error_handler(_gather_metrics)(
            repo, targets, rev, recursive, onerror=onerror
        )

    if hide_workspace:
        # Hide workspace metrics if they are the same as in the active branch
        try:
            active_branch = repo.scm.active_branch()
        except (SCMError, NoSCMError):
            # SCMError - detached head
            # NoSCMError - no repo case
            pass
        else:
            if res.get("workspace") == res.get(active_branch):
                res.pop("workspace", None)

    errored = errored_revisions(res)
    if errored:
        from dvc.ui import ui

        ui.error_write(
            "DVC failed to load some metrics for following revisions:"
            f" '{', '.join(errored)}'."
        )

    return res




dvc/repo/params/__init__.py
class Params:
    def __init__(self, repo):
        self.repo = repo

    def show(self, *args, **kwargs):
        from .show import show

        return show(self.repo, *args, **kwargs)

    def diff(self, *args, **kwargs):
        from .diff import diff

        return diff(self.repo, *args, **kwargs)




dvc/repo/params/diff.py
from dvc.utils.diff import diff as _diff
from dvc.utils.diff import format_dict


def diff(repo, *args, a_rev=None, b_rev=None, **kwargs):
    if repo.scm.no_commits:
        return {}

    with_unchanged = kwargs.pop("all", False)

    a_rev = a_rev or "HEAD"
    b_rev = b_rev or "workspace"

    params = repo.params.show(
        *args, **kwargs, revs=[a_rev, b_rev], hide_workspace=False
    )

    old = params.get(a_rev, {}).get("data", {})
    new = params.get(b_rev, {}).get("data", {})

    return _diff(format_dict(old), format_dict(new), with_unchanged=with_unchanged)




dvc/repo/params/show.py
import logging
import os
from collections import defaultdict
from copy import copy
from typing import TYPE_CHECKING, Callable, Dict, Iterable, List, Optional, Tuple

from scmrepo.exceptions import SCMError

from dvc.dependency.param import ParamsDependency
from dvc.repo import locked
from dvc.repo.collect import collect
from dvc.scm import NoSCMError
from dvc.stage import PipelineStage
from dvc.ui import ui
from dvc.utils import as_posix, error_handler, errored_revisions, onerror_collect
from dvc.utils.collections import ensure_list
from dvc.utils.serialize import load_path

if TYPE_CHECKING:
    from dvc.output import Output
    from dvc.repo import Repo

logger = logging.getLogger(__name__)


def _is_params(dep: "Output"):
    return isinstance(dep, ParamsDependency)


def _collect_top_level_params(repo):
    top_params = repo.index._params  # pylint: disable=protected-access
    for dvcfile, params in top_params.items():
        wdir = repo.fs.path.relpath(repo.fs.path.parent(dvcfile), repo.root_dir)
        for file in params:
            path = repo.fs.path.join(wdir, as_posix(file))
            yield repo.fs.path.normpath(path)


def _collect_configs(
    repo: "Repo", targets=None, deps=False, stages=None
) -> Tuple[List["Output"], List[str]]:
    params, fs_paths = collect(
        repo,
        targets=targets or [],
        deps=True,
        output_filter=_is_params,
        duplicates=deps or stages is not None,
    )
    all_fs_paths = fs_paths + [p.fs_path for p in params]
    if not any([deps, targets, stages]):
        default_params = repo.fs.path.join(
            repo.root_dir, ParamsDependency.DEFAULT_PARAMS_FILE
        )
        if default_params not in all_fs_paths and repo.fs.exists(default_params):
            fs_paths.append(default_params)
    if targets and (deps or stages) and not params:
        # A target has been provided but it is not used in the stages
        fs_paths = []
    return params, fs_paths


@error_handler
def _read_fs_path(fs, fs_path, **kwargs):
    return load_path(fs_path, fs)


def _read_params(
    repo,
    params,
    params_fs_paths,
    deps=False,
    onerror: Optional[Callable] = None,
    stages: Optional[Iterable[str]] = None,
):
    res: Dict[str, Dict] = defaultdict(lambda: defaultdict(dict))
    fs_paths = copy(params_fs_paths)

    if deps or stages:
        for param in params:
            if stages and param.stage.addressing not in stages:
                continue
            params_dict = error_handler(param.read_params)(
                onerror=onerror, flatten=False
            )
            if params_dict:
                name = os.sep.join(repo.fs.path.relparts(param.fs_path))
                res[name]["data"].update(params_dict["data"])
                if name in fs_paths:
                    fs_paths.remove(name)
    else:
        fs_paths += [param.fs_path for param in params]

    for fs_path in fs_paths:
        from_path = _read_fs_path(repo.fs, fs_path, onerror=onerror)
        if from_path:
            name = os.sep.join(repo.fs.path.relparts(fs_path))
            res[name] = from_path

    return res


def _collect_vars(repo, params, stages=None) -> Dict:
    vars_params: Dict[str, Dict] = defaultdict(dict)

    for stage in repo.index.stages:
        if isinstance(stage, PipelineStage) and stage.tracked_vars:
            if stages and stage.addressing not in stages:
                continue
            for file, vars_ in stage.tracked_vars.items():
                # `params` file are shown regardless of `tracked` or not
                # to reduce noise and duplication, they are skipped
                if file in params:
                    continue

                name = os.sep.join(repo.fs.path.parts(file))
                vars_params[name].update(vars_)
    return vars_params


@locked
def show(
    repo,
    revs=None,
    targets=None,
    deps=False,
    onerror: Optional[Callable] = None,
    stages=None,
    hide_workspace=True,
):
    if onerror is None:
        onerror = onerror_collect
    res = {}

    targets = ensure_list(targets)
    targets = [repo.dvcfs.from_os_path(target) for target in targets]

    for branch in repo.brancher(revs=revs):
        params = error_handler(_gather_params)(
            repo=repo,
            targets=targets,
            deps=deps,
            onerror=onerror,
            stages=stages,
        )

        if params:
            res[branch] = params

    if hide_workspace:
        # Hide workspace params if they are the same as in the active branch
        try:
            active_branch = repo.scm.active_branch()
        except (SCMError, NoSCMError):
            # SCMError - detached head
            # NoSCMError - no repo case
            pass
        else:
            if res.get("workspace") == res.get(active_branch):
                res.pop("workspace", None)

    errored = errored_revisions(res)
    if errored:
        ui.error_write(
            "DVC failed to load some parameters for following revisions:"
            f" '{', '.join(errored)}'."
        )

    return res


def _gather_params(repo, targets=None, deps=False, onerror=None, stages=None):
    param_outs, params_fs_paths = _collect_configs(
        repo, targets=targets, deps=deps, stages=stages
    )
    params_fs_paths.extend(_collect_top_level_params(repo=repo))
    params = _read_params(
        repo,
        params=param_outs,
        params_fs_paths=params_fs_paths,
        deps=deps,
        onerror=onerror,
        stages=stages,
    )
    vars_params = _collect_vars(repo, params, stages=stages)

    # NOTE: only those that are not added as a ParamDependency are
    # included so we don't need to recursively merge them yet.
    for key, vals in vars_params.items():
        params[key]["data"] = vals
    return params




dvc/repo/plots/__init__.py
import csv
import io
import logging
import os
from collections import defaultdict
from copy import deepcopy
from functools import partial
from multiprocessing import cpu_count
from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    Dict,
    Iterator,
    List,
    Optional,
    Set,
    Union,
)

import dpath
import dpath.options
from funcy import first, ldistinct, project, reraise

from dvc.exceptions import DvcException
from dvc.utils import error_handler, errored_revisions, onerror_collect
from dvc.utils.objects import cached_property
from dvc.utils.serialize import PARSERS, EncodingError
from dvc.utils.threadpool import ThreadPoolExecutor
from dvc_render.image import ImageRenderer

if TYPE_CHECKING:
    from dvc.fs import FileSystem
    from dvc.output import Output
    from dvc.repo import Repo
    from dvc.types import DictStrAny, StrPath

dpath.options.ALLOW_EMPTY_STRING_KEYS = True

logger = logging.getLogger(__name__)


SUPPORTED_IMAGE_EXTENSIONS = ImageRenderer.EXTENSIONS


class PlotMetricTypeError(DvcException):
    def __init__(self, file):
        super().__init__(
            "'{}' - file type error\n"
            "Only JSON, YAML, CSV and TSV formats are supported.".format(file)
        )


class NotAPlotError(DvcException):
    def __init__(self, out):
        super().__init__(
            f"'{out}' is not a known plot. Use `dvc plots modify` to turn it into one."
        )


class PropsNotFoundError(DvcException):
    pass


@error_handler
def _unpack_dir_files(fs, path, **kwargs):
    ret = list(fs.find(path))
    if not ret:
        # This will raise FileNotFoundError if it is a broken symlink or TreeError
        next(iter(fs.ls(path)), None)
    return ret


class Plots:
    def __init__(self, repo):
        self.repo = repo

    def collect(
        self,
        targets: Optional[List[str]] = None,
        revs: Optional[List[str]] = None,
        recursive: bool = False,
        onerror: Optional[Callable] = None,
        props: Optional[Dict] = None,
    ) -> Iterator[Dict]:
        """Collects plots definitions and data sources.

        Generator yielding a structure like:
            {
                revision:
                {
                    "definitions":
                    {
                        "data":
                        {
                            "config_file":
                            {
                                "data":
                                {
                                    plot_id:
                                    {
                                        plot_config
                                    }
                                }
                            }
                        }
                    },
                    "sources":
                    {
                        "data":
                        {
                            "filename":
                            {
                                "data_source": callable loading the data,
                                "props": properties for the file if it is
                                         plots type output
                            }
                        }
                    }
                }

            }
        """
        from dvc.repo.experiments.brancher import switch_repo
        from dvc.utils.collections import ensure_list

        targets = ensure_list(targets)
        targets = [self.repo.dvcfs.from_os_path(target) for target in targets]

        if revs is None:
            revs = ["workspace"]
        else:
            revs = list(revs)
            if "workspace" in revs:
                # reorder revs to match repo.brancher ordering
                revs.remove("workspace")
                revs = ["workspace"] + revs
        for rev in revs:
            with switch_repo(self.repo, rev) as (repo, _):
                res: Dict = {}
                definitions = _collect_definitions(
                    repo,
                    targets=targets,
                    revision=rev,
                    onerror=onerror,
                    props=props,
                )
                if definitions:
                    res[rev] = {"definitions": definitions}

                    data_targets = _get_data_targets(definitions)

                    res[rev]["sources"] = self._collect_data_sources(
                        repo,
                        targets=data_targets,
                        recursive=recursive,
                        props=props,
                        onerror=onerror,
                    )
                yield res

    @error_handler
    def _collect_data_sources(
        self,
        repo: "Repo",
        targets: Optional[List[str]] = None,
        recursive: bool = False,
        props: Optional[Dict] = None,
        onerror: Optional[Callable] = None,
    ):
        fs = repo.dvcfs

        props = props or {}

        plots = _collect_plots(repo, targets, recursive)
        res: Dict[str, Any] = {}
        for fs_path, rev_props in plots.items():
            joined_props = {**rev_props, **props}
            res[fs_path] = {"props": joined_props}
            res[fs_path].update(
                {
                    "data_source": partial(
                        parse,
                        fs,
                        fs_path,
                        props=joined_props,
                        onerror=onerror,
                    )
                }
            )
        return res

    def show(
        self,
        targets: Optional[List[str]] = None,
        revs=None,
        props=None,
        recursive=False,
        onerror=None,
    ):
        if onerror is None:
            onerror = onerror_collect

        result: Dict[str, Dict] = {}
        for data in self.collect(
            targets,
            revs,
            recursive,
            onerror=onerror,
            props=props,
        ):
            short_rev = "workspace"
            if rev := getattr(self.repo.fs, "rev", None):
                short_rev = rev[:7]
            _resolve_data_sources(data, short_rev, cache_remote_stream=True)
            result.update(data)

        errored = errored_revisions(result)
        if errored:
            from dvc.ui import ui

            ui.error_write(
                "DVC failed to load some plots for following revisions: "
                f"'{', '.join(errored)}'."
            )

        return result

    def diff(self, *args, **kwargs):
        from .diff import diff

        return diff(self.repo, *args, **kwargs)

    @staticmethod
    def _unset(out, props):
        missing = list(set(props) - set(out.plot.keys()))
        if missing:
            raise PropsNotFoundError(
                f"display properties {missing} not found in plot '{out}'"
            )

        for prop in props:
            out.plot.pop(prop)

    def modify(self, path, props=None, unset=None):
        from dvc_render.vega_templates import get_template

        props = props or {}
        template = props.get("template")
        if template:
            get_template(template, self.templates_dir)

        (out,) = self.repo.find_outs_by_path(path)
        if not out.plot and unset is not None:
            raise NotAPlotError(out)

        # This out will become a plot unless it is one already
        if not isinstance(out.plot, dict):
            out.plot = {}

        if unset:
            self._unset(out, unset)

        out.plot.update(props)

        # Empty dict will move it to non-plots
        if not out.plot:
            out.plot = True

        out.verify_metric()
        out.stage.dump(update_lock=False)

    @cached_property
    def templates_dir(self) -> Optional[str]:
        if self.repo.dvc_dir:
            return os.path.join(self.repo.dvc_dir, "plots")
        return None


def _is_plot(out: "Output") -> bool:
    return bool(out.plot)


def _resolve_data_sources(
    plots_data: Dict, rev: str, cache_remote_stream: bool = False
):
    from dvc.progress import Tqdm

    values = list(plots_data.values())
    to_resolve = []
    while values:
        value = values.pop()
        if isinstance(value, dict):
            if "data_source" in value:
                to_resolve.append(value)
            values.extend(value.values())

    def resolve(value):
        data_source = value.pop("data_source")
        assert callable(data_source)
        value.update(data_source(cache_remote_stream=cache_remote_stream))

    if not to_resolve:
        return

    executor = ThreadPoolExecutor(
        max_workers=min(16, 4 * cpu_count()),
        thread_name_prefix="resolve_data",
        cancel_on_error=True,
    )
    with executor:
        iterable = executor.imap_unordered(resolve, to_resolve)
        with Tqdm(
            iterable,
            total=len(to_resolve),
            desc=f"Reading plot's data from {rev}",
            unit="files",
            unit_scale=False,
        ) as progress_iterable:
            list(progress_iterable)


def _collect_plots(
    repo: "Repo",
    targets: Optional[List[str]] = None,
    recursive: bool = False,
) -> Dict[str, Dict]:
    from dvc.repo.collect import collect

    plots, fs_paths = collect(
        repo,
        output_filter=_is_plot,
        targets=targets,
        recursive=recursive,
    )

    result = {
        repo.dvcfs.from_os_path(plot.fs_path): _plot_props(plot) for plot in plots
    }
    result.update({fs_path: {} for fs_path in fs_paths})
    return result


def _get_data_targets(definitions: Dict):
    result: Set = set()
    if "data" in definitions:
        for content in definitions["data"].values():
            if "data" in content:
                for plot_id, config in content["data"].items():
                    result = result.union(infer_data_sources(plot_id, config))
    return result


def infer_data_sources(plot_id, config=None):
    y = config.get("y", None)

    if isinstance(y, dict):
        sources = list(y.keys())
    else:
        sources = [plot_id]

    x = config.get("x", None)
    if isinstance(x, dict):
        sources.append(first(x.keys()))

    return ldistinct(source for source in sources)


def _matches(targets, config_file, plot_id):
    import re

    from dvc.utils.plots import get_plot_id

    if not targets:
        return True

    full_id = get_plot_id(plot_id, config_file)
    if any(
        (re.match(target, plot_id) or re.match(target, full_id)) for target in targets
    ):
        return True
    return False


def _normpath(path):
    # TODO dvcfs.path.normopath normalizes to windows path on Windows
    # even though other methods work as expected
    import posixpath

    return posixpath.normpath(path)


def _relpath(fs, path):
    # TODO from_os_path changes abs to relative
    # TODO we should be using `dvcfile.relpath` - in case of GitFS (plots diff)
    # and invoking from some subdir `dvcfile.relpath` returns strange long
    # relative paths
    # ("../../../../../../dvc.yaml") - investigate
    return fs.path.relpath(fs.path.join("/", fs.from_os_path(path)), fs.path.getcwd())


def _collect_output_plots(repo, targets, props, onerror: Optional[Callable] = None):
    fs = repo.dvcfs
    result: Dict[str, Dict] = {}
    for plot in repo.index.plots:
        plot_props = _plot_props(plot)
        dvcfile = plot.stage.dvcfile
        config_path = _relpath(fs, dvcfile.path)
        wdir_relpath = _relpath(fs, plot.stage.wdir)
        if _matches(targets, config_path, str(plot)):
            unpacked = unpack_if_dir(
                fs,
                _normpath(fs.path.join(wdir_relpath, plot.def_path)),
                props={**plot_props, **props},
                onerror=onerror,
            )

            dpath.merge(
                result,
                {"": unpacked},
            )
    return result


def _id_is_path(plot_props=None):
    if not plot_props:
        return True

    y_def = plot_props.get("y")
    return not isinstance(y_def, dict)


def _adjust_sources(fs, plot_props, config_dir):
    new_plot_props = deepcopy(plot_props)
    old_y = new_plot_props.pop("y", {})
    new_y = {}
    for filepath, val in old_y.items():
        new_y[_normpath(fs.path.join(config_dir, filepath))] = val
    new_plot_props["y"] = new_y
    return new_plot_props


def _resolve_definitions(
    fs: "FileSystem",
    targets: List[str],
    props: Dict[str, Any],
    config_path: "StrPath",
    definitions: "DictStrAny",
    onerror: Optional[Callable[[Any], Any]] = None,
):
    config_dir = fs.path.dirname(config_path)
    result: Dict[str, Dict] = {}
    for plot_id, plot_props in definitions.items():
        if plot_props is None:
            plot_props = {}
        if _id_is_path(plot_props):
            data_path = _normpath(fs.path.join(config_dir, plot_id))
            if _matches(targets, config_path, plot_id):
                unpacked = unpack_if_dir(
                    fs,
                    data_path,
                    props={**plot_props, **props},
                    onerror=onerror,
                )
                dpath.merge(
                    result,
                    unpacked,
                )
        elif _matches(targets, config_path, plot_id):
            adjusted_props = _adjust_sources(fs, plot_props, config_dir)
            dpath.merge(result, {"data": {plot_id: {**adjusted_props, **props}}})

    return result


def _collect_pipeline_files(repo, targets: List[str], props, onerror=None):
    result: Dict[str, Dict] = {}
    top_plots = repo.index._plots  # pylint: disable=protected-access
    for dvcfile, plots_def in top_plots.items():
        dvcfile_path = _relpath(repo.dvcfs, dvcfile)
        dvcfile_defs_dict: Dict[str, Union[Dict, None]] = {}
        for elem in plots_def:
            if isinstance(elem, str):
                dvcfile_defs_dict[elem] = None
            else:
                k, v = list(elem.items())[0]
                dvcfile_defs_dict[k] = v

        resolved = _resolve_definitions(
            repo.dvcfs,
            targets,
            props,
            dvcfile_path,
            dvcfile_defs_dict,
            onerror=onerror,
        )
        dpath.merge(
            result,
            {dvcfile_path: resolved},
        )
    return result


@error_handler
def _collect_definitions(
    repo: "Repo",
    targets=None,
    props: Optional[Dict] = None,
    onerror: Optional[Callable] = None,
    **kwargs,
) -> Dict:
    result: Dict = defaultdict(dict)
    props = props or {}

    fs = repo.dvcfs
    dpath.merge(
        result,
        _collect_pipeline_files(repo, targets, props, onerror=onerror),
    )

    dpath.merge(
        result,
        _collect_output_plots(repo, targets, props, onerror=onerror),
    )

    for target in targets:
        if not result or fs.exists(target):
            unpacked = unpack_if_dir(fs, target, props=props, onerror=onerror)
            dpath.merge(result[""], unpacked)

    return dict(result)


def unpack_if_dir(fs, path, props: Dict[str, str], onerror: Optional[Callable] = None):
    result: Dict[str, Dict] = defaultdict(dict)
    if fs.isdir(path):
        unpacked = _unpack_dir_files(fs, path, onerror=onerror)
    else:
        unpacked = {"data": [path]}

    if "data" in unpacked:
        for subpath in unpacked["data"]:
            result["data"].update({subpath: props})
    else:
        result.update(unpacked)

    return dict(result)


@error_handler
def parse(fs, path, props=None, **fs_kwargs):
    props = props or {}
    _, extension = os.path.splitext(path)
    if extension in SUPPORTED_IMAGE_EXTENSIONS:
        with fs.open(path, mode="rb", **fs_kwargs) as fd:
            return fd.read()

    if extension not in PARSERS.keys() | {".yml", ".yaml", ".csv", ".tsv"}:
        raise PlotMetricTypeError(path)

    with reraise(UnicodeDecodeError, EncodingError(path, "utf8")):
        with fs.open(path, mode="r", encoding="utf8", **fs_kwargs) as fd:
            contents = fd.read()

    if extension in (".csv", ".tsv"):
        header = props.get("header", True)
        delim = "\t" if extension == ".tsv" else ","
        return _load_sv(contents, delimiter=delim, header=header)
    return PARSERS[extension](contents, path)


def _plot_props(out: "Output") -> Dict:
    from dvc.schema import PLOT_PROPS

    if not (out.plot):
        raise NotAPlotError(out)
    if isinstance(out.plot, list):
        raise DvcException("Multiple plots per data file not supported.")
    if isinstance(out.plot, bool):
        return {}

    return project(out.plot, PLOT_PROPS)


def _load_sv(content, delimiter=",", header=True):
    if header:
        reader = csv.DictReader(io.StringIO(content), delimiter=delimiter)
    else:
        first_row = first(csv.reader(io.StringIO(content)))
        reader = csv.DictReader(
            io.StringIO(content),
            delimiter=delimiter,
            fieldnames=[str(i) for i in range(len(first_row))],
        )
    return list(reader)




dvc/repo/plots/diff.py
def _revisions(repo, revs, experiment):
    revisions = revs or []
    if experiment and len(revisions) == 1:
        baseline = repo.experiments.get_baseline(revisions[0])
        if baseline:
            revisions.append(baseline[:7])
    if len(revisions) <= 1:
        if len(revisions) == 0 and repo.scm.is_dirty(untracked_files=False):
            revisions.append("HEAD")
        revisions.append("workspace")
    return revisions


def diff(repo, *args, revs=None, experiment=False, **kwargs):
    if repo.scm.no_commits:
        return {}
    return repo.plots.show(*args, revs=_revisions(repo, revs, experiment), **kwargs)




dvc/stage/__init__.py
import logging
import os
import string
from collections import defaultdict
from contextlib import suppress
from dataclasses import dataclass
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Iterable,
    List,
    Optional,
    Set,
    Tuple,
    Type,
    TypeVar,
    Union,
)

from funcy import project

from dvc import prompt
from dvc.exceptions import CacheLinkError, CheckoutError, DvcException, MergeError
from dvc.utils import relpath
from dvc.utils.objects import cached_property

from . import params
from .decorators import rwlocked
from .exceptions import StageUpdateError
from .imports import sync_import, update_import
from .run import run_stage
from .utils import (
    check_circular_dependency,
    check_duplicated_arguments,
    check_missing_outputs,
    check_no_externals,
    check_stage_path,
    compute_md5,
    fill_stage_dependencies,
    fill_stage_outputs,
    get_dump,
)

if TYPE_CHECKING:
    from dvc.dvcfile import ProjectFile, SingleStageFile
    from dvc.output import Output
    from dvc.repo import Repo
    from dvc.types import StrPath
    from dvc_data.hashfile.hash_info import HashInfo
    from dvc_objects.db import ObjectDB

logger = logging.getLogger(__name__)
# Disallow all punctuation characters except hyphen and underscore
INVALID_STAGENAME_CHARS = set(string.punctuation) - {"_", "-"}
Env = Dict[str, str]
ChangedEntries = Tuple[List[str], List[str], Optional[str]]

_T = TypeVar("_T")


def loads_from(
    cls: Type[_T], repo: "Repo", path: str, wdir: str, data: Dict[str, Any]
) -> _T:
    kw = {
        "repo": repo,
        "path": path,
        "wdir": wdir,
        **project(
            data,
            [
                Stage.PARAM_CMD,
                Stage.PARAM_LOCKED,
                Stage.PARAM_FROZEN,
                Stage.PARAM_ALWAYS_CHANGED,
                Stage.PARAM_MD5,
                Stage.PARAM_DESC,
                Stage.PARAM_META,
                "name",
            ],
        ),
    }
    return cls(**kw)


@dataclass
class RawData:
    parametrized: bool = False
    generated_from: Optional[str] = None


def create_stage(cls: Type[_T], repo, path, external=False, **kwargs) -> _T:
    from dvc.dvcfile import check_dvcfile_path

    wdir = os.path.abspath(kwargs.get("wdir", None) or os.curdir)
    path = os.path.abspath(path)

    check_dvcfile_path(repo, path)
    check_stage_path(repo, wdir, is_wdir=kwargs.get("wdir"))
    check_stage_path(repo, os.path.dirname(path))

    stage = loads_from(cls, repo, path, wdir, kwargs)
    fill_stage_outputs(stage, **kwargs)
    if not external:
        check_no_externals(stage)
    fill_stage_dependencies(
        stage, **project(kwargs, ["deps", "erepo", "params", "fs_config"])
    )
    check_circular_dependency(stage)
    check_duplicated_arguments(stage)

    return stage


def restore_fields(stage: "Stage") -> None:
    from .exceptions import StageNotFound

    if not stage.dvcfile.exists():
        return

    try:
        old = stage.reload()
    except StageNotFound:
        return

    # will be used to restore comments later
    # pylint: disable=protected-access
    stage._stage_text = old._stage_text
    stage.meta = old.meta
    stage.desc = old.desc

    old_outs = {out.def_path: out for out in old.outs}
    for out in stage.outs:
        old_out = old_outs.get(out.def_path, None)
        if old_out is not None:
            out.restore_fields(old_out)


class Stage(params.StageParams):
    # pylint:disable=no-value-for-parameter
    # rwlocked() confuses pylint

    def __init__(  # noqa: PLR0913
        self,
        repo,
        path=None,
        cmd=None,
        wdir=os.curdir,
        deps=None,
        outs=None,
        md5=None,
        locked=False,  # backward compatibility
        frozen=False,
        always_changed=False,
        stage_text=None,
        dvcfile=None,
        desc: Optional[str] = None,
        meta=None,
    ):
        if deps is None:
            deps = []
        if outs is None:
            outs = []

        self.repo = repo
        self._path = path
        self.cmd = cmd
        self.wdir = wdir
        self.outs = outs
        self.deps = deps
        self.md5 = md5
        self.frozen = locked or frozen
        self.always_changed = always_changed
        self._stage_text = stage_text
        self._dvcfile = dvcfile
        self.desc: Optional[str] = desc
        self.meta = meta
        self.raw_data = RawData()

    @property
    def path(self) -> str:
        return self._path

    @path.setter
    def path(self, path: str):
        self._path = path
        self.__dict__.pop("path_in_repo", None)
        self.__dict__.pop("relpath", None)

    @property
    def dvcfile(self) -> Union["ProjectFile", "SingleStageFile"]:
        if self.path and self._dvcfile and self.path == self._dvcfile.path:
            return self._dvcfile

        if not self.path:
            raise DvcException(
                "Stage does not have any path set and is detached from dvcfile."
            )

        from dvc.dvcfile import load_file

        self._dvcfile = load_file(self.repo, self.path)
        return self._dvcfile

    @dvcfile.setter
    def dvcfile(self, dvcfile: Union["ProjectFile", "SingleStageFile"]) -> None:
        self._dvcfile = dvcfile

    def __repr__(self):
        return f"Stage: '{self.addressing}'"

    def __str__(self):
        return f"stage: '{self.addressing}'"

    @property
    def addressing(self) -> str:
        """
        Useful for alternative presentations where we don't need
        `Stage:` prefix.
        """
        return self.relpath if self.path else "No path"

    def __hash__(self):
        return hash(self.path_in_repo)

    def __eq__(self, other):
        return (
            self.__class__ == other.__class__
            and self.repo is other.repo
            and self.path_in_repo == other.path_in_repo
        )

    @cached_property
    def path_in_repo(self) -> str:
        return relpath(self.path, self.repo.root_dir)

    @cached_property
    def relpath(self) -> str:
        return relpath(self.path)

    @property
    def is_data_source(self) -> bool:
        """Whether the DVC file was created with `dvc add` or `dvc import`"""
        return self.cmd is None

    @property
    def is_callback(self) -> bool:
        """
        A callback stage is always considered as changed,
        so it runs on every `dvc repro` call.
        """
        return self.cmd and not any((self.deps, self.outs))

    @property
    def is_import(self) -> bool:
        """Whether the DVC file was created with `dvc import`."""
        return not self.cmd and len(self.deps) == 1 and len(self.outs) == 1

    @property
    def is_partial_import(self) -> bool:
        """
        Whether the DVC file was created using `dvc import --no-download`
        or `dvc import-url --no-download`.
        """
        return self.is_import and (not self.outs[0].hash_info)

    @property
    def is_repo_import(self) -> bool:
        if not self.is_import:
            return False

        from dvc.dependency import RepoDependency

        return isinstance(self.deps[0], RepoDependency)

    @property
    def is_versioned_import(self) -> bool:
        return self.is_import and self.deps[0].fs.version_aware

    def short_description(self) -> Optional["str"]:
        desc: Optional["str"] = None
        if self.desc:
            with suppress(ValueError):
                # try to use first non-empty line as a description
                line = next(filter(None, self.desc.splitlines()))
                return line.strip()
        return desc

    def env(self) -> Env:
        from dvc.env import DVC_ROOT

        env: Env = {}
        if self.repo:
            env.update({DVC_ROOT: self.repo.root_dir})

        return env

    def changed_deps(self, allow_missing: bool = False) -> bool:
        if self.frozen:
            return False

        if self.is_callback or self.always_changed:
            return True

        return self._changed_deps(allow_missing=allow_missing)

    @rwlocked(read=["deps"])
    def _changed_deps(self, allow_missing: bool = False) -> bool:
        for dep in self.deps:
            status = dep.status()
            if status:
                if allow_missing and status[str(dep)] == "deleted":
                    continue
                logger.debug(
                    "Dependency '%s' of %s changed because it is '%s'.",
                    dep,
                    self,
                    status[str(dep)],
                )
                return True
        return False

    @rwlocked(read=["outs"])
    def changed_outs(self, allow_missing: bool = False) -> bool:
        for out in self.outs:
            status = out.status()
            if status:
                if allow_missing and status[str(out)] == "not in cache":
                    continue
                logger.debug(
                    "Output '%s' of %s changed because it is '%s'.",
                    out,
                    self,
                    status[str(out)],
                )
                return True

        return False

    def changed_stage(self) -> bool:
        changed = self.md5 != self.compute_md5()
        if changed:
            logger.debug(self._changed_stage_entry())
        return changed

    @rwlocked(read=["deps", "outs"])
    def changed(self, allow_missing: bool = False) -> bool:
        is_changed = (
            # Short-circuit order: stage md5 is fast,
            # deps are expected to change
            self.changed_stage()
            or self.changed_deps(allow_missing=allow_missing)
            or self.changed_outs(allow_missing=allow_missing)
        )
        if is_changed:
            logger.debug("%s changed.", self)
        return is_changed

    @rwlocked(write=["outs"])
    def remove_outs(self, ignore_remove=False, force=False) -> None:
        """Used mainly for `dvc remove --outs` and :func:`Stage.reproduce`."""
        for out in self.outs:
            if out.persist and not force:
                out.unprotect()
                continue

            logger.debug("Removing output '%s' of %s.", out, self)
            out.remove(ignore_remove=ignore_remove)

    def unprotect_outs(self) -> None:
        for out in self.outs:
            out.unprotect()

    def ignore_remove_outs(self) -> None:
        for out in self.outs:
            out.ignore_remove()

    @rwlocked(write=["outs"])
    def remove(self, force=False, remove_outs=True, purge=True) -> None:
        if remove_outs:
            self.remove_outs(ignore_remove=True, force=force)
        else:
            self.unprotect_outs()
            self.ignore_remove_outs()
        if purge:
            self.dvcfile.remove_stage(self)

    def transfer(
        self,
        source: str,
        odb: Optional["ObjectDB"] = None,
        to_remote: bool = False,
        jobs: Optional[int] = None,
        force: bool = False,
    ) -> None:
        assert len(self.outs) == 1
        (out,) = self.outs
        out.transfer(source, odb=odb, jobs=jobs)
        if not to_remote:
            out.checkout(force=force)
            out.ignore()

    @rwlocked(read=["deps"], write=["outs"])
    def reproduce(self, interactive=False, **kwargs) -> Optional["Stage"]:
        if not (
            kwargs.get("force", False)
            or self.changed(kwargs.get("allow_missing", False))
        ):
            if not isinstance(self, PipelineStage) and self.is_data_source:
                logger.info("'%s' didn't change, skipping", self.addressing)
            else:
                logger.info("Stage '%s' didn't change, skipping", self.addressing)
            return None

        msg = "Going to reproduce {stage}. Are you sure you want to continue?".format(
            stage=self
        )

        if interactive and not prompt.confirm(msg):
            raise DvcException("reproduction aborted by the user")

        self.run(**kwargs)

        logger.debug("%s was reproduced", self)

        return self

    def update(
        self,
        rev=None,
        to_remote=False,
        remote=None,
        no_download=None,
        jobs=None,
    ) -> None:
        if not (self.is_repo_import or self.is_import):
            raise StageUpdateError(self.relpath)
        update_import(
            self,
            rev=rev,
            to_remote=to_remote,
            remote=remote,
            no_download=no_download,
            jobs=jobs,
        )

    def reload(self) -> "Stage":
        return self.dvcfile.stage

    def dumpd(self, **kwargs) -> Dict[str, Any]:
        return get_dump(self, **kwargs)

    def compute_md5(self) -> Optional[str]:
        # `dvc add`ed files don't need stage md5
        if self.is_data_source and not (self.is_import or self.is_repo_import):
            m = None
        else:
            m = compute_md5(self)
        logger.debug("Computed %s md5: '%s'", self, m)
        return m

    def save(self, allow_missing: bool = False, run_cache: bool = True):
        self.save_deps(allow_missing=allow_missing)

        self.save_outs(allow_missing=allow_missing)

        self.md5 = self.compute_md5()

        if run_cache:
            self.repo.stage_cache.save(self)

    def save_deps(self, allow_missing=False):
        from dvc.dependency.base import DependencyDoesNotExistError

        for dep in self.deps:
            try:
                dep.save()
            except DependencyDoesNotExistError:
                if not allow_missing:
                    raise

    def get_versioned_outs(self) -> Dict[str, "Output"]:
        from .exceptions import StageFileDoesNotExistError, StageNotFound

        try:
            old = self.reload()
        except (StageFileDoesNotExistError, StageNotFound):
            return {}

        return {
            out.def_path: out
            for out in old.outs
            if out.files is not None
            or (out.meta is not None and out.meta.version_id is not None)
        }

    def save_outs(self, allow_missing: bool = False):
        from dvc.output import OutputDoesNotExistError

        old_versioned_outs = self.get_versioned_outs()
        for out in self.outs:
            try:
                out.save()
            except OutputDoesNotExistError:
                if not allow_missing:
                    raise

            if old_out := old_versioned_outs.get(out.def_path):
                out.merge_version_meta(old_out)

    def ignore_outs(self) -> None:
        for out in self.outs:
            out.ignore()

    @staticmethod
    def _changed_entries(entries) -> List[str]:
        return [str(entry) for entry in entries if entry.workspace_status()]

    def _changed_stage_entry(self) -> str:
        return f"'md5' of {self} changed."

    def changed_entries(self) -> ChangedEntries:
        changed_deps = self._changed_entries(self.deps)
        changed_outs = self._changed_entries(self.outs)
        return (
            changed_deps,
            changed_outs,
            self._changed_stage_entry() if self.changed_stage() else None,
        )

    @rwlocked(write=["outs"])
    def commit(self, allow_missing=False, filter_info=None, **kwargs) -> None:
        from dvc.output import OutputDoesNotExistError

        link_failures = []
        for out in self.filter_outs(filter_info):
            try:
                out.commit(filter_info=filter_info, **kwargs)
            except OutputDoesNotExistError:
                if not allow_missing:
                    raise
            except CacheLinkError:
                link_failures.append(out.fs_path)
        if link_failures:
            raise CacheLinkError(link_failures)

    @rwlocked(write=["outs"])
    def add_outs(  # noqa: C901
        self, filter_info=None, allow_missing: bool = False, **kwargs
    ):
        from dvc.output import OutputDoesNotExistError

        link_failures = []
        old_versioned_outs = self.get_versioned_outs()
        for out in self.filter_outs(filter_info):
            try:
                out.add(filter_info, **kwargs)
            except (FileNotFoundError, OutputDoesNotExistError):
                if not allow_missing:
                    raise
            except CacheLinkError:
                link_failures.append(filter_info or out.fs_path)

            if old_out := old_versioned_outs.get(out.def_path):
                out.merge_version_meta(old_out)

        if link_failures:
            raise CacheLinkError(link_failures)

    @rwlocked(read=["deps", "outs"])
    def run(  # noqa: C901
        self,
        dry=False,
        no_commit=False,
        force=False,
        allow_missing=False,
        no_download=False,
        **kwargs,
    ) -> None:
        if (self.cmd or self.is_import) and not self.frozen and not dry:
            self.remove_outs(ignore_remove=False, force=False)

        if (
            self.is_import and (not self.frozen or kwargs.get("pull"))
        ) or self.is_partial_import:
            self._sync_import(dry, force, kwargs.get("jobs", None), no_download)
        elif not self.frozen and self.cmd:
            self._run_stage(dry, force, allow_missing=allow_missing, **kwargs)
        elif kwargs.get("pull"):
            logger.info("Pulling data for %s", self)
            self.repo.pull(self.addressing, jobs=kwargs.get("jobs", None))
            self.checkout()
        elif not dry:
            args = ("outputs", "frozen ") if self.frozen else ("data sources", "")
            logger.info("Verifying %s in %s%s", *args, self)
            self._check_missing_outputs()

        if not dry:
            if no_download:
                allow_missing = True

            no_cache_outs = any(
                not out.use_cache
                for out in self.outs
                if not (out.is_metric or out.is_plot)
            )
            self.save(
                allow_missing=allow_missing,
                run_cache=not no_commit and not no_cache_outs,
            )

            if no_download:
                self.ignore_outs()
            if not no_commit:
                self.commit(allow_missing=allow_missing)

    @rwlocked(read=["deps"], write=["outs"])
    def _run_stage(self, dry, force, **kwargs) -> None:
        return run_stage(self, dry, force, **kwargs)

    @rwlocked(read=["deps"], write=["outs"])
    def _sync_import(self, dry, force, jobs, no_download) -> None:
        sync_import(self, dry, force, jobs, no_download)

    @rwlocked(read=["outs"])
    def _check_missing_outputs(self) -> None:
        check_missing_outputs(self)

    def filter_outs(self, fs_path) -> Iterable["Output"]:
        def _func(o):
            return o.fs.path.isin_or_eq(fs_path, o.fs_path)

        return filter(_func, self.outs) if fs_path else self.outs

    @rwlocked(write=["outs"])
    def checkout(
        self, allow_missing: bool = False, **kwargs
    ) -> Dict[str, List["StrPath"]]:
        stats: Dict[str, List["StrPath"]] = defaultdict(list)
        if self.is_partial_import:
            return stats

        for out in self.filter_outs(kwargs.get("filter_info")):
            key, outs = self._checkout(
                out,
                allow_missing=allow_missing,
                **kwargs,
            )
            if key:
                stats[key].extend(outs)
        return stats

    @staticmethod
    def _checkout(out, **kwargs) -> Tuple[Optional[str], List[str]]:
        try:
            result = out.checkout(**kwargs)
            added, modified = result or (None, None)
            if not (added or modified):
                return None, []
            return "modified" if modified else "added", [str(out)]
        except CheckoutError as exc:
            return "failed", exc.target_infos

    @rwlocked(read=["deps", "outs"])
    def status(
        self, check_updates: bool = False, filter_info: Optional[bool] = None
    ) -> Dict[str, List[Union[str, Dict[str, str]]]]:
        ret: List[Union[str, Dict[str, str]]] = []
        show_import = (
            self.is_repo_import or self.is_versioned_import
        ) and check_updates

        if not self.frozen or show_import:
            self._status_deps(ret)
        self._status_outs(ret, filter_info=filter_info)
        self._status_always_changed(ret)
        self._status_stage(ret)
        return {self.addressing: ret} if ret else {}

    @staticmethod
    def _status(entries: Iterable["Output"]) -> Dict[str, str]:
        ret = {}

        for entry in entries:
            ret.update(entry.status())

        return ret

    def _status_deps(self, ret) -> None:
        deps_status = self._status(self.deps)
        if deps_status:
            ret.append({"changed deps": deps_status})

    def _status_outs(self, ret, filter_info) -> None:
        filter_outs = self.filter_outs(filter_info)
        outs_status = self._status(filter_outs)
        if outs_status:
            ret.append({"changed outs": outs_status})

    def _status_always_changed(self, ret) -> None:
        if self.is_callback or self.always_changed:
            ret.append("always changed")

    def _status_stage(self, ret) -> None:
        if self.changed_stage():
            ret.append("changed checksum")

    def already_cached(self) -> bool:
        return not self.changed_stage() and self.deps_cached() and self.outs_cached()

    def deps_cached(self) -> bool:
        return all(not dep.changed() for dep in self.deps)

    def outs_cached(self) -> bool:
        return all(
            not out.changed_cache() if out.use_cache else not out.changed()
            for out in self.outs
        )

    def get_all_files_number(self, filter_info=None) -> int:
        return sum(
            out.get_files_number(filter_info) for out in self.filter_outs(filter_info)
        )

    def get_used_objs(
        self, *args, **kwargs
    ) -> Dict[Optional["ObjectDB"], Set["HashInfo"]]:
        """Return set of object IDs used by this stage."""
        if self.is_partial_import and not self.is_repo_import:
            return {}

        used_objs = defaultdict(set)
        for out in self.filter_outs(kwargs.get("filter_info")):
            for odb, objs in out.get_used_objs(*args, **kwargs).items():
                used_objs[odb].update(objs)
        return used_objs

    @staticmethod
    def _check_can_merge(stage, ancestor_out=None) -> None:
        if isinstance(stage, PipelineStage):
            raise MergeError("unable to auto-merge pipeline stages")

        if not stage.is_data_source or stage.deps or len(stage.outs) > 1:
            raise MergeError(
                "unable to auto-merge DVC files that weren't created by `dvc add`"
            )

        if ancestor_out and not stage.outs:
            raise MergeError("unable to auto-merge DVC files with deleted outputs")

    def merge(self, ancestor, other, allowed=None) -> None:
        assert other

        if not other.outs:
            return

        if not self.outs:
            self.outs = other.outs
            return

        if ancestor:
            self._check_can_merge(ancestor)
            outs = ancestor.outs
            ancestor_out = outs[0] if outs else None
        else:
            ancestor_out = None

        self._check_can_merge(self, ancestor_out)
        self._check_can_merge(other, ancestor_out)

        self.outs[0].merge(ancestor_out, other.outs[0], allowed=allowed)

    def dump(self, **kwargs) -> None:
        self.dvcfile.dump(self, **kwargs)


class PipelineStage(Stage):
    def __init__(self, *args, name: Optional[str] = None, **kwargs):
        super().__init__(*args, **kwargs)
        self.name = name
        self.cmd_changed = False
        self.tracked_vars: Dict[str, Dict[str, Dict[str, str]]] = {}

    def __eq__(self, other):
        return super().__eq__(other) and self.name == other.name

    def __hash__(self) -> int:
        return hash((self.path_in_repo, self.name))

    @property
    def addressing(self):
        from dvc.dvcfile import PROJECT_FILE

        if self.path and self.relpath == PROJECT_FILE:
            return self.name
        return f"{super().addressing}:{self.name}"

    def reload(self) -> Stage:
        from dvc.dvcfile import ProjectFile

        assert isinstance(self.dvcfile, ProjectFile)

        self.dvcfile._reset()  # pylint: disable=protected-access
        return self.dvcfile.stages[self.name]

    def _status_stage(self, ret) -> None:
        if self.cmd_changed:
            ret.append("changed command")

    def changed_stage(self) -> bool:
        if self.cmd_changed:
            logger.debug(self._changed_stage_entry())
        return self.cmd_changed

    def _changed_stage_entry(self) -> str:
        return f"'cmd' of {self} has changed."

    def merge(self, ancestor, other, allowed=None):
        raise NotImplementedError




dvc/stage/cache.py
import logging
import os
from contextlib import contextmanager
from typing import TYPE_CHECKING, List, Optional, Tuple

from funcy import first

from dvc import fs
from dvc.exceptions import DvcException
from dvc.utils import dict_sha256, relpath

if TYPE_CHECKING:
    from dvc_objects.db import ObjectDB

logger = logging.getLogger(__name__)


class RunCacheNotFoundError(DvcException):
    def __init__(self, stage):
        super().__init__(f"No run-cache for {stage.addressing}")


class RunCacheNotSupported(DvcException):
    pass


def _get_cache_hash(cache, key=False):
    from dvc_data.hashfile.meta import Meta

    if key:
        cache["outs"] = [out["path"] for out in cache.get("outs", [])]
    return dict_sha256(cache, exclude=[Meta.PARAM_SIZE, Meta.PARAM_NFILES])


def _can_hash(stage):
    if stage.is_callback or stage.always_changed:
        return False

    if not all([stage.cmd, stage.deps, stage.outs]):
        return False

    for dep in stage.deps:
        if not (dep.protocol == "local" and dep.def_path and dep.get_hash()):
            return False

    for out in stage.outs:
        if out.protocol != "local" or not out.def_path or out.persist:
            return False

    return True


def _get_stage_hash(stage):
    from .serialize import to_single_stage_lockfile

    assert _can_hash(stage)
    return _get_cache_hash(to_single_stage_lockfile(stage), key=True)


class StageCache:
    def __init__(self, repo):
        self.repo = repo
        self.cache_dir = os.path.join(self.repo.cache.local.path, "runs")

    def _get_cache_dir(self, key):
        return os.path.join(self.cache_dir, key[:2], key)

    def _get_cache_path(self, key, value):
        return os.path.join(self._get_cache_dir(key), value)

    def _load_cache(self, key, value):
        from voluptuous import Invalid

        from dvc.schema import COMPILED_LOCK_FILE_STAGE_SCHEMA
        from dvc.utils.serialize import YAMLFileCorruptedError, load_yaml

        path = self._get_cache_path(key, value)

        try:
            return COMPILED_LOCK_FILE_STAGE_SCHEMA(load_yaml(path))
        except FileNotFoundError:
            return None
        except (YAMLFileCorruptedError, Invalid):
            logger.warning("corrupted cache file '%s'.", relpath(path))
            os.unlink(path)
            return None

    def _load(self, stage):
        key = _get_stage_hash(stage)
        if not key:
            return None

        cache_dir = self._get_cache_dir(key)
        if not os.path.exists(cache_dir):
            return None

        for value in os.listdir(cache_dir):
            cache = self._load_cache(key, value)
            if cache:
                return cache

        return None

    def _create_stage(self, cache, wdir=None):
        from . import PipelineStage, create_stage
        from .loader import StageLoader

        stage = create_stage(
            PipelineStage,
            repo=self.repo,
            path="dvc.yaml",
            cmd=cache["cmd"],
            wdir=wdir,
            outs=[out["path"] for out in cache["outs"]],
            external=True,
        )
        StageLoader.fill_from_lock(stage, cache)
        return stage

    @contextmanager
    def _cache_type_copy(self):
        cache_types = self.repo.cache.local.cache_types
        self.repo.cache.local.cache_types = ["copy"]
        try:
            yield
        finally:
            self.repo.cache.local.cache_types = cache_types

    def _uncached_outs(self, stage, cache):
        # NOTE: using temporary stage to avoid accidentally modifying original
        # stage and to workaround `commit/checkout` not working for uncached
        # outputs.
        cached_stage = self._create_stage(cache, wdir=stage.wdir)

        outs_no_cache = [out.def_path for out in stage.outs if not out.use_cache]

        # NOTE: using copy link to make it look like a git-tracked file
        with self._cache_type_copy():
            for out in cached_stage.outs:
                if out.def_path in outs_no_cache:
                    yield out

    def save(self, stage):
        from .serialize import to_single_stage_lockfile

        if not _can_hash(stage):
            return

        cache_key = _get_stage_hash(stage)
        cache = to_single_stage_lockfile(stage)
        cache_value = _get_cache_hash(cache)

        existing_cache = self._load_cache(cache_key, cache_value)
        cache = existing_cache or cache

        for out in self._uncached_outs(stage, cache):
            out.commit()

        if existing_cache:
            return

        from dvc.schema import COMPILED_LOCK_FILE_STAGE_SCHEMA
        from dvc.utils.serialize import dump_yaml

        # sanity check
        COMPILED_LOCK_FILE_STAGE_SCHEMA(cache)

        path = self._get_cache_path(cache_key, cache_value)
        local_fs = self.repo.cache.local.fs
        parent = local_fs.path.parent(path)
        self.repo.cache.local.makedirs(parent)
        tmp = local_fs.path.join(parent, fs.utils.tmp_fname())
        assert os.path.exists(parent)
        assert os.path.isdir(parent)
        dump_yaml(tmp, cache)
        self.repo.cache.local.move(tmp, path)

    def restore(self, stage, run_cache=True, pull=False, dry=False):
        from .serialize import to_single_stage_lockfile

        if not _can_hash(stage):
            raise RunCacheNotFoundError(stage)

        if (
            not stage.changed_stage()
            and stage.deps_cached()
            and all(bool(out.hash_info) for out in stage.outs)
        ):
            cache = to_single_stage_lockfile(stage)
        else:
            if not run_cache:  # backward compatibility
                raise RunCacheNotFoundError(stage)
            if not dry:
                stage.save_deps()
            cache = self._load(stage)
            if not cache:
                raise RunCacheNotFoundError(stage)

        cached_stage = self._create_stage(cache, wdir=stage.wdir)

        if pull and not dry:
            for objs in cached_stage.get_used_objs().values():
                self.repo.cloud.pull(objs)

        if not cached_stage.outs_cached():
            raise RunCacheNotFoundError(stage)

        logger.info(
            "Stage '%s' is cached - skipping run, checking out outputs",
            stage.addressing,
        )
        if not dry:
            cached_stage.checkout()

    def transfer(self, from_odb, to_odb):
        from dvc.fs import HTTPFileSystem, LocalFileSystem
        from dvc.fs.callbacks import Callback

        from_fs = from_odb.fs
        to_fs = to_odb.fs
        func = fs.generic.log_exceptions(fs.generic.copy)
        runs = from_fs.path.join(from_odb.path, "runs")

        http_odb = next(
            (odb for odb in (from_odb, to_odb) if isinstance(odb.fs, HTTPFileSystem)),
            None,
        )
        if http_odb:
            path = http_odb.path
            message = f"run-cache is not supported for http filesystem: {path}"
            raise RunCacheNotSupported(message)

        ret: List[Tuple[str, str]] = []
        if not from_fs.exists(runs):
            return ret

        for src in from_fs.find(runs):
            rel = from_fs.path.relpath(src, from_odb.path)
            if not isinstance(to_fs, LocalFileSystem):
                rel = from_fs.path.as_posix(rel)

            dst = to_fs.path.join(to_odb.path, rel)
            key = to_fs.path.parent(dst)
            # check if any build cache already exists for this key
            # TODO: check if MaxKeys=1 or something like that applies
            # or otherwise this will take a lot of time!
            if to_fs.exists(key) and first(to_fs.find(key)):
                continue

            src_name = from_fs.path.name(src)
            parent_name = from_fs.path.name(from_fs.path.parent(src))
            with Callback.as_tqdm_callback(
                desc=src_name,
                bytes=True,
            ) as cb:
                func(from_fs, src, to_fs, dst, callback=cb)
            ret.append((parent_name, src_name))
        return ret

    def push(self, remote: Optional[str], odb: Optional["ObjectDB"] = None):
        dest_odb = odb or self.repo.cloud.get_remote_odb(remote, "push --run-cache")
        return self.transfer(self.repo.cache.local, dest_odb)

    def pull(self, remote: Optional[str], odb: Optional["ObjectDB"] = None):
        odb = odb or self.repo.cloud.get_remote_odb(remote, "fetch --run-cache")
        return self.transfer(odb, self.repo.cache.local)

    def get_used_objs(self, used_run_cache, *args, **kwargs):
        """Return used cache for the specified run-cached stages."""
        from collections import defaultdict

        used_objs = defaultdict(set)
        for key, value in used_run_cache:
            entry = self._load_cache(key, value)
            if not entry:
                continue
            stage = self._create_stage(entry)
            for odb, objs in stage.get_used_objs(*args, **kwargs).items():
                used_objs[odb].update(objs)
        return used_objs




dvc/stage/decorators.py
from functools import wraps

from funcy import decorator


@decorator
def rwlocked(call, read=None, write=None):
    import sys

    from dvc.dependency.repo import RepoDependency
    from dvc.rwlock import rwlock

    if read is None:
        read = []

    if write is None:
        write = []

    stage = call._args[0]  # pylint: disable=protected-access

    assert stage.repo.lock.is_locked

    def _chain(names):
        return [
            item.fs_path
            for attr in names
            for item in getattr(stage, attr)
            # There is no need to lock RepoDependency deps, as there is no
            # corresponding OutputREPO, so we can't even write it.
            if not isinstance(item, RepoDependency)
        ]

    cmd = " ".join(sys.argv)

    with rwlock(
        stage.repo.tmp_dir,
        stage.repo.fs,
        cmd,
        _chain(read),
        _chain(write),
        stage.repo.config["core"].get("hardlink_lock", False),
    ):
        return call()


def unlocked_repo(f):
    @wraps(f)
    def wrapper(stage, *args, **kwargs):
        stage.repo.lock.unlock()
        stage.repo._reset()  # pylint: disable=protected-access
        try:
            ret = f(stage, *args, **kwargs)
        finally:
            stage.repo.lock.lock()
        return ret

    return wrapper


def relock_repo(f):
    @wraps(f)
    def wrapper(stage, *args, **kwargs):
        stage.repo.lock.lock()
        try:
            ret = f(stage, *args, **kwargs)
        finally:
            stage.repo.lock.unlock()
            stage.repo._reset()  # pylint: disable=protected-access
        return ret

    return wrapper




dvc/stage/exceptions.py
from dvc.exceptions import DvcException


class StageCmdFailedError(DvcException):
    def __init__(self, cmd, status=None):
        msg = f"failed to run: {cmd}"
        if status is not None:
            msg += f", exited with {status}"
        super().__init__(msg)


class StageFileDoesNotExistError(DvcException):
    DVC_IGNORED = "is dvc-ignored"
    DOES_NOT_EXIST = "does not exist"

    def __init__(self, fname, dvc_ignored=False):
        self.file = fname
        message = self.DVC_IGNORED if dvc_ignored else self.DOES_NOT_EXIST
        super().__init__(f"'{self.file}' {message}")


class StageFileAlreadyExistsError(DvcException):
    pass


class StageFileIsNotDvcFileError(DvcException):
    def __init__(self, fname):
        from dvc.dvcfile import DVC_FILE_SUFFIX, is_dvc_file

        msg = f"'{fname}' is not a .dvc file"

        sname = fname + DVC_FILE_SUFFIX
        if is_dvc_file(sname):
            msg += f". Do you mean '{sname}'?"

        super().__init__(msg)


class StageFileBadNameError(DvcException):
    pass


class StagePathOutsideError(DvcException):
    pass


class StagePathNotFoundError(DvcException):
    pass


class StagePathNotDirectoryError(DvcException):
    pass


class StageCommitError(DvcException):
    pass


class StageExternalOutputsError(DvcException):
    pass


class StageUpdateError(DvcException):
    def __init__(self, path):
        super().__init__(f"update is not supported for '{path}' that is not an import.")


class MissingDataSource(DvcException):
    def __init__(self, missing_files):
        assert len(missing_files) > 0

        source = "source"
        if len(missing_files) > 1:
            source += "s"

        msg = "missing data '{}': {}".format(source, ", ".join(missing_files))
        super().__init__(msg)


class DataSourceChanged(DvcException):
    def __init__(self, path: str):
        super().__init__(f"data source changed: {path}")


class StageNotFound(DvcException, KeyError):
    def __init__(self, file, name):
        self.file = file.relpath
        self.name = name
        super().__init__(f"Stage '{self.name}' not found inside '{self.file}' file")

    def __str__(self):
        # `KeyError` quotes the message
        # see: https://bugs.python.org/issue2651
        return self.msg


class StageNameUnspecified(DvcException):
    def __init__(self, file):
        super().__init__(
            "Stage name not provided."
            "Please specify the name as: `{}:stage_name`".format(file.relpath)
        )


class DuplicateStageName(DvcException):
    pass


class InvalidStageName(DvcException):
    def __init__(self):
        super().__init__("Stage name cannot contain punctuation characters.")




dvc/stage/imports.py
import logging

from dvc.exceptions import InvalidArgumentError

logger = logging.getLogger(__name__)


def _update_import_on_remote(stage, remote, jobs):
    if stage.is_repo_import:
        raise InvalidArgumentError(
            "Data imported from other DVC or Git repositories can't "
            "be updated with --to-remote"
        )

    url = stage.deps[0].def_path
    odb = stage.repo.cloud.get_remote_odb(remote, "update")
    stage.outs[0].transfer(url, odb=odb, jobs=jobs, update=True)


def update_import(
    stage, rev=None, to_remote=False, remote=None, no_download=None, jobs=None
):
    stage.deps[0].update(rev=rev)

    frozen = stage.frozen
    stage.frozen = False
    changed = stage.changed()

    try:
        if to_remote:
            _update_import_on_remote(stage, remote, jobs)
        else:
            stage.reproduce(no_download=no_download, jobs=jobs)
    finally:
        if no_download and changed:
            # Avoid retaining stale information
            stage.outs[0].clear()
        stage.frozen = frozen


def sync_import(
    stage,
    dry=False,
    force=False,
    jobs=None,
    no_download=False,
):
    """Synchronize import's outs to the workspace."""
    logger.info("Importing '%s' -> '%s'", stage.deps[0], stage.outs[0])
    if dry:
        return

    if not force and stage.already_cached():
        stage.outs[0].checkout()
    else:
        stage.save_deps()
        if no_download:
            if stage.is_repo_import:
                stage.deps[0].update()
        else:
            stage.deps[0].download(
                stage.outs[0],
                jobs=jobs,
            )




dvc/stage/loader.py
import logging
from collections.abc import Mapping
from copy import deepcopy
from itertools import chain
from typing import TYPE_CHECKING, Any, Dict, Iterable, Optional, Tuple

from funcy import get_in, lcat, once, project

from dvc import dependency, output
from dvc.parsing import FOREACH_KWD, JOIN, EntryNotFound
from dvc.utils.objects import cached_property
from dvc_data.hashfile.hash_info import HashInfo
from dvc_data.hashfile.meta import Meta

from . import PipelineStage, Stage, loads_from
from .exceptions import StageNameUnspecified, StageNotFound
from .params import StageParams
from .utils import fill_stage_dependencies, resolve_paths

if TYPE_CHECKING:
    from dvc.dvcfile import ProjectFile, SingleStageFile
    from dvc.output import Output

logger = logging.getLogger(__name__)


class StageLoader(Mapping):
    def __init__(
        self,
        dvcfile: "ProjectFile",
        data,
        lockfile_data=None,
    ):
        self.dvcfile = dvcfile
        self.resolver = self.dvcfile.resolver
        self.data = data or {}
        self.stages_data = self.data.get("stages", {})
        self.repo = self.dvcfile.repo

        lockfile_data = lockfile_data or {}
        self._lockfile_data = lockfile_data.get("stages", {})

    @cached_property
    def lockfile_data(self) -> Dict[str, Any]:
        if not self._lockfile_data:
            logger.debug("Lockfile for '%s' not found", self.dvcfile.relpath)
        return self._lockfile_data

    @staticmethod
    def fill_from_lock(stage, lock_data=None):
        """Fill values for params, checksums for outs and deps from lock."""
        if not lock_data:
            return

        from dvc.output import merge_file_meta_from_cloud

        assert isinstance(lock_data, dict)
        items: Iterable[Tuple[str, "Output"]] = chain(
            ((StageParams.PARAM_DEPS, dep) for dep in stage.deps),
            ((StageParams.PARAM_OUTS, out) for out in stage.outs),
        )

        checksums = {
            key: {item["path"]: item for item in lock_data.get(key, {})}
            for key in [StageParams.PARAM_DEPS, StageParams.PARAM_OUTS]
        }
        for key, item in items:
            path = item.def_path
            if isinstance(item, dependency.ParamsDependency):
                item.fill_values(get_in(lock_data, [stage.PARAM_PARAMS, path]))
                continue
            info = get_in(checksums, [key, path], {})
            info = info.copy()
            info.pop("path", None)

            item.meta = Meta.from_dict(merge_file_meta_from_cloud(info))
            hash_value = getattr(item.meta, item.hash_name, None)
            item.hash_info = HashInfo(item.hash_name, hash_value)
            files = get_in(checksums, [key, path, item.PARAM_FILES], None)
            if files:
                item.files = [merge_file_meta_from_cloud(f) for f in files]
            # pylint: disable-next=protected-access
            item._compute_meta_hash_info_from_files()

    @classmethod
    def load_stage(cls, dvcfile: "ProjectFile", name, stage_data, lock_data=None):
        assert all([name, dvcfile, dvcfile.repo, dvcfile.path])
        assert stage_data
        assert isinstance(stage_data, dict)

        path, wdir = resolve_paths(
            dvcfile.repo.fs, dvcfile.path, stage_data.get(Stage.PARAM_WDIR)
        )
        stage = loads_from(PipelineStage, dvcfile.repo, path, wdir, stage_data)
        stage.name = name
        stage.desc = stage_data.get(Stage.PARAM_DESC)
        stage.meta = stage_data.get(Stage.PARAM_META)

        deps = project(stage_data, [stage.PARAM_DEPS, stage.PARAM_PARAMS])
        fill_stage_dependencies(stage, **deps)

        outs = project(
            stage_data,
            [
                stage.PARAM_OUTS,
                stage.PARAM_METRICS,
                stage.PARAM_PLOTS,
            ],
        )
        stage.outs = lcat(
            output.load_from_pipeline(stage, data, typ=key)
            for key, data in outs.items()
        )

        if lock_data:
            stage.cmd_changed = lock_data.get(Stage.PARAM_CMD) != stage.cmd

        cls.fill_from_lock(stage, lock_data)
        return stage

    @once
    def lockfile_needs_update(self):
        # if lockfile does not have all of the entries that dvc.yaml says it
        # should have, provide a debug message once
        # pylint: disable=protected-access
        lockfile = self.dvcfile._lockfile.relpath
        logger.debug("Lockfile '%s' needs to be updated.", lockfile)

    def __getitem__(self, name):
        if not name:
            raise StageNameUnspecified(self.dvcfile)

        try:
            resolved_data = self.resolver.resolve_one(name)
        except EntryNotFound:
            raise StageNotFound(self.dvcfile, name)  # noqa: B904

        if self.lockfile_data and name not in self.lockfile_data:
            self.lockfile_needs_update()
            logger.trace(  # type: ignore[attr-defined]
                "No lock entry found for '%s:%s'", self.dvcfile.relpath, name
            )

        resolved_stage = resolved_data[name]
        stage = self.load_stage(
            self.dvcfile,
            name,
            resolved_stage,
            self.lockfile_data.get(name, {}),
        )

        stage.tracked_vars = self.resolver.tracked_vars.get(name, {})
        group, *keys = name.rsplit(JOIN, maxsplit=1)
        if group and keys and name not in self.stages_data:
            stage.raw_data.generated_from = group

        stage.raw_data.parametrized = self.stages_data.get(name, {}) != resolved_stage
        return stage

    def __iter__(self):
        return iter(self.resolver.get_keys())

    def __len__(self):
        return len(self.resolver.get_keys())

    def __contains__(self, name):
        return self.resolver.has_key(name)  # noqa: W601

    def is_foreach_generated(self, name: str):
        return name in self.stages_data and FOREACH_KWD in self.stages_data[name]


class SingleStageLoader(Mapping):
    def __init__(
        self,
        dvcfile: "SingleStageFile",
        stage_data: Dict[Any, str],
        stage_text: Optional[str] = None,
    ):
        self.dvcfile = dvcfile
        self.stage_data = stage_data or {}
        self.stage_text = stage_text

    def __getitem__(self, item):
        if item:
            logger.warning(
                "Ignoring name '%s' for single stage in '%s'.",
                item,
                self.dvcfile,
            )
        # during `load`, we remove attributes from stage data, so as to
        # not duplicate, therefore, for MappingView, we need to deepcopy.
        return self.load_stage(self.dvcfile, deepcopy(self.stage_data), self.stage_text)

    @classmethod
    def load_stage(
        cls,
        dvcfile: "SingleStageFile",
        d: Dict[str, Any],
        stage_text: Optional[str],
    ) -> Stage:
        path, wdir = resolve_paths(
            dvcfile.repo.fs, dvcfile.path, d.get(Stage.PARAM_WDIR)
        )
        stage = loads_from(Stage, dvcfile.repo, path, wdir, d)
        stage._stage_text = stage_text  # pylint: disable=protected-access
        stage.deps = dependency.loadd_from(stage, d.get(Stage.PARAM_DEPS) or [])
        stage.outs = output.loadd_from(stage, d.get(Stage.PARAM_OUTS) or [])
        return stage

    def __iter__(self):
        return iter([None])

    def __contains__(self, item):
        return False

    def __len__(self):
        return 1




dvc/stage/params.py
class StageParams:
    PARAM_MD5 = "md5"
    PARAM_CMD = "cmd"
    PARAM_WDIR = "wdir"
    PARAM_DEPS = "deps"
    PARAM_OUTS = "outs"
    PARAM_LOCKED = "locked"  # backward compatibility
    PARAM_FROZEN = "frozen"
    PARAM_META = "meta"
    PARAM_ALWAYS_CHANGED = "always_changed"
    PARAM_PARAMS = "params"
    PARAM_METRICS = "metrics"
    PARAM_PLOTS = "plots"
    PARAM_DESC = "desc"




dvc/stage/run.py
import logging
import os
import signal
import subprocess  # nosec B404
import threading

from dvc.utils import fix_env

from .decorators import unlocked_repo
from .exceptions import StageCmdFailedError

logger = logging.getLogger(__name__)


def _make_cmd(executable, cmd):
    if executable is None:
        return cmd
    opts = {"zsh": ["--no-rcs"], "bash": ["--noprofile", "--norc"]}
    name = os.path.basename(executable).lower()
    return [executable, *opts.get(name, []), "-c", cmd]


def warn_if_fish(executable):
    if executable is None or os.path.basename(os.path.realpath(executable)) != "fish":
        return

    logger.warning(
        "DVC detected that you are using fish as your default "
        "shell. Be aware that it might cause problems by overwriting "
        "your current environment variables with values defined "
        "in '.fishrc', which might affect your command. See "
        "https://github.com/iterative/dvc/issues/1307. "
    )


def _enforce_cmd_list(cmd):
    assert cmd
    return cmd if isinstance(cmd, list) else cmd.splitlines()


def prepare_kwargs(stage, run_env=None):
    kwargs = {"cwd": stage.wdir, "env": fix_env(None), "close_fds": True}

    if run_env:
        kwargs["env"].update(run_env)

    # NOTE: when you specify `shell=True`, `Popen` [1] will default to
    # `/bin/sh` on *nix and will add ["/bin/sh", "-c"] to your command.
    # But we actually want to run the same shell that we are running
    # from right now, which is usually determined by the `SHELL` env
    # var. So instead, we compose our command on our own, making sure
    # to include special flags to prevent shell from reading any
    # configs and modifying env, which may change the behavior or the
    # command we are running. See [2] for more info.
    #
    # [1] https://github.com/python/cpython/blob/3.7/Lib/subprocess.py
    #                                                            #L1426
    # [2] https://github.com/iterative/dvc/issues/2506
    #                                           #issuecomment-535396799
    kwargs["shell"] = os.name == "nt"
    return kwargs


def display_command(cmd):
    logger.info("%s %s", ">", cmd)


def get_executable():
    return (os.getenv("SHELL") or "/bin/sh") if os.name != "nt" else None


def _run(executable, cmd, **kwargs):
    # pylint: disable=protected-access
    main_thread = isinstance(
        threading.current_thread(),
        threading._MainThread,  # type: ignore[attr-defined]
    )
    old_handler = None

    exec_cmd = _make_cmd(executable, cmd)

    try:
        p = subprocess.Popen(exec_cmd, **kwargs)  # nosec B603
        if main_thread:
            old_handler = signal.signal(signal.SIGINT, signal.SIG_IGN)

        p.communicate()

        if p.returncode != 0:
            raise StageCmdFailedError(cmd, p.returncode)
    finally:
        if old_handler:
            signal.signal(signal.SIGINT, old_handler)


def cmd_run(stage, dry=False, run_env=None):
    logger.info("Running stage '%s':", stage.addressing)
    commands = _enforce_cmd_list(stage.cmd)
    kwargs = prepare_kwargs(stage, run_env=run_env)
    executable = get_executable()

    if not dry:
        warn_if_fish(executable)

    for cmd in commands:
        display_command(cmd)
        if dry:
            continue

        _run(executable, cmd, **kwargs)


def _pull_missing_deps(stage):
    for dep in stage.deps:
        if not dep.exists:
            stage.repo.pull(dep.def_path)


def run_stage(
    stage,
    dry=False,
    force=False,
    run_env=None,
    allow_missing: bool = False,
    **kwargs,
):
    if not force:
        if allow_missing and kwargs.get("pull") and not dry:
            _pull_missing_deps(stage)

        from .cache import RunCacheNotFoundError

        try:
            stage.repo.stage_cache.restore(stage, dry=dry, **kwargs)
            if not dry:
                return
        except RunCacheNotFoundError:
            if not dry:
                stage.save_deps()

    run = cmd_run if dry else unlocked_repo(cmd_run)
    run(stage, dry=dry, run_env=run_env)




dvc/stage/serialize.py
from collections import OrderedDict
from operator import attrgetter
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Iterable,
    List,
    Optional,
    Union,
    no_type_check,
)

from funcy import post_processing

from dvc.dependency import ParamsDependency
from dvc.output import Annotation, Output
from dvc.utils.collections import apply_diff
from dvc.utils.serialize import parse_yaml_for_update

from .params import StageParams
from .utils import resolve_wdir, split_params_deps

if TYPE_CHECKING:
    from dvc.stage import PipelineStage, Stage

PARAM_PARAMS = ParamsDependency.PARAM_PARAMS
PARAM_PATH = ParamsDependency.PARAM_PATH

PARAM_DEPS = StageParams.PARAM_DEPS
PARAM_OUTS = StageParams.PARAM_OUTS

PARAM_CACHE = Output.PARAM_CACHE
PARAM_METRIC = Output.PARAM_METRIC
PARAM_PLOT = Output.PARAM_PLOT
PARAM_PERSIST = Output.PARAM_PERSIST
PARAM_DESC = Annotation.PARAM_DESC
PARAM_REMOTE = Output.PARAM_REMOTE
PARAM_PUSH = Output.PARAM_PUSH

DEFAULT_PARAMS_FILE = ParamsDependency.DEFAULT_PARAMS_FILE


@post_processing(OrderedDict)
def _get_flags(out):
    annot = out.annot.to_dict()
    yield from annot.items()

    if not out.use_cache:
        yield PARAM_CACHE, False
    if out.persist:
        yield PARAM_PERSIST, True
    if out.plot and isinstance(out.plot, dict):
        # notice `out.plot` is not sorted
        # `out.plot` is in the same order as is in the file when read
        # and, should be dumped as-is without any sorting
        yield from out.plot.items()
    if out.remote:
        yield PARAM_REMOTE, out.remote
    if not out.can_push:
        yield PARAM_PUSH, False


def _serialize_out(out):
    flags = _get_flags(out)
    return out.def_path if not flags else {out.def_path: flags}


@no_type_check
def _serialize_outs(outputs: List[Output]):
    outs, metrics, plots = [], [], []
    for out in sorted(outputs, key=attrgetter("def_path")):
        bucket = outs
        if out.plot:
            bucket = plots
        elif out.metric:
            bucket = metrics
        bucket.append(_serialize_out(out))
    return outs, metrics, plots


def _serialize_params_keys(params: Iterable["ParamsDependency"]):
    """
    Returns the following format of data:
     ['lr', 'train', {'params2.yaml': ['lr']}]

    The output is sorted, with keys of params from default params file being
    at the first, and then followed by entry of other files in lexicographic
    order. The keys of those custom files are also sorted in the same order.
    """
    keys: List[Union[str, Dict[str, Optional[List[str]]]]] = []
    for param_dep in sorted(params, key=attrgetter("def_path")):
        # when on no_exec, params are not filled and are saved as list
        k: List[str] = sorted(param_dep.params)
        if k and param_dep.def_path == DEFAULT_PARAMS_FILE:
            keys = k + keys  # type: ignore[operator,assignment]
        else:
            keys.append({param_dep.def_path: k or None})
    return keys


@no_type_check
def _serialize_params_values(params: List[ParamsDependency]):
    """Returns output of following format, used for lockfile:
        {'params.yaml': {'lr': '1', 'train': 2}, {'params2.yaml': {'lr': '1'}}

    Default params file are always kept at the start, followed by others in
    alphabetical order. The param values are sorted too(not recursively though)
    """
    key_vals = OrderedDict()
    for param_dep in sorted(params, key=attrgetter("def_path")):
        dump = param_dep.dumpd()
        path, params = dump[PARAM_PATH], dump[PARAM_PARAMS]
        if isinstance(params, dict):
            kv = [(key, params[key]) for key in sorted(params.keys())]
            key_vals[path] = OrderedDict(kv)
            if path == DEFAULT_PARAMS_FILE:
                key_vals.move_to_end(path, last=False)
    return key_vals


def to_pipeline_file(stage: "PipelineStage"):
    wdir = resolve_wdir(stage.wdir, stage.path)
    param_objs, deps_objs = split_params_deps(stage)
    deps = sorted(d.def_path for d in deps_objs)
    params = _serialize_params_keys(param_objs)

    outs, metrics, plots = _serialize_outs(stage.outs)

    cmd = stage.cmd
    assert cmd, (
        f"'{stage.PARAM_CMD}' cannot be empty for stage '{stage.name}', "
        f"got: '{cmd}'(type: '{type(cmd).__name__}')"
    )
    res = [
        (stage.PARAM_DESC, stage.desc),
        (stage.PARAM_CMD, stage.cmd),
        (stage.PARAM_WDIR, wdir),
        (stage.PARAM_DEPS, deps),
        (stage.PARAM_PARAMS, params),
        (stage.PARAM_OUTS, outs),
        (stage.PARAM_METRICS, metrics),
        (stage.PARAM_PLOTS, plots),
        (stage.PARAM_FROZEN, stage.frozen),
        (stage.PARAM_ALWAYS_CHANGED, stage.always_changed),
        (stage.PARAM_META, stage.meta),
    ]
    return {stage.name: OrderedDict([(key, value) for key, value in res if value])}


def to_single_stage_lockfile(stage: "Stage", **kwargs) -> dict:
    from dvc.output import _serialize_tree_obj_to_files, split_file_meta_from_cloud
    from dvc_data.hashfile.tree import Tree

    assert stage.cmd

    def _dumpd(item: "Output"):
        ret: Dict[str, Any] = {item.PARAM_PATH: item.def_path}
        if item.hash_info.isdir and kwargs.get("with_files"):
            obj = item.obj or item.get_obj()
            if obj:
                assert isinstance(obj, Tree)
                ret[item.PARAM_FILES] = [
                    split_file_meta_from_cloud(f)
                    for f in _serialize_tree_obj_to_files(obj)
                ]
        else:
            meta_d = item.meta.to_dict()
            meta_d.pop("isdir", None)
            ret.update(item.hash_info.to_dict())
            ret.update(split_file_meta_from_cloud(meta_d))
        return ret

    res = OrderedDict([("cmd", stage.cmd)])
    params, deps = split_params_deps(stage)
    deps, outs = (
        [_dumpd(item) for item in sorted(items, key=attrgetter("def_path"))]
        for items in [deps, stage.outs]
    )
    params = _serialize_params_values(params)
    if deps:
        res[PARAM_DEPS] = deps
    if params:
        res[PARAM_PARAMS] = params
    if outs:
        res[PARAM_OUTS] = outs

    return res


def to_lockfile(stage: "PipelineStage", **kwargs) -> dict:
    assert stage.name
    return {stage.name: to_single_stage_lockfile(stage, **kwargs)}


def to_single_stage_file(stage: "Stage", **kwargs):
    state = stage.dumpd(**kwargs)

    # When we load a stage we parse yaml with a fast parser, which strips
    # off all the comments and formatting. To retain those on update we do
    # a trick here:
    # - reparse the same yaml text with a slow but smart ruamel yaml parser
    # - apply changes to a returned structure
    # - serialize it
    text = stage._stage_text  # pylint: disable=protected-access
    if text is None:
        return state

    saved_state = parse_yaml_for_update(text, stage.path)
    apply_diff(state, saved_state)
    return saved_state




dvc/stage/utils.py
import os
import pathlib
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union

from funcy import concat, first, lsplit, rpartial

from dvc.annotations import ANNOTATION_FIELDS
from dvc.exceptions import InvalidArgumentError
from dvc_data.hashfile.meta import Meta

from .exceptions import (
    MissingDataSource,
    StageExternalOutputsError,
    StagePathNotDirectoryError,
    StagePathNotFoundError,
    StagePathOutsideError,
)

if TYPE_CHECKING:
    from dvc.dependency import Dependency, ParamsDependency
    from dvc.repo import Repo

    from . import PipelineStage, Stage


def check_stage_path(repo, path, is_wdir=False):
    from dvc.utils.fs import path_isin

    assert repo is not None

    error_msg = "{wdir_or_path} '{path}' {{}}".format(
        wdir_or_path="stage working dir" if is_wdir else "file path", path=path
    )

    real_path = os.path.realpath(path)
    if not os.path.exists(real_path):
        raise StagePathNotFoundError(error_msg.format("does not exist"))

    if not os.path.isdir(real_path):
        raise StagePathNotDirectoryError(error_msg.format("is not directory"))

    proj_dir = os.path.realpath(repo.root_dir)
    if real_path != proj_dir and not path_isin(real_path, proj_dir):
        raise StagePathOutsideError(error_msg.format("is outside of DVC repo"))


def fill_stage_outputs(stage, **kwargs):
    from dvc.output import loads_from

    assert not stage.outs

    keys = [
        "outs_persist",
        "outs_persist_no_cache",
        "metrics",
        "metrics_persist",
        "metrics_no_cache",
        "metrics_persist_no_cache",
        "plots",
        "plots_persist",
        "plots_no_cache",
        "plots_persist_no_cache",
        "outs_no_cache",
        "outs",
    ]

    stage.outs = []

    for key in keys:
        stage.outs += loads_from(
            stage,
            kwargs.get(key, []),
            use_cache="no_cache" not in key,
            persist="persist" in key,
            metric="metrics" in key,
            plot="plots" in key,
        )


def fill_stage_dependencies(stage, deps=None, erepo=None, params=None, fs_config=None):
    from dvc.dependency import loads_from, loads_params

    assert not stage.deps
    stage.deps = []
    stage.deps += loads_from(stage, deps or [], erepo=erepo, fs_config=fs_config)
    stage.deps += loads_params(stage, params or [])


def check_no_externals(stage):
    from urllib.parse import urlparse

    from dvc.utils import format_link

    # NOTE: preventing users from accidentally using external outputs. See
    # https://github.com/iterative/dvc/issues/1545 for more details.

    def _is_external(out):
        # NOTE: in case of `remote://` notation, the user clearly knows that
        # this is an advanced feature and so we shouldn't error-out.
        if out.is_in_repo or urlparse(out.def_path).scheme == "remote":
            return False
        return True

    outs = [str(out) for out in stage.outs if _is_external(out)]
    if not outs:
        return

    str_outs = ", ".join(outs)
    link = format_link("https://dvc.org/doc/user-guide/managing-external-data")
    raise StageExternalOutputsError(
        f"Output(s) outside of DVC project: {str_outs}. See {link} for more info."
    )


def check_circular_dependency(stage):
    from dvc.exceptions import CircularDependencyError

    circular_dependencies = {d.fs_path for d in stage.deps} & {
        o.fs_path for o in stage.outs
    }

    if circular_dependencies:
        raise CircularDependencyError(str(circular_dependencies.pop()))


def check_duplicated_arguments(stage):
    from collections import Counter

    from dvc.exceptions import ArgumentDuplicationError

    path_counts = Counter(edge.fs_path for edge in stage.deps + stage.outs)

    for path, occurrence in path_counts.items():
        if occurrence > 1:
            raise ArgumentDuplicationError(str(path))


def check_missing_outputs(stage):
    paths = [str(out) for out in stage.outs if not out.exists]
    if paths:
        raise MissingDataSource(paths)


def compute_md5(stage):
    from dvc.output import Output
    from dvc.utils import dict_md5

    d = stage.dumpd()

    # Remove md5 and meta, these should not affect stage md5
    d.pop(stage.PARAM_MD5, None)
    d.pop(stage.PARAM_META, None)
    d.pop(stage.PARAM_DESC, None)

    # Ignore the wdir default value. In this case DVC file w/o
    # wdir has the same md5 as a file with the default value specified.
    # It's important for backward compatibility with pipelines that
    # didn't have WDIR in their DVC files.
    if d.get(stage.PARAM_WDIR) == ".":
        del d[stage.PARAM_WDIR]

    return dict_md5(
        d,
        exclude=[
            *ANNOTATION_FIELDS,
            stage.PARAM_LOCKED,  # backward compatibility
            stage.PARAM_FROZEN,
            Output.PARAM_METRIC,
            Output.PARAM_PERSIST,
            Meta.PARAM_ISEXEC,
            Meta.PARAM_SIZE,
            Meta.PARAM_NFILES,
        ],
    )


def resolve_wdir(wdir, path):
    from dvc.utils import relpath

    rel_wdir = relpath(wdir, os.path.dirname(path))
    return pathlib.PurePath(rel_wdir).as_posix() if rel_wdir != "." else None


def resolve_paths(fs, path, wdir=None):
    path = fs.path.abspath(path)
    wdir = wdir or os.curdir
    wdir = fs.path.abspath(fs.path.join(fs.path.dirname(path), wdir))
    return path, wdir


def get_dump(stage: "Stage", **kwargs):
    return {
        key: value
        for key, value in {
            stage.PARAM_DESC: stage.desc,
            stage.PARAM_MD5: stage.md5,
            stage.PARAM_CMD: stage.cmd,
            stage.PARAM_WDIR: resolve_wdir(stage.wdir, stage.path),
            stage.PARAM_FROZEN: stage.frozen,
            stage.PARAM_DEPS: [d.dumpd(**kwargs) for d in stage.deps],
            stage.PARAM_OUTS: [o.dumpd(**kwargs) for o in stage.outs],
            stage.PARAM_ALWAYS_CHANGED: stage.always_changed,
            stage.PARAM_META: stage.meta,
        }.items()
        if value
    }


def split_params_deps(
    stage: "Stage",
) -> Tuple[List["ParamsDependency"], List["Dependency"]]:
    from dvc.dependency import ParamsDependency

    return lsplit(rpartial(isinstance, ParamsDependency), stage.deps)


def is_valid_name(name: str) -> bool:
    from . import INVALID_STAGENAME_CHARS

    return not INVALID_STAGENAME_CHARS & set(name)


def prepare_file_path(kwargs) -> str:
    """Determine file path from the first output name.

    Used in creating .dvc files.
    """
    from dvc.dvcfile import DVC_FILE, DVC_FILE_SUFFIX

    out = first(
        concat(
            kwargs.get("outs", []),
            kwargs.get("outs_no_cache", []),
            kwargs.get("metrics", []),
            kwargs.get("metrics_no_cache", []),
            kwargs.get("plots", []),
            kwargs.get("plots_no_cache", []),
            kwargs.get("outs_persist", []),
            kwargs.get("outs_persist_no_cache", []),
        )
    )

    return (
        os.path.basename(os.path.normpath(out)) + DVC_FILE_SUFFIX if out else DVC_FILE
    )


def check_stage_exists(repo: "Repo", stage: Union["Stage", "PipelineStage"], path: str):
    from dvc.dvcfile import load_file
    from dvc.stage import PipelineStage
    from dvc.stage.exceptions import DuplicateStageName, StageFileAlreadyExistsError

    dvcfile = load_file(repo, path)
    if not dvcfile.exists():
        return

    hint = "Use '--force' to overwrite."
    if not isinstance(stage, PipelineStage):
        raise StageFileAlreadyExistsError(f"'{stage.relpath}' already exists. {hint}")
    if stage.name and stage.name in dvcfile.stages:
        raise DuplicateStageName(
            f"Stage '{stage.name}' already exists in '{stage.relpath}'. {hint}"
        )


def validate_kwargs(
    single_stage: bool = False, fname: Optional[str] = None, **kwargs
) -> Dict[str, Any]:
    """Prepare, validate and process kwargs passed from cli"""
    cmd = kwargs.get("cmd")
    if not cmd and not single_stage:
        raise InvalidArgumentError("command is not specified")

    stage_name = kwargs.get("name")
    if stage_name and single_stage:
        raise InvalidArgumentError("`-n|--name` is incompatible with `--single-stage`")
    if stage_name and fname:
        raise InvalidArgumentError(
            "`--file` is currently incompatible with `-n|--name` "
            "and requires `--single-stage`"
        )
    if not stage_name and not single_stage:
        raise InvalidArgumentError("`-n|--name` is required")

    if single_stage:
        kwargs.pop("name", None)

    return kwargs




dvc/testing/__init__.py




dvc/testing/api_tests.py
import pytest

from dvc import api
from dvc.api import DVCFileSystem
from dvc.utils.fs import remove

# pylint: disable=unused-argument


class TestAPI:
    def test_get_url(self, tmp_dir, dvc, remote):
        tmp_dir.dvc_gen("foo", "foo")

        expected_url = (remote / "ac/bd18db4cc2f85cedef654fccc4a4d8").url
        assert api.get_url("foo") == expected_url

    def test_open(self, tmp_dir, dvc, remote):
        tmp_dir.dvc_gen(
            {
                "foo": "foo-text",
                "dir": {"bar": "bar-text"},
            }
        )
        dvc.push()

        # Remove cache to force download
        remove(dvc.cache.local.path)

        with api.open("foo") as fobj:
            assert fobj.read() == "foo-text"

        with api.open("dir/bar") as fobj:
            assert fobj.read() == "bar-text"

    @pytest.mark.parametrize("clear_cache", [True, False], ids=["cache", "no_cache"])
    @pytest.mark.parametrize(
        "fs_kwargs",
        [
            {},
            {"url": "{path}"},
            {"url": "{path}", "rev": "master"},
            {"url": "file://{posixpath}"},
            {"url": "file://{posixpath}", "rev": "master"},
        ],
        ids=["current", "local", "local_rev", "git", "git_rev"],
    )
    def test_filesystem(
        self,
        M,
        tmp_dir,
        make_tmp_dir,
        scm,
        dvc,
        remote,
        fs_kwargs,
        clear_cache,
    ):
        fs_kwargs = fs_kwargs.copy()

        tmp_dir.scm_gen({"scripts": {"script1": "script1"}}, commit="scripts")
        tmp_dir.dvc_gen({"data": {"foo": "foo", "bar": "bar"}}, commit="data")
        dvc.push()

        if clear_cache:
            remove(dvc.cache.repo.path)

        if url := fs_kwargs.get("url"):
            fs_kwargs["url"] = url.format(path=tmp_dir, posixpath=tmp_dir.as_posix())

        fs = DVCFileSystem(**fs_kwargs)

        assert fs.ls("/", detail=False) == M.unordered(
            "/.gitignore", "/scripts", "/data"
        )
        assert fs.ls("scripts", detail=False) == ["scripts/script1"]
        assert fs.ls("data", detail=False) == M.unordered("data/foo", "data/bar")

        data_info = M.dict(
            name="/data",
            type="directory",
            dvc_info=M.dict(isdvc=True, isout=True),
        )
        scripts_info = M.dict(name="/scripts", type="directory", isexec=False)

        assert sorted(fs.ls("/"), key=lambda i: i["name"]) == [
            M.dict(name="/.gitignore", type="file", isexec=False),
            data_info,
            scripts_info,
        ]

        with pytest.raises(FileNotFoundError):
            fs.info("/not-existing-path")

        assert fs.info("/") == M.dict(name="/", isexec=False, type="directory")
        assert fs.info("/data") == data_info
        assert fs.info("/scripts") == scripts_info
        assert fs.info("/data/foo") == M.dict(name="/data/foo", type="file")
        assert fs.info("/scripts/script1") == M.dict(
            name="/scripts/script1", type="file"
        )

        assert not fs.isdvc("/")
        assert fs.isdvc("/data")
        assert fs.isdvc("/data/foo")
        assert not fs.isdvc("/scripts")
        assert not fs.isdvc("/scripts/script1")

        with pytest.raises((IsADirectoryError, PermissionError)):
            fs.open("data")
        with pytest.raises((IsADirectoryError, PermissionError)):
            fs.open("scripts")
        with fs.open("/data/foo") as fobj:
            assert fobj.read() == b"foo"
        with fs.open("/scripts/script1") as fobj:
            assert fobj.read() == b"script1"

        tmp = make_tmp_dir("temp-download")
        fs.get_file("data/foo", tmp / "foo")
        assert (tmp / "foo").read_text() == "foo"

        fs.get_file("scripts/script1", tmp / "script1")
        assert (tmp / "script1").read_text() == "script1"

        fs.get("/", (tmp / "all").fs_path, recursive=True)
        assert (tmp / "all").read_text() == {
            ".gitignore": "/data\n",
            "data": {"bar": "bar", "foo": "foo"},
            "scripts": {"script1": "script1"},
        }




dvc/testing/cloud.py
import locale
import pathlib
from abc import ABC, abstractmethod


class Cloud(ABC):
    IS_OBJECT_STORAGE = False

    @abstractmethod
    def is_file(self):
        pass

    @abstractmethod
    def is_dir(self):
        pass

    @abstractmethod
    def exists(self):
        pass

    @abstractmethod
    def mkdir(self, mode=0o777, parents=False, exist_ok=False):
        pass

    def write_text(self, contents, encoding=None, errors=None):
        if not encoding:
            encoding = locale.getpreferredencoding(False)
        assert errors is None
        self.write_bytes(contents.encode(encoding))

    @abstractmethod
    def write_bytes(self, contents):
        raise NotImplementedError

    def read_text(self, encoding=None, errors=None):
        if not encoding:
            encoding = locale.getpreferredencoding(False)
        assert errors is None
        return self.read_bytes().decode(encoding)

    @abstractmethod
    def read_bytes(self):
        pass

    def _gen(self, struct, prefix=None):
        for name, contents in struct.items():
            path = (prefix or self) / name

            if isinstance(contents, dict):
                if not contents:
                    path.mkdir(parents=True, exist_ok=True)
                else:
                    self._gen(contents, prefix=path)
            else:
                path.parent.mkdir(parents=True, exist_ok=True)
                if isinstance(contents, bytes):
                    path.write_bytes(contents)
                else:
                    path.write_text(contents, encoding="utf-8")

    def gen(self, struct, text=""):
        if isinstance(struct, (str, bytes, pathlib.PurePath)):
            struct = {struct: text}

        self._gen(struct)
        return struct.keys()

    def close(self):  # noqa: B027
        pass

    @staticmethod
    def should_test():
        return True

    @staticmethod
    def get_url():
        raise NotImplementedError

    @property
    @abstractmethod
    def config(self):
        pass




dvc/testing/conftest.py
from .fixtures import *  # noqa, pylint: disable=wildcard-import




dvc/testing/fixtures.py
import os
import subprocess  # nosec B404
from typing import Dict, Tuple

import pytest

# pylint: disable=redefined-outer-name,unused-argument

__all__ = [
    "make_tmp_dir",
    "tmp_dir",
    "scm",
    "dvc",
    "make_cloud",
    "make_cloud_version_aware",
    "make_local",
    "cloud",
    "local_cloud",
    "make_remote",
    "make_remote_version_aware",
    "make_remote_worktree",
    "remote",
    "remote_version_aware",
    "remote_worktree",
    "local_remote",
    "workspace",
    "make_workspace",
    "local_workspace",
    "docker_compose_project_name",
    "docker_services",
]

CACHE: Dict[Tuple[bool, bool, bool], str] = {}


@pytest.fixture(scope="session")
def make_tmp_dir(tmp_path_factory, request, worker_id):
    def make(name, *, scm=False, dvc=False, subdir=False):  # pylint: disable=W0621
        from shutil import copytree, ignore_patterns

        from dvc.repo import Repo
        from dvc.scm import Git

        from .tmp_dir import TmpDir

        cache = CACHE.get((scm, dvc, subdir))
        if not cache:
            cache_dir = tmp_path_factory.mktemp("dvc-test-cache" + worker_id)
            TmpDir(cache_dir).init(scm=scm, dvc=dvc, subdir=subdir)
            CACHE[(scm, dvc, subdir)] = cache = os.fspath(cache_dir)

        assert cache
        path = tmp_path_factory.mktemp(name) if isinstance(name, str) else name

        # ignore sqlite files from .dvc/tmp. We might not be closing the cache
        # connection resulting in PermissionErrors in Windows.
        ignore = ignore_patterns("cache.db*")
        copytree(cache, path, dirs_exist_ok=True, ignore=ignore)
        new_dir = TmpDir(path)
        str_path = os.fspath(new_dir)
        if dvc:
            new_dir.dvc = Repo(str_path)
        if scm:
            new_dir.scm = new_dir.dvc.scm if hasattr(new_dir, "dvc") else Git(str_path)
        request.addfinalizer(new_dir.close)
        return new_dir

    return make


@pytest.fixture
def tmp_dir(tmp_path, make_tmp_dir, request, monkeypatch):
    monkeypatch.chdir(tmp_path)
    fixtures = request.fixturenames
    return make_tmp_dir(tmp_path, scm="scm" in fixtures, dvc="dvc" in fixtures)


@pytest.fixture
def scm(tmp_dir):
    return tmp_dir.scm


@pytest.fixture
def dvc(tmp_dir):
    with tmp_dir.dvc as _dvc:
        yield _dvc


@pytest.fixture
def make_local(make_tmp_dir):
    def _make_local():
        return make_tmp_dir("local-cloud")

    return _make_local


@pytest.fixture
def make_cloud(request):
    def _make_cloud(typ):
        return request.getfixturevalue(f"make_{typ}")()

    return _make_cloud


@pytest.fixture
def make_cloud_version_aware(request):
    def _make_cloud(typ):
        return request.getfixturevalue(f"make_{typ}_version_aware")()

    return _make_cloud


@pytest.fixture
def cloud(make_cloud, request):
    typ = getattr(request, "param", "local")
    return make_cloud(typ)


@pytest.fixture
def local_cloud(make_cloud):
    return make_cloud("local")


@pytest.fixture
def make_remote(tmp_dir, dvc, make_cloud):  # noqa: ARG001
    def _make_remote(name, typ="local", **kwargs):
        cloud = make_cloud(typ)  # pylint: disable=W0621
        tmp_dir.add_remote(name=name, config=cloud.config, **kwargs)
        return cloud

    return _make_remote


@pytest.fixture
def make_remote_version_aware(tmp_dir, dvc, make_cloud_version_aware):  # noqa: ARG001
    def _make_remote(name, typ="local", **kwargs):
        cloud = make_cloud_version_aware(typ)  # pylint: disable=W0621
        config = dict(cloud.config)
        config["version_aware"] = True
        tmp_dir.add_remote(name=name, config=config, **kwargs)
        return cloud

    return _make_remote


@pytest.fixture
def make_remote_worktree(tmp_dir, dvc, make_cloud_version_aware):  # noqa: ARG001
    def _make_remote(name, typ="local", **kwargs):
        cloud = make_cloud_version_aware(typ)  # pylint: disable=W0621
        config = dict(cloud.config)
        config["worktree"] = True
        tmp_dir.add_remote(name=name, config=config, **kwargs)
        return cloud

    return _make_remote


@pytest.fixture
def remote(make_remote, request):
    typ = getattr(request, "param", "local")
    return make_remote("upstream", typ=typ)


@pytest.fixture
def remote_version_aware(make_remote_version_aware, request):
    typ = getattr(request, "param", "local")
    return make_remote_version_aware("upstream", typ=typ)


@pytest.fixture
def remote_worktree(make_remote_worktree, request):
    typ = getattr(request, "param", "local")
    return make_remote_worktree("upstream", typ=typ)


@pytest.fixture
def local_remote(make_remote):
    return make_remote("upstream", typ="local")


@pytest.fixture
def make_workspace(tmp_dir, dvc, make_cloud):
    def _make_workspace(name, typ="local"):
        from dvc.cachemgr import CacheManager

        cloud = make_cloud(typ)  # pylint: disable=W0621

        tmp_dir.add_remote(name=name, config=cloud.config, default=False)
        tmp_dir.add_remote(
            name=f"{name}-cache", url="remote://workspace/cache", default=False
        )

        scheme = getattr(cloud, "scheme", "local")
        if scheme != "http":
            with dvc.config.edit() as conf:
                conf["cache"][scheme] = f"{name}-cache"

            dvc.cache = CacheManager(dvc)

        return cloud

    return _make_workspace


@pytest.fixture
def workspace(make_workspace, request):
    typ = getattr(request, "param", "local")
    return make_workspace("workspace", typ=typ)


@pytest.fixture
def local_workspace(make_workspace):
    return make_workspace("workspace", typ="local")


@pytest.fixture(scope="session")
def docker_compose_project_name():
    return "pytest-dvc-test"


@pytest.fixture(scope="session")
def docker_services(
    tmp_path_factory,
    docker_compose_command,
    docker_compose_file,
    docker_compose_project_name,
    docker_setup,
):
    from filelock import FileLock
    from pytest_docker.plugin import DockerComposeExecutor, Services

    if os.environ.get("CI") and os.name == "nt":
        pytest.skip("disabled for Windows on CI")

    try:
        subprocess.check_output(  # nosec B607, B602
            "docker ps", stderr=subprocess.STDOUT, shell=True
        )
    except subprocess.CalledProcessError as err:
        out = (err.output or b"").decode("utf-8")
        pytest.skip(f"docker is not installed or the daemon is not running: {out}")

    try:
        cmd = "docker-compose version"
        subprocess.check_output(cmd, stderr=subprocess.STDOUT, shell=True)  # nosec
    except subprocess.CalledProcessError as err:
        out = (err.output or b"").decode("utf-8")
        pytest.skip(f"docker-compose is not installed: {out}")

    executor = DockerComposeExecutor(
        docker_compose_command, docker_compose_file, docker_compose_project_name
    )

    # making sure we don't accidentally launch docker-compose in parallel,
    # as it might result in network conflicts. Inspired by:
    # https://github.com/pytest-dev/pytest-xdist#making-session-scoped-fixtures-execute-only-once
    lockfile = tmp_path_factory.getbasetemp().parent / "docker-compose.lock"
    with FileLock(os.fspath(lockfile)):
        executor.execute(docker_setup)
        # note: we are not tearing down the containers here
        return Services(executor)




dvc/testing/path_info.py
# pylint: disable=protected-access
import os
import pathlib
import posixpath
from typing import Callable
from urllib.parse import urlparse

from dvc.utils import relpath
from dvc.utils.objects import cached_property


class _BasePath:
    def overlaps(self, other):
        if isinstance(other, (str, bytes)):
            other = self.__class__(other)  # type: ignore[call-arg]
        elif self.__class__ != other.__class__:
            return False
        return self.isin_or_eq(other) or other.isin(self)

    def isin_or_eq(self, other):
        # pylint: disable-next=no-member
        return self == other or self.isin(other)  # type: ignore[attr-defined]


class PathInfo(pathlib.PurePath, _BasePath):
    # Use __slots__ in PathInfo objects following PurePath implementation.
    # This makes objects smaller and speeds up attribute access.
    # We don't add any fields so it's empty.
    __slots__ = ()
    scheme = "local"

    def __new__(cls, *args):
        # Construct a proper subclass depending on current os
        if cls is PathInfo:
            cls = (  # pylint: disable=self-cls-assignment
                WindowsPathInfo if os.name == "nt" else PosixPathInfo
            )

        return cls._from_parts(args)  # type: ignore[attr-defined]

    def as_posix(self):
        # pylint: disable-next=no-member
        f = self._flavour  # type: ignore[attr-defined]
        # Unlike original implementation [1] that uses `str()` we actually need
        # to use `fspath`, because we've overridden `__str__` method to return
        # relative paths, which will break original `as_posix`.
        #
        # [1] https://github.com/python/cpython/blob/v3.7.0/Lib/pathlib.py#L692
        return self.fspath.replace(f.sep, "/")

    def __str__(self):
        path = self.__fspath__()
        return relpath(path)

    def __repr__(self):
        return f"{type(self).__name__}: '{self}'"

    # This permits passing it to file utils directly in Python 3.6+
    def __fspath__(self):
        return pathlib.PurePath.__str__(self)

    @property
    def fspath(self):
        return os.fspath(self)

    url = fspath

    path = fspath

    def relpath(self, other):
        return self.__class__(relpath(self, other))

    def isin(self, other):
        if isinstance(other, (str, bytes)):
            other = self.__class__(other)
        elif self.__class__ != other.__class__:
            return False
        # Use cached casefolded parts to compare paths
        n = len(other._cparts)
        return (
            len(self._cparts) > n  # type: ignore[attr-defined]
            and self._cparts[:n] == other._cparts  # type: ignore[attr-defined]
        )

    def relative_to(self, other):  # pylint: disable=arguments-differ
        # pathlib relative_to raises exception when one path is not a direct
        # descendant of the other when os.path.relpath would return abspath.
        # For DVC PathInfo we only need the relpath behavior.
        # See: https://bugs.python.org/issue40358
        try:
            path = super().relative_to(other)
        except ValueError:
            path = relpath(self, other)
        return self.__class__(path)


class WindowsPathInfo(PathInfo, pathlib.PureWindowsPath):
    pass


class PosixPathInfo(PathInfo, pathlib.PurePosixPath):
    pass


class _URLPathInfo(PosixPathInfo):
    def __str__(self):
        return self.__fspath__()

    __unicode__ = __str__


class _URLPathParents:
    def __init__(self, src):
        self.src = src
        self._parents = self.src._path.parents

    def __len__(self):
        return len(self._parents)

    def __getitem__(self, idx):
        return self.src.replace(path=self._parents[idx])

    def __repr__(self):
        return f"<{self.src}.parents>"


class URLInfo(_BasePath):
    DEFAULT_PORTS = {"http": 80, "https": 443, "ssh": 22, "hdfs": 0}

    def __init__(self, url):
        p = urlparse(url)
        assert not p.query
        assert not p.params
        assert not p.fragment
        assert p.password is None
        self._fill_parts(p.scheme, p.hostname, p.username, p.port, p.path)

    @classmethod
    def from_parts(
        cls, scheme=None, host=None, user=None, port=None, path="", netloc=None
    ):
        assert bool(host) ^ bool(netloc)

        if netloc is not None:
            return cls(f"{scheme}://{netloc}{path}")

        obj = cls.__new__(cls)
        obj._fill_parts(scheme, host, user, port, path)
        return obj

    def _fill_parts(self, scheme, host, user, port, path):
        assert scheme != "remote"
        assert isinstance(path, (str, bytes, _URLPathInfo))

        self.scheme, self.host, self.user = scheme, host, user
        self.port = int(port) if port else self.DEFAULT_PORTS.get(self.scheme)

        if isinstance(path, _URLPathInfo):
            self._spath = str(path)
            self._path = path
        else:
            if path and path[0] != "/":
                path = "/" + path  # type: ignore[operator]
            self._spath = path

    @property
    def _base_parts(self):
        return (self.scheme, self.host, self.user, self.port)

    @property
    def parts(self):
        return self._base_parts + self._path.parts

    def replace(self, path=None):
        return self.from_parts(*self._base_parts, path=path)  # type: ignore[misc]

    @cached_property
    def url(self) -> str:
        return f"{self.scheme}://{self.netloc}{self._spath}"

    def __str__(self):
        return self.url

    def __repr__(self):
        return f"{type(self).__name__}: '{self}'"

    def __eq__(self, other):
        if isinstance(other, (str, bytes)):
            other = self.__class__(other)
        return (
            self.__class__ == other.__class__
            and self._base_parts == other._base_parts
            and self._path == other._path
        )

    def __hash__(self):
        return hash(self.parts)

    def __div__(self, other):
        return self.replace(path=posixpath.join(self._spath, other))

    def joinpath(self, *args):
        return self.replace(path=posixpath.join(self._spath, *args))

    __truediv__ = __div__

    @property
    def path(self):
        return self._spath

    @cached_property
    def _path(  # pylint: disable=method-hidden
        self,
    ) -> "_URLPathInfo":
        return _URLPathInfo(self._spath)

    @property
    def name(self) -> str:
        return self._path.name

    @cached_property
    def netloc(self) -> str:
        netloc = self.host
        if self.user:
            netloc = self.user + "@" + netloc
        if self.port and int(self.port) != self.DEFAULT_PORTS.get(self.scheme):
            netloc += ":" + str(self.port)
        return netloc

    @property
    def bucket(self) -> str:
        return self.netloc

    @property
    def parent(self):
        return self.replace(path=self._path.parent)

    @property
    def parents(self):
        return _URLPathParents(self)

    def relative_to(self, other):
        if isinstance(other, (str, bytes)):
            other = self.__class__(other)
        if self.__class__ != other.__class__:
            msg = f"'{self}' has incompatible class with '{other}'"
            raise ValueError(msg)
        if self._base_parts != other._base_parts:
            msg = f"'{self}' does not start with '{other}'"
            raise ValueError(msg)
        return self._path.relative_to(other._path)

    def isin(self, other):
        if isinstance(other, (str, bytes)):
            other = self.__class__(other)
        elif self.__class__ != other.__class__:
            return False
        return self._base_parts == other._base_parts and self._path.isin(other._path)


class CloudURLInfo(URLInfo):
    @property
    def path(self):
        return self._spath.lstrip("/")


class HTTPURLInfo(URLInfo):
    __hash__: Callable[["HTTPURLInfo"], int] = URLInfo.__hash__

    def __init__(self, url):
        p = urlparse(url)
        stripped = p._replace(params=None, query=None, fragment=None)
        super().__init__(stripped.geturl())
        self.params = p.params
        self.query = p.query
        self.fragment = p.fragment

    def replace(self, path=None):
        return self.from_parts(  # type: ignore[misc]
            *self._base_parts,
            params=self.params,
            query=self.query,
            fragment=self.fragment,
            path=path,
        )

    @classmethod
    def from_parts(
        cls,
        scheme=None,
        host=None,
        user=None,
        port=None,
        path="",
        netloc=None,
        params=None,
        query=None,
        fragment=None,
    ):  # pylint: disable=arguments-differ
        assert bool(host) ^ bool(netloc)

        if netloc is not None:
            return cls(
                "{}://{}{}{}{}{}".format(
                    scheme,
                    netloc,
                    path,
                    (";" + params) if params else "",
                    ("?" + query) if query else "",
                    ("#" + fragment) if fragment else "",
                )
            )

        obj = cls.__new__(cls)
        obj._fill_parts(scheme, host, user, port, path)
        obj.params = params
        obj.query = query
        obj.fragment = fragment
        return obj

    @property
    def _extra_parts(self):
        return (self.params, self.query, self.fragment)

    @property
    def parts(self):
        return self._base_parts + self._path.parts + self._extra_parts

    @cached_property
    def url(self) -> str:
        return "{}://{}{}{}{}{}".format(
            self.scheme,
            self.netloc,
            self._spath,
            (";" + self.params) if self.params else "",
            ("?" + self.query) if self.query else "",
            ("#" + self.fragment) if self.fragment else "",
        )

    def __eq__(self, other):
        if isinstance(other, (str, bytes)):
            other = self.__class__(other)
        return (
            self.__class__ == other.__class__
            and self._base_parts == other._base_parts
            and self._path == other._path
            and self._extra_parts == other._extra_parts
        )


class WebDAVURLInfo(URLInfo):
    @cached_property
    def url(self) -> str:
        return "{}://{}{}".format(
            self.scheme.replace("webdav", "http"), self.netloc, self._spath
        )




dvc/testing/plugin.py
from .benchmarks.fixtures import *  # noqa, pylint: disable=wildcard-import
from .fixtures import *  # noqa, pylint: disable=wildcard-import


def pytest_generate_tests(metafunc):
    from .benchmarks.plugin import pytest_generate_tests as bench_generate_tests

    bench_generate_tests(metafunc)


def pytest_addoption(parser):
    from .benchmarks.plugin import pytest_addoption as bench_addoption

    bench_addoption(parser)


def pytest_configure(config):
    from .benchmarks.plugin import pytest_configure as bench_configure

    bench_configure(config)




dvc/testing/README.rst
DVC pytest plugin

dvc.testing.benchmarks
======================

Benchmark test definitions are now part of ``dvc.testing``.
For CLI usage and `bench.dvc.org <https://bench.dvc.org>`_ details see `dvc-bench <https://github.com/iterative/dvc-bench>`_.

``dvc.testing.benchmarks`` structure:

* cli: should be able to run these with any dvc (rpm, deb, pypi, snap, etc) (could be used in dvc-test repo too)

  * commands: granular tests for individual commands. These should have a cached setup, so that we could use them during rapid development instead of our hand-written scripts. Every test could be run in a separate machine.
  * stories: multistage start-to-end benchmarks, useful for testing workflows (e.g. in documentation, see test_sharing inspired by `Storing and sharing <https://dvc.org/doc/start/data-management/data-versioning#storing-and-sharing>`_. Every full story could be run in a separate machine.

* api: for python api only.

  * methods: granular tests for individual methods (e.g. ``api.open/read``). Same reasoning as in ``cli.commands``
  * stories: same as ``cli.stories`` but for our api. E.g. imagine using our api with pandas or smth like that.




dvc/testing/remote_tests.py
import os
import shutil

import pytest

from dvc.stage.cache import RunCacheNotSupported
from dvc.utils.fs import remove
from dvc_data.hashfile.tree import Tree

# pylint: disable=unused-argument


def _check_status(status, **kwargs):
    for key in ("ok", "missing", "new", "deleted"):
        expected = kwargs.get(key, set())
        assert expected == set(getattr(status, key))


class TestRemote:
    def test(self, tmp_dir, dvc, remote):  # pylint: disable=W0613
        (stage,) = tmp_dir.dvc_gen("foo", "foo")
        out = stage.outs[0]
        cache = out.cache_path
        foo_hash = out.hash_info
        foo_hashes = out.get_used_objs().get(None, set())

        (stage_dir,) = tmp_dir.dvc_gen(
            {
                "data_dir": {
                    "data_sub_dir": {"data_sub": "data_sub"},
                    "data": "data",
                    "empty": "",
                }
            }
        )

        out_dir = stage_dir.outs[0]
        cache_dir = out_dir.cache_path
        dir_hash = out_dir.hash_info
        dir_hashes = {dir_hash} | {oid for _, _, oid in out_dir.obj}

        # Check status
        status = dvc.cloud.status(foo_hashes)
        _check_status(status, new={foo_hash})

        status_dir = dvc.cloud.status(dir_hashes)
        _check_status(status_dir, new=dir_hashes)

        # Move cache and check status
        # See issue https://github.com/iterative/dvc/issues/4383 for details
        backup_dir = dvc.cache.local.path + ".backup"
        shutil.move(dvc.cache.local.path, backup_dir)
        status = dvc.cloud.status(foo_hashes)
        _check_status(status, missing={foo_hash})

        status_dir = dvc.cloud.status(dir_hashes)
        _check_status(status_dir, missing=dir_hashes)

        # Restore original cache:
        remove(dvc.cache.local.path)
        shutil.move(backup_dir, dvc.cache.local.path)

        # Push and check status
        dvc.cloud.push(foo_hashes)
        assert os.path.exists(cache)
        assert os.path.isfile(cache)

        dvc.cloud.push(dir_hashes)
        assert os.path.isfile(cache_dir)

        status = dvc.cloud.status(foo_hashes)
        _check_status(status, ok={foo_hash})

        status_dir = dvc.cloud.status(dir_hashes)
        _check_status(status_dir, ok=dir_hashes)

        # Remove and check status
        dvc.cache.local.clear()

        status = dvc.cloud.status(foo_hashes)
        _check_status(status, deleted={foo_hash})

        status_dir = dvc.cloud.status(dir_hashes)
        _check_status(status_dir, deleted=dir_hashes)

        # Pull and check status
        dvc.cloud.pull(foo_hashes)
        assert os.path.exists(cache)
        assert os.path.isfile(cache)
        with open(cache, encoding="utf-8") as fd:
            assert fd.read() == "foo"

        dvc.cloud.pull(dir_hashes)
        assert os.path.isfile(cache_dir)

        status = dvc.cloud.status(foo_hashes)
        _check_status(status, ok={foo_hash})

        status_dir = dvc.cloud.status(dir_hashes)
        _check_status(status_dir, ok=dir_hashes)

    @pytest.mark.xfail(raises=RunCacheNotSupported, strict=False)
    def test_stage_cache_push_pull(self, tmp_dir, dvc, remote):
        tmp_dir.gen("foo", "foo")
        stage = dvc.stage.add(
            deps=["foo"], outs=["bar"], name="copy-foo-bar", cmd="cp foo bar"
        )
        dvc.reproduce(stage.addressing)
        assert dvc.push(run_cache=True) == 2

        stage_cache_dir = tmp_dir / dvc.stage_cache.cache_dir
        expected = list(stage_cache_dir.rglob("*"))
        shutil.rmtree(stage_cache_dir)

        dvc.pull(run_cache=True)
        assert list(stage_cache_dir.rglob("*")) == expected

    @pytest.mark.xfail(raises=NotImplementedError, strict=False)
    def test_pull_00_prefix(self, tmp_dir, dvc, remote, monkeypatch):
        # Related: https://github.com/iterative/dvc/issues/6089

        fs_type = type(dvc.cloud.get_remote_odb("upstream").fs)
        monkeypatch.setattr(fs_type, "_ALWAYS_TRAVERSE", True, raising=False)
        monkeypatch.setattr(fs_type, "LIST_OBJECT_PAGE_SIZE", 256, raising=False)

        # foo's md5 checksum is 00411460f7c92d2124a67ea0f4cb5f85
        # bar's md5 checksum is 0000000018e6137ac2caab16074784a6
        foo_out = tmp_dir.dvc_gen("foo", "363")[0].outs[0]
        bar_out = tmp_dir.dvc_gen("bar", "jk8ssl")[0].outs[0]
        expected_hashes = {foo_out.hash_info, bar_out.hash_info}

        dvc.push()
        status = dvc.cloud.status(expected_hashes)
        _check_status(status, ok=expected_hashes)

        dvc.cache.local.clear()
        remove(tmp_dir / "foo")
        remove(tmp_dir / "bar")

        stats = dvc.pull()
        assert stats["fetched"] == 2
        assert set(stats["added"]) == {"foo", "bar"}

    @pytest.mark.xfail(raises=NotImplementedError, strict=False)
    def test_pull_no_00_prefix(self, tmp_dir, dvc, remote, monkeypatch):
        # Related: https://github.com/iterative/dvc/issues/6244

        fs_type = type(dvc.cloud.get_remote_odb("upstream").fs)
        monkeypatch.setattr(fs_type, "_ALWAYS_TRAVERSE", True, raising=False)
        monkeypatch.setattr(fs_type, "LIST_OBJECT_PAGE_SIZE", 256, raising=False)

        # foo's md5 checksum is 14ffd92a6cbf5f2f657067df0d5881a6
        # bar's md5 checksum is 64020400f00960c0ef04052547b134b3
        foo_out = tmp_dir.dvc_gen("foo", "dvc")[0].outs[0]
        bar_out = tmp_dir.dvc_gen("bar", "cml")[0].outs[0]
        expected_hashes = {foo_out.hash_info, bar_out.hash_info}

        dvc.push()
        status = dvc.cloud.status(expected_hashes)
        _check_status(status, ok=expected_hashes)

        dvc.cache.local.clear()
        remove(tmp_dir / "foo")
        remove(tmp_dir / "bar")

        stats = dvc.pull()
        assert stats["fetched"] == 2
        assert set(stats["added"]) == {"foo", "bar"}


class TestRemoteVersionAware:
    def test_file(self, tmp_dir, dvc, remote_version_aware):  # pylint: disable=W0613
        (stage,) = tmp_dir.dvc_gen("foo", "foo")

        dvc.push()
        assert "version_id" in (tmp_dir / "foo.dvc").read_text()
        stage = stage.reload()
        out = stage.outs[0]
        assert out.meta.version_id

        remove(dvc.cache.local.path)
        remove(tmp_dir / "foo")

        dvc.pull()
        assert (tmp_dir / "foo").read_text() == "foo"

    def test_dir(self, tmp_dir, dvc, remote_version_aware):  # pylint: disable=W0613
        (stage,) = tmp_dir.dvc_gen(
            {
                "data_dir": {
                    "data_sub_dir": {"data_sub": "data_sub"},
                    "data": "data",
                    "empty": "",
                }
            }
        )

        dvc.push()
        assert "files" in (tmp_dir / "data_dir.dvc").read_text()
        assert "version_id" in (tmp_dir / "data_dir.dvc").read_text()
        stage = stage.reload()
        out = stage.outs[0]
        assert out.files
        for file in out.files:
            assert file["version_id"]
            assert file["remote"] == "upstream"

        remove(dvc.cache.local.path)
        remove(tmp_dir / "data_dir")

        dvc.pull()
        assert (tmp_dir / "data_dir" / "data").read_text() == "data"
        assert (
            tmp_dir / "data_dir" / "data_sub_dir" / "data_sub"
        ).read_text() == "data_sub"


class TestRemoteWorktree:
    def test_file(self, tmp_dir, dvc, remote_worktree):  # pylint: disable=W0613
        (stage,) = tmp_dir.dvc_gen("foo", "foo")

        dvc.push()
        assert "version_id" in (tmp_dir / "foo.dvc").read_text()
        stage = stage.reload()
        out = stage.outs[0]
        assert out.meta.version_id

        remove(dvc.cache.local.path)
        remove(tmp_dir / "foo")

        dvc.pull()
        assert (tmp_dir / "foo").read_text() == "foo"

    def test_dir(self, tmp_dir, dvc, remote_worktree):  # pylint: disable=W0613
        (stage,) = tmp_dir.dvc_gen(
            {
                "data_dir": {
                    "data_sub_dir": {"data_sub": "data_sub"},
                    "data": "data",
                    "empty": "",
                }
            }
        )

        dvc.push()
        assert "files" in (tmp_dir / "data_dir.dvc").read_text()
        assert "version_id" in (tmp_dir / "data_dir.dvc").read_text()
        stage = stage.reload()
        out = stage.outs[0]
        assert out.files
        for file in out.files:
            assert file["version_id"]
            assert file["remote"] == "upstream"

        remove(dvc.cache.local.path)
        remove(tmp_dir / "data_dir")

        dvc.pull()
        assert (tmp_dir / "data_dir" / "data").read_text() == "data"
        assert (
            tmp_dir / "data_dir" / "data_sub_dir" / "data_sub"
        ).read_text() == "data_sub"

    def test_deletion(
        self, tmp_dir, dvc, scm, remote_worktree  # pylint: disable=W0613
    ):
        tmp_dir.dvc_gen(
            {
                "data_dir": {
                    "data_sub_dir": {"data_sub": "data_sub"},
                    "data": "data",
                    "empty": "",
                }
            }
        )
        dvc.push()
        assert (remote_worktree / "data_dir" / "data").exists()
        tmp_dir.scm_add([tmp_dir / "data_dir.dvc"], commit="v1")
        v1 = scm.get_rev()
        remove(tmp_dir / "data_dir" / "data")
        dvc.add(str(tmp_dir / "data_dir"))

        # data_dir/data should show as deleted in the remote
        dvc.push()
        tmp_dir.scm_add([tmp_dir / "data_dir.dvc"], commit="v2")
        assert not (remote_worktree / "data_dir" / "data").exists()

        remove(dvc.cache.local.path)
        remove(tmp_dir / "data_dir")
        # pulling the original pushed version should still succeed
        scm.checkout(v1)
        dvc.pull()
        assert (tmp_dir / "data_dir" / "data").read_text() == "data"

    def test_update(self, tmp_dir, dvc, remote_worktree):  # pylint: disable=W0613
        (foo_stage,) = tmp_dir.dvc_gen("foo", "foo")
        (data_dir_stage,) = tmp_dir.dvc_gen(
            {
                "data_dir": {
                    "data_sub_dir": {"data_sub": "data_sub"},
                    "data": "data",
                    "empty": "",
                }
            }
        )
        dvc.push()
        orig_foo = foo_stage.reload().outs[0]
        orig_data_dir = data_dir_stage.reload().outs[0]
        (remote_worktree / "foo").write_text("bar")
        (remote_worktree / "data_dir" / "data").write_text("modified")
        (remote_worktree / "data_dir" / "new_data").write_text("new data")

        dvc.update([str(tmp_dir / "foo.dvc"), str(tmp_dir / "data_dir.dvc")])
        updated_foo = foo_stage.reload().outs[0]
        updated_data_dir = data_dir_stage.reload().outs[0]

        assert updated_foo.meta.version_id
        assert updated_foo.meta.version_id != orig_foo.meta.version_id
        updated_data_dir = data_dir_stage.reload().outs[0]
        orig_tree = orig_data_dir.get_obj()
        updated_tree = Tree.from_list(updated_data_dir.files, hash_name="md5")
        assert orig_tree.get(("data_sub_dir", "data_sub")) == updated_tree.get(
            ("data_sub_dir", "data_sub")
        )
        orig_meta, _ = orig_tree.get(("data",))
        updated_meta, _ = updated_tree.get(("data",))
        assert orig_meta.version_id
        assert updated_meta.version_id
        assert orig_meta.version_id != updated_meta.version_id
        meta, hash_info = updated_tree.get(("new_data",))
        assert meta
        assert hash_info

        assert (tmp_dir / "foo").read_text() == "bar"
        assert (tmp_dir / "data_dir" / "data").read_text() == "modified"
        assert (tmp_dir / "data_dir" / "new_data").read_text() == "new data"

        remove(dvc.cache.local.path)
        remove(tmp_dir / "foo")
        remove(tmp_dir / "data_dir")
        dvc.pull()
        assert (tmp_dir / "foo").read_text() == "bar"
        assert (tmp_dir / "data_dir" / "data").read_text() == "modified"
        assert (tmp_dir / "data_dir" / "new_data").read_text() == "new data"




dvc/testing/tmp_dir.py
"""
The goal of this module is making dvc functional tests setup a breeze. This
includes a temporary dir, initializing git and DVC repos and bootstrapping some
file structure.

The cornerstone of these fixtures is `tmp_dir`, which creates a temporary dir
and changes path to it, it might be combined with `scm` and `dvc` to initialize
empty git and DVC repos. `tmp_dir` returns a Path instance, which should save
you from using `open()`, `os` and `os.path` utils many times:

    (tmp_dir / "some_file").write_text("some text")
    # ...
    assert "some text" == (tmp_dir / "some_file").read_text()
    assert (tmp_dir / "some_file").exists()

Additionally it provides `.gen()`, `.scm_gen()` and `.dvc_gen()` methods to
bootstrap a required file structure in a single call:

    # Generate a dir with files
    tmp_dir.gen({"dir": {"file": "file text", "second_file": "..."}})

    # Generate a single file, dirs will be created along the way
    tmp_dir.gen("dir/file", "file text")

    # Generate + git add
    tmp_dir.scm_gen({"file1": "...", ...})

    # Generate + git add + git commit
    tmp_dir.scm_gen({"file1": "...", ...}, commit="add files")

    # Generate + dvc add
    tmp_dir.dvc_gen({"file1": "...", ...})

    # Generate + dvc add + git commit -am "..."
    # This commits stages to git not the generated files.
    tmp_dir.dvc_gen({"file1": "...", ...}, commit="add files")

Making it easier to bootstrap things has a supergoal of incentivizing a move
from global repo template to creating everything inplace, which:

    - makes all path references local to test, enhancing readability
    - allows using telling filenames, e.g. "git_tracked_file" instead of "foo"
    - does not create unnecessary files
"""

# pylint: disable=redefined-outer-name, attribute-defined-outside-init

import os
import pathlib
import sys
from contextlib import contextmanager
from functools import partialmethod

from dvc.utils import serialize


class TmpDir(pathlib.Path):
    scheme = "local"

    @property
    def fs_path(self):
        return os.fspath(self)

    @property
    def url(self):
        return self.fs_path

    @property
    def config(self):
        return {"url": self.url}

    def __new__(cls, *args, **kwargs):
        if cls is TmpDir:
            cls = (  # pylint: disable=self-cls-assignment
                WindowsTmpDir if os.name == "nt" else PosixTmpDir
            )
        # init parameter and `_init` method has been removed in Python 3.10.
        kw = {"init": False} if sys.version_info < (3, 10) else {}
        # pylint: disable-next=unexpected-keyword-arg
        self = cls._from_parts(args, **kw)  # type: ignore[attr-defined]
        if not self._flavour.is_supported:
            raise NotImplementedError(
                f"cannot instantiate {cls.__name__!r} on your system"
            )
        if sys.version_info < (3, 10):
            self._init()  # pylint: disable=no-member
        return self

    def init(self, *, scm=False, dvc=False, subdir=False):
        from dvc.repo import Repo
        from dvc.scm import Git

        assert not scm or not hasattr(self, "scm")
        assert not dvc or not hasattr(self, "dvc")

        if scm:
            Git.init(self.fs_path).close()
        if dvc:
            self.dvc = Repo.init(
                self.fs_path,
                no_scm=not scm and not hasattr(self, "scm"),
                subdir=subdir,
            )
        if scm:
            self.scm = self.dvc.scm if hasattr(self, "dvc") else Git(self.fs_path)
        if dvc and hasattr(self, "scm"):
            self.scm.commit("init dvc")

    def close(self):
        if hasattr(self, "scm"):
            self.scm.close()
        if hasattr(self, "dvc"):
            self.dvc.close()

    def _require(self, name):
        if not hasattr(self, name):
            raise TypeError(
                "Can't use {name} for this temporary dir. "
                'Did you forget to use "{name}" fixture?'.format(name=name)
            )

    # Bootstrapping methods
    def gen(self, struct, text=""):
        if isinstance(struct, (str, bytes, pathlib.PurePath)):
            struct = {struct: text}

        return self._gen(struct)

    def _gen(self, struct, prefix=None):
        paths = []
        for name, contents in struct.items():
            path = (prefix or self) / name

            if isinstance(contents, dict):
                if not contents:
                    os.makedirs(path, exist_ok=True)
                else:
                    self._gen(contents, prefix=path)
            else:
                os.makedirs(path.parent, exist_ok=True)
                if isinstance(contents, bytes):
                    path.write_bytes(contents)
                else:
                    path.write_text(contents, encoding="utf-8")
            paths.append(path)
        return paths

    def dvc_gen(self, struct, text="", commit=None):
        paths = self.gen(struct, text)
        return self.dvc_add(paths, commit=commit)

    def scm_gen(self, struct, text="", commit=None, force=False):
        paths = self.gen(struct, text)
        return self.scm_add(paths, commit=commit, force=force)

    def commit(self, output_paths, msg, force=False):
        def to_gitignore(stage_path):
            from dvc.scm import Git

            return os.path.join(os.path.dirname(stage_path), Git.GITIGNORE)

        gitignores = [
            to_gitignore(s) for s in output_paths if os.path.exists(to_gitignore(s))
        ]
        return self.scm_add(output_paths + gitignores, commit=msg, force=force)

    def dvc_add(self, filenames, commit=None):
        self._require("dvc")
        filenames = _coerce_filenames(filenames)

        stages = self.dvc.add(filenames)
        if commit:
            self.commit([s.path for s in stages], msg=commit)
        return stages

    def scm_add(self, filenames, commit=None, force=False):
        from dvc.scm import Git

        self._require("scm")
        filenames = _coerce_filenames(filenames)
        assert isinstance(self.scm, Git)
        self.scm.add(filenames, force=force)
        if commit:
            self.scm.commit(commit)

    def add_remote(self, *, url=None, config=None, name="upstream", default=True):
        self._require("dvc")

        assert bool(url) ^ bool(config)

        if url:
            config = {"url": url}

        with self.dvc.config.edit() as conf:
            conf["remote"][name] = config
            if default:
                conf["core"]["remote"] = name

        if hasattr(self, "scm"):
            self.scm.add(self.dvc.config.files["repo"])
            self.scm.commit(f"add '{name}' remote")

        return url or config["url"]

    # contexts
    @contextmanager
    def chdir(self):
        old = os.getcwd()
        try:
            os.chdir(self)
            yield
        finally:
            os.chdir(old)

    @contextmanager
    def branch(self, name, new=False):
        self._require("scm")
        old = self.scm.active_branch()
        try:
            self.scm.checkout(name, create_new=new)
            yield
        finally:
            self.scm.checkout(old)

    def read_text(self, *args, **kwargs):  # pylint: disable=signature-differs
        # NOTE: on windows we'll get PermissionError instead of
        # IsADirectoryError when we try to `open` a directory, so we can't
        # rely on exception flow control
        if self.is_dir():
            return {
                path.name: path.read_text(*args, **kwargs) for path in self.iterdir()
            }
        kwargs.setdefault("encoding", "utf-8")  # type: ignore[call-overload]
        return super().read_text(*args, **kwargs)

    def oid_to_path(self, hash_):
        return str(self / hash_[0:2] / hash_[2:])

    def dump(self, *args, **kwargs):
        return serialize.DUMPERS[self.suffix](self, *args, **kwargs)

    def parse(self, *args, **kwargs):
        return serialize.LOADERS[self.suffix](self, *args, **kwargs)

    def modify(self, *args, **kwargs):
        return serialize.MODIFIERS[self.suffix](self, *args, **kwargs)

    load_yaml = partialmethod(serialize.load_yaml)
    dump_yaml = partialmethod(serialize.dump_yaml)
    load_json = partialmethod(serialize.load_json)
    dump_json = partialmethod(serialize.dump_json)
    load_toml = partialmethod(serialize.load_toml)
    dump_toml = partialmethod(serialize.dump_toml)


def make_subrepo(dir_: TmpDir, scm, config=None):
    dir_.mkdir(parents=True, exist_ok=True)
    with dir_.chdir():
        dir_.scm = scm
        dir_.init(dvc=True, subdir=True)
        if config:
            dir_.add_remote(config=config)


def _coerce_filenames(filenames):
    if isinstance(filenames, (str, bytes, pathlib.PurePath)):
        filenames = [filenames]
    return list(map(os.fspath, filenames))


class WindowsTmpDir(TmpDir, pathlib.PureWindowsPath):
    pass


class PosixTmpDir(TmpDir, pathlib.PurePosixPath):
    pass




dvc/testing/workspace_tests.py
import os
from typing import Dict, Union

import pytest
from funcy import first

from dvc.exceptions import URLMissingError
from dvc.repo import Repo
from dvc.repo.ls_url import ls_url, parse_external_url
from dvc.utils.fs import remove


class TestImport:
    def test_import(self, tmp_dir, dvc, workspace):
        workspace.gen("file", "file")
        assert not (tmp_dir / "file").exists()  # sanity check
        dvc.imp_url("remote://workspace/file")
        assert (tmp_dir / "file").read_text() == "file"
        assert dvc.status() == {}

    @pytest.fixture
    def stage_md5(self):
        pytest.skip()

    @pytest.fixture
    def dir_md5(self):
        pytest.skip()

    def test_import_dir(self, tmp_dir, dvc, workspace, stage_md5, dir_md5):
        from dvc.cachemgr import CacheManager

        workspace.gen({"dir": {"file": "file", "subdir": {"subfile": "subfile"}}})

        # remove external cache to make sure that we don't need it
        # to import dirs
        with dvc.config.edit() as conf:
            del conf["cache"]
        dvc.cache = CacheManager(dvc)

        assert not (tmp_dir / "dir").exists()  # sanity check
        dvc.imp_url("remote://workspace/dir")
        assert set(os.listdir(tmp_dir / "dir")) == {"file", "subdir"}
        assert (tmp_dir / "dir" / "file").read_text() == "file"
        assert list(os.listdir(tmp_dir / "dir" / "subdir")) == ["subfile"]
        assert (tmp_dir / "dir" / "subdir" / "subfile").read_text() == "subfile"

        assert dvc.status() == {}

        if stage_md5 is not None and dir_md5 is not None:
            assert (tmp_dir / "dir.dvc").read_text() == (
                f"md5: {stage_md5}\n"
                "frozen: true\n"
                "deps:\n"
                f"- md5: {dir_md5}\n"
                "  size: 11\n"
                "  nfiles: 2\n"
                "  path: remote://workspace/dir\n"
                "outs:\n"
                "- md5: b6dcab6ccd17ca0a8bf4a215a37d14cc.dir\n"
                "  size: 11\n"
                "  nfiles: 2\n"
                "  path: dir\n"
            )

    @pytest.fixture
    def is_object_storage(self):
        pytest.skip()

    def test_import_empty_dir(self, tmp_dir, dvc, workspace, is_object_storage):
        # prefix based storage services (e.g s3) doesn't have the real concept
        # of directories. So instead we create an empty file that ends with a
        # trailing slash in order to actually support this operation
        if is_object_storage:
            contents: Union[str, Dict[str, str]] = ""
        else:
            contents = {}

        workspace.gen({"empty_dir/": contents})

        dvc.imp_url("remote://workspace/empty_dir/")

        empty_dir = tmp_dir / "empty_dir"
        assert empty_dir.is_dir()
        assert tuple(empty_dir.iterdir()) == ()


class TestImportURLVersionAware:
    def test_import_file(self, tmp_dir, dvc, remote_version_aware):
        remote_version_aware.gen("file", "file")
        dvc.imp_url("remote://upstream/file", version_aware=True)
        stage = first(dvc.index.stages)
        assert not stage.outs[0].can_push
        assert (tmp_dir / "file").read_text() == "file"
        assert dvc.status() == {}

        dvc.cache.local.clear()
        remove(tmp_dir / "file")
        dvc.pull()
        assert (tmp_dir / "file").read_text() == "file"

        (remote_version_aware / "file").write_text("modified")
        assert dvc.status().get("file.dvc") == [
            {"changed deps": {"remote://upstream/file": "update available"}},
        ]
        dvc.update(str(tmp_dir / "file.dvc"))
        assert (tmp_dir / "file").read_text() == "modified"
        assert dvc.status() == {}

        dvc.cache.local.clear()
        remove(tmp_dir / "file")
        dvc.pull()
        assert (tmp_dir / "file").read_text() == "modified"

    def test_import_dir(self, tmp_dir, dvc, remote_version_aware):
        remote_version_aware.gen({"data_dir": {"subdir": {"file": "file"}}})
        dvc.imp_url("remote://upstream/data_dir", version_aware=True)
        stage = first(dvc.index.stages)
        assert not stage.outs[0].can_push
        assert (tmp_dir / "data_dir" / "subdir" / "file").read_text() == "file"
        assert dvc.status() == {}

        dvc.cache.local.clear()
        remove(tmp_dir / "data_dir")
        dvc.pull()
        assert (tmp_dir / "data_dir" / "subdir" / "file").read_text() == "file"

        (remote_version_aware / "data_dir" / "subdir" / "file").write_text("modified")
        (remote_version_aware / "data_dir" / "new_file").write_text("new")
        assert dvc.status().get("data_dir.dvc") == [
            {"changed deps": {"remote://upstream/data_dir": "modified"}},
        ]
        dvc.update(str(tmp_dir / "data_dir.dvc"))
        assert (tmp_dir / "data_dir" / "subdir" / "file").read_text() == "modified"
        assert (tmp_dir / "data_dir" / "new_file").read_text() == "new"
        assert dvc.status() == {}

        dvc.cache.local.clear()
        remove(tmp_dir / "data_dir")
        dvc.pull()
        assert (tmp_dir / "data_dir" / "subdir" / "file").read_text() == "modified"
        assert (tmp_dir / "data_dir" / "new_file").read_text() == "new"

    def test_import_no_download(self, tmp_dir, dvc, remote_version_aware):
        remote_version_aware.gen({"data_dir": {"subdir": {"file": "file"}}})
        dvc.imp_url("remote://upstream/data_dir", version_aware=True, no_download=True)
        stage = first(dvc.index.stages)
        assert not stage.outs[0].can_push

        dvc.pull()
        assert (tmp_dir / "data_dir" / "subdir" / "file").read_text() == "file"
        assert dvc.status() == {}


class TestAdd:
    @pytest.fixture
    def hash_name(self):
        pytest.skip()

    @pytest.fixture
    def hash_value(self):
        pytest.skip()

    @pytest.fixture
    def dir_hash_value(self):
        pytest.skip()

    def test_add(self, tmp_dir, dvc, workspace, hash_name, hash_value):
        from dvc.stage.exceptions import StageExternalOutputsError

        workspace.gen("file", "file")

        with pytest.raises(StageExternalOutputsError):
            dvc.add(workspace.url)

        dvc.add("remote://workspace/file")
        assert (tmp_dir / "file.dvc").read_text() == (
            "outs:\n"
            f"- {hash_name}: {hash_value}\n"
            "  size: 4\n"
            "  path: remote://workspace/file\n"
        )
        assert (workspace / "file").read_text() == "file"
        assert (
            workspace / "cache" / hash_value[:2] / hash_value[2:]
        ).read_text() == "file"

        assert dvc.status() == {}

    # pylint: disable-next=unused-argument
    def test_add_dir(self, tmp_dir, dvc, workspace, hash_name, dir_hash_value):
        workspace.gen({"dir": {"file": "file", "subdir": {"subfile": "subfile"}}})

        dvc.add("remote://workspace/dir")
        assert (workspace / "cache" / dir_hash_value[:2] / dir_hash_value[2:]).is_file()


def match_files(fs, entries, expected):
    entries_content = {(fs.path.normpath(d["path"]), d["isdir"]) for d in entries}
    expected_content = {(fs.path.normpath(d["path"]), d["isdir"]) for d in expected}
    assert entries_content == expected_content


class TestLsUrl:
    @pytest.mark.parametrize("fname", ["foo", "foo.dvc", "dir/foo"])
    def test_file(self, cloud, fname):
        cloud.gen({fname: "foo contents"})
        fs, fs_path = parse_external_url(cloud.url, cloud.config)
        result = ls_url(str(cloud / fname), config=cloud.config)
        match_files(
            fs,
            result,
            [{"path": fs.path.join(fs_path, fname), "isdir": False}],
        )

    def test_dir(self, cloud):
        cloud.gen({"dir/foo": "foo contents", "dir/subdir/bar": "bar contents"})
        if not (cloud / "dir").is_dir():
            pytest.skip("Cannot create directories on this cloud")
        fs, _ = parse_external_url(cloud.url, cloud.config)
        result = ls_url(str(cloud / "dir"), config=cloud.config)
        match_files(
            fs,
            result,
            [
                {"path": "foo", "isdir": False},
                {"path": "subdir", "isdir": True},
            ],
        )

    def test_recursive(self, cloud):
        cloud.gen({"dir/foo": "foo contents", "dir/subdir/bar": "bar contents"})
        if not (cloud / "dir").is_dir():
            pytest.skip("Cannot create directories on this cloud")
        fs, _ = parse_external_url(cloud.url, cloud.config)
        result = ls_url(str(cloud / "dir"), config=cloud.config, recursive=True)
        match_files(
            fs,
            result,
            [
                {"path": "foo", "isdir": False},
                {"path": "subdir/bar", "isdir": False},
            ],
        )

    def test_nonexistent(self, cloud):
        with pytest.raises(URLMissingError):
            ls_url(str(cloud / "dir"), config=cloud.config)


class TestGetUrl:
    def test_get_file(self, cloud, tmp_dir):
        cloud.gen({"foo": "foo contents"})

        Repo.get_url(str(cloud / "foo"), "foo_imported", config=cloud.config)

        assert (tmp_dir / "foo_imported").is_file()
        assert (tmp_dir / "foo_imported").read_text() == "foo contents"

    def test_get_dir(self, cloud, tmp_dir):
        cloud.gen({"foo": {"foo": "foo contents"}})
        if not (cloud / "foo").is_dir():
            pytest.skip("Cannot create directories on this cloud")

        Repo.get_url(str(cloud / "foo"), "foo_imported", config=cloud.config)

        assert (tmp_dir / "foo_imported").is_dir()
        assert (tmp_dir / "foo_imported" / "foo").is_file()
        assert (tmp_dir / "foo_imported" / "foo").read_text() == "foo contents"

    @pytest.mark.parametrize("dname", [".", "dir", "dir/subdir"])
    def test_get_url_to_dir(self, cloud, tmp_dir, dname):
        cloud.gen({"src": {"foo": "foo contents"}})
        if not (cloud / "src").is_dir():
            pytest.skip("Cannot create directories on this cloud")
        tmp_dir.gen({"dir": {"subdir": {}}})

        Repo.get_url(str(cloud / "src" / "foo"), dname, config=cloud.config)

        assert (tmp_dir / dname).is_dir()
        assert (tmp_dir / dname / "foo").read_text() == "foo contents"

    def test_get_url_nonexistent(self, cloud):
        with pytest.raises(URLMissingError):
            Repo.get_url(str(cloud / "nonexistent"), config=cloud.config)


class TestToRemote:
    def test_add_to_remote(self, tmp_dir, dvc, remote, workspace):
        workspace.gen("foo", "foo")

        url = "remote://workspace/foo"
        [stage] = dvc.add(url, to_remote=True)

        assert not (tmp_dir / "foo").exists()
        assert (tmp_dir / "foo.dvc").exists()

        assert len(stage.deps) == 0
        assert len(stage.outs) == 1

        hash_info = stage.outs[0].hash_info
        meta = stage.outs[0].meta
        assert hash_info.name == "md5"
        assert hash_info.value == "acbd18db4cc2f85cedef654fccc4a4d8"
        assert (remote / "ac" / "bd18db4cc2f85cedef654fccc4a4d8").read_text() == "foo"
        assert meta.size == len("foo")

    def test_import_url_to_remote_file(self, tmp_dir, dvc, workspace, remote):
        workspace.gen("foo", "foo")

        url = "remote://workspace/foo"
        stage = dvc.imp_url(url, to_remote=True)

        assert stage.deps[0].hash_info.value is not None
        assert not (tmp_dir / "foo").exists()
        assert (tmp_dir / "foo.dvc").exists()

        assert len(stage.deps) == 1
        assert stage.deps[0].def_path == url
        assert len(stage.outs) == 1

        hash_info = stage.outs[0].hash_info
        assert hash_info.name == "md5"
        assert hash_info.value == "acbd18db4cc2f85cedef654fccc4a4d8"
        assert (remote / "ac" / "bd18db4cc2f85cedef654fccc4a4d8").read_text() == "foo"
        assert stage.outs[0].meta.size == len("foo")

    def test_import_url_to_remote_dir(self, tmp_dir, dvc, workspace, remote):
        import json

        workspace.gen(
            {
                "data": {
                    "foo": "foo",
                    "bar": "bar",
                    "sub_dir": {"baz": "sub_dir/baz"},
                }
            }
        )

        url = "remote://workspace/data"
        stage = dvc.imp_url(url, to_remote=True)

        assert not (tmp_dir / "data").exists()
        assert (tmp_dir / "data.dvc").exists()

        assert len(stage.deps) == 1
        assert stage.deps[0].def_path == url
        assert len(stage.outs) == 1

        hash_info = stage.outs[0].hash_info
        assert hash_info.name == "md5"
        assert hash_info.value == "55d05978954d1b2cd7b06aedda9b9e43.dir"
        file_parts = json.loads(
            (remote / "55" / "d05978954d1b2cd7b06aedda9b9e43.dir").read_text()
        )

        assert len(file_parts) == 3
        assert {file_part["relpath"] for file_part in file_parts} == {
            "foo",
            "bar",
            "sub_dir/baz",
        }

        for file_part in file_parts:
            md5 = file_part["md5"]
            assert (remote / md5[:2] / md5[2:]).read_text() == file_part["relpath"]




dvc/testing/benchmarks/__init__.py




dvc/testing/benchmarks/conftest.py
from .fixtures import *  # noqa, pylint: disable=wildcard-import




dvc/testing/benchmarks/fixtures.py
import os
import shutil
from pathlib import Path
from subprocess import check_output  # nosec B404

import pytest
from dulwich.porcelain import clone
from funcy import first
from packaging import version

# pylint: disable=redefined-outer-name,unused-argument


@pytest.fixture(scope="session")
def bench_config(request):
    return request.config.bench_config


@pytest.fixture(scope="session")
def make_dvc_venv(tmp_path_factory):
    def _make_dvc_venv(name):
        from pytest_virtualenv import VirtualEnv

        name = _sanitize_venv_name(name)
        venv_dir = tmp_path_factory.mktemp(f"dvc-venv-{name}")
        return VirtualEnv(workspace=venv_dir)

    return _make_dvc_venv


def _sanitize_venv_name(name):
    return name.replace("/", "-").replace("\\", "-")


@pytest.fixture(scope="session")
def dvc_venvs():
    return {}


@pytest.fixture(scope="session")
def dvc_git_repo(tmp_path_factory, bench_config):
    url = bench_config.dvc_git_repo

    if os.path.isdir(url):
        return url

    tmp_path = tmp_path_factory.mktemp("dvc-git-repo")
    clone(url, os.fspath(tmp_path))

    return tmp_path


@pytest.fixture(scope="session")
def dvc_bench_git_repo(tmp_path_factory, bench_config):
    url = bench_config.dvc_bench_git_repo

    if os.path.isdir(url):
        return Path(url)

    tmp_path = tmp_path_factory.mktemp("dvc-bench-git-repo")
    clone(url, os.fspath(tmp_path))

    return tmp_path


@pytest.fixture(scope="session")
def make_dvc_bin(
    dvc_rev,
    dvc_venvs,
    make_dvc_venv,
    dvc_git_repo,
    bench_config,
    request,
):
    if dvc_rev:
        venv = dvc_venvs.get(dvc_rev)
        if not venv:
            venv = make_dvc_venv(dvc_rev)
            venv.run("pip install -U pip")
            if bench_config.dvc_install_deps:
                egg = f"dvc[{bench_config.dvc_install_deps}]"
            else:
                egg = "dvc"
            venv.run(f"pip install git+file://{dvc_git_repo}@{dvc_rev}#egg={egg}")
            if dvc_rev in ["2.18.1", "2.11.0", "2.6.3"]:
                venv.run("pip install fsspec==2022.11.0")
            dvc_venvs[dvc_rev] = venv
        dvc_bin = venv.virtualenv / "bin" / "dvc"
    else:
        dvc_bin = bench_config.dvc_bin

    def _dvc_bin(*args):
        return check_output([dvc_bin, *args], text=True)  # nosec B603

    _dvc_bin.version = parse_tuple(_dvc_bin("--version"))  # type: ignore[attr-defined]
    return _dvc_bin


def parse_tuple(version_string):
    from packaging.version import Version

    parsed = version.parse(version_string)
    assert isinstance(parsed, Version)
    return (parsed.major, parsed.minor, parsed.micro)


@pytest.fixture
def dvc_bin(request, make_dvc_bin):
    if marker := request.node.get_closest_marker("requires"):
        minversion = marker.kwargs.get("minversion") or first(marker.args)
        assert minversion, (
            "'minversion' needs to be specified as"
            " a positional or a keyword argument"
        )
        reason = marker.kwargs.get("reason", "")
        if isinstance(minversion, str):
            minversion = parse_tuple(minversion)
        if make_dvc_bin.version < minversion:
            version_repr = ".".join(map(str, minversion))
            pytest.skip(f"requires dvc>={version_repr}: {reason}")
    return make_dvc_bin


@pytest.fixture
def make_bench(request):
    def _make_bench(name):
        import pytest_benchmark.plugin

        # hack from https://github.com/ionelmc/pytest-benchmark/issues/166
        bench = pytest_benchmark.plugin.benchmark.__pytest_wrapped__.obj(request)

        suffix = f"-{name}"

        def add_suffix(_name):
            start, sep, end = _name.partition("[")
            return start + suffix + sep + end

        bench.name = add_suffix(bench.name)
        bench.fullname = add_suffix(bench.fullname)

        return bench

    return _make_bench


@pytest.fixture
def bench_dvc(dvc_bin, make_bench):
    def _bench_dvc(*args, **kwargs):
        name = kwargs.pop("name", None)
        name = f"-{name}" if name else ""
        bench = make_bench(args[0] + name)
        return bench.pedantic(dvc_bin, args=args, **kwargs)

    return _bench_dvc


def _pull(repo, *args):
    from dvc.exceptions import CheckoutError, DownloadError

    while True:
        try:
            return repo.pull(*args)
        except (CheckoutError, DownloadError):
            pass


@pytest.fixture
def make_dataset(request, bench_config, tmp_dir, dvc_bench_git_repo):
    def _make_dataset(
        dvcfile=False, files=True, cache=False, commit=False, remote=False
    ):
        from dvc.repo import Repo

        path = tmp_dir / "dataset"
        root = dvc_bench_git_repo
        src = root / "data" / bench_config.size / "dataset"
        src_dvc = src.with_suffix(".dvc")

        dvc = Repo(root)

        _pull(dvc, [str(src_dvc)])
        if files:
            shutil.copytree(src, path)
        if dvcfile:
            shutil.copy(src.with_suffix(".dvc"), path.with_suffix(".dvc"))
        if cache:
            shutil.copytree(root / ".dvc" / "cache", tmp_dir / ".dvc" / "cache")
        if remote:
            assert dvcfile
            assert not cache
            assert tmp_dir.dvc
            # FIXME temporary hack, we should try to push from home repo
            # directly to this remote instead
            shutil.copytree(root / ".dvc" / "cache", tmp_dir / ".dvc" / "cache")
            tmp_dir.dvc.push([str(path.with_suffix(".dvc").relative_to(tmp_dir))])
            shutil.rmtree(tmp_dir / ".dvc" / "cache")
        if commit:
            assert dvcfile
            assert tmp_dir.scm
            tmp_dir.scm.add([str(path.with_suffix(".dvc").relative_to(tmp_dir))])
            tmp_dir.scm.commit("add dataset")
        return path

    return _make_dataset


@pytest.fixture
def dataset(make_dataset):
    return make_dataset(dvcfile=False, files=True, cache=False)


@pytest.fixture
def remote_dataset():
    pytest.skip("fixme")


@pytest.fixture
def make_project(tmp_path_factory):
    def _make_project(url, rev=None):
        path = os.fspath(tmp_path_factory.mktemp("dvc-project"))

        if rev:
            rev = rev.encode("ascii")

        clone(url, path, branch=rev)
        return path

    return _make_project


@pytest.fixture
def project(bench_config, monkeypatch, make_project):
    rev = bench_config.project_rev
    url = bench_config.project_git_repo

    if os.path.isdir(url):
        path = url
        assert not rev
    else:
        path = make_project(url, rev=rev)

    monkeypatch.chdir(path)




dvc/testing/benchmarks/plugin.py
DEFAULT_DVC_BIN = "dvc"
DEFAULT_DVC_GIT_REPO = "https://github.com/iterative/dvc"
DEFAULT_DVC_BENCH_GIT_REPO = "https://github.com/iterative/dvc-bench"
DEFAULT_PROJECT_GIT_REPO = "https://github.com/iterative/example-get-started"


def pytest_generate_tests(metafunc):
    str_revs = metafunc.config.getoption("--dvc-revs")
    revs = str_revs.split(",") if str_revs else [None]
    if "dvc_rev" in metafunc.fixturenames:
        metafunc.parametrize("dvc_rev", revs, scope="session")


class DVCBenchConfig:
    def __init__(self):
        self.size = "small"
        self.dvc_bin = DEFAULT_DVC_BIN
        self.dvc_revs = None
        self.dvc_git_repo = DEFAULT_DVC_GIT_REPO
        self.dvc_install_deps = None
        self.dvc_bench_git_repo = DEFAULT_DVC_BENCH_GIT_REPO
        self.project_rev = None
        self.project_git_repo = DEFAULT_PROJECT_GIT_REPO


def pytest_configure(config):
    config.addinivalue_line(
        "markers", "requires(minversion): mark a test as requiring minimum DVC version"
    )

    config.bench_config = DVCBenchConfig()
    config.bench_config.size = config.getoption("--size")
    config.bench_config.dvc_bin = config.getoption("--dvc-bin")
    config.bench_config.dvc_revs = config.getoption("--dvc-revs")
    config.bench_config.dvc_git_repo = config.getoption("--dvc-git-repo")
    config.bench_config.dvc_install_deps = config.getoption("--dvc-install-deps")
    config.bench_config.dvc_bench_git_repo = config.getoption("--dvc-bench-git-repo")
    config.bench_config.project_rev = config.getoption("--project-rev")
    config.bench_config.project_git_repo = config.getoption("--project-git-repo")


def pytest_addoption(parser):
    parser.addoption(
        "--size",
        choices=["tiny", "small", "large", "mnist"],
        default="small",
        help="Size of the dataset/datafile to use in tests",
    )

    parser.addoption(
        "--dvc-bin",
        type=str,
        default=DEFAULT_DVC_BIN,
        help="Path to dvc binary",
    )

    parser.addoption(
        "--dvc-revs",
        type=str,
        help=("Comma-separated list of DVC revisions to test (overrides `--dvc-bin`)"),
    )

    parser.addoption(
        "--dvc-git-repo",
        type=str,
        default=DEFAULT_DVC_GIT_REPO,
        help="Path or url to dvc git repo",
    )

    parser.addoption(
        "--dvc-install-deps",
        type=str,
        default="",
        help="Comma-separated list of DVC installation packages",
    )

    parser.addoption(
        "--dvc-bench-git-repo",
        type=str,
        default=DEFAULT_DVC_BENCH_GIT_REPO,
        help="Path or url to dvc-bench git repo (for loading benchmark datasets)",
    )

    parser.addoption(
        "--project-rev",
        type=str,
        help="Project revision to test",
    )

    parser.addoption(
        "--project-git-repo",
        type=str,
        default=DEFAULT_PROJECT_GIT_REPO,
        help="Path or url to dvc project",
    )




dvc/testing/benchmarks/api/__init__.py




dvc/testing/benchmarks/cli/__init__.py




dvc/testing/benchmarks/cli/commands/__init__.py




dvc/testing/benchmarks/cli/commands/test_add.py
# pylint: disable=unused-argument
def test_add(bench_dvc, tmp_dir, dvc, dataset):
    bench_dvc("add", dataset)




dvc/testing/benchmarks/cli/commands/test_checkout.py
# pylint: disable=unused-argument
import os

import pytest

from dvc.fs import localfs
from dvc_objects.fs import generic
from dvc_objects.fs.utils import tmp_fname


def _skip_unsupported_link(src, dest, link_type):
    src_test_file = os.path.join(src, tmp_fname())
    dest_test_file = os.path.join(dest, tmp_fname())
    if not generic.test_links(
        [link_type], localfs, src_test_file, localfs, dest_test_file
    ):
        pytest.skip(f"{link_type} not supported")


def generate_test(*, link_type="copy"):
    def _test_checkout_func(bench_dvc, tmp_dir, dvc, make_dataset):
        dataset = make_dataset(dvcfile=True, cache=True, files=False)

        _skip_unsupported_link((tmp_dir / ".dvc" / "cache"), tmp_dir, link_type)

        with dvc.config.edit() as conf:
            conf["cache"]["type"] = link_type

        bench_dvc("checkout", dataset)
        bench_dvc("checkout", name="noop")
        (dataset / "new").write_text("new")
        bench_dvc("checkout", "--force", name="update")

    return _test_checkout_func


test_checkout_copy = generate_test(link_type="copy")
test_checkout_symlink = generate_test(link_type="symlink")
test_checkout_hardlink = generate_test(link_type="hardlink")




dvc/testing/benchmarks/cli/commands/test_data_status.py
# pylint: disable=unused-argument
from shutil import rmtree

import pytest

pytestmark = pytest.mark.requires(minversion=(2, 15, 0), reason="new command")


def test_data_status(
    bench_dvc,
    tmp_dir,
    scm,
    dvc,
    make_dataset,
):
    args = ("data", "status")
    dataset = make_dataset(cache=True, files=True, dvcfile=True, commit=False)
    rmtree(dvc.tmp_dir)

    bench_dvc(*args, name="new")
    bench_dvc(*args, name="noop")

    tmp_dir.scm_add(dataset.with_suffix(".dvc").name, commit="add dataset")

    (dataset / "new").write_text("new")
    bench_dvc(*args, name="changed")
    bench_dvc(*args, name="changed-noop")


def test_data_status_all_flags(bench_dvc, tmp_dir, scm, dvc, make_dataset):
    args = (
        "data",
        "status",
        "--granular",
        "--unchanged",
        "--untracked-files",
        "--json",
    )
    dataset = make_dataset(cache=True, files=True, dvcfile=True, commit=False)
    rmtree(dvc.tmp_dir)

    bench_dvc(*args, name="new")
    bench_dvc(*args, name="noop")

    tmp_dir.scm_add(dataset.with_suffix(".dvc").name, commit="add dataset")

    (dataset / "new").write_text("new")
    bench_dvc(*args, name="changed")
    bench_dvc(*args, name="changed-noop")




dvc/testing/benchmarks/cli/commands/test_diff.py
# pylint: disable=unused-argument
def test_diff(bench_dvc, tmp_dir, scm, dvc, make_dataset):
    dataset = make_dataset(cache=True, files=True, dvcfile=True, commit=True)
    bench_dvc("diff")
    bench_dvc("diff", name="noop")

    (dataset / "new").write_text("new")
    bench_dvc("diff", name="changed")
    bench_dvc("diff", name="changed-noop")




dvc/testing/benchmarks/cli/commands/test_exp_show.py
# pylint: disable=unused-argument
import pytest


@pytest.mark.requires(minversion=(2, 10, 0))
def test_exp_show(make_project, monkeypatch, bench_dvc, dvc_bin):
    url = "https://github.com/efiop/lstm_seq2seq"
    rev = "dvc-bench"
    path = make_project(url, rev=rev)
    monkeypatch.chdir(path)

    dvc_bin("exp", "pull", "-A", "--no-cache", "origin")

    bench_dvc("exp", "show", "-A", "--no-pager")




dvc/testing/benchmarks/cli/commands/test_gc.py
# pylint: disable=unused-argument
def test_gc(bench_dvc, tmp_dir, dvc, make_dataset):
    make_dataset(files=False, cache=True, dvcfile=False)
    bench_dvc("gc", "-f", "-w")




dvc/testing/benchmarks/cli/commands/test_get.py
# pylint: disable=unused-argument
def test_get(bench_dvc, tmp_dir, scm, dvc, make_dataset, remote):
    dataset = make_dataset(
        cache=False, files=False, dvcfile=True, commit=True, remote=True
    )
    bench_dvc("get", tmp_dir, dataset.name, "-o", "new")




dvc/testing/benchmarks/cli/commands/test_get_url.py
# pylint: disable=unused-argument
def test_get_url(bench_dvc, tmp_dir, scm, dvc, make_dataset):
    dataset = make_dataset(
        cache=False, files=True, dvcfile=False, commit=False, remote=False
    )
    bench_dvc("get-url", str(dataset), "new")




dvc/testing/benchmarks/cli/commands/test_help.py
def test_help(bench_dvc):
    bench_dvc("--help", rounds=100)




dvc/testing/benchmarks/cli/commands/test_import.py
# pylint: disable=unused-argument
def test_import(bench_dvc, tmp_dir, scm, dvc, make_dataset, remote):
    dataset = make_dataset(
        cache=False, files=False, dvcfile=True, commit=True, remote=True
    )
    bench_dvc("import", tmp_dir, dataset.name, "-o", "new")




dvc/testing/benchmarks/cli/commands/test_import_url.py
# pylint: disable=unused-argument
def test_import_url(bench_dvc, tmp_dir, scm, dvc, make_dataset):
    dataset = make_dataset(
        cache=False, files=True, dvcfile=False, commit=False, remote=False
    )
    bench_dvc("import-url", str(dataset), "new")




dvc/testing/benchmarks/cli/commands/test_init.py
# pylint: disable=unused-argument
import shutil


def test_init(bench_dvc, tmp_dir, scm):
    def _cleanup_dir():
        for item in tmp_dir.iterdir():
            if item.is_dir():
                if item.name != ".git":
                    shutil.rmtree(item)
            else:
                item.unlink()

    bench_dvc("init", setup=_cleanup_dir, rounds=100, warmup_rounds=1)




dvc/testing/benchmarks/cli/commands/test_ls.py
# pylint: disable=unused-argument
def test_list(bench_dvc, tmp_dir, scm, dvc, make_dataset, remote):
    make_dataset(cache=False, files=False, dvcfile=True, commit=True, remote=True)
    bench_dvc("list", tmp_dir)
    bench_dvc("list", tmp_dir, "--dvc-only", name="dvc-only")
    bench_dvc("list", tmp_dir, "--recursive", name="recursive")
    bench_dvc("list", tmp_dir, "dataset", name="shallow")




dvc/testing/benchmarks/cli/commands/test_plots.py
import pytest

from dvc.repo import Repo
from dvc.testing.benchmarks.fixtures import _pull

# pylint: disable=redefined-outer-name,unused-argument

# pylint: disable=unused-argument


@pytest.mark.requires(minversion=(2, 34, 0), reason="top-level plots not supported")
def test_plots(project, bench_dvc):
    with Repo() as dvc:
        _pull(dvc)

    kwargs = {"rounds": 5, "iterations": 3, "warmup_rounds": 2}
    bench_dvc("plots", "show", name="show", **kwargs)
    bench_dvc("plots", "show", "--json", name="show-json", **kwargs)
    bench_dvc("plots", "diff", "HEAD", name="diff", **kwargs)
    bench_dvc("plots", "diff", "HEAD", "--json", name="diff-json", **kwargs)




dvc/testing/benchmarks/cli/commands/test_pull.py
# pylint: disable=unused-argument
def test_pull(bench_dvc, tmp_dir, dvc, make_dataset, remote):
    make_dataset(cache=False, dvcfile=True, files=False, remote=True)
    bench_dvc("pull")




dvc/testing/benchmarks/cli/commands/test_push.py
# pylint: disable=unused-argument
def test_push(bench_dvc, tmp_dir, dvc, make_dataset, remote):
    make_dataset(cache=True, dvcfile=True, files=False)
    bench_dvc("push")




dvc/testing/benchmarks/cli/commands/test_status.py
# pylint: disable=unused-argument
def test_status(bench_dvc, tmp_dir, dvc, make_dataset):
    dataset = make_dataset(files=True, dvcfile=True, cache=True)
    bench_dvc("status")
    bench_dvc("status", name="noop")

    (dataset / "new").write_text("new")
    bench_dvc("status", name="changed")
    bench_dvc("status", name="changed-noop")




dvc/testing/benchmarks/cli/commands/test_update.py
# pylint: disable=unused-argument
def test_update(bench_dvc, tmp_dir, dvc, make_dataset):
    dataset = make_dataset(
        cache=False, files=True, dvcfile=False, commit=False, remote=False
    )
    dvc.imp_url(str(dataset), "new")
    (dataset / "new").write_text("new")
    bench_dvc("update", "new.dvc")
    bench_dvc("update", "new.dvc", name="noop")




dvc/testing/benchmarks/cli/stories/__init__.py




dvc/testing/benchmarks/cli/stories/test_modify_data.py
"""
Tests for modifications to an existing dataset.
"""
import glob
import os
import random
import shutil
import sys

import pytest

# pylint: disable=unused-argument,unexpected-keyword-arg


@pytest.mark.skipif(sys.version_info < (3, 10), reason="requires 3.10 glob.glob")
def test_partial_add(bench_dvc, tmp_dir, dvc, dataset, remote):
    random.seed(4231)
    # Move some files to create a partial dataset
    os.makedirs("partial-copy")
    for f in glob.glob("*", root_dir=dataset, recursive=True):  # type: ignore[call-arg]
        if random.random() > 0.5:  # noqa: S311 # nosec
            shutil.move(dataset / f, tmp_dir / "partial-copy" / f)

    # Add/push partial dataset
    dvc.add(str(dataset))
    dvc.push()

    # Add more files to the dataset
    shutil.copytree("partial-copy", dataset, dirs_exist_ok=True)

    # Benchmark operations for adding files to a dataset
    bench_dvc("add", dataset, name="partial-add")
    bench_dvc("push", name="partial-add")
    bench_dvc("gc", "-f", "-w", name="noop")
    bench_dvc("gc", "-f", "-w", "-c", name="cloud-noop")


@pytest.mark.skipif(sys.version_info < (3, 10), reason="requires 3.10 glob.glob")
def test_partial_remove(bench_dvc, tmp_dir, dvc, dataset, remote):
    random.seed(5232)
    # Add/push full dataset
    dvc.add(str(dataset))
    dvc.push()

    # Remove some files
    for f in glob.glob("*", root_dir=dataset, recursive=True):  # type: ignore[call-arg]
        if random.random() > 0.5:  # noqa: S311 # nosec
            if os.path.isfile(dataset / f):
                os.remove(dataset / f)
            elif os.path.isdir(dataset / f):
                shutil.rmtree(dataset / f)

    # Benchmark operations for removing files from dataset
    bench_dvc("add", dataset)
    bench_dvc("push")
    bench_dvc("gc", "-f", "-w")
    bench_dvc("gc", "-f", "-w", "-c", name="cloud")




dvc/testing/benchmarks/cli/stories/use_cases/__init__.py




dvc/testing/benchmarks/cli/stories/use_cases/test_sharing.py
import shutil

# pylint: disable=unused-argument


def test_sharing(bench_dvc, tmp_dir, dvc, dataset, remote):
    bench_dvc("add", dataset)
    bench_dvc("add", dataset, name="noop")

    bench_dvc("push")
    bench_dvc("push", name="noop")

    shutil.rmtree(dataset)
    shutil.rmtree(tmp_dir / ".dvc" / "cache")

    bench_dvc("pull")
    bench_dvc("pull", name="noop")

    bench_dvc("checkout", name="noop")




dvc/ui/__init__.py
from contextlib import contextmanager, nullcontext
from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    Dict,
    Iterable,
    Iterator,
    Optional,
    Sequence,
    TextIO,
    Type,
    Union,
)

import colorama

from dvc.utils.objects import cached_property

if TYPE_CHECKING:
    from rich.console import Console as RichConsole
    from rich.console import JustifyMethod, OverflowMethod
    from rich.status import Status
    from rich.style import Style
    from rich.text import Text as RichText

    from dvc.progress import Tqdm
    from dvc.types import StrPath
    from dvc.ui.table import Headers, Styles, TableData


@contextmanager
def disable_colorama():
    import sys

    colorama.deinit()
    try:
        yield
    finally:
        if sys.stdout:
            sys.stdout.flush()
        if sys.stderr:
            sys.stderr.flush()
        colorama.reinit()


class Formatter:
    def __init__(
        self, theme: Optional[Dict] = None, defaults: Optional[Dict] = None
    ) -> None:
        from collections import defaultdict

        theme = theme or {
            "success": {"color": "green", "style": "bold"},
            "warn": {"color": "yellow"},
            "error": {"color": "red", "style": "bold"},
        }
        self.theme = defaultdict(lambda: defaults or {}, theme)

    def format(  # noqa: A003
        self, message: str, style: Optional[str] = None, **kwargs
    ) -> str:
        from dvc.utils import colorize

        return colorize(message, **self.theme[style])


class Console:
    def __init__(
        self, formatter: Optional[Formatter] = None, enable: bool = False
    ) -> None:
        from contextvars import ContextVar

        self.formatter: Formatter = formatter or Formatter()
        self._enabled: bool = enable
        self._paginate: ContextVar[bool] = ContextVar("_paginate", default=False)

    def enable(self) -> None:
        self._enabled = True

    def success(self, message: str) -> None:
        self.write(message, style="success")

    def error(self, message: str) -> None:
        self.error_write(message, style="error")

    def warn(self, message: str) -> None:
        self.error_write(message, style="warn")

    def error_write(
        self,
        *objects: Any,
        style: Optional[str] = None,
        sep: Optional[str] = None,
        end: Optional[str] = None,
        styled: bool = False,
        force: bool = True,
    ) -> None:
        return self.write(
            *objects,
            style=style,
            sep=sep,
            end=end,
            stderr=True,
            force=force,
            styled=styled,
        )

    def write_json(  # noqa: PLR0913
        self,
        data: Any,
        indent: Optional[int] = None,
        highlight: Optional[bool] = None,
        stderr: bool = False,
        skip_keys: bool = False,
        ensure_ascii: bool = True,
        check_circular: bool = True,
        allow_nan: bool = True,
        default: Optional[Callable[[Any], Any]] = None,
        sort_keys: bool = False,
    ) -> None:
        if highlight is None:
            highlight = self.isatty()
        if indent is None and self.isatty():
            indent = 2

        from rich.json import JSON

        json = JSON.from_data(
            data=data,
            indent=indent,
            highlight=bool(highlight),
            skip_keys=skip_keys,
            ensure_ascii=ensure_ascii,
            check_circular=check_circular,
            allow_nan=allow_nan,
            default=default,
            sort_keys=sort_keys,
        )
        if not highlight:
            import os

            # we don't need colorama to try to strip ansi codes
            # when highlighting is disabled
            ctx = nullcontext() if "DVC_TEST" in os.environ else disable_colorama()
            with ctx:
                return self.write(json.text, stderr=stderr)
        return self.rich_print(json, stderr=stderr, soft_wrap=True)

    def rich_print(
        self,
        *objects: Any,
        sep: str = " ",
        end: str = "\n",
        stderr: bool = False,
        style: Optional[Union[str, "Style"]] = None,
        justify: Optional["JustifyMethod"] = None,
        overflow: Optional["OverflowMethod"] = None,
        no_wrap: Optional[bool] = None,
        emoji: Optional[bool] = None,
        markup: Optional[bool] = None,
        highlight: Optional[bool] = None,
        width: Optional[int] = None,
        height: Optional[int] = None,
        crop: bool = True,
        soft_wrap: Optional[bool] = None,
        new_line_start: bool = False,
    ) -> None:
        if stderr:
            console = self.error_console
        else:
            console = self.rich_console
        return console.print(
            *objects,
            sep=sep,
            end=end,
            style=style,
            justify=justify,
            overflow=overflow,
            no_wrap=no_wrap,
            emoji=emoji,
            markup=markup,
            highlight=highlight,
            width=width,
            height=height,
            crop=crop,
            soft_wrap=soft_wrap,
            new_line_start=new_line_start,
        )

    def write(
        self,
        *objects: Any,
        style: Optional[str] = None,
        sep: Optional[str] = None,
        end: Optional[str] = None,
        stderr: bool = False,
        force: bool = False,
        styled: bool = False,
        file: Optional[TextIO] = None,
    ) -> None:
        import sys

        from dvc.progress import Tqdm

        sep = " " if sep is None else sep
        end = "\n" if end is None else end
        if not self._enabled and not force:
            return

        file = file or (sys.stderr if stderr else sys.stdout)
        with Tqdm.external_write_mode(file=file):
            # if we are inside pager context, send the output to rich's buffer
            if styled or self._paginate.get():
                if styled:
                    return self.rich_print(*objects, sep=sep, end=end, stderr=stderr)
                return self.rich_print(
                    sep.join(str(_object) for _object in objects),
                    style=None,
                    highlight=False,
                    emoji=False,
                    markup=False,
                    no_wrap=True,
                    overflow="ignore",
                    crop=False,
                    sep=sep,
                    end=end,
                    stderr=stderr,
                )

            values = (self.formatter.format(obj, style) for obj in objects)
            return print(*values, sep=sep, end=end, file=file)

    @property
    def rich_text(self) -> "Type[RichText]":
        from rich.text import Text

        return Text

    @staticmethod
    def progress(*args, **kwargs) -> "Tqdm":
        from dvc.progress import Tqdm

        return Tqdm(*args, **kwargs)

    @contextmanager
    def pager(self, styles: bool = True) -> Iterator[None]:
        from .pager import DvcPager

        tok = self._paginate.set(True)
        try:
            with self.rich_console.pager(pager=DvcPager(), styles=styles):
                yield
        finally:
            self._paginate.reset(tok)

    def prompt(
        self,
        text: str,
        choices: Optional[Iterable[str]] = None,
        password: bool = False,
    ) -> Optional[str]:
        while True:
            try:
                response = self.rich_console.input(
                    text + " ", markup=False, password=password
                )
            except EOFError:
                return None

            answer = response.lower()
            if not choices:
                return answer

            if answer in choices:
                return answer

            self.write(f"Your response must be one of: {choices}. Please try again.")

    def confirm(self, statement: str) -> bool:
        """Ask the user for confirmation about the specified statement.

        Args:
            statement: statement to ask the user confirmation about.
        """
        text = f"{statement} [y/n]:"
        answer = self.prompt(text, choices=["yes", "no", "y", "n"])
        if not answer:
            return False
        return answer.startswith("y")

    @cached_property
    def rich_console(self) -> "RichConsole":
        """rich_console is only set to stdout for now."""
        from rich import console

        return console.Console()

    @cached_property
    def error_console(self) -> "RichConsole":
        from rich import console

        return console.Console(stderr=True)

    def table(
        self,
        data: "TableData",
        headers: Optional["Headers"] = None,
        markdown: bool = False,
        rich_table: bool = False,
        force: bool = True,
        pager: bool = False,
        header_styles: Optional[Union[Dict[str, "Styles"], Sequence["Styles"]]] = None,
        row_styles: Optional[Sequence["Styles"]] = None,
        borders: Union[bool, str] = False,
    ) -> None:
        from dvc.ui import table as t

        if not data and not markdown:
            return

        if not markdown and rich_table:
            if force or self._enabled:
                return t.rich_table(
                    self,
                    data,
                    headers,
                    pager=pager,
                    header_styles=header_styles,
                    row_styles=row_styles,
                    borders=borders,
                )

            return

        return t.plain_table(
            self, data, headers, markdown=markdown, pager=pager, force=force
        )

    def status(self, status: str, **kwargs: Any) -> "Status":
        return self.error_console.status(status, **kwargs)

    def isatty(self) -> bool:
        import sys

        return sys.stdout.isatty()

    def open_browser(self, file: "StrPath") -> int:
        import webbrowser
        from pathlib import Path
        from platform import uname

        from dvc.utils import relpath

        path = Path(file).resolve()
        url = relpath(path) if "microsoft" in uname().release.lower() else path.as_uri()

        opened = webbrowser.open(url)

        if not opened:
            ui.error_write(f"Failed to open {url}. Please try opening it manually.")
            return 1

        return 0


ui = Console()


if __name__ == "__main__":
    ui.enable()

    ui.write("No default remote set")
    ui.success("Everything is up to date.")
    ui.warn("Run queued experiments will be removed.")
    ui.error("too few arguments.")

    ui.table([("scores.json", "0.5674")], headers=["Path", "auc"])
    ui.table([("scores.json", "0.5674")], headers=["Path", "auc"], markdown=True)




dvc/ui/_rich_progress.py
from funcy import split
from rich.progress import (
    BarColumn,
    DownloadColumn,
    MofNCompleteColumn,
    Progress,
    TextColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
    TransferSpeedColumn,
)


class MofNCompleteColumnWithUnit(MofNCompleteColumn):
    """Requires `task.fields[unit]` to be set."""

    def render(self, task):
        ret = super().render(task)
        unit = task.fields.get("unit")
        return ret.append(f" {unit}") if unit else ret


class RichProgress(Progress):
    def clear_task(self, task):
        try:
            self.remove_task(task)
        except KeyError:
            pass


class RichTransferProgress(RichProgress):
    SUMMARY_COLS = (
        TextColumn("[magenta]{task.description}[bold green]"),
        MofNCompleteColumnWithUnit(),
        TimeElapsedColumn(),
    )
    TRANSFER_COLS = (
        TextColumn("  [blue]{task.description}"),
        BarColumn(),
        DownloadColumn(),
        TransferSpeedColumn(),
        TextColumn("eta"),
        TimeRemainingColumn(),
    )

    def get_renderables(self):
        summary_tasks, other_tasks = split(
            lambda task: task.fields.get("progress_type") == "summary",
            self.tasks,
        )
        self.columns = self.SUMMARY_COLS
        yield self.make_tasks_table(summary_tasks)
        self.columns = self.TRANSFER_COLS
        yield self.make_tasks_table(other_tasks)




dvc/ui/pager.py
"""Draws DAG in ASCII."""

import logging
import os
import pydoc
import sys

from rich.pager import Pager

from dvc.env import DVC_PAGER
from dvc.utils import format_link

logger = logging.getLogger(__name__)


DEFAULT_PAGER = "less"
LESS = "LESS"
PAGER_ENV = "PAGER"


def prepare_default_pager(
    clear_screen: bool = False,
    quit_if_one_screen: bool = True,
    ansi_escapes: bool = True,
    chop_long_lines: bool = True,
    no_init: bool = True,
    no_tilde: bool = False,
) -> str:
    args = [DEFAULT_PAGER]
    if clear_screen:
        args.append("--clear-screen")  # -c
    if quit_if_one_screen:
        args.append("--quit-if-one-screen")  # -F
    if ansi_escapes:
        args.append("--RAW-CONTROL-CHARS")  # -R
    if chop_long_lines:
        args.append("--chop-long-lines")  # -S
    if no_init:
        args.append("--no-init")  # -X
    if no_tilde:
        args.append("--tilde")  # -~

    return " ".join(args)


def make_pager(cmd=None):
    def _pager(text):
        return pydoc.tempfilepager(pydoc.plain(text), cmd)

    return _pager if cmd else pydoc.plainpager


def find_pager():
    if not sys.stdout.isatty():
        return None

    # pylint: disable=redefined-outer-name
    pager = os.getenv(DVC_PAGER)
    if not pager:
        pager = os.getenv(PAGER_ENV)
    if not pager:
        if os.system(f"({DEFAULT_PAGER}) 2>{os.devnull}") != 0:  # nosec B605
            logger.warning(
                "Unable to find `less` in the PATH. Check out %s for more info.",
                format_link("https://man.dvc.org/pipeline/show"),
            )
        else:
            pager = DEFAULT_PAGER

    if pager == DEFAULT_PAGER:
        # if pager is less (i.e. default), regardless of `$LESS`, apply `-RS`.
        # `-R` is required to render ansi escape sequences for exp show
        # and, `-S` is required for horizontal scrolling.
        less_env = bool(os.getenv(LESS))
        return prepare_default_pager(
            ansi_escapes=True,
            chop_long_lines=True,
            quit_if_one_screen=not less_env,
            no_init=not less_env,
        )

    return pager


def pager(text: str) -> None:
    _pager = find_pager()
    logger.trace("Using pager: '%s'", _pager)  # type: ignore[attr-defined]
    make_pager(_pager)(text)


class DvcPager(Pager):
    def show(self, content: str) -> None:
        pager(content)




dvc/ui/table.py
from collections import abc
from contextlib import ExitStack, contextmanager
from itertools import zip_longest
from typing import TYPE_CHECKING, Dict, Iterator, Optional, Sequence, Union

from dvc.types import DictStrAny

if TYPE_CHECKING:
    from rich.console import Console as RichConsole
    from rich.table import Table

    from dvc.ui import Console, RichText

SHOW_MAX_WIDTH = 1024


CellT = Union[str, "RichText", None]  # RichText is mostly compatible with str
Row = Sequence[CellT]
TableData = Sequence[Row]
Headers = Sequence[str]
Styles = DictStrAny


def plain_table(
    ui: "Console",
    data: TableData,
    headers: Optional[Headers] = None,
    markdown: bool = False,
    pager: bool = False,
    force: bool = True,
) -> None:
    from funcy import nullcontext
    from tabulate import tabulate

    text: str = tabulate(
        data,
        headers if headers is not None else (),
        tablefmt="github" if markdown else "plain",
        disable_numparse=True,
        # None will be shown as "" by default, overriding
        missingval="-",
    )
    if markdown:
        # NOTE: md table is incomplete without the trailing newline
        text += "\n"

    cm = ui.pager() if pager else nullcontext()
    with cm:
        ui.write(text, force=force)


@contextmanager
def console_width(table: "Table", console: "RichConsole", val: int) -> Iterator[None]:
    # NOTE: rich does not have native support for unlimited width
    # via pager. we override rich table compression by setting
    # console width to the full width of the table
    # pylint: disable=protected-access

    console_options = console.options
    original = console_options.max_width
    con_width = console._width

    try:
        console_options.max_width = val
        measurement = table.__rich_measure__(console, console_options)
        console._width = measurement.maximum

        yield
    finally:
        console_options.max_width = original
        console._width = con_width


def rich_table(
    ui: "Console",
    data: TableData,
    headers: Optional[Headers] = None,
    pager: bool = False,
    header_styles: Optional[Union[Dict[str, Styles], Sequence[Styles]]] = None,
    row_styles: Optional[Sequence[Styles]] = None,
    borders: Union[bool, str] = False,
) -> None:
    from rich import box

    from dvc.utils.table import Table

    border_style = {
        True: box.HEAVY_HEAD,  # is a default in rich,
        False: None,
        "simple": box.SIMPLE,
        "minimal": box.MINIMAL,
        "horizontals": box.HORIZONTALS,
    }

    table = Table(box=border_style[borders])

    if isinstance(header_styles, abc.Sequence):
        hs: Dict[str, Styles] = dict(zip(headers or [], header_styles))
    else:
        hs = header_styles or {}

    for header in headers or []:
        table.add_column(header, **hs.get(header, {}))

    rs: Sequence[Styles] = row_styles or []
    for row, style in zip_longest(data, rs):
        table.add_row(*row, **(style or {}))

    stack = ExitStack()
    if pager:
        stack.enter_context(console_width(table, ui.rich_console, SHOW_MAX_WIDTH))
        stack.enter_context(ui.pager())

    with stack:
        ui.write(table, styled=True)
        return




dvc/utils/.gitignore
/build.py




dvc/utils/__init__.py
"""Helpers for other modules."""

import hashlib
import json
import logging
import os
import re
import sys
from typing import Dict, List, Optional, Tuple

import colorama

logger = logging.getLogger(__name__)

LARGE_DIR_SIZE = 100
TARGET_REGEX = re.compile(r"(?P<path>.*?)(:(?P<name>[^\\/:]*))??$")


def bytes_hash(byts, typ):
    hasher = getattr(hashlib, typ)()
    hasher.update(byts)
    return hasher.hexdigest()


def dict_filter(d, exclude=()):
    """
    Exclude specified keys from a nested dict
    """
    if not exclude or not isinstance(d, (list, dict)):
        return d

    if isinstance(d, list):
        return [dict_filter(e, exclude) for e in d]

    return {k: dict_filter(v, exclude) for k, v in d.items() if k not in exclude}


def dict_hash(d, typ, exclude=()):
    filtered = dict_filter(d, exclude)
    byts = json.dumps(filtered, sort_keys=True).encode("utf-8")
    return bytes_hash(byts, typ)


def dict_md5(d, **kwargs):
    return dict_hash(d, "md5", **kwargs)


def dict_sha256(d, **kwargs):
    return dict_hash(d, "sha256", **kwargs)


def _split(list_to_split, chunk_size):
    return [
        list_to_split[i : i + chunk_size]
        for i in range(0, len(list_to_split), chunk_size)
    ]


# NOTE: Check if we are in a bundle
# https://pythonhosted.org/PyInstaller/runtime-information.html
def is_binary():
    return getattr(sys, "frozen", False)


def fix_env(env=None):
    """Fix env variables modified by PyInstaller [1] and pyenv [2].
    [1] http://pyinstaller.readthedocs.io/en/stable/runtime-information.html
    [2] https://github.com/pyenv/pyenv/issues/985
    """
    if env is None:
        env = os.environ.copy()
    else:
        env = env.copy()

    if is_binary():
        lp_key = "LD_LIBRARY_PATH"
        lp_orig = env.get(lp_key + "_ORIG", None)
        if lp_orig is not None:
            env[lp_key] = lp_orig
        else:
            env.pop(lp_key, None)

    # Unlike PyInstaller, pyenv doesn't leave backups of original env vars
    # when it modifies them. If we look into the shim, pyenv and pyenv-exec,
    # we can figure out that the PATH is modified like this:
    #
    #     PATH=$PYENV_BIN_PATH:${bin_path}:${plugin_bin}:$PATH
    #
    # where
    #
    #     PYENV_BIN_PATH - might not start with $PYENV_ROOT if we are running
    #         `system` version of the command, see pyenv-exec source code.
    #     bin_path - might not start with $PYENV_ROOT as it runs realpath on
    #         it, but always has `libexec` part in it, see pyenv source code.
    #     plugin_bin - might contain more than 1 entry, which start with
    #         $PYENV_ROOT, see pyenv source code.
    #
    # Also, we know that whenever pyenv is running, it exports these env vars:
    #
    #     PYENV_DIR
    #     PYENV_HOOK_PATH
    #     PYENV_VERSION
    #     PYENV_ROOT
    #
    # So having this, we can make a rightful assumption about what parts of the
    # PATH we need to remove in order to get the original PATH.
    path = env.get("PATH", "")
    parts = path.split(":")
    bin_path = parts[1] if len(parts) > 2 else ""
    pyenv_dir = env.get("PYENV_DIR")
    pyenv_hook_path = env.get("PYENV_HOOK_PATH")
    pyenv_version = env.get("PYENV_VERSION")
    pyenv_root = env.get("PYENV_ROOT")

    env_matches = all([pyenv_dir, pyenv_hook_path, pyenv_version, pyenv_root])

    bin_path_matches = os.path.basename(bin_path) == "libexec"

    # NOTE: we don't support pyenv-win
    if os.name != "nt" and env_matches and bin_path_matches:
        # removing PYENV_BIN_PATH and bin_path
        parts = parts[2:]

        if parts:
            # removing plugin_bin from the left
            plugin_bin = os.path.join(pyenv_root, "plugins")
            while parts[0].startswith(plugin_bin):
                del parts[0]

        env["PATH"] = ":".join(parts)

    return env


def colorize(message, color=None, style=None):
    """Returns a message in a specified color."""
    if not color:
        return message

    styles = {"dim": colorama.Style.DIM, "bold": colorama.Style.BRIGHT}

    colors = {
        "green": colorama.Fore.GREEN,
        "yellow": colorama.Fore.YELLOW,
        "blue": colorama.Fore.BLUE,
        "red": colorama.Fore.RED,
        "magenta": colorama.Fore.MAGENTA,
        "cyan": colorama.Fore.CYAN,
    }

    return "{style}{color}{message}{reset}".format(
        style=styles.get(style, ""),
        color=colors.get(color, ""),
        message=message,
        reset=colorama.Style.RESET_ALL,
    )


def boxify(message, border_color=None):
    """Put a message inside a box.

    Args:
        message (unicode): message to decorate.
        border_color (unicode): name of the color to outline the box with.
    """
    lines = message.split("\n")
    max_width = max(_visual_width(line) for line in lines)

    padding_horizontal = 5
    padding_vertical = 1

    box_size_horizontal = max_width + (padding_horizontal * 2)

    chars = {"corner": "+", "horizontal": "-", "vertical": "|", "empty": " "}

    margin = "{corner}{line}{corner}\n".format(
        corner=chars["corner"], line=chars["horizontal"] * box_size_horizontal
    )

    padding_lines = [
        "{border}{space}{border}\n".format(
            border=colorize(chars["vertical"], color=border_color),
            space=chars["empty"] * box_size_horizontal,
        )
        * padding_vertical
    ]

    content_lines = [
        "{border}{space}{content}{space}{border}\n".format(
            border=colorize(chars["vertical"], color=border_color),
            space=chars["empty"] * padding_horizontal,
            content=_visual_center(line, max_width),
        )
        for line in lines
    ]

    return "{margin}{padding}{content}{padding}{margin}".format(
        margin=colorize(margin, color=border_color),
        padding="".join(padding_lines),
        content="".join(content_lines),
    )


def _visual_width(line):
    """Get the the number of columns required to display a string"""

    return len(re.sub(colorama.ansitowin32.AnsiToWin32.ANSI_CSI_RE, "", line))


def _visual_center(line, width):
    """Center align string according to it's visual width"""

    spaces = max(width - _visual_width(line), 0)
    left_padding = int(spaces / 2)
    right_padding = spaces - left_padding

    return (left_padding * " ") + line + (right_padding * " ")


def relpath(path, start=os.curdir):
    path = os.fspath(path)
    start = os.path.abspath(os.fspath(start))

    # Windows path on different drive than curdir doesn't have relpath
    if os.name == "nt":
        # Since python 3.8 os.realpath resolves network shares to their UNC
        # path. So, to be certain that relative paths correctly captured,
        # we need to resolve to UNC path first. We resolve only the drive
        # name so that we don't follow any 'real' symlinks on the path
        def resolve_network_drive_windows(path_to_resolve):
            drive, tail = os.path.splitdrive(path_to_resolve)
            return os.path.join(os.path.realpath(drive), tail)

        path = resolve_network_drive_windows(os.path.abspath(path))
        start = resolve_network_drive_windows(start)
        if not os.path.commonprefix([start, path]):
            return path
    return os.path.relpath(path, start)


def as_posix(path: str) -> str:
    import ntpath
    import posixpath

    return path.replace(ntpath.sep, posixpath.sep)


def env2bool(var, undefined=False):
    """
    undefined: return value if env var is unset
    """
    var = os.getenv(var, None)
    if var is None:
        return undefined
    return bool(re.search("1|y|yes|true", var, flags=re.I))


def resolve_output(inp, out, force=False):
    from urllib.parse import urlparse

    from dvc.exceptions import FileExistsLocallyError

    name = os.path.basename(os.path.normpath(urlparse(inp).path))
    if not out:
        ret = name
    elif os.path.isdir(out):
        ret = os.path.join(out, name)
    else:
        ret = out

    if os.path.exists(ret) and not force:
        hint = "\nTo override it, re-run with '--force'."
        raise FileExistsLocallyError(ret, hint=hint)

    return ret


def resolve_paths(repo, out, always_local=False):
    from urllib.parse import urlparse

    from dvc.dvcfile import DVC_FILE_SUFFIX
    from dvc.exceptions import DvcException
    from dvc.fs import localfs

    from .fs import contains_symlink_up_to

    abspath = os.path.abspath(out)
    dirname = os.path.dirname(abspath)
    base = os.path.basename(os.path.normpath(out))

    scheme = urlparse(out).scheme

    if os.name == "nt" and scheme == os.path.splitdrive(abspath)[0][0].lower():
        # urlparse interprets windows drive letters as URL scheme
        scheme = ""

    if scheme or not localfs.path.isin_or_eq(abspath, repo.root_dir):
        wdir = os.getcwd()
    elif contains_symlink_up_to(dirname, repo.root_dir) or (
        os.path.isdir(abspath) and localfs.is_symlink(abspath)
    ):
        msg = (
            "Cannot add files inside symlinked directories to DVC. "
            "See {} for more information."
        ).format(
            format_link("https://dvc.org/doc/user-guide/troubleshooting#add-symlink")
        )
        raise DvcException(msg)
    else:
        wdir = dirname
        out = base

    if always_local:
        out = base

    path = os.path.join(wdir, base + DVC_FILE_SUFFIX)

    return (path, wdir, out)


def format_link(link):
    return "<{blue}{link}{nc}>".format(
        blue=colorama.Fore.CYAN, link=link, nc=colorama.Fore.RESET
    )


def error_link(name):
    return format_link(f"https://error.dvc.org/{name}")


def parse_target(
    target: str, default: Optional[str] = None, isa_glob: bool = False
) -> Tuple[Optional[str], Optional[str]]:
    from dvc.dvcfile import LOCK_FILE, PROJECT_FILE, is_valid_filename
    from dvc.exceptions import DvcException
    from dvc.parsing import JOIN

    if not target:
        return None, None

    default = default or PROJECT_FILE
    if isa_glob:
        path, _, glob = target.rpartition(":")
        return path or default, glob or None

    # look for first "@", so as not to assume too much about stage name
    # eg: it might contain ":" in a generated stages from dict which might
    # affect further parsing with the regex.
    group, _, key = target.partition(JOIN)
    match = TARGET_REGEX.match(group)

    if not match:
        return target, None

    path, name = (match.group("path"), match.group("name"))

    if name and key:
        name += f"{JOIN}{key}"

    if path:
        if os.path.basename(path) == LOCK_FILE:
            raise DvcException(
                "Did you mean: `{}`?".format(target.replace(".lock", ".yaml", 1))
            )
        if not name:
            ret = (target, None)
            return ret if is_valid_filename(target) else ret[::-1]

    if not path:
        logger.trace("Assuming file to be '%s'", default)  # type: ignore[attr-defined]

    return path or default, name


def glob_targets(targets, glob=True, recursive=True):
    from dvc.exceptions import DvcException

    if not glob:
        return targets

    from glob import iglob

    results = [
        exp_target
        for target in targets
        for exp_target in iglob(target, recursive=recursive)
    ]

    if not results:
        msg = f"Glob {targets} has no matches."
        raise DvcException(msg)

    return results


def error_handler(func):
    def wrapper(*args, **kwargs):
        onerror = kwargs.get("onerror", None)
        result = {}

        try:
            vals = func(*args, **kwargs)
            if vals:
                result["data"] = vals
        except Exception as e:  # noqa: BLE001, pylint: disable=broad-except
            if onerror is not None:
                onerror(result, e, **kwargs)
        return result

    return wrapper


def onerror_collect(result: Dict, exception: Exception, *args, **kwargs):
    logger.debug("", exc_info=True)
    result["error"] = exception


def errored_revisions(rev_data: Dict) -> List:
    from dvc.utils.collections import nested_contains

    result = []
    for revision, data in rev_data.items():
        if nested_contains(data, "error"):
            result.append(revision)
    return result




dvc/utils/cli_parse.py
from collections import defaultdict
from typing import Dict, Iterable, List


def parse_params(path_params: Iterable[str]) -> List[Dict[str, List[str]]]:
    """Normalizes the shape of params from the CLI to dict."""
    from dvc.dependency.param import ParamsDependency

    ret: Dict[str, List[str]] = defaultdict(list)
    for path_param in path_params:
        path, _, params_str = path_param.rpartition(":")
        # remove empty strings from params, on condition such as `-p "file1:"`
        params = filter(bool, params_str.split(","))
        if not path:
            path = ParamsDependency.DEFAULT_PARAMS_FILE
        ret[path].extend(params)
    return [{path: params} for path, params in ret.items()]


def to_path_overrides(
    path_params: Iterable[str],
) -> Dict[str, List[str]]:
    """Group overrides by path"""
    from dvc.dependency.param import ParamsDependency

    path_overrides = defaultdict(list)
    for path_param in path_params:
        path_and_name = path_param.partition("=")[0]
        if ":" not in path_and_name:
            override = path_param
            path = ParamsDependency.DEFAULT_PARAMS_FILE
        else:
            path, _, override = path_param.partition(":")

        path_overrides[path].append(override)

    return dict(path_overrides)




dvc/utils/collections.py
from collections.abc import Mapping
from typing import Dict, Iterable, List, Union, no_type_check


@no_type_check
def apply_diff(src, dest):  # noqa: C901
    """Recursively apply changes from src to dest.

    Preserves dest type and hidden info in dest structure,
    like ruamel.yaml leaves when parses files. This includes comments,
    ordering and line foldings.

    Used in Stage load/dump cycle to preserve comments and custom formatting.
    """
    Seq = (list, tuple)  # noqa: N806
    Container = (Mapping, list, tuple)  # noqa: N806

    def is_same_type(a, b):
        return any(
            isinstance(a, t) and isinstance(b, t) for t in [str, Mapping, Seq, bool]
        )

    if isinstance(src, Mapping) and isinstance(dest, Mapping):
        for key, value in src.items():
            if isinstance(value, Container) and is_same_type(value, dest.get(key)):
                apply_diff(value, dest[key])
            elif key not in dest or value != dest[key]:
                dest[key] = value
        for key in set(dest) - set(src):
            del dest[key]
    elif isinstance(src, Seq) and isinstance(dest, Seq):
        if len(src) != len(dest):
            dest[:] = src
        else:
            for i, value in enumerate(src):
                if isinstance(value, Container) and is_same_type(value, dest[i]):
                    apply_diff(value, dest[i])
                elif value != dest[i]:
                    dest[i] = value
    else:
        raise AssertionError(  # noqa: TRY004
            "Can't apply diff from {} to {}".format(
                src.__class__.__name__, dest.__class__.__name__
            )
        )


def to_omegaconf(item):
    """
    Some parsers return custom classes (i.e. parse_yaml_for_update)
    that can mess up with omegaconf logic.
    Cast the custom classes to Python primitives.
    """
    if isinstance(item, dict):
        return {k: to_omegaconf(v) for k, v in item.items()}
    if isinstance(item, list):
        return [to_omegaconf(x) for x in item]
    return item


def remove_missing_keys(src, to_update):
    keys = list(src.keys())
    for key in keys:
        if key not in to_update:
            del src[key]
        elif isinstance(src[key], dict):
            remove_missing_keys(src[key], to_update[key])

    return src


def _merge_item(d, key, value):
    if key in d:
        item = d.get(key, None)
        if isinstance(item, dict) and isinstance(value, dict):
            merge_dicts(item, value)
        else:
            d[key] = value
    else:
        d[key] = value


def merge_dicts(src: Dict, to_update: Dict) -> Dict:
    """Recursively merges dictionaries.

    Args:
        src (dict): source dictionary of parameters
        to_update (dict): dictionary of parameters to merge into src
    """
    for key, value in to_update.items():
        _merge_item(src, key, value)
    return src


def ensure_list(item: Union[Iterable[str], str, None]) -> List[str]:
    if item is None:
        return []
    if isinstance(item, str):
        return [item]
    return list(item)


def nested_contains(dictionary: Dict, phrase: str) -> bool:
    for key, val in dictionary.items():
        if key == phrase and val:
            return True

        if isinstance(val, dict) and nested_contains(val, phrase):
            return True
    return False




dvc/utils/diff.py
import json
from collections import defaultdict
from typing import Dict

from .flatten import flatten


def _parse(raw):
    if raw is None or isinstance(raw, (dict, list, int, float)):
        return raw

    assert isinstance(raw, str)
    try:
        return json.loads(raw)
    except json.JSONDecodeError:
        return raw


def _diff_vals(old, new, with_unchanged):
    if isinstance(new, list) and isinstance(old, list) and len(old) == len(new) == 1:
        return _diff_vals(old[0], new[0], with_unchanged)

    if not with_unchanged and old == new:
        return {}

    res = {"old": old, "new": new}
    if isinstance(new, (int, float)) and isinstance(old, (int, float)):
        res["diff"] = new - old

    return res


def _flatten(d):
    if not d:
        return defaultdict(lambda: None)

    if isinstance(d, dict):
        return defaultdict(lambda: None, flatten(d))

    return defaultdict(lambda: "unable to parse")


def _diff_dicts(old_dict, new_dict, with_unchanged):
    new = _flatten(new_dict)
    old = _flatten(old_dict)

    res: Dict[str, Dict] = defaultdict(dict)

    xpaths = set(old.keys())
    xpaths.update(set(new.keys()))
    for xpath in xpaths:
        old_val = old[xpath]
        new_val = new[xpath]
        val_diff = _diff_vals(old_val, new_val, with_unchanged)
        if val_diff:
            res[xpath] = val_diff
    return dict(res)


def _diff(old_raw, new_raw, with_unchanged):
    old = _parse(old_raw)
    new = _parse(new_raw)

    if isinstance(new, dict) or isinstance(old, dict):
        return _diff_dicts(old, new, with_unchanged)

    val_diff = _diff_vals(old, new, with_unchanged)
    if val_diff:
        return {"": val_diff}

    return {}


def diff(old, new, with_unchanged=False):
    paths = set(old.keys())
    paths.update(set(new.keys()))

    res: Dict[str, Dict] = defaultdict(dict)
    for path in paths:
        path_diff = _diff(
            old.get(path, {}).get("data", {}),
            new.get(path, {}).get("data", {}),
            with_unchanged,
        )
        if path_diff:
            res[path] = path_diff
    return dict(res)


def format_dict(d):
    ret = {}
    for key, val in d.items():
        if isinstance(val, dict):
            new_val = format_dict(val)
        elif isinstance(val, list):
            new_val = str(val)
        else:
            new_val = val
        ret[key] = new_val
    return ret




dvc/utils/flatten.py
def flatten(d):
    import flatten_dict

    return flatten_dict.flatten(d, reducer="dot")


def unflatten(d):
    import flatten_dict

    return flatten_dict.unflatten(d, splitter="dot")




dvc/utils/fs.py
import errno
import logging
import os
import shutil
import stat
import sys
from typing import TYPE_CHECKING

from dvc.exceptions import DvcException

if TYPE_CHECKING:
    from dvc.types import StrPath

logger = logging.getLogger(__name__)


class BasePathNotInCheckedPathException(DvcException):
    def __init__(self, path, base_path):
        msg = f"Path: {path} does not overlap with base path: {base_path}"
        super().__init__(msg)


def contains_symlink_up_to(path: "StrPath", base_path: "StrPath"):
    from dvc.fs import system

    base_path = os.path.normcase(os.fspath(base_path))
    path = os.path.normcase(os.fspath(path))

    if base_path not in path:
        raise BasePathNotInCheckedPathException(path, base_path)

    if path == base_path:
        return False
    if system.is_symlink(path):
        return True
    if os.path.dirname(path) == path:
        return False
    return contains_symlink_up_to(os.path.dirname(path), base_path)


def _chmod(func, p, excinfo):  # noqa: ARG001, pylint: disable=unused-argument
    perm = os.lstat(p).st_mode
    perm |= stat.S_IWRITE

    try:
        os.chmod(p, perm)
    except OSError as exc:
        # broken symlink or file is not owned by us
        if exc.errno not in [errno.ENOENT, errno.EPERM]:
            raise

    func(p)


def _unlink(path, onerror):
    try:
        os.unlink(path)
    except OSError:
        onerror(os.unlink, path, sys.exc_info())


def remove(path):
    logger.debug("Removing '%s'", path)

    try:
        if os.path.isdir(path):
            shutil.rmtree(path, onerror=_chmod)
        else:
            _unlink(path, _chmod)
    except OSError as exc:
        if exc.errno != errno.ENOENT:
            raise


def path_isin(child: "StrPath", parent: "StrPath") -> bool:
    """Check if given `child` path is inside `parent`."""

    def normalize_path(path) -> str:
        return os.path.normcase(os.path.normpath(path))

    parent = os.path.join(normalize_path(parent), "")
    child = normalize_path(child)
    return child != parent and child.startswith(parent)




dvc/utils/humanize.py
from funcy import is_seq


def join(words):
    words = list(words)
    if not words:
        return ""

    return (
        "{before} and {after}".format(before=", ".join(words[:-1]), after=words[-1])
        if len(words) > 1
        else words[0]
    )


def get_summary(stats):
    status = (
        (state, len(data) if is_seq(data) else data) for state, data in stats if data
    )
    return join(
        "{} file{} {}".format(num, "s" if num > 1 else "", state)
        for state, num in status
    )


ELLIPSIS = "‚Ä¶"


def truncate_text(text: str, max_length: int, with_ellipsis: bool = True) -> str:
    if with_ellipsis and len(text) > max_length:
        return text[: max_length - 1] + ELLIPSIS

    return text[:max_length]




dvc/utils/hydra.py
import logging
from pathlib import Path
from typing import TYPE_CHECKING, List

from dvc.exceptions import InvalidArgumentError

from .collections import merge_dicts, remove_missing_keys, to_omegaconf

if TYPE_CHECKING:
    from dvc.types import StrPath


logger = logging.getLogger(__name__)


def compose_and_dump(
    output_file: "StrPath",
    config_dir: str,
    config_name: str,
    overrides: List[str],
) -> None:
    """Compose Hydra config and dumpt it to `output_file`.

    Args:
        output_file: File where the composed config will be dumped.
        config_dir: Folder containing the Hydra config files.
            Must be absolute file system path.
        config_name: Name of the config file containing defaults,
            without the .yaml extension.
        overrides: List of `Hydra Override`_ patterns.

    .. _Hydra Override:
        https://hydra.cc/docs/advanced/override_grammar/basic/
    """
    from hydra import compose, initialize_config_dir
    from omegaconf import OmegaConf

    from .serialize import DUMPERS

    with initialize_config_dir(config_dir, version_base=None):
        cfg = compose(config_name=config_name, overrides=overrides)

    OmegaConf.resolve(cfg)

    suffix = Path(output_file).suffix.lower()
    if suffix not in [".yml", ".yaml"]:
        dumper = DUMPERS[suffix]
        dumper(output_file, OmegaConf.to_object(cfg))
    else:
        Path(output_file).write_text(OmegaConf.to_yaml(cfg), encoding="utf-8")
    logger.trace(  # type: ignore[attr-defined]
        "Hydra composition enabled. Contents dumped to %s:\n %s", output_file, cfg
    )


def apply_overrides(path: "StrPath", overrides: List[str]) -> None:
    """Update `path` params with the provided `Hydra Override`_ patterns.

    Args:
        overrides: List of `Hydra Override`_ patterns.

    .. _Hydra Override:
        https://hydra.cc/docs/next/advanced/override_grammar/basic/
    """
    from hydra._internal.config_loader_impl import ConfigLoaderImpl
    from hydra.errors import ConfigCompositionException, OverrideParseException
    from omegaconf import OmegaConf

    from .serialize import MODIFIERS

    suffix = Path(path).suffix.lower()

    hydra_errors = (ConfigCompositionException, OverrideParseException)

    modify_data = MODIFIERS[suffix]
    with modify_data(path) as original_data:
        try:
            parsed = to_hydra_overrides(overrides)

            new_data = OmegaConf.create(
                to_omegaconf(original_data),
                flags={"allow_objects": True},
            )
            OmegaConf.set_struct(new_data, True)
            # pylint: disable=protected-access
            ConfigLoaderImpl._apply_overrides_to_config(parsed, new_data)
            new_data = OmegaConf.to_object(new_data)
        except hydra_errors as e:
            raise InvalidArgumentError("Invalid `--set-param` value") from e

        merge_dicts(original_data, new_data)
        remove_missing_keys(original_data, new_data)


def to_hydra_overrides(path_overrides):
    from hydra.core.override_parser.overrides_parser import OverridesParser

    parser = OverridesParser.create()
    return parser.parse_overrides(overrides=path_overrides)


def dict_product(dicts):
    import itertools

    return [dict(zip(dicts, x)) for x in itertools.product(*dicts.values())]


def get_hydra_sweeps(path_overrides):
    from hydra._internal.core_plugins.basic_sweeper import BasicSweeper
    from hydra.core.override_parser.types import ValueType

    path_sweeps = {}
    for path, overrides in path_overrides.items():
        overrides = to_hydra_overrides(overrides)
        for override in overrides:
            if override.value_type == ValueType.GLOB_CHOICE_SWEEP:
                raise InvalidArgumentError(
                    f"Glob override '{override.input_line}' is not supported."
                )
        path_sweeps[path] = BasicSweeper.split_arguments(overrides, None)[0]
    return dict_product(path_sweeps)




dvc/utils/objects.py
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from functools import cached_property
else:
    from funcy import cached_property  # noqa: TID251

__all__ = ["cached_property"]




dvc/utils/pkg.py
try:
    # file is created during dvc build
    # pylint:disable=unused-import
    from .build import PKG  # type: ignore[import]
except ImportError:
    PKG = None  # type: ignore[assignment]




dvc/utils/plots.py
def get_plot_id(config_plot_id: str, config_file_path: str = ""):
    return (
        f"{config_file_path}::{config_plot_id}" if config_file_path else config_plot_id
    )




dvc/utils/strictyaml.py
"""
This module combines schema and yaml parser into one, to provide better error
messages through a single entrypoint `load`.

Used for parsing dvc.yaml, dvc.lock and .dvc files.

Not to be confused with strictyaml, a python library with similar motivations.
"""
import re
import typing
from contextlib import suppress
from typing import TYPE_CHECKING, Any, Callable, List, Optional, Tuple, TypeVar

from dvc.exceptions import PrettyDvcException
from dvc.ui import ui
from dvc.utils.serialize import (
    EncodingError,
    YAMLFileCorruptedError,
    parse_yaml,
    parse_yaml_for_update,
)

if TYPE_CHECKING:
    from rich.syntax import Syntax
    from ruamel.yaml import StreamMark
    from voluptuous import MultipleInvalid

    from dvc.fs import FileSystem
    from dvc.ui import RichText


_T = TypeVar("_T")
merge_conflict_marker = re.compile("^([<=>]{7}) .*$", re.MULTILINE)


def make_relpath(path: str) -> str:
    import os

    from dvc.utils import relpath

    rel = relpath(path)
    prefix = ""
    if not rel.startswith(".."):
        prefix = "./" if os.name == "posix" else ".\\"
    return prefix + rel


def _prepare_message(message: str) -> "RichText":
    return ui.rich_text(message, style="red")


def _prepare_cause(cause: str) -> "RichText":
    return ui.rich_text(cause, style="bold")


def _prepare_code_snippets(code: str, start_line: int = 1, **kwargs: Any) -> "Syntax":
    from rich.syntax import Syntax

    kwargs.setdefault("start_line", start_line)
    return Syntax(
        code,
        "yaml",
        theme="ansi_dark",
        word_wrap=True,
        line_numbers=True,
        indent_guides=True,
        **kwargs,
    )


class YAMLSyntaxError(PrettyDvcException, YAMLFileCorruptedError):
    def __init__(
        self,
        path: str,
        yaml_text: str,
        exc: Exception,
        rev: Optional[str] = None,
    ) -> None:
        self.path: str = path
        self.yaml_text: str = yaml_text
        self.exc: Exception = exc

        merge_conflicts = merge_conflict_marker.search(self.yaml_text)
        self.hint = " (possible merge conflicts)" if merge_conflicts else ""
        self.rev: Optional[str] = rev
        super().__init__(self.path)

    def __pretty_exc__(self, **kwargs: Any) -> None:  # noqa: C901
        from ruamel.yaml.error import MarkedYAMLError

        exc = self.exc.__cause__

        if not isinstance(exc, MarkedYAMLError):
            raise ValueError("nothing to pretty-print here.")  # noqa: TRY004

        source = self.yaml_text.splitlines()

        def prepare_linecol(mark: "StreamMark") -> str:
            return f"in line {mark.line + 1}, column {mark.column + 1}"

        def prepare_message(
            message: str, mark: Optional["StreamMark"] = None
        ) -> "RichText":
            cause = ", ".join(
                [message.capitalize(), prepare_linecol(mark) if mark else ""]
            )
            return _prepare_cause(cause)

        def prepare_code(mark: "StreamMark") -> "Syntax":
            line = mark.line + 1
            code = "" if line > len(source) else source[line - 1]
            return _prepare_code_snippets(code, line)

        lines: List[object] = []
        if hasattr(exc, "context"):
            if exc.context_mark is not None:
                lines.append(prepare_message(str(exc.context), exc.context_mark))
            if exc.context_mark is not None and (
                exc.problem is None
                or exc.problem_mark is None
                or exc.context_mark.name != exc.problem_mark.name
                or exc.context_mark.line != exc.problem_mark.line
                or exc.context_mark.column != exc.problem_mark.column
            ):
                lines.extend([prepare_code(exc.context_mark), ""])
            if exc.problem is not None:
                lines.append(prepare_message(str(exc.problem), exc.problem_mark))
            if exc.problem_mark is not None:
                lines.append(prepare_code(exc.problem_mark))

        if lines:
            # we should not add a newline after the main message
            # if there are no other outputs
            lines.insert(0, "")

        rel = make_relpath(self.path)
        rev_msg = f" in revision '{self.rev[:7]}'" if self.rev else ""
        msg_fmt = f"'{rel}' is invalid{self.hint}{rev_msg}."
        lines.insert(0, _prepare_message(msg_fmt))
        for line in lines:
            ui.error_write(line, styled=True)


def determine_linecol(
    data, paths, max_steps=5
) -> Tuple[Optional[int], Optional[int], int]:
    """Determine linecol from the CommentedMap for the `paths` location.

    CommentedMap from `ruamel.yaml` has `.lc` property from which we can read
    `.line` and `.col`. This is available in the collections type,
    i.e. list and dictionaries.

    But this may fail on non-collection types. For example, if the `paths` is
    ['stages', 'metrics'], metrics being a boolean type does not have `lc`
    prop.
    ```
    stages:
      metrics: true
    ```

    To provide some context to the user, we step up to the
    path ['stages'], which being a collection type, will have `lc` prop
    with which we can find line and col.

    This may end up being not accurate, so we try to show the same amount of
    lines of code for `n` number of steps taken upwards. In a worst case,
    it may be just 1 step (as non-collection item cannot have child items),
    but `schema validator` may provide us arbitrary path. So, this caps the
    number of steps upward to just 5. If it does not find any linecols, it'll
    abort.
    """
    from dpath import get

    step = 1
    line, col = None, None
    while paths and step < max_steps:
        value = get(data, paths, default=None)
        if value is not None:
            with suppress(AttributeError, TypeError):
                line = value.lc.line + 1  # type: ignore[attr-defined]
                col = value.lc.col + 1  # type: ignore[attr-defined]
                break
        step += 1
        *paths, _ = paths

    return line, col, step


class YAMLValidationError(PrettyDvcException):
    def __init__(
        self,
        exc: "MultipleInvalid",
        path: Optional[str] = None,
        text: Optional[str] = None,
        rev: Optional[str] = None,
    ) -> None:
        self.text = text or ""
        self.exc = exc

        rel = make_relpath(path) if path else ""
        self.path = path or ""

        message = f"'{rel}' validation failed"
        message += f" in revision '{rev[:7]}'" if rev else ""
        if len(self.exc.errors) > 1:
            message += f": {len(self.exc.errors)} errors"
        super().__init__(f"{message}")

    def _prepare_context(self, data: typing.Mapping) -> List[object]:
        lines: List[object] = []
        for index, error in enumerate(self.exc.errors):
            if index and lines[-1]:
                lines.append("")
            line, col, step = determine_linecol(data, error.path)
            parts = [error.error_message]
            if error.path:
                parts.append("in " + " -> ".join(str(p) for p in error.path))
            if line:
                parts.append(f"line {line}")
            if col:
                parts.append(f"column {col}")
            lines.append(_prepare_cause(", ".join(parts)))

            if line:
                # we show one line above the error
                # we try to show few more lines if we could not
                # reliably figure out where the error was
                lr = (line - 1, line + step - 1)
                code = _prepare_code_snippets(self.text, line_range=lr)
                lines.append(code)
        return lines

    def __pretty_exc__(self, **kwargs: Any) -> None:
        """Prettify exception message."""
        from collections.abc import Mapping

        lines: List[object] = []
        data = parse_yaml_for_update(self.text, self.path)
        if isinstance(data, Mapping):
            lines.extend(self._prepare_context(data))

        cause = ""
        if lines:
            # we should not add a newline after the main message
            # if there are no other outputs
            lines.insert(0, "")
        else:
            # if we don't have any context to show, we'll fallback to what we
            # got from voluptuous and print them in the same line.
            cause = f": {self.exc}"

        lines.insert(0, _prepare_message(f"{self}{cause}."))
        for line in lines:
            ui.error_write(line, styled=True)


def validate(
    data: _T,
    schema: Callable[[_T], _T],
    text: Optional[str] = None,
    path: Optional[str] = None,
    rev: Optional[str] = None,
) -> _T:
    from voluptuous import MultipleInvalid

    try:
        return schema(data)
    except MultipleInvalid as exc:
        raise YAMLValidationError(exc, path, text, rev=rev) from exc


def load(
    path: str,
    schema: Optional[Callable[[_T], _T]] = None,
    fs: Optional["FileSystem"] = None,
    encoding: str = "utf-8",
    round_trip: bool = False,
) -> Any:
    open_fn = fs.open if fs else open
    rev = getattr(fs, "rev", None)

    try:
        with open_fn(path, encoding=encoding) as fd:  # type: ignore[operator]
            text = fd.read()
        data = parse_yaml(text, path, typ="rt" if round_trip else "safe")
    except UnicodeDecodeError as exc:
        raise EncodingError(path, encoding) from exc
    except YAMLFileCorruptedError as exc:
        cause = exc.__cause__
        raise YAMLSyntaxError(path, text, exc, rev=rev) from cause

    if schema:
        # not returning validated data, as it may remove
        # details from CommentedMap that we get from roundtrip parser
        validate(data, schema, text=text, path=path, rev=rev)
    return data, text




dvc/utils/studio.py
import logging
from typing import TYPE_CHECKING, Any, Dict, List, Optional, cast
from urllib.parse import urljoin

import requests
from funcy import compact, ignore
from requests.adapters import HTTPAdapter

from dvc.env import (
    DVC_STUDIO_OFFLINE,
    DVC_STUDIO_REPO_URL,
    DVC_STUDIO_TOKEN,
    DVC_STUDIO_URL,
)

if TYPE_CHECKING:
    from requests import Response

logger = logging.getLogger(__name__)

STUDIO_URL = "https://studio.iterative.ai"


def post(
    url: str,
    token: str,
    data: Dict[str, Any],
    base_url: Optional[str] = STUDIO_URL,
    max_retries: int = 3,
    timeout: int = 5,
) -> "Response":
    url = urljoin(base_url or STUDIO_URL, url)
    session = requests.Session()
    session.mount(url, HTTPAdapter(max_retries=max_retries))

    logger.trace("Sending %s to %s", data, url)  # type: ignore[attr-defined]

    headers = {"Authorization": f"token {token}"}
    r = session.post(
        url, json=data, headers=headers, timeout=timeout, allow_redirects=False
    )
    r.raise_for_status()
    return r


def notify_refs(
    repo_url: str,
    token: str,
    *,
    base_url: Optional[str] = STUDIO_URL,
    **refs: List[str],
) -> Dict[str, Any]:
    extra_keys = refs.keys() - {"pushed", "removed"}
    assert not extra_keys, f"got extra args: {extra_keys}"

    refs = compact(refs)
    if not refs:
        return {}

    logger.debug(
        "notifying Studio%s about updated experiments",
        f" ({base_url})" if base_url else "",
    )
    data = {"repo_url": repo_url, "client": "dvc", "refs": refs}

    try:
        r = post("webhook/dvc", token, data, base_url=base_url)
    except requests.RequestException as e:
        logger.trace("", exc_info=True)  # type: ignore[attr-defined]

        msg = str(e)
        if e.response is None:
            logger.warning("failed to notify Studio: %s", msg.lower())
            return {}

        r = cast("Response", e.response)
        d = ignore(Exception, default={})(r.json)()
        status = r.status_code
        if detail := d.get("detail"):
            msg = f"{detail} ({status=})"
        logger.warning("failed to notify Studio: %s", msg.lower())
    else:
        d = r.json()

    if d:
        logger.trace(  # type: ignore[attr-defined]
            "received response: %s (status=%r)", d, r.status_code
        )
    return d


def config_to_env(config: Dict[str, Any]) -> Dict[str, Any]:
    env = {}
    if "offline" in config:
        env[DVC_STUDIO_OFFLINE] = config["offline"]
    if "repo_url" in config:
        env[DVC_STUDIO_REPO_URL] = config["repo_url"]
    if "token" in config:
        env[DVC_STUDIO_TOKEN] = config["token"]
    if "url" in config:
        env[DVC_STUDIO_URL] = config["url"]
    return env


def env_to_config(env: Dict[str, Any]) -> Dict[str, Any]:
    config = {}
    if DVC_STUDIO_OFFLINE in env:
        config["offline"] = env[DVC_STUDIO_OFFLINE]
    if DVC_STUDIO_REPO_URL in env:
        config["repo_url"] = env[DVC_STUDIO_REPO_URL]
    if DVC_STUDIO_TOKEN in env:
        config["token"] = env[DVC_STUDIO_TOKEN]
    if DVC_STUDIO_URL in env:
        config["url"] = env[DVC_STUDIO_URL]
    return config




dvc/utils/table.py
from typing import TYPE_CHECKING, Any, List

from rich.table import Table as RichTable

if TYPE_CHECKING:
    from rich.console import Console, ConsoleOptions


class Table(RichTable):
    def add_column(  # pylint: disable=arguments-differ
        self, *args: Any, collapse: bool = False, **kwargs: Any
    ) -> None:
        super().add_column(*args, **kwargs)
        self.columns[-1].collapse = collapse  # type: ignore[attr-defined]

    def _calculate_column_widths(
        self, console: "Console", options: "ConsoleOptions"
    ) -> List[int]:
        """Calculate the widths of each column, including padding, not
        including borders.

        Adjacent collapsed columns will be removed until there is only a single
        truncated column remaining.
        """
        widths = super()._calculate_column_widths(console, options)
        last_collapsed = -1
        columns = self.columns
        for i in range(len(columns) - 1, -1, -1):
            if widths[i] == 0 and columns[i].collapse:  # type: ignore[attr-defined]
                if last_collapsed >= 0:
                    del widths[last_collapsed]
                    del columns[last_collapsed]
                    if self.box:
                        options.max_width += 1
                    for column in columns[last_collapsed:]:
                        column._index -= 1
                last_collapsed = i
                padding = self._get_padding_width(i)
                if (
                    columns[i].overflow == "ellipsis"
                    and (sum(widths) + padding) <= options.max_width
                ):
                    # Set content width to 1 (plus padding) if we can fit a
                    # single unicode ellipsis in this column
                    widths[i] = 1 + padding
            else:
                last_collapsed = -1
        return widths

    def _collapse_widths(  # type: ignore[override]
        # pylint: disable=arguments-differ
        self,
        widths: List[int],
        wrapable: List[bool],
        max_width: int,
    ) -> List[int]:
        """Collapse columns right-to-left if possible to fit table into
        max_width.

        If table is still too wide after collapsing, rich's automatic overflow
        handling will be used.
        """
        collapsible = [
            column.collapse for column in self.columns  # type: ignore[attr-defined]
        ]
        total_width = sum(widths)
        excess_width = total_width - max_width
        if any(collapsible):
            for i in range(len(widths) - 1, -1, -1):
                if collapsible[i]:
                    excess_width -= widths[i]
                    widths[i] = 0
                    if excess_width <= 0:
                        break
        return super()._collapse_widths(widths, wrapable, max_width)




dvc/utils/threadpool.py
import queue
import sys
from concurrent import futures
from itertools import islice
from typing import Any, Callable, Iterable, Iterator, Optional, Set, TypeVar

_T = TypeVar("_T")


class ThreadPoolExecutor(futures.ThreadPoolExecutor):
    _max_workers: int

    def __init__(
        self,
        max_workers: Optional[int] = None,
        cancel_on_error: bool = False,
        **kwargs,
    ):
        super().__init__(max_workers=max_workers, **kwargs)
        self._cancel_on_error = cancel_on_error

    @property
    def max_workers(self) -> int:
        return self._max_workers

    def imap_unordered(
        self, fn: Callable[..., _T], *iterables: Iterable[Any]
    ) -> Iterator[_T]:
        """Lazier version of map that does not preserve ordering of results.

        It does not create all the futures at once to reduce memory usage.
        """

        def create_taskset(n: int) -> Set[futures.Future]:
            return {self.submit(fn, *args) for args in islice(it, n)}

        it = zip(*iterables)
        tasks = create_taskset(self.max_workers * 5)
        while tasks:
            done, tasks = futures.wait(tasks, return_when=futures.FIRST_COMPLETED)
            for fut in done:
                yield fut.result()
            tasks.update(create_taskset(len(done)))

    def shutdown(self, wait=True, *, cancel_futures=False):
        if sys.version_info > (3, 9):  # pylint: disable=no-else-return
            # pylint: disable=unexpected-keyword-arg
            return super().shutdown(wait=wait, cancel_futures=cancel_futures)
        else:
            with self._shutdown_lock:
                self._shutdown = True
                if cancel_futures:
                    # Drain all work items from the queue, and then cancel
                    # their associated futures.
                    while True:
                        try:
                            work_item = self._work_queue.get_nowait()
                        except queue.Empty:
                            break
                        if work_item is not None:
                            work_item.future.cancel()

                # Send a wake-up to prevent threads calling
                # _work_queue.get(block=True) from permanently blocking.
                self._work_queue.put(None)  # type: ignore[arg-type]
            if wait:
                for t in self._threads:
                    t.join()

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self._cancel_on_error:
            self.shutdown(wait=True, cancel_futures=exc_val is not None)
        else:
            self.shutdown(wait=True)
        return False




dvc/utils/serialize/__init__.py
from collections import defaultdict
from typing import DefaultDict

from ._common import *  # noqa, pylint: disable=wildcard-import
from ._json import *  # noqa, pylint: disable=wildcard-import
from ._py import *  # noqa, pylint: disable=wildcard-import
from ._toml import *  # noqa, pylint: disable=wildcard-import
from ._yaml import *  # noqa, pylint: disable=wildcard-import

LOADERS: DefaultDict[str, LoaderFn] = defaultdict(  # noqa: F405
    lambda: load_yaml  # noqa: F405
)
LOADERS.update({".toml": load_toml, ".json": load_json, ".py": load_py})  # noqa: F405

PARSERS: DefaultDict[str, ParserFn] = defaultdict(  # noqa: F405
    lambda: parse_yaml  # noqa: F405
)
PARSERS.update(
    {".toml": parse_toml, ".json": parse_json, ".py": parse_py}  # noqa: F405
)


def load_path(fs_path, fs):
    suffix = fs.path.suffix(fs_path).lower()
    loader = LOADERS[suffix]
    return loader(fs_path, fs=fs)


DUMPERS: DefaultDict[str, DumperFn] = defaultdict(  # noqa: F405
    lambda: dump_yaml  # noqa: F405
)
DUMPERS.update({".toml": dump_toml, ".json": dump_json, ".py": dump_py})  # noqa: F405

MODIFIERS: DefaultDict[str, ModifierFn] = defaultdict(  # noqa: F405
    lambda: modify_yaml  # noqa: F405
)
MODIFIERS.update(
    {
        ".toml": modify_toml,  # noqa: F405
        ".json": modify_json,  # noqa: F405
        ".py": modify_py,  # noqa: F405
    }
)




dvc/utils/serialize/_common.py
"""Common utilities for serialize."""
import os
from contextlib import contextmanager
from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    ContextManager,
    Dict,
    Optional,
    Protocol,
    TextIO,
    Union,
)

from funcy import reraise

from dvc.exceptions import DvcException

if TYPE_CHECKING:
    from dvc.fs import FileSystem
    from dvc.types import StrPath


class DumperFn(Protocol):
    def __call__(
        self, path: "StrPath", data: Any, fs: Optional["FileSystem"] = None
    ) -> Any:
        ...


class DumpersFn(Protocol):
    def __call__(self, data: Any, stream: TextIO) -> Any:
        ...


class ModifierFn(Protocol):
    def __call__(
        self, path: "StrPath", fs: Optional["FileSystem"] = None
    ) -> ContextManager[Dict]:
        ...


class LoaderFn(Protocol):
    def __call__(self, path: "StrPath", fs: Optional["FileSystem"] = None) -> Any:
        ...


ReadType = Union[bytes, None, str]
ParserFn = Callable[[ReadType, "StrPath"], dict]


class ParseError(DvcException):
    """Errors while parsing files"""

    def __init__(self, path: "StrPath", message: str):
        from dvc.utils import relpath

        path = relpath(path)
        self.path = path
        super().__init__(f"unable to read: '{path}', {message}")


class EncodingError(ParseError):
    """We could not read a file with the given encoding"""

    def __init__(self, path: "StrPath", encoding: str):
        self.encoding = encoding
        super().__init__(path, f"is not valid {encoding}")


def _load_data(path: "StrPath", parser: ParserFn, fs: Optional["FileSystem"] = None):
    open_fn = fs.open if fs else open
    encoding = "utf-8"
    with open_fn(path, encoding=encoding) as fd:  # type: ignore[operator]
        with reraise(UnicodeDecodeError, EncodingError(path, encoding)):
            return parser(fd.read(), path)


def _dump_data(
    path,
    data: Any,
    dumper: DumpersFn,
    fs: Optional["FileSystem"] = None,
    **dumper_args,
):
    open_fn = fs.open if fs else open
    with open_fn(path, "w+", encoding="utf-8") as fd:  # type: ignore[operator]
        dumper(data, fd, **dumper_args)


@contextmanager
def _modify_data(
    path: "StrPath",
    parser: ParserFn,
    dumper: DumpersFn,
    fs: Optional["FileSystem"] = None,
):
    file_exists = fs.exists(os.fspath(path)) if fs else os.path.exists(path)
    data = _load_data(path, parser=parser, fs=fs) if file_exists else {}
    yield data
    _dump_data(path, data, dumper=dumper, fs=fs)




dvc/utils/serialize/_json.py
import json

from funcy import contextmanager, reraise

from ._common import ParseError, _dump_data, _load_data, _modify_data


class JSONFileCorruptedError(ParseError):
    def __init__(self, path):
        super().__init__(path, "JSON file structure is corrupted")


def load_json(path, fs=None):
    return _load_data(path, parser=parse_json, fs=fs)


def parse_json(text, path, **kwargs):
    with reraise(json.JSONDecodeError, JSONFileCorruptedError(path)):
        return json.loads(text, **kwargs) or {}


def _dump_json(data, stream, **kwargs):
    return json.dump(data, stream, **kwargs)


def dump_json(path, data, fs=None, **kwargs):
    return _dump_data(path, data, dumper=_dump_json, fs=fs, **kwargs)


@contextmanager
def modify_json(path, fs=None):
    with _modify_data(path, parse_json, _dump_json, fs=fs) as d:
        yield d


def encode_exception(o):
    if isinstance(o, Exception):
        return {"type": type(o).__name__, "msg": str(o)}
    raise TypeError




dvc/utils/serialize/_py.py
import ast
from contextlib import contextmanager
from typing import Any

from funcy import reraise

from ._common import ParseError, _dump_data, _load_data, _modify_data

_PARAMS_KEY = "__params_old_key_for_update__"
_PARAMS_TEXT_KEY = "__params_text_key_for_update__"


class PythonFileCorruptedError(ParseError):
    def __init__(self, path, message="Python file structure is corrupted"):
        super().__init__(path, message)


def load_py(path, fs=None):
    return _load_data(path, parser=parse_py, fs=fs)


def parse_py(text, path):
    """Parses text from .py file into Python structure."""
    with reraise(SyntaxError, PythonFileCorruptedError(path)):
        tree = ast.parse(text, filename=path)

    return _ast_tree_to_dict(tree)


def parse_py_for_update(text, path):
    """Parses text into dict for update params."""
    with reraise(SyntaxError, PythonFileCorruptedError(path)):
        tree = ast.parse(text, filename=path)

    result = _ast_tree_to_dict(tree)
    result.update({_PARAMS_KEY: _ast_tree_to_dict(tree, lineno=True)})
    result.update({_PARAMS_TEXT_KEY: text})
    return result


def _dump(data, stream):
    old_params = data[_PARAMS_KEY]
    new_params = {
        key: value
        for key, value in data.items()
        if key not in [_PARAMS_KEY, _PARAMS_TEXT_KEY]
    }
    old_lines = data[_PARAMS_TEXT_KEY].splitlines(True)

    def _update_lines(lines, old_dct, new_dct):
        for key, value in new_dct.items():
            if isinstance(value, dict):
                lines = _update_lines(lines, old_dct[key], value)
            elif value != old_dct[key]["value"]:
                lineno = old_dct[key]["lineno"]
                lines[lineno] = lines[lineno].replace(
                    f" = {old_dct[key]['value']}", f" = {value}"
                )
            else:
                continue
        return lines

    new_lines = _update_lines(old_lines, old_params, new_params)
    new_text = "".join(new_lines)

    try:
        ast.parse(new_text)
    except SyntaxError:
        raise PythonFileCorruptedError(  # noqa: B904
            stream.name,
            "Python file structure is corrupted after update params",
        )

    stream.write(new_text)
    stream.close()


def dump_py(path, data, fs=None):
    return _dump_data(path, data, dumper=_dump, fs=fs)


@contextmanager
def modify_py(path, fs=None):
    with _modify_data(path, parse_py_for_update, _dump, fs=fs) as d:
        yield d


def _ast_tree_to_dict(tree, only_self_params=False, lineno=False):
    """Parses ast trees to dict.

    :param tree: ast.Tree
    :param only_self_params: get only self params from class __init__ function
    :param lineno: add params line number (needed for update)
    :return:
    """
    result = {}
    for _body in tree.body:
        try:
            if isinstance(_body, (ast.Assign, ast.AnnAssign)):
                result.update(_ast_assign_to_dict(_body, only_self_params, lineno))
            elif isinstance(_body, ast.ClassDef):
                result.update({_body.name: _ast_tree_to_dict(_body, lineno=lineno)})
            elif isinstance(_body, ast.FunctionDef) and _body.name == "__init__":
                result.update(
                    _ast_tree_to_dict(_body, only_self_params=True, lineno=lineno)
                )
        except ValueError:
            continue
        except AttributeError:
            continue
    return result


def _ast_assign_to_dict(assign, only_self_params=False, lineno=False):  # noqa: PLR0912
    result = {}

    if isinstance(assign, ast.AnnAssign):
        name = _get_ast_name(assign.target, only_self_params)
    elif len(assign.targets) == 1:
        name = _get_ast_name(assign.targets[0], only_self_params)
    else:
        raise AttributeError

    value: Any
    if isinstance(assign.value, ast.Dict):
        value = {}
        for key, val in zip(assign.value.keys, assign.value.values):
            if lineno:
                value[ast.literal_eval(key)] = {  # type: ignore[arg-type]
                    "lineno": assign.lineno - 1,
                    "value": ast.literal_eval(val),
                }
            else:
                v = ast.literal_eval(val)
                value[ast.literal_eval(key)] = v  # type: ignore[arg-type]
    elif isinstance(assign.value, ast.List):
        value = [ast.literal_eval(val) for val in assign.value.elts]
    elif isinstance(assign.value, ast.Set):
        values = [ast.literal_eval(val) for val in assign.value.elts]
        value = set(values)
    elif isinstance(assign.value, ast.Tuple):
        values = [ast.literal_eval(val) for val in assign.value.elts]
        value = tuple(values)
    else:
        value = ast.literal_eval(assign.value)

    if lineno and not isinstance(assign.value, ast.Dict):
        result[name] = {"lineno": assign.lineno - 1, "value": value}
    else:
        result[name] = value

    return result


def _get_ast_name(target, only_self_params=False):
    if hasattr(target, "id") and not only_self_params:
        return target.id
    if hasattr(target, "attr") and target.value.id == "self":
        return target.attr
    raise AttributeError




dvc/utils/serialize/_toml.py
from contextlib import contextmanager

from funcy import reraise

from ._common import ParseError, _dump_data, _load_data, _modify_data


class TOMLFileCorruptedError(ParseError):
    def __init__(self, path):
        super().__init__(path, "TOML file structure is corrupted")


def load_toml(path, fs=None):
    return _load_data(path, parser=parse_toml, fs=fs)


def _parse_toml(text, path):
    from tomlkit import loads
    from tomlkit.exceptions import ParseError as TomlkitParseError

    with reraise(TomlkitParseError, TOMLFileCorruptedError(path)):
        return loads(text)


def parse_toml(text, path, preserve_comments=False):
    rval = _parse_toml(text, path)

    if preserve_comments:
        return rval

    return rval.unwrap()


def parse_toml_for_update(text, path):
    return parse_toml(text, path, preserve_comments=True)


def _dump(data, stream, sort_keys=False):
    import tomlkit

    return tomlkit.dump(data, stream, sort_keys=sort_keys)


def dump_toml(path, data, fs=None, **kwargs):
    return _dump_data(path, data, dumper=_dump, fs=fs, **kwargs)


@contextmanager
def modify_toml(path, fs=None):
    with _modify_data(path, parse_toml_for_update, _dump, fs=fs) as d:
        yield d




dvc/utils/serialize/_yaml.py
import io
from collections import OrderedDict
from contextlib import contextmanager
from typing import Any, TextIO

from funcy import reraise

from ._common import ParseError, _dump_data, _load_data, _modify_data


class YAMLError(ParseError):
    pass


class YAMLFileCorruptedError(YAMLError):
    def __init__(self, path):
        super().__init__(path, "YAML file structure is corrupted")


def load_yaml(path, fs=None):
    return _load_data(path, parser=parse_yaml, fs=fs)


def parse_yaml(text, path, typ="safe"):
    from ruamel.yaml import YAML
    from ruamel.yaml import YAMLError as _YAMLError

    yaml = YAML(typ=typ)
    with reraise(_YAMLError, YAMLFileCorruptedError(path)):
        return yaml.load(text) or {}


def parse_yaml_for_update(text, path):
    """Parses text into Python structure.

    Unlike `parse_yaml()` this returns ordered dicts, values have special
    attributes to store comments and line breaks. This allows us to preserve
    all of those upon dump.

    This one is, however, several times slower than simple `parse_yaml()`.
    """
    return parse_yaml(text, path, typ="rt")


def _get_yaml():
    from ruamel.yaml import YAML

    yaml = YAML()
    yaml.default_flow_style = False

    # tell Dumper to represent OrderedDict as normal dict
    yaml_repr_cls = yaml.Representer
    yaml_repr_cls.add_representer(OrderedDict, yaml_repr_cls.represent_dict)
    return yaml


def _dump(data: Any, stream: TextIO) -> Any:
    yaml = _get_yaml()
    return yaml.dump(data, stream)


def dump_yaml(path, data, fs=None, **kwargs):
    return _dump_data(path, data, dumper=_dump, fs=fs, **kwargs)


def loads_yaml(s, typ="safe"):
    from ruamel.yaml import YAML

    return YAML(typ=typ).load(s)


def dumps_yaml(d):
    stream = io.StringIO()
    _dump(d, stream)
    return stream.getvalue()


@contextmanager
def modify_yaml(path, fs=None):
    with _modify_data(path, parse_yaml_for_update, _dump, fs=fs) as d:
        yield d




scripts/build.py
import argparse
import pathlib
import sys
from subprocess import STDOUT, check_call

path = pathlib.Path(__file__).parent.absolute()
dvc = path.parent / "dvc"
pyinstaller = path / "pyinstaller"
innosetup = path / "innosetup"
fpm = path / "fpm"

parser = argparse.ArgumentParser()
parser.add_argument("pkg", choices=["deb", "rpm", "osxpkg", "exe"], help="package type")
parser.add_argument("--sign-application", default=False, action="store_true")
parser.add_argument("--application-id")
parser.add_argument("--sign-installer", default=False, action="store_true")
parser.add_argument("--installer-id")
parser.add_argument("--notarize", default=False, action="store_true")
parser.add_argument("--apple-id-username")
parser.add_argument("--apple-id-password")
args = parser.parse_args()

(dvc / "utils" / "build.py").write_text(f'PKG = "{args.pkg}"')

if not (dvc / "_dvc_version.py").exists():
    raise Exception("no version info found")

check_call(
    ["python", "build.py"],
    cwd=pyinstaller,
    stderr=STDOUT,
)

if args.sign_application:
    if args.pkg != "osxpkg":
        raise NotImplementedError
    if not args.application_id:
        print("--sign-application requires --application-id")
        sys.exit(1)
    check_call(
        ["python", "sign.py", "--application-id", args.application_id],
        cwd=pyinstaller,
        stderr=STDOUT,
    )

if args.pkg == "exe":
    check_call(
        ["python", "build.py"],
        cwd=innosetup,
        stderr=STDOUT,
    )
else:
    check_call(
        ["python", "build.py", args.pkg],
        cwd=fpm,
        stderr=STDOUT,
    )

if args.sign_installer:
    if args.pkg != "osxpkg":
        raise NotImplementedError
    if not all([args.installer_id, args.apple_id_username, args.apple_id_password]):
        print("--sign-installer requires --installer-id")
        sys.exit(1)
    check_call(
        ["python", "sign.py", "--installer-id", args.installer_id],
        cwd=fpm,
        stderr=STDOUT,
    )

if args.notarize:
    if args.pkg != "osxpkg":
        raise NotImplementedError
    if not all([args.apple_id_username, args.apple_id_password]):
        print("--notarize requires --apple-id-username and --apple-id-password")
        sys.exit(1)
    check_call(
        [
            "python",
            "notarize.py",
            "--apple-id-username",
            args.apple_id_username,
            "--apple-id-password",
            args.apple_id_password,
        ],
        cwd=fpm,
        stderr=STDOUT,
    )




scripts/build-requirements.txt
# NOTE: using strict version due to
# https://github.com/iterative/dvc/issues/7949
PyInstaller==5.9.0




scripts/build_package.sh
#!/bin/bash

set -e
set -x

if [ ! -d "dvc" ]; then
  echo "Please run this script from repository root" >&2
  exit 1
fi

echo 'PKG = "pip"' >dvc/utils/build.py

python -m pip install build twine
python -m build
twine check dist/*




scripts/fpm/.gitignore
/build
/*.deb
/*.rpm
/*.pkg




scripts/fpm/after-install.sh
#!/bin/sh
ln -sf /usr/local/lib/dvc/dvc /usr/local/bin/dvc




scripts/fpm/after-remove.sh
#!/bin/sh
rm -rf /usr/local/bin/dvc




scripts/fpm/build.py
import argparse
import os
import pathlib
import shutil
from subprocess import STDOUT, check_call, check_output

path = pathlib.Path(__file__).parent.absolute()
dvc = path.parent.parent / "dvc"
pyinstaller = path.parent / "pyinstaller"

build = path / "build"
install = build / "usr"

parser = argparse.ArgumentParser()
parser.add_argument("pkg", choices=["deb", "rpm", "osxpkg"], help="package type")
args = parser.parse_args()

flags = [
    "--description",
    '"Data Version Control | Git for Data & Models"',
    "-n",
    "dvc",
    "-s",
    "dir",
    "-f",
    "--license",
    '"Apache License 2.0"',
]

if args.pkg == "osxpkg":
    install /= "local"
    bash_dir = install / "etc" / "bash_completion.d"
    dirs = ["usr"]
    flags.extend(
        [
            "--osxpkg-identifier-prefix",
            "com.iterative",
            "--after-install",
            path / "after-install.sh",
            "--after-remove",
            path / "after-remove.sh",
        ]
    )
else:
    if args.pkg == "rpm":
        # https://github.com/jordansissel/fpm/issues/1503
        flags.extend(["--rpm-rpmbuild-define", "_build_id_links none"])

    bash_dir = build / "etc" / "bash_completion.d"
    dirs = ["usr", "etc"]
    flags.extend(["--depends", "git >= 1.7.0"])  # needed for gitpython

try:
    shutil.rmtree(build)
except FileNotFoundError:
    pass

lib = install / "lib"
lib.mkdir(parents=True)
shutil.copytree(pyinstaller / "dist" / "dvc", lib / "dvc")

if args.pkg != "osxpkg":
    # NOTE: in osxpkg fpm replaces symlinks with actual file that it
    # points to, so we need to use after-install hook.
    (install / "bin").mkdir()
    os.symlink("../lib/dvc/dvc", install / "bin" / "dvc")

bash_dir.mkdir(parents=True)
bash_completion = check_output(
    [lib / "dvc" / "dvc", "completion", "-s", "bash"], text=True
)
(bash_dir / "dvc").write_text(bash_completion)

zsh_dir = install / "share" / "zsh" / "site-functions"
zsh_dir.mkdir(parents=True)
zsh_completion = check_output(
    [lib / "dvc" / "dvc", "completion", "-s", "zsh"], text=True
)
(zsh_dir / "_dvc").write_text(zsh_completion)

version = check_output([lib / "dvc" / "dvc", "--version"], text=True).strip()

check_call(
    [
        "fpm",
        "--verbose",
        "-t",
        args.pkg,
        *flags,
        "-v",
        version,
        "-C",
        build,
        *dirs,
    ],
    cwd=path,
    stderr=STDOUT,
)




scripts/fpm/notarize.py
import argparse
import json
import os
import pathlib
import sys
from subprocess import STDOUT, check_call

if sys.platform != "darwin":
    raise NotImplementedError

parser = argparse.ArgumentParser()
parser.add_argument(
    "path",
    nargs="?",
    help="Path to the osxpkg to notarize. If not specified - try to find one.",
)
parser.add_argument("--apple-id-username", required=True, help="Apple ID username.")
parser.add_argument(
    "--apple-id-password",
    required=True,
    help=(
        "Apple ID app-specific password. Note that this is not a regular "
        "Apple ID password, so you need to generate one at "
        "https://appleid.apple.com/account/manage"
    ),
)
args = parser.parse_args()

path = pathlib.Path(__file__).parent.absolute()

if args.path:
    pkg = pathlib.Path(args.path)
else:
    pkgs = list(path.glob("*.pkg"))
    if not pkgs:
        print("No pkgs found")
        sys.exit(1)

    if len(pkgs) > 1:
        print("Too many packages")
        sys.exit(1)

    (pkg,) = pkgs


config = {
    "notarize": {
        "path": os.fspath(pkg),
        "bundle_id": "com.iterative.dvc",
        "staple": True,
    },
    "apple_id": {
        "username": args.apple_id_username,
        "password": args.apple_id_password,
    },
}

(path / "config.json").write_text(json.dumps(config))

check_call(
    ["gon", "config.json"],
    cwd=path,
    stderr=STDOUT,
)




scripts/fpm/sign.py
import argparse
import os
import pathlib
import sys
from subprocess import STDOUT, check_call

if sys.platform != "darwin":
    raise NotImplementedError

parser = argparse.ArgumentParser()
parser.add_argument(
    "path",
    nargs="?",
    help="Path to the osxpkg to sign. If not specified - try to find one.",
)
parser.add_argument(
    "--installer-id",
    required=True,
    help="Certificate ID (should be added to the keychain).",
)
args = parser.parse_args()

path = pathlib.Path(__file__).parent.absolute()

if args.path:
    pkg = pathlib.Path(args.path)
else:
    pkgs = list(path.glob("*.pkg"))
    if not pkgs:
        print("No pkgs found")
        sys.exit(1)

    if len(pkgs) > 1:
        print("Too many packages")
        sys.exit(1)

    (pkg,) = pkgs

unsigned = pkg.with_suffix(".unsigned")
os.rename(pkg, unsigned)
check_call(
    [
        "productsign",
        "--sign",
        args.installer_id,
        os.fspath(unsigned),
        os.fspath(pkg),
    ],
    stderr=STDOUT,
)

check_call(
    ["pkgutil", "--check-signature", os.fspath(pkg)],
    stderr=STDOUT,
)




scripts/innosetup/.gitignore
/dvc.ico
/dvc_left.bmp
/dvc_up.bmp
/config.ini
/build
/dvc*.exe




scripts/innosetup/addsymlink.iss
procedure AddSymLink();
var
    ErrorCode: Integer;
    SRCdir: String;
begin
    SRCdir := ExpandConstant('{app}');
    if isUninstaller() = false then begin
        if not ShellExec('', 'powershell.exe', '-noninteractive -windowstyle hidden -executionpolicy bypass -File "' + SRCdir + '\addSymLinkPermissions.ps1" -mytype ' + SymLinkType, '', SW_HIDE, ewWaitUntilTerminated, ErrorCode) then begin
            SuppressibleMsgBox('Failed to automatically grant SeCreateSymbolicLinkPrivilege. Please download Polsedit(www.southsoftware.com/polsedit.zip). Launch polseditx32.exe or polseeditx64.exe (depending on your Windows version), navigate to "Security Settings" -> "User Rights Assignment", add the account(s) to the list named "Create symbolic links", logoff and login back into your account. More info at https://github.com/git-for-windows/git/wiki/Symbolic-Links.', mbInformation, MB_OK, IDOK);
        end else begin
            SuppressibleMsgBox('Automatically added SeCreateSymbolicLinkPrivilege. Please logoff and login back into your account in order for the change to take effect.', mbInformation, MB_OK, IDOK);
        end;
    end;
end;




scripts/innosetup/addSymLinkPermissions.ps1
param (
	[string]$mytype = 'system'
)

function addSymLinkPermissions($accountToAdd){
    Write-Host "Checking SymLink permissions.."
    $sidstr = $null
    if ( "$accountToAdd" -eq "Everyone" ) {
    	$sidstr = "S-1-1-0"
    } else {
        try {
            $ntprincipal = new-object System.Security.Principal.NTAccount "$accountToAdd"
            $sid = $ntprincipal.Translate([System.Security.Principal.SecurityIdentifier])
            $sidstr = $sid.Value.ToString()
        } catch {
            $sidstr = $null
        }
    }
    Write-Host "Account: $($accountToAdd)" -ForegroundColor DarkCyan
    if( [string]::IsNullOrEmpty($sidstr) ) {
        Write-Host "Account not found!" -ForegroundColor Red
        exit -1
    }
    Write-Host "Account SID: $($sidstr)" -ForegroundColor DarkCyan
    $tmp = [System.IO.Path]::GetTempFileName()
    Write-Host "Export current Local Security Policy" -ForegroundColor DarkCyan
    secedit.exe /export /cfg "$($tmp)"
    $c = Get-Content -Path $tmp
    $currentSetting = ""
    foreach($s in $c) {
        if( $s -like "SECreateSymbolicLinkPrivilege*") {
            $x = $s.split("=",[System.StringSplitOptions]::RemoveEmptyEntries)
            $currentSetting = $x[1].Trim()
        }
    }
    if( $currentSetting -notlike "*$($sidstr)*" ) {
        Write-Host "Need to add permissions to SymLink" -ForegroundColor Yellow

        Write-Host "Modify Setting ""Create SymLink""" -ForegroundColor DarkCyan

        if( [string]::IsNullOrEmpty($currentSetting) ) {
            $currentSetting = "*$($sidstr)"
        } else {
            $currentSetting = "*$($sidstr),$($currentSetting)"
        }
        Write-Host "$currentSetting"
    $outfile = @"
[Unicode]
Unicode=yes
[Version]
signature="`$CHICAGO`$"
Revision=1
[Privilege Rights]
SECreateSymbolicLinkPrivilege = $($currentSetting)
"@
    $tmp2 = [System.IO.Path]::GetTempFileName()
        Write-Host "Import new settings to Local Security Policy" -ForegroundColor DarkCyan
        $outfile | Set-Content -Path $tmp2 -Encoding Unicode -Force
        Push-Location (Split-Path $tmp2)
        try {
            secedit.exe /configure /db "secedit.sdb" /cfg "$($tmp2)" /areas USER_RIGHTS
        } finally {
            Pop-Location
        }
    } else {
        Write-Host "NO ACTIONS REQUIRED! Account already in ""Create SymLink""" -ForegroundColor DarkCyan
        Write-Host "Account $accountToAdd already has permissions to SymLink" -ForegroundColor Green
        return $true;
    }
}

if ( "$mytype" -eq "user" ) {
    addSymLinkPermissions $(Get-WMIObject -class Win32_ComputerSystem | select username).username
} else {
    addSymLinkPermissions Everyone
}




scripts/innosetup/build.py
import argparse
import configparser
import os
import pathlib
import shutil
from subprocess import STDOUT, check_call, check_output

path = pathlib.Path(__file__).parent.absolute()
config = path / "config.ini"

dvc = path.parent.parent / "dvc"
pyinstaller = path.parent / "pyinstaller"

build = path / "build"
install = build / "usr"

parser = argparse.ArgumentParser()
args = parser.parse_args()

try:
    shutil.rmtree(build)
except FileNotFoundError:
    pass

build.mkdir()
shutil.copytree(pyinstaller / "dist" / "dvc", build / "dvc")
shutil.copy(path / "addSymLinkPermissions.ps1", build)

version = check_output(
    [os.fspath(build / "dvc" / "dvc"), "--version"], text=True
).strip()

cfg = configparser.ConfigParser()
cfg.add_section("Version")
cfg.set("Version", "version", version)

with (path / "config.ini").open("w") as fobj:
    cfg.write(fobj)

check_call(
    ["iscc", "setup.iss"],
    cwd=path,
    stderr=STDOUT,
)




scripts/innosetup/dvc.ico.dvc
outs:
- md5: 90104d9e83cfb825cf45507e90aadd27
  path: dvc.ico




scripts/innosetup/dvc_left.bmp.dvc
outs:
- md5: 9106cda08aa427e73492389a0f17c72d
  path: dvc_left.bmp




scripts/innosetup/dvc_up.bmp.dvc
outs:
- md5: 94614d6650e062655f9f77507dc9c1f2
  path: dvc_up.bmp




scripts/innosetup/modpath.iss
// ----------------------------------------------------------------------------
// Script based on:
//
// Inno Setup Ver:	5.4.2
// Script Version:	1.4.2
// Author:			Jared Breland <jbreland@legroom.net>
// Homepage:		http://www.legroom.net/software
// License:			GNU Lesser General Public License (LGPL), version 3
//						http://www.gnu.org/licenses/lgpl.html
//
// Script Function:
//	Allow modification of environmental path directly from Inno Setup installers
//
// Instructions:
//	Copy modpath.iss to the same directory as your setup script
//
//	Add this statement to your [Setup] section
//		ChangesEnvironment=true
//
//	Add this statement to your [Tasks] section
//	You can change the Description or Flags
//	You can change the Name, but it must match the ModPathName setting below
//		Name: modifypath; Description: &Add application directory to your environmental path; Flags: unchecked
//
//	Add the following to the end of your [Code] section
//	ModPathName defines the name of the task defined above
//	ModPathType defines whether the 'user' or 'system' path will be modified;
//		this will default to user if anything other than system is set
//	setArrayLength must specify the total number of dirs to be added
//	Result[0] contains first directory, Result[1] contains second, etc.
//		const
//			ModPathName = 'modifypath';
//			ModPathType = 'user';
//
//		function ModPathDir(): TArrayOfString;
//		begin
//			setArrayLength(Result, 1);
//			Result[0] := ExpandConstant('{app}');
//		end;
//		#include "modpath.iss"
// ----------------------------------------------------------------------------

function ModPathDir(): TArrayOfString;
begin
	setArrayLength(Result, 1)
	Result[0] := ExpandConstant(ModPathPath);
end;

procedure ModPath();
var
	oldpath:	String;
	newpath:	String;
	updatepath:	Boolean;
	pathArr:	TArrayOfString;
	aExecFile:	String;
	aExecArr:	TArrayOfString;
	i, d:		Integer;
	pathdir:	TArrayOfString;
	regroot:	Integer;
	regpath:	String;

begin
	// Get constants from main script and adjust behavior accordingly
	// ModPathType MUST be 'system' or 'user'; force 'user' if invalid
	if ModPathType = 'system' then begin
		regroot := HKEY_LOCAL_MACHINE;
		regpath := 'SYSTEM\CurrentControlSet\Control\Session Manager\Environment';
	end else begin
		regroot := HKEY_CURRENT_USER;
		regpath := 'Environment';
	end;

	// Get array of new directories and act on each individually
	pathdir := ModPathDir();
	for d := 0 to GetArrayLength(pathdir)-1 do begin
		updatepath := true;

		// Modify WinNT path
		if UsingWinNT() = true then begin

			// Get current path, split into an array
			RegQueryStringValue(regroot, regpath, 'Path', oldpath);
			oldpath := oldpath + ';';
			i := 0;

			while (Pos(';', oldpath) > 0) do begin
				SetArrayLength(pathArr, i+1);
				pathArr[i] := Copy(oldpath, 0, Pos(';', oldpath)-1);
				oldpath := Copy(oldpath, Pos(';', oldpath)+1, Length(oldpath));
				i := i + 1;

				// Check if current directory matches app dir
				if pathdir[d] = pathArr[i-1] then begin
					// if uninstalling, remove dir from path
					if IsUninstaller() = true then begin
						continue;
					// if installing, flag that dir already exists in path
					end else begin
						updatepath := false;
					end;
				end;

				// Add current directory to new path
				if i = 1 then begin
					newpath := pathArr[i-1];
				end else begin
					newpath := newpath + ';' + pathArr[i-1];
				end;
			end;

			// Append app dir to path if not already included
			if (IsUninstaller() = false) AND (updatepath = true) then
				newpath := newpath + ';' + pathdir[d];

			// Write new path
			RegWriteStringValue(regroot, regpath, 'Path', newpath);

		// Modify Win9x path
		end else begin

			// Convert to shortened dirname
			pathdir[d] := GetShortName(pathdir[d]);

			// If autoexec.bat exists, check if app dir already exists in path
			aExecFile := 'C:\AUTOEXEC.BAT';
			if FileExists(aExecFile) then begin
				LoadStringsFromFile(aExecFile, aExecArr);
				for i := 0 to GetArrayLength(aExecArr)-1 do begin
					if IsUninstaller() = false then begin
						// If app dir already exists while installing, skip add
						if (Pos(pathdir[d], aExecArr[i]) > 0) then
							updatepath := false;
							break;
					end else begin
						// If app dir exists and = what we originally set, then delete at uninstall
						if aExecArr[i] = 'SET PATH=%PATH%;' + pathdir[d] then
							aExecArr[i] := '';
					end;
				end;
			end;

			// If app dir not found, or autoexec.bat didn't exist, then (create and) append to current path
			if (IsUninstaller() = false) AND (updatepath = true) then begin
				SaveStringToFile(aExecFile, #13#10 + 'SET PATH=%PATH%;' + pathdir[d], True);

			// If uninstalling, write the full autoexec out
			end else begin
				SaveStringsToFile(aExecFile, aExecArr, False);
			end;
		end;
	end;
end;

// Split a string into an array using passed delimiter
procedure MPExplode(var Dest: TArrayOfString; Text: String; Separator: String);
var
	i: Integer;
begin
	i := 0;
	repeat
		SetArrayLength(Dest, i+1);
		if Pos(Separator,Text) > 0 then	begin
			Dest[i] := Copy(Text, 1, Pos(Separator, Text)-1);
			Text := Copy(Text, Pos(Separator,Text) + Length(Separator), Length(Text));
			i := i + 1;
		end else begin
			 Dest[i] := Text;
			 Text := '';
		end;
	until Length(Text)=0;
end;

procedure CurUninstallStepChanged(CurUninstallStep: TUninstallStep);
var
	aSelectedTasks:	TArrayOfString;
	i:				Integer;
	taskname:		String;
	regpath:		String;
	regstring:		String;
	appid:			String;
begin
	// only run during actual uninstall
	if CurUninstallStep = usUninstall then begin
		// get list of selected tasks saved in registry at install time
		appid := '{#emit SetupSetting("AppId")}';
		if appid = '' then appid := '{#emit SetupSetting("AppName")}';
		regpath := ExpandConstant('Software\Microsoft\Windows\CurrentVersion\Uninstall\'+appid+'_is1');
		RegQueryStringValue(HKLM, regpath, 'Inno Setup: Selected Tasks', regstring);
		if regstring = '' then RegQueryStringValue(HKCU, regpath, 'Inno Setup: Selected Tasks', regstring);

		// check each task; if matches modpath taskname, trigger patch removal
		if regstring <> '' then begin
			taskname := ModPathName;
			MPExplode(aSelectedTasks, regstring, ',');
			if GetArrayLength(aSelectedTasks) > 0 then begin
				for i := 0 to GetArrayLength(aSelectedTasks)-1 do begin
					if comparetext(aSelectedTasks[i], taskname) = 0 then
						ModPath();
				end;
			end;
		end;
	end;
end;

function NeedRestart(): Boolean;
var
	taskname:	String;
begin
	taskname := ModPathName;
	if IsTaskSelected(taskname) and not UsingWinNT() then begin
		Result := True;
	end else begin
		Result := False;
	end;
end;




scripts/innosetup/setup.iss
#define MyAppName "DVC (Data Version Control)"
#define MyAppVersion ReadIni(SourcePath + "\config.ini", "Version", "version", "unknown")
#define MyAppPublisher "Dmitry Petrov"
#define MyAppURL "https://dvc.org"
#define MyAppDir SourcePath + "\build\dvc"

[Setup]
AppId={{8258CE8A-110E-4E0D-AE60-FEE00B15F041}
AppName={#MyAppName}
AppVersion={#MyAppVersion}
AppPublisher={#MyAppPublisher}
AppPublisherURL={#MyAppURL}
AppSupportURL={#MyAppURL}
AppUpdatesURL={#MyAppURL}
DefaultDirName={code:GetDefaultDirName}
DefaultGroupName={#MyAppName}
AllowNoIcons=yes
LicenseFile=..\..\LICENSE
OutputBaseFilename=dvc-{#MyAppVersion}
Compression=lzma2/max
SolidCompression=yes
OutputDir=.
ChangesEnvironment=yes
SetupIconFile=dvc.ico
WizardSmallImageFile=dvc_up.bmp
WizardImageFile=dvc_left.bmp
DisableWelcomePage=no

[Languages]
Name: "english"; MessagesFile: "compiler:Default.isl"

[Files]
Source: "{#MyAppDir}\*"; DestDir: "{app}"; Flags: ignoreversion recursesubdirs createallsubdirs

[Tasks]
Name: modifypath; Description: Adds dvc's application directory to environmental path; Flags: checkablealone;
Name: modifypath\system; Description: Adds dvc's application directory to environmental path for all users;
Name: addsymlinkpermissions; Description: Add permission for creating symbolic links; Flags: checkablealone;
Name: addsymlinkpermissions\system; Description: Add permissions for creating symbolic links for all users;

[Code]
const
	ModPathName = 'modifypath';
	ModPathPath = '{app}';
	SymLinkName = 'addsymlinkpermissions';

var
	ModPathType: String;
	SymLinkType: String;

function GetDefaultDirName(Dummy: string): string;
begin
	if IsAdminLoggedOn then begin
		Result := ExpandConstant('{pf}\{#MyAppName}');
	end else begin
		Result := ExpandConstant('{userpf}\{#MyAppName}');
	end;
end;

#include "modpath.iss"
#include "addsymlink.iss"

procedure CurStepChanged(CurStep: TSetupStep);
begin
	if CurStep = ssPostInstall then begin
		if IsTaskSelected(ModPathName + '\system') then begin
			ModPathType := 'system';
		end else begin
			ModPathType := 'user';
		end;

		if IsTaskSelected(SymLinkName + '\system') then begin
			SymLinkType := 'system';
		end else begin
			SymLinkType := 'user';
		end;

		if IsTaskSelected(ModPathName) then
			ModPath();
		if IsTaskSelected(SymLinkName) then
			AddSymLink();
	end;
end;




scripts/pyinstaller/.gitignore
/build
/dist
/dvc.spec
/dvc




scripts/pyinstaller/build.py
import os
import pathlib
from subprocess import STDOUT, check_call, check_output

path = pathlib.Path(__file__).parent.absolute()
hooks = path / "hooks"
dvc = path.parent.parent / "dvc"
entry = dvc / "__main__.py"

check_call(
    [
        "pyinstaller",
        "--additional-hooks-dir",
        os.fspath(hooks),
        "--name",
        "dvc",
        "-y",
        os.fspath(entry),
    ],
    cwd=path,
    stderr=STDOUT,
)

out = check_output(
    [
        path / "dist" / "dvc" / "dvc",
        "doctor",
    ],
    stderr=STDOUT,
).decode()

remotes = [
    "s3",
    "oss",
    "gdrive",
    "gs",
    "hdfs",
    "http",
    "webhdfs",
    "azure",
    "ssh",
    "webdav",
]

print(out)
for remote in remotes:
    assert f"\t{remote}" in out, f"Missing support for {remote}"




scripts/pyinstaller/entitlements.plist
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<!-- These are required for binaries built by PyInstaller -->
	<key>com.apple.security.cs.allow-jit</key>
	<true/>
	<key>com.apple.security.cs.allow-unsigned-executable-memory</key>
	<true/>
</dict>
</plist>




scripts/pyinstaller/sign.py
import argparse
import os
import pathlib
import sys
from subprocess import STDOUT, check_call

if sys.platform != "darwin":
    raise NotImplementedError

parser = argparse.ArgumentParser()
parser.add_argument(
    "--application-id",
    required=True,
    help="Certificate ID (should be added to the keychain).",
)
args = parser.parse_args()

path = pathlib.Path(__file__).parent.absolute()
dvc = path / "dist" / "dvc"
for root, _, fnames in os.walk(dvc):
    for fname in fnames:
        fpath = os.path.join(root, fname)
        print(f"signing {fpath}")
        check_call(
            [
                "codesign",
                "--force",
                "--verbose",
                "-s",
                args.application_id,
                "-o",
                "runtime",
                "--entitlements",
                "entitlements.plist",
                fpath,
            ],
            stderr=STDOUT,
            timeout=180,
        )




scripts/pyinstaller/hooks/hook-asyncssh.py
hiddenimports = ["win32timezone"]




scripts/pyinstaller/hooks/hook-celery.py
# pylint: disable=import-error
from PyInstaller.utils.hooks import collect_submodules, is_module_or_submodule

# Celery dynamically imports most celery internals at runtime
# pyinstaller hook must expose all modules loaded by
# kombu.utils.imports:symbol_by_name()
_EXCLUDES = (
    "celery.bin",
    "celery.contrib",
)
hiddenimports = collect_submodules(
    "celery",
    filter=lambda name: not any(
        is_module_or_submodule(name, module) for module in _EXCLUDES
    ),
)




scripts/pyinstaller/hooks/hook-dvc.py
from PyInstaller.utils.hooks import copy_metadata  # pylint:disable=import-error

# needed for `dvc doctor` to show dep versions
datas = copy_metadata("adlfs", recursive=True)
datas += copy_metadata("knack")
datas += copy_metadata("gcsfs")
datas += copy_metadata("pyarrow")
datas += copy_metadata("pydrive2")
datas += copy_metadata("s3fs", recursive=True)
datas += copy_metadata("boto3")
datas += copy_metadata("ossfs")
datas += copy_metadata("sshfs")
datas += copy_metadata("webdav4")
datas += copy_metadata("aiohttp")
datas += copy_metadata("aiohttp_retry")

hiddenimports = [
    "dvc_azure",
    "dvc_gdrive",
    "dvc_gs",
    "dvc_hdfs",
    "dvc_oss",
    "dvc_s3",
    "dvc_webdav",
    "dvc_webhdfs",
    # https://github.com/pypa/setuptools/issues/1963
    "pkg_resources.py2_warn",
]




scripts/pyinstaller/hooks/hook-dvc.system.py
import os

if os.name == "nt":
    hiddenimports = ["win32file"]




scripts/pyinstaller/hooks/hook-dvc.tree.gs.py
from PyInstaller.utils.hooks import copy_metadata  # pylint:disable=import-error

datas = copy_metadata("google-cloud-storage")

# NOTE: including "requests" metadata because of google-resumable-media
# bug. See [1] for more info.
#
# [1] https://github.com/googleapis/google-resumable-media-python/issues/59
datas += copy_metadata("requests")




scripts/pyinstaller/hooks/hook-dvc.utils.flatten.py
from PyInstaller.utils.hooks import copy_metadata  # pylint:disable=import-error

datas = copy_metadata("flatten-dict")




scripts/pyinstaller/hooks/hook-dvc_task.py
# pylint: disable=import-error
from PyInstaller.utils.hooks import collect_submodules

hiddenimports = collect_submodules("dvc_task")




scripts/pyinstaller/hooks/hook-fsspec.py
hiddenimports = ["fsspec.implementations.memory"]




scripts/pyinstaller/hooks/hook-google_compute_engine.logger.py
from PyInstaller.utils.hooks import copy_metadata  # pylint:disable=import-error

datas = copy_metadata("google-compute-engine")
hiddenimports = ["google_cloud_engine"]




scripts/pyinstaller/hooks/hook-pydrive2.py
from PyInstaller.utils.hooks import copy_metadata  # pylint:disable=import-error

datas = copy_metadata("pydrive2")




tests/__init__.py
import os

# Increasing fd ulimit for tests
if os.name == "nt":
    try:
        import win32file
    except ImportError:
        pass
    else:
        win32file._setmaxstdio(4096)
else:
    import resource

    resource.setrlimit(resource.RLIMIT_NOFILE, (4096, 4096))

    nproc_soft, nproc_hard = resource.getrlimit(resource.RLIMIT_NPROC)
    resource.setrlimit(resource.RLIMIT_NPROC, (nproc_hard, nproc_hard))




tests/__main__.py
if __name__ == "__main__":
    import sys

    import pytest

    sys.exit(pytest.main(["-v", "-n=logical", *sys.argv[1:]]))




tests/conftest.py
import json
import os
import sys
from contextlib import suppress

import pytest

from dvc import env
from dvc.testing.fixtures import *  # noqa, pylint: disable=wildcard-import

from .dir_helpers import *  # noqa, pylint: disable=wildcard-import
from .remotes import *  # noqa, pylint: disable=wildcard-import
from .scripts import *  # noqa, pylint: disable=wildcard-import

# Prevent updater and analytics from running their processes
os.environ["DVC_TEST"] = "true"
# Ensure progress output even when not outputting to raw sys.stderr console
os.environ["DVC_IGNORE_ISATTY"] = "true"
# Disable system git config
os.environ["GIT_CONFIG_NOSYSTEM"] = "1"

REMOTES = {
    # remote: enabled_by_default?
    "azure": False,
    "gdrive": False,
    "gs": False,
    "hdfs": True,
    "real_hdfs": False,
    "http": True,
    "oss": False,
    "s3": False,
    "ssh": False,
    "webdav": True,
}


@pytest.fixture(autouse=True)
def reset_loglevel(request, caplog):
    """
    Use it to ensure log level at the start of each test
    regardless of dvc.logger.setup(), Repo configs or whatever.
    """
    ini_opt = None
    with suppress(ValueError):
        ini_opt = request.config.getini("log_level")

    level = request.config.getoption("--log-level") or ini_opt
    if level:
        with caplog.at_level(level.upper(), logger="dvc"):
            yield
    else:
        yield


@pytest.fixture(autouse=True)
def enable_ui():
    from dvc.ui import ui

    ui.enable()


@pytest.fixture(autouse=True)
def clean_repos():
    from dvc.repo.open_repo import clean_repos

    clean_repos()


def _get_opt(remote_name, action):
    return f"--{action}-{remote_name}"


def pytest_addoption(parser):
    """Adds remote-related flags to selectively disable/enable for tests
    Eg: If some remotes, eg: ssh is enabled to be tested for by default
    (see above `REMOTES`), then, `--disable-ssh` flag is added. If remotes
    like `hdfs` are disabled by default, `--enable-hdfs` is added to make them
    run.

    You can also make everything run-by-default with `--all` flag, which takes
    precedence on all previous `--enable-*`/`--disable-*` flags.
    """
    parser.addoption(
        "--all",
        action="store_true",
        default=False,
        help="Test all of the remotes, unless other flags also supplied",
    )
    for remote_name in REMOTES:
        for action in ("enable", "disable"):
            opt = _get_opt(remote_name, action)
            parser.addoption(
                opt,
                action="store_true",
                default=None,
                help=f"{action} tests for {remote_name}",
            )


class DVCTestConfig:
    def __init__(self):
        self.enabled_remotes = set()

    def requires(self, remote_name):
        if remote_name not in REMOTES or remote_name in self.enabled_remotes:
            return

        pytest.skip(f"{remote_name} tests not enabled through CLI")

    def apply_marker(self, marker):
        self.requires(marker.name)


def pytest_runtest_setup(item):
    # Apply test markers to skip tests selectively
    # NOTE: this only works on individual tests,
    # for fixture, use `test_config` fixture and
    # run `test_config.requires(remote_name)`.
    for marker in item.iter_markers():
        item.config.dvc_config.apply_marker(marker)

    if "CI" in os.environ and item.get_closest_marker("needs_internet") is not None:
        # remotes that need internet connection might be flaky,
        # so we rerun them in case it fails.
        item.add_marker(pytest.mark.flaky(max_runs=5, min_passes=1))


@pytest.fixture(scope="session")
def test_config(request):
    return request.config.dvc_config


def pytest_configure(config):
    config.dvc_config = DVCTestConfig()

    for remote_name in REMOTES:
        config.addinivalue_line(
            "markers", f"{remote_name}: mark test as requiring {remote_name}"
        )

    enabled_remotes = config.dvc_config.enabled_remotes
    if config.getoption("--all"):
        enabled_remotes.update(REMOTES)
    else:
        default_enabled = {k for k, v in REMOTES.items() if v}
        enabled_remotes.update(default_enabled)

    for remote_name in REMOTES:
        enabled_opt = _get_opt(remote_name, "enable")
        disabled_opt = _get_opt(remote_name, "disable")

        enabled = config.getoption(enabled_opt)
        disabled = config.getoption(disabled_opt)
        if disabled and enabled:
            continue  # default behavior if both flags are supplied

        if disabled:
            enabled_remotes.discard(remote_name)
        if enabled:
            enabled_remotes.add(remote_name)


@pytest.fixture
def custom_template(tmp_dir, dvc):
    from dvc_render.vega_templates import SimpleLinearTemplate

    template = tmp_dir / "custom_template.json"
    template.write_text(json.dumps(SimpleLinearTemplate.DEFAULT_CONTENT))
    return template


@pytest.fixture(autouse=True)
def mocked_webbrowser_open(mocker):
    mocker.patch("webbrowser.open")


@pytest.fixture(scope="session", autouse=True)
def isolate(tmp_path_factory):
    path = tmp_path_factory.mktemp("mock")
    home_dir = path / "home"
    home_dir.mkdir()

    monkeypatch = pytest.MonkeyPatch()
    if sys.platform == "win32":
        home_drive, home_path = os.path.splitdrive(home_dir)
        monkeypatch.setenv("USERPROFILE", str(home_dir))
        monkeypatch.setenv("HOMEDRIVE", home_drive)
        monkeypatch.setenv("HOMEPATH", home_path)

        for env_var, sub_path in (("APPDATA", "Roaming"), ("LOCALAPPDATA", "Local")):
            path = home_dir / "AppData" / sub_path
            path.mkdir(parents=True)
            monkeypatch.setenv(env_var, os.fspath(path))
    else:
        monkeypatch.setenv("HOME", str(home_dir))

    monkeypatch.setenv("GIT_CONFIG_NOSYSTEM", "1")
    contents = b"""
[user]
name=DVC Tester
email=dvctester@example.com
[init]
defaultBranch=master
"""
    (home_dir / ".gitconfig").write_bytes(contents)

    import pygit2

    pygit2.settings.search_path[pygit2.GIT_CONFIG_LEVEL_GLOBAL] = str(home_dir)

    monkeypatch.setenv(env.DVC_SYSTEM_CONFIG_DIR, os.fspath(path / "system"))
    monkeypatch.setenv(env.DVC_GLOBAL_CONFIG_DIR, os.fspath(path / "global"))
    monkeypatch.setenv(env.DVC_SITE_CACHE_DIR, os.fspath(path / "site_cache_dir"))

    yield

    monkeypatch.undo()


@pytest.fixture
def run_copy_metrics(tmp_dir, copy_script):
    def run(
        file1,
        file2,
        commit=None,
        tag=None,
        single_stage=True,
        name=None,
        **kwargs,
    ):
        if name:
            single_stage = False

        stage = tmp_dir.dvc.run(
            cmd=f"python copy.py {file1} {file2}",
            deps=[file1],
            single_stage=single_stage,
            name=name,
            **kwargs,
        )

        if hasattr(tmp_dir.dvc, "scm"):
            files = [stage.path]
            files += [out.fs_path for out in stage.outs if not out.use_cache]
            tmp_dir.dvc.scm.add(files)
            if commit:
                tmp_dir.dvc.scm.commit(commit)
            if tag:
                tmp_dir.dvc.scm.tag(tag)
        return stage

    return run




tests/dir_helpers.py
"""
The goal of this module is making dvc functional tests setup a breeze. This
includes a temporary dir, initializing git and DVC repos and bootstrapping some
file structure.

The cornerstone of these fixtures is `tmp_dir`, which creates a temporary dir
and changes path to it, it might be combined with `scm` and `dvc` to initialize
empty git and DVC repos. `tmp_dir` returns a Path instance, which should save
you from using `open()`, `os` and `os.path` utils many times:

    (tmp_dir / "some_file").write_text("some text")
    # ...
    assert "some text" == (tmp_dir / "some_file").read_text()
    assert (tmp_dir / "some_file").exists()

Additionally it provides `.gen()`, `.scm_gen()` and `.dvc_gen()` methods to
bootstrap a required file structure in a single call:

    # Generate a dir with files
    tmp_dir.gen({"dir": {"file": "file text", "second_file": "..."}})

    # Generate a single file, dirs will be created along the way
    tmp_dir.gen("dir/file", "file text")

    # Generate + git add
    tmp_dir.scm_gen({"file1": "...", ...})

    # Generate + git add + git commit
    tmp_dir.scm_gen({"file1": "...", ...}, commit="add files")

    # Generate + dvc add
    tmp_dir.dvc_gen({"file1": "...", ...})

    # Generate + dvc add + git commit -am "..."
    # This commits stages to git not the generated files.
    tmp_dir.dvc_gen({"file1": "...", ...}, commit="add files")

Making it easier to bootstrap things has a supergoal of incentivizing a move
from global repo template to creating everything inplace, which:

    - makes all path references local to test, enhancing readability
    - allows using telling filenames, e.g. "git_tracked_file" instead of "foo"
    - does not create unnecessary files
"""


import os
import pathlib

import pytest

__all__ = [
    "run_copy",
    "run_head",
    "erepo_dir",
    "git_dir",
    "git_upstream",
    "git_downstream",
]


@pytest.fixture
def run_copy(tmp_dir, copy_script, dvc):
    def run_copy(src, dst, **run_kwargs):
        wdir = pathlib.Path(run_kwargs.get("wdir", "."))
        wdir = pathlib.Path("../" * len(wdir.parts))
        script_path = wdir / "copy.py"

        return dvc.run(
            cmd=f"python {script_path} {src} {dst}",
            outs=[dst],
            deps=[src, f"{script_path}"],
            **run_kwargs,
        )

    return run_copy


@pytest.fixture
def run_head(tmp_dir, head_script, dvc):
    script = os.path.abspath(tmp_dir / "head.py")

    def run(*args, **run_kwargs):
        return dvc.run(
            **{
                "cmd": "python {} {}".format(script, " ".join(args)),
                "outs": [dep + "-1" for dep in args],
                "deps": list(args),
                **run_kwargs,
            }
        )

    return run


@pytest.fixture
def erepo_dir(make_tmp_dir):
    return make_tmp_dir("erepo", scm=True, dvc=True)


@pytest.fixture
def git_dir(make_tmp_dir):
    path = make_tmp_dir("git-erepo", scm=True)
    path.scm.commit("init repo")
    return path


class GitRemote:
    def __init__(self, tmp_dir, name, url):
        self.tmp_dir = tmp_dir
        self.remote = name
        self.url = url


@pytest.fixture
def git_upstream(tmp_dir, erepo_dir, git_dir, request):
    remote = erepo_dir if "dvc" in request.fixturenames else git_dir
    url = f"file://{remote.resolve().as_posix()}"
    tmp_dir.scm.gitpython.repo.create_remote("upstream", url)
    return GitRemote(remote, "upstream", url)


@pytest.fixture
def git_downstream(tmp_dir, erepo_dir, git_dir, request):
    remote = erepo_dir if "dvc" in request.fixturenames else git_dir
    url = f"file://{tmp_dir.resolve().as_posix()}"
    remote.scm.gitpython.repo.create_remote("upstream", url)
    return GitRemote(remote, "upstream", url)




tests/docker-compose.yml
---
version: '3.2'
services:
  git-server:
    image: ghcr.io/linuxserver/openssh-server
    environment:
      - USER_NAME=user
      - PUBLIC_KEY_FILE=/tmp/key
    ports:
      - 2222
    volumes:
      - ./remotes/user.key.pub:/tmp/key
      - ./remotes/git-init:/custom-cont-init.d




tests/remotes_env.sample
# Uncomment and fill in to test against S3
# export AWS_ACCESS_KEY_ID="...SET-ME..."
# export AWS_SECRET_ACCESS_KEY="...SET-ME..."
# export DVC_TEST_AWS_REPO_BUCKET="...SET-ME..."

# Uncomment and set a bucket name to test against Google Storage
# export GOOGLE_APPLICATION_CREDENTIALS="scripts/ci/gcp-creds.json"

# Uncomment to test against Microsoft Azure via Azurite
# export AZURE_STORAGE_CONTAINER_NAME="dvc-test"
# export AZURE_STORAGE_CONNECTION_STRING="DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://127.0.0.1:10000/devstoreaccount1;"

# Uncomment and fill in to test against Alibaba OSS
# export OSS_ENDPOINT="...SET-ME.."
# export OSS_ACCESS_KEY_ID="...SET-ME.."
# export OSS_ACCESS_KEY_SECRET="...SET-ME.."
# export DVC_TEST_OSS=1
# export DVC_TEST_OSS_REPO_BUCKET="...SET-ME..."




tests/ruff.toml
extend = "../pyproject.toml"
extend-select = ["RUF005"]
extend-ignore = ["S", "RUF001", "ARG001", "ARG002", "TRY002"]




tests/scripts.py
import pytest

COPY_SCRIPT = """
import os
import shutil
import sys

if os.path.isfile(sys.argv[1]):
    shutil.copyfile(sys.argv[1], sys.argv[2])
else:
    shutil.copytree(sys.argv[1], sys.argv[2])
"""


def _add_script(tmp_dir, path, contents=""):
    script, *_ = tmp_dir.gen(path, contents)
    if hasattr(tmp_dir, "scm"):
        tmp_dir.scm_add(path, commit=f"add {path}")
    return script.fs_path


@pytest.fixture
def append_foo_script(tmp_dir):
    return _add_script(
        tmp_dir,
        "append_foo.py",
        """
import sys
from pathlib import Path

with Path(sys.argv[1]).open("a+", encoding="utf-8") as f:
    f.write("foo")
""",
    )


@pytest.fixture
def copy_script(
    tmp_dir,
):
    return _add_script(tmp_dir, "copy.py", COPY_SCRIPT)


@pytest.fixture
def head_script(tmp_dir):
    """Output first line of each file to different file with '-1' appended.
    Useful for tracking multiple outputs/dependencies which are not a copy
    of each others.
    """
    return _add_script(
        tmp_dir,
        "head.py",
        """
import sys
for file in sys.argv[1:]:
    with open(file) as f, open(file +"-1","w+") as w:
        w.write(f.readline())
""",
    )




tests/func/__init__.py




tests/func/test_add.py
import errno
import filecmp
import os
import shutil
import stat
import textwrap
from unittest.mock import call

import pytest

import dvc_data
from dvc.cachemgr import CacheManager
from dvc.cli import main
from dvc.config import ConfigError
from dvc.dvcfile import DVC_FILE_SUFFIX
from dvc.exceptions import (
    DvcException,
    InvalidArgumentError,
    OverlappingOutputPathsError,
)
from dvc.fs import LocalFileSystem, system
from dvc.output import (
    OutputAlreadyTrackedError,
    OutputDoesNotExistError,
    OutputIsStageFileError,
)
from dvc.stage import Stage
from dvc.stage.exceptions import StageExternalOutputsError, StagePathNotFoundError
from dvc.testing.workspace_tests import TestAdd
from dvc.utils.fs import path_isin
from dvc.utils.serialize import YAMLFileCorruptedError
from dvc_data.hashfile.hash import file_md5
from dvc_data.hashfile.hash_info import HashInfo
from tests.utils import get_gitignore_content


def test_add(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    (stage,) = dvc.add("foo")
    md5 = file_md5("foo", dvc.fs)

    assert stage is not None

    assert isinstance(stage, Stage)
    assert os.path.isfile(stage.path)
    assert len(stage.outs) == 1
    assert len(stage.deps) == 0
    assert stage.cmd is None
    assert stage.outs[0].hash_info == HashInfo("md5", md5)
    assert stage.md5 is None

    assert (tmp_dir / "foo.dvc").parse() == {
        "outs": [
            {
                "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
                "path": "foo",
                "size": 3,
            }
        ]
    }


@pytest.mark.skipif(os.name == "nt", reason="can't set exec bit on Windows")
def test_add_executable(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    st = os.stat("foo")
    os.chmod("foo", st.st_mode | stat.S_IEXEC)
    dvc.add("foo")

    assert (tmp_dir / "foo.dvc").parse() == {
        "outs": [
            {
                "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
                "path": "foo",
                "size": 3,
                "isexec": True,
            }
        ]
    }
    assert os.stat("foo").st_mode & stat.S_IEXEC


def test_add_unicode(tmp_dir, dvc):
    with open("\xe1", "wb", encoding=None) as fd:
        fd.write(b"something")

    (stage,) = dvc.add("\xe1")

    assert os.path.isfile(stage.path)


def test_add_unsupported_file(dvc):
    with pytest.raises(ConfigError, match="Unsupported URL type"):
        dvc.add("unsupported://unsupported")


def test_add_directory(tmp_dir, dvc):
    from dvc_data.hashfile import load

    (stage,) = tmp_dir.dvc_gen({"dir": {"file": "file"}})

    assert stage is not None
    assert len(stage.deps) == 0
    assert len(stage.outs) == 1

    hash_info = stage.outs[0].hash_info

    obj = load(dvc.cache.local, hash_info)
    for key, _, _ in obj:
        for part in key:
            assert "\\" not in part


def test_add_directory_with_forward_slash(tmp_dir, dvc):
    tmp_dir.gen("directory", {"file": "file"})
    (stage,) = dvc.add("directory/")
    assert stage.relpath == "directory.dvc"


def test_add_tracked_file(tmp_dir, scm, dvc):
    path = "tracked_file"
    tmp_dir.scm_gen(path, "...", commit="add tracked file")
    msg = f""" output '{path}' is already tracked by SCM \\(e.g. Git\\).
    You can remove it from Git, then add to DVC.
        To stop tracking from Git:
            git rm -r --cached '{path}'
            git commit -m "stop tracking {path}" """

    with pytest.raises(OutputAlreadyTrackedError, match=msg):
        dvc.add(path)


def test_add_dir_with_existing_cache(tmp_dir, dvc):
    tmp_dir.gen({"foo": "foo", "dir": {"file": "foo"}})

    (stage,) = dvc.add("foo")
    assert stage is not None
    (stage,) = dvc.add("dir")
    assert stage is not None


def test_add_modified_dir(tmp_dir, dvc):
    tmp_dir.gen("data", {"foo": "foo", "sub": {"bar": "bar"}})
    (stage,) = dvc.add("data")
    assert stage is not None

    (tmp_dir / "data" / "foo").unlink()
    (stage,) = dvc.add("data")
    assert stage is not None


def test_add_file_in_dir(tmp_dir, dvc):
    tmp_dir.gen({"dir": {"subdir": {"subdata": "subdata content"}}})
    subdir_path = os.path.join("dir", "subdir", "subdata")

    (stage,) = dvc.add(subdir_path)

    assert stage is not None
    assert len(stage.deps) == 0
    assert len(stage.outs) == 1
    assert stage.relpath == subdir_path + ".dvc"

    # Current dir should not be taken into account
    assert stage.wdir == os.path.dirname(stage.path)
    assert stage.outs[0].def_path == "subdata"


@pytest.mark.parametrize(
    "target, expected_def_paths, expected_rel_paths",
    [
        (
            os.path.join("dir", "subdir", "subdata*"),
            ["subdata", "subdata123"],
            [
                os.path.join("dir", "subdir", "subdata") + ".dvc",
                os.path.join("dir", "subdir", "subdata123") + ".dvc",
            ],
        ),
        (
            os.path.join("dir", "subdir", "?subdata"),
            ["esubdata", "isubdata"],
            [
                os.path.join("dir", "subdir", "esubdata") + ".dvc",
                os.path.join("dir", "subdir", "isubdata") + ".dvc",
            ],
        ),
        (
            os.path.join("dir", "subdir", "[aiou]subdata"),
            ["isubdata"],
            [os.path.join("dir", "subdir", "isubdata") + ".dvc"],
        ),
        (
            os.path.join("dir", "**", "subdata*"),
            ["subdata", "subdata123", "subdata4", "subdata5"],
            [
                os.path.join("dir", "subdir", "subdata") + ".dvc",
                os.path.join("dir", "subdir", "subdata123") + ".dvc",
                os.path.join("dir", "anotherdir", "subdata4") + ".dvc",
                os.path.join("dir", "subdata5") + ".dvc",
            ],
        ),
    ],
)
def test_add_filtered_files_in_dir(
    tmp_dir, dvc, target, expected_def_paths, expected_rel_paths
):
    tmp_dir.gen(
        {
            "dir": {
                "subdir": {
                    "subdata": "subdata content",
                    "esubdata": "extra subdata content",
                    "isubdata": "i subdata content",
                    "subdata123": "subdata content 123",
                },
                "anotherdir": {
                    "subdata4": "subdata 4 content",
                    "esubdata": "extra 2 subdata content",
                },
                "subdata5": "subdata 5 content",
            }
        }
    )

    stages = dvc.add(target, glob=True)

    assert len(stages) == len(expected_def_paths)
    for stage in stages:
        assert stage is not None
        assert len(stage.deps) == 0
        assert len(stage.outs) == 1
        assert stage.relpath in expected_rel_paths

        # Current dir should not be taken into account
        assert stage.wdir == os.path.dirname(stage.path)
        assert stage.outs[0].def_path in expected_def_paths


class TestAddExternal(TestAdd):
    @pytest.fixture
    def hash_name(self):
        return "md5"

    @pytest.fixture
    def hash_value(self):
        return "8c7dd922ad47494fc02c388e12c00eac"

    @pytest.fixture
    def dir_hash_value(self):
        return "b6dcab6ccd17ca0a8bf4a215a37d14cc.dir"


def test_add_external_relpath(tmp_dir, dvc, local_cloud):
    (fpath,) = local_cloud.gen("file", "file")
    rel = os.path.relpath(fpath)

    with pytest.raises(StageExternalOutputsError):
        dvc.add(rel)

    dvc.add(rel, external=True)
    assert (tmp_dir / "file.dvc").read_text() == (
        "outs:\n"
        "- md5: 8c7dd922ad47494fc02c388e12c00eac\n"
        "  size: 4\n"
        f"  path: {rel}\n"
    )
    assert fpath.read_text() == "file"
    assert dvc.status() == {}


def test_add_local_remote_file(tmp_dir, dvc):
    """
    Making sure that 'remote' syntax is handled properly for local outs.
    """
    tmp_dir.gen({"foo": "foo", "bar": "bar"})
    tmp_dir.add_remote(url=tmp_dir.fs_path, name="myremote")

    assert main(["add", "remote://myremote/foo"]) == 0
    d = (tmp_dir / "foo.dvc").load_yaml()
    assert d["outs"][0]["path"] == "remote://myremote/foo"

    assert main(["add", (tmp_dir / "bar").fs_path]) == 0
    d = (tmp_dir / "bar.dvc").load_yaml()
    assert d["outs"][0]["path"] == "bar"


def test_cmd_add(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    ret = main(["add", "foo"])
    assert ret == 0

    ret = main(["add", "non-existing-file"])
    assert ret != 0


def test_double_add_unchanged_file(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    ret = main(["add", "foo"])
    assert ret == 0

    ret = main(["add", "foo"])
    assert ret == 0


def test_double_add_unchanged_dir(tmp_dir, dvc):
    tmp_dir.gen("data", {"foo": "foo"})
    ret = main(["add", "data"])
    assert ret == 0

    ret = main(["add", "data"])
    assert ret == 0


@pytest.mark.skipif(os.name == "nt", reason="unsupported on Windows")
def test_add_colon_in_filename(tmp_dir, dvc):
    tmp_dir.gen("fo:o", "foo")
    ret = main(["add", "fo:o"])
    assert ret == 0


def test_should_update_state_entry_for_file_after_add(mocker, dvc, tmp_dir):
    file_md5_counter = mocker.spy(dvc_data.hashfile.hash, "file_md5")
    tmp_dir.gen("foo", "foo")

    ret = main(["config", "cache.type", "copy"])
    assert ret == 0

    ret = main(["add", "foo"])
    assert ret == 0
    assert file_md5_counter.mock.call_count == 1

    ret = main(["status"])
    assert ret == 0
    assert file_md5_counter.mock.call_count == 1

    os.rename("foo", "foo.back")
    ret = main(["checkout"])
    assert ret == 0
    assert file_md5_counter.mock.call_count == 1

    ret = main(["status"])
    assert ret == 0
    assert file_md5_counter.mock.call_count == 1


def test_should_update_state_entry_for_directory_after_add(mocker, dvc, tmp_dir):
    file_md5_counter = mocker.spy(dvc_data.hashfile.hash, "file_md5")

    tmp_dir.gen({"data/data": "foo", "data/data_sub/sub_data": "foo"})

    ret = main(["config", "cache.type", "copy"])
    assert ret == 0

    ret = main(["add", "data"])
    assert ret == 0
    assert file_md5_counter.mock.call_count == 4

    ret = main(["status"])
    assert ret == 0
    assert file_md5_counter.mock.call_count == 5

    os.rename("data", "data.back")
    ret = main(["checkout"])
    assert ret == 0
    assert file_md5_counter.mock.call_count == 5

    ret = main(["status"])
    assert ret == 0
    assert file_md5_counter.mock.call_count == 6


def test_add_commit(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    ret = main(["add", "foo", "--no-commit"])
    assert ret == 0
    assert os.path.isfile("foo")
    assert not os.path.exists(dvc.cache.local.path)

    ret = main(["commit", "foo.dvc"])
    assert ret == 0
    assert os.path.isfile("foo")
    assert dvc.cache.local.exists("acbd18db4cc2f85cedef654fccc4a4d8")


def test_should_collect_dir_cache_only_once(mocker, tmp_dir, dvc):
    tmp_dir.gen({"data/data": "foo"})
    counter = mocker.spy(dvc_data.hashfile.build, "_build_tree")
    ret = main(["add", "data"])
    assert ret == 0
    assert counter.mock.call_count == 2

    ret = main(["status"])
    assert ret == 0
    assert counter.mock.call_count == 3

    ret = main(["status"])
    assert ret == 0
    assert counter.mock.call_count == 4


def test_should_place_stage_in_data_dir_if_repository_below_symlink(
    mocker, tmp_dir, dvc
):
    def is_symlink_true_below_dvc_root(path):
        return path == os.path.dirname(dvc.root_dir)

    tmp_dir.gen({"data": {"foo": "foo"}})
    mocker.patch.object(
        system, "is_symlink", side_effect=is_symlink_true_below_dvc_root
    )
    ret = main(["add", os.path.join("data", "foo")])
    assert ret == 0

    assert not (tmp_dir / "foo.dvc").exists()
    assert (tmp_dir / "data" / "foo.dvc").exists()


def test_should_throw_proper_exception_on_corrupted_stage_file(caplog, tmp_dir, dvc):
    tmp_dir.gen({"foo": "foo", "bar": " bar"})
    assert main(["add", "foo"]) == 0

    with (tmp_dir / "foo.dvc").open("a+") as f:
        f.write("this will break yaml file structure")

    caplog.clear()
    assert main(["add", "bar"]) == 1
    expected_error = "unable to read: 'foo.dvc', YAML file structure is corrupted"
    assert expected_error in caplog.text


def test_should_throw_proper_exception_on_existing_out(caplog, tmp_dir, dvc):
    tmp_dir.gen({"foo": "foo"})
    (tmp_dir / "out").write_text("old contents")

    assert main(["add", "foo", "--out", "out"]) == 1

    assert (tmp_dir / "out").read_text() == "old contents"
    expected_error_lines = [
        "Error: The file 'out' already exists locally.",
        "To override it, re-run with '--force'.",
    ]
    assert all(line in caplog.text for line in expected_error_lines)


def test_add_force_overwrite_out(caplog, tmp_dir, dvc):
    tmp_dir.gen({"foo": "foo"})
    (tmp_dir / "out").write_text("old contents")

    assert main(["add", "foo", "--out", "out", "--force"]) == 0
    assert (tmp_dir / "foo").read_text() == "foo"


def test_add_filename(tmp_dir, dvc):
    tmp_dir.gen({"foo": "foo", "bar": "bar", "data": {"file": "file"}})
    ret = main(["add", "foo", "bar", "--file", "error.dvc"])
    assert ret != 0

    ret = main(["add", "data", "--file", "data_directory.dvc"])
    assert ret == 0
    assert (tmp_dir / "data_directory.dvc").exists()

    ret = main(["add", "foo", "--file", "bar.dvc"])
    assert ret == 0
    assert (tmp_dir / "bar.dvc").exists()
    assert not (tmp_dir / "foo.dvc").exists()

    (tmp_dir / "bar.dvc").unlink()

    ret = main(["add", "foo", "--file", "bar.dvc"])
    assert ret == 0
    assert (tmp_dir / "bar.dvc").exists()
    assert not (tmp_dir / "foo.dvc").exists()


def test_failed_add_cleanup(tmp_dir, scm, dvc):
    tmp_dir.gen({"foo": "foo", "bar": "bar"})

    # Add and corrupt a stage file
    dvc.add("foo")
    tmp_dir.gen("foo.dvc", "- broken\nyaml")

    with pytest.raises(YAMLFileCorruptedError):
        dvc.add("bar")

    assert not os.path.exists("bar.dvc")

    gitignore_content = get_gitignore_content()
    assert "/bar" not in gitignore_content


def test_add_unprotected(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    ret = main(["config", "cache.type", "hardlink"])
    assert ret == 0

    ret = main(["add", "foo"])
    assert ret == 0

    assert not os.access("foo", os.W_OK)
    assert system.is_hardlink("foo")

    ret = main(["unprotect", "foo"])
    assert ret == 0

    ret = main(["add", "foo"])
    assert ret == 0

    assert not os.access("foo", os.W_OK)
    assert system.is_hardlink("foo")


@pytest.fixture
def temporary_windows_drive(tmp_path_factory):
    import string
    from ctypes import windll

    try:
        import win32api
        from win32con import DDD_REMOVE_DEFINITION
    except ImportError:
        pytest.skip("pywin32 not installed")

    drives = [
        s[0].upper()
        for s in win32api.GetLogicalDriveStrings().split("\000")
        if len(s) > 0
    ]

    new_drive_name = [
        letter for letter in string.ascii_uppercase if letter not in drives
    ][0]
    new_drive = f"{new_drive_name}:"

    target_path = tmp_path_factory.mktemp("tmp_windows_drive")

    set_up_result = windll.kernel32.DefineDosDeviceW(
        0, new_drive, os.fspath(target_path)
    )
    if set_up_result == 0:
        raise RuntimeError("Failed to mount windows drive!")

    # NOTE: new_drive has form of `A:` and joining it with some relative
    # path might result in non-existing path (A:path\\to)
    yield os.path.join(new_drive, os.sep)

    tear_down_result = windll.kernel32.DefineDosDeviceW(
        DDD_REMOVE_DEFINITION, new_drive, os.fspath(target_path)
    )
    if tear_down_result == 0:
        raise RuntimeError("Could not unmount windows drive!")


@pytest.mark.skipif(os.name != "nt", reason="Windows specific")
def test_windows_should_add_when_cache_on_different_drive(
    tmp_dir, dvc, temporary_windows_drive
):
    dvc.config["cache"]["dir"] = temporary_windows_drive
    dvc.cache = CacheManager(dvc)

    (stage,) = tmp_dir.dvc_gen({"file": "file"})
    cache_path = stage.outs[0].cache_path

    assert path_isin(cache_path, temporary_windows_drive)
    assert os.path.isfile(cache_path)
    filecmp.cmp("file", cache_path)


def test_readding_dir_should_not_unprotect_all(tmp_dir, dvc, mocker):
    tmp_dir.gen("dir/data", "data")

    dvc.cache.local.cache_types = ["symlink"]

    dvc.add("dir")
    tmp_dir.gen("dir/new_file", "new_file_content")

    unprotect_spy = mocker.spy(dvc.cache.local, "unprotect")
    dvc.add("dir")

    assert not unprotect_spy.mock.called
    assert system.is_symlink(os.path.join("dir", "new_file"))


def test_should_not_checkout_when_adding_cached_copy(tmp_dir, dvc, mocker):
    dvc.cache.local.cache_types = ["copy"]

    tmp_dir.dvc_gen({"foo": "foo", "bar": "bar"})

    shutil.copy("bar", "foo")

    copy_spy = mocker.spy(dvc.cache.local.fs, "copy")

    dvc.add("foo")

    assert copy_spy.mock.call_count == 0


@pytest.mark.parametrize(
    "link,new_link,link_test_func",
    [
        ("hardlink", "copy", lambda path: not system.is_hardlink(path)),
        ("symlink", "copy", lambda path: not system.is_symlink(path)),
        ("copy", "hardlink", system.is_hardlink),
        ("copy", "symlink", system.is_symlink),
    ],
)
def test_should_relink_on_repeated_add(link, new_link, link_test_func, tmp_dir, dvc):
    dvc.config["cache"]["type"] = link

    tmp_dir.dvc_gen({"foo": "foo", "bar": "bar"})

    os.remove("foo")
    getattr(dvc.cache.local.fs, link)(
        (tmp_dir / "bar").fs_path, (tmp_dir / "foo").fs_path
    )

    dvc.cache.local.cache_types = [new_link]

    dvc.add("foo")

    assert link_test_func("foo")


@pytest.mark.parametrize("link", ["hardlink", "symlink", "copy"])
def test_should_protect_on_repeated_add(link, tmp_dir, dvc):
    dvc.cache.local.cache_types = [link]

    tmp_dir.dvc_gen({"foo": "foo"})

    dvc.unprotect("foo")

    dvc.add("foo")

    assert not os.access(
        os.path.join(".dvc", "cache", "ac", "bd18db4cc2f85cedef654fccc4a4d8"),
        os.W_OK,
    )

    # NOTE: Windows symlink perms don't propagate to the target
    if link == "copy" or (link == "symlink" and os.name == "nt"):
        assert os.access("foo", os.W_OK)
    else:
        assert not os.access("foo", os.W_OK)


def test_escape_gitignore_entries(tmp_dir, scm, dvc):
    fname = "file!with*weird#naming_[1].t?t"
    ignored_fname = r"/file\!with\*weird\#naming_\[1\].t\?t"

    if os.name == "nt":
        # Some characters are not supported by Windows in the filename
        # https://docs.microsoft.com/en-us/windows/win32/fileio/naming-a-file
        fname = "file!with_weird#naming_[1].txt"
        ignored_fname = r"/file\!with_weird\#naming_\[1\].txt"

    tmp_dir.dvc_gen(fname, "...")
    assert ignored_fname in get_gitignore_content()


def test_add_from_data_dir(tmp_dir, scm, dvc):
    tmp_dir.dvc_gen({"dir": {"file1": "file1 content"}})

    tmp_dir.gen({"dir": {"file2": "file2 content"}})

    dvc.add(os.path.join("dir", "file2"))


def test_add_parent_dir(tmp_dir, scm, dvc):
    tmp_dir.gen({"dir": {"file1": "file1 content"}})
    out_path = os.path.join("dir", "file1")
    dvc.add(out_path, file=out_path + ".dvc")

    with pytest.raises(OverlappingOutputPathsError) as e:
        dvc.add("dir", file="dir.dvc")
    assert str(e.value) == (
        "Cannot add 'dir', because it is overlapping with other DVC "
        "tracked output: '{out}'.\n"
        "To include '{out}' in 'dir', run 'dvc remove {out}.dvc' "
        "and then 'dvc add dir'"
    ).format(out=os.path.join("dir", "file1"))


def test_not_raises_on_re_add(tmp_dir, dvc):
    tmp_dir.dvc_gen("file", "file content")

    tmp_dir.gen({"file2": "file2 content", "file": "modified file"})
    dvc.add(["file2", "file"])


@pytest.mark.parametrize("link", ["hardlink", "symlink", "copy"])
def test_add_empty_files(tmp_dir, dvc, link):
    file = "foo"
    dvc.cache.local.cache_types = [link]
    stages = tmp_dir.dvc_gen(file, "")

    assert (tmp_dir / file).exists()
    assert (tmp_dir / (file + DVC_FILE_SUFFIX)).exists()
    assert os.path.exists(stages[0].outs[0].cache_path)


def test_add_optimization_for_hardlink_on_empty_files(tmp_dir, dvc, mocker):
    dvc.cache.local.cache_types = ["hardlink"]
    tmp_dir.gen({"foo": "", "bar": "", "lorem": "lorem", "ipsum": "ipsum"})
    m = mocker.spy(LocalFileSystem, "is_hardlink")
    stages = dvc.add(["foo", "bar", "lorem", "ipsum"])

    assert m.call_count == 8
    assert m.call_args != call(tmp_dir / "foo")
    assert m.call_args != call(tmp_dir / "bar")

    for stage in stages[:2]:
        # hardlinks are not created for empty files
        assert not system.is_hardlink(stage.outs[0].fs_path)

    for stage in stages[2:]:
        assert system.is_hardlink(stage.outs[0].fs_path)

    for stage in stages:
        assert os.path.exists(stage.path)
        assert os.path.exists(stage.outs[0].cache_path)


def test_try_adding_pipeline_tracked_output(tmp_dir, dvc, run_copy):
    tmp_dir.dvc_gen("foo", "foo")
    run_copy("foo", "bar", name="copy-foo-bar")
    with pytest.raises(DvcException, match="cannot update 'bar': not a data source"):
        dvc.add("bar")


def test_add_pipeline_file(tmp_dir, dvc, run_copy):
    from dvc.dvcfile import PROJECT_FILE

    tmp_dir.dvc_gen("foo", "foo")
    run_copy("foo", "bar", name="copy-foo-bar")

    with pytest.raises(OutputIsStageFileError):
        dvc.add(PROJECT_FILE)


def test_add_symlink_file(tmp_dir, dvc):
    tmp_dir.gen({"dir": {"bar": "bar"}})

    (tmp_dir / "dir" / "foo").symlink_to(os.path.join(".", "bar"))

    dvc.add(os.path.join("dir", "foo"))

    assert not (tmp_dir / "foo.dvc").exists()
    assert (tmp_dir / "dir" / "foo.dvc").exists()
    assert not (tmp_dir / "dir" / "foo").is_symlink()
    assert not (tmp_dir / "dir" / "bar").is_symlink()
    assert (tmp_dir / "dir" / "foo").read_text() == "bar"
    assert (tmp_dir / "dir" / "bar").read_text() == "bar"

    assert (
        tmp_dir / ".dvc" / "cache" / "37" / "b51d194a7513e45b56f6524f2d51f2"
    ).read_text() == "bar"
    assert not (
        tmp_dir / ".dvc" / "cache" / "37" / "b51d194a7513e45b56f6524f2d51f2"
    ).is_symlink()

    # Test that subsequent add succeeds
    # See https://github.com/iterative/dvc/issues/4654
    dvc.add(os.path.join("dir", "foo"))


@pytest.mark.parametrize("external", [True, False])
def test_add_symlink_dir(make_tmp_dir, tmp_dir, dvc, external):
    if external:
        data_dir = make_tmp_dir("data")
        data_dir.gen({"foo": "foo"})
        target = os.fspath(data_dir)
    else:
        tmp_dir.gen({"data": {"foo": "foo"}})
        target = os.path.join(".", "data")

    tmp_dir.gen({"data": {"foo": "foo"}})

    (tmp_dir / "dir").symlink_to(target)

    msg = "Cannot add files inside symlinked directories to DVC"
    with pytest.raises(DvcException, match=msg):
        dvc.add("dir")


@pytest.mark.parametrize("external", [True, False])
def test_add_file_in_symlink_dir(make_tmp_dir, tmp_dir, dvc, external):
    if external:
        data_dir = make_tmp_dir("data")
        data_dir.gen({"dir": {"foo": "foo"}})
        target = os.fspath(data_dir / "dir")
    else:
        tmp_dir.gen({"data": {"foo": "foo"}})
        target = os.path.join(".", "data")

    (tmp_dir / "dir").symlink_to(target)

    msg = "Cannot add files inside symlinked directories to DVC"
    with pytest.raises(DvcException, match=msg):
        dvc.add(os.path.join("dir", "foo"))


def test_add_with_cache_link_error(tmp_dir, dvc, mocker, capsys):
    tmp_dir.gen("foo", "foo")

    mocker.patch(
        "dvc_data.hashfile.checkout.test_links",
        return_value=[],
    )
    dvc.add("foo")
    err = capsys.readouterr()[1]
    assert "reconfigure cache types" in err

    assert (tmp_dir / "foo").exists()
    assert (tmp_dir / "foo.dvc").exists()
    assert (
        tmp_dir / ".dvc" / "cache" / "ac" / "bd18db4cc2f85cedef654fccc4a4d8"
    ).read_text() == "foo"


def test_add_preserve_fields(tmp_dir, dvc):
    text = textwrap.dedent(
        """\
        # top comment
        desc: top desc
        outs:
        - path: foo # out comment
          desc: out desc
          type: mytype
          labels:
          - label1
          - label2
          remote: testremote
        meta: some metadata
    """
    )
    tmp_dir.gen("foo.dvc", text)
    tmp_dir.dvc_gen("foo", "foo")
    assert (tmp_dir / "foo.dvc").read_text() == textwrap.dedent(
        """\
        # top comment
        desc: top desc
        outs:
        - path: foo # out comment
          desc: out desc
          type: mytype
          labels:
          - label1
          - label2
          remote: testremote
          md5: acbd18db4cc2f85cedef654fccc4a4d8
          size: 3
        meta: some metadata
    """
    )


# NOTE: unless long paths are enabled on Windows, PATH_MAX and NAME_MAX
# are the same 260 chars, which makes the test unnecessarily complex
@pytest.mark.skipif(os.name == "nt", reason="unsupported on Windows")
def test_add_long_fname(tmp_dir, dvc):
    name_max = os.pathconf(tmp_dir, "PC_NAME_MAX")
    name = "a" * name_max
    tmp_dir.gen({"data": {name: "foo"}})

    # nothing we can do in this case, as the resulting dvcfile
    # will definitely exceed NAME_MAX
    with pytest.raises(OSError, match=f"File name too long: .*{name}") as info:
        dvc.add(os.path.join("data", name))
    assert info.value.errno == errno.ENAMETOOLONG

    dvc.add("data")
    assert (tmp_dir / "data").read_text() == {name: "foo"}


def test_add_to_remote_absolute(tmp_dir, make_tmp_dir, dvc, remote):
    tmp_abs_dir = make_tmp_dir("abs")
    tmp_foo = tmp_abs_dir / "foo"
    tmp_foo.write_text("foo")

    dvc.add(str(tmp_foo), to_remote=True)
    tmp_foo.unlink()

    foo = tmp_dir / "foo"
    assert foo.with_suffix(".dvc").exists()
    assert not os.path.exists(tmp_foo)

    dvc.pull("foo")
    assert not os.path.exists(tmp_foo)
    assert foo.read_text() == "foo"

    tmp_bar = tmp_abs_dir / "bar"
    with pytest.raises(StageExternalOutputsError):
        dvc.add(str(tmp_foo), out=str(tmp_bar), to_remote=True)


@pytest.mark.parametrize(
    "invalid_opt, kwargs",
    [
        ("multiple targets", {"targets": ["foo", "bar", "baz"]}),
        ("--no-commit", {"targets": ["foo"], "no_commit": True}),
        ("--external", {"targets": ["foo"], "external": True}),
    ],
)
def test_add_to_remote_invalid_combinations(dvc, invalid_opt, kwargs):
    with pytest.raises(InvalidArgumentError, match=invalid_opt):
        dvc.add(to_remote=True, **kwargs)


def test_add_to_cache_dir(tmp_dir, dvc, local_cloud):
    local_cloud.gen({"data": {"foo": "foo", "bar": "bar"}})

    (stage,) = dvc.add(str(local_cloud / "data"), out="data")
    assert len(stage.deps) == 0
    assert len(stage.outs) == 1
    assert stage.outs[0].meta.size == len("foo") + len("bar")
    assert stage.outs[0].meta.nfiles == 2

    data = tmp_dir / "data"
    assert data.read_text() == {"foo": "foo", "bar": "bar"}
    assert (tmp_dir / "data.dvc").exists()

    shutil.rmtree(data)
    status = dvc.checkout(str(data))
    assert status["added"] == ["data" + os.sep]
    assert data.read_text() == {"foo": "foo", "bar": "bar"}


def test_add_to_cache_file(tmp_dir, dvc, local_cloud):
    local_cloud.gen("foo", "foo")

    (stage,) = dvc.add(str(local_cloud / "foo"), out="foo")
    assert len(stage.deps) == 0
    assert len(stage.outs) == 1

    foo = tmp_dir / "foo"
    assert foo.read_text() == "foo"
    assert (tmp_dir / "foo.dvc").exists()

    foo.unlink()
    status = dvc.checkout(str(foo))
    assert status["added"] == ["foo"]
    assert foo.read_text() == "foo"


def test_add_with_out(tmp_dir, scm, dvc):
    tmp_dir.gen({"foo": "foo"})
    dvc.add("foo", out="out_foo")
    gitignore_content = get_gitignore_content()
    assert "/out_foo" in gitignore_content


def test_add_to_cache_different_name(tmp_dir, dvc, local_cloud):
    local_cloud.gen({"data": {"foo": "foo", "bar": "bar"}})

    dvc.add(str(local_cloud / "data"), out="not_data")

    not_data = tmp_dir / "not_data"
    assert not_data.read_text() == {"foo": "foo", "bar": "bar"}
    assert (tmp_dir / "not_data.dvc").exists()

    assert not (tmp_dir / "data").exists()
    assert not (tmp_dir / "data.dvc").exists()

    shutil.rmtree(not_data)
    dvc.checkout(str(not_data))
    assert not_data.read_text() == {"foo": "foo", "bar": "bar"}
    assert not (tmp_dir / "data").exists()


def test_add_to_cache_not_exists(tmp_dir, dvc, local_cloud):
    local_cloud.gen({"data": {"foo": "foo", "bar": "bar"}})

    dest_dir = tmp_dir / "dir" / "that" / "does" / "not" / "exist"
    with pytest.raises(StagePathNotFoundError):
        dvc.add(str(local_cloud / "data"), out=str(dest_dir))

    dest_dir.parent.mkdir(parents=True)
    dvc.add(str(local_cloud / "data"), out=str(dest_dir))

    assert dest_dir.read_text() == {"foo": "foo", "bar": "bar"}
    assert dest_dir.with_suffix(".dvc").exists()


@pytest.mark.parametrize(
    "invalid_opt, kwargs",
    [
        ("multiple targets", {"targets": ["foo", "bar", "baz"]}),
        ("--no-commit", {"targets": ["foo"], "no_commit": True}),
    ],
)
def test_add_to_cache_invalid_combinations(dvc, invalid_opt, kwargs):
    with pytest.raises(InvalidArgumentError, match=invalid_opt):
        dvc.add(out="bar", **kwargs)


def test_add_to_cache_from_remote(tmp_dir, dvc, workspace):
    workspace.gen("foo", "foo")

    url = "remote://workspace/foo"
    dvc.add(url, out="foo")

    foo = tmp_dir / "foo"
    assert foo.read_text() == "foo"
    assert (tmp_dir / "foo.dvc").exists()

    # Change the contents of the remote location, in order to
    # ensure it retrieves file from the cache and not re-fetches it
    (workspace / "foo").write_text("bar")

    foo.unlink()
    dvc.checkout(str(foo))
    assert foo.read_text() == "foo"


def test_add_ignored(tmp_dir, scm, dvc):
    from dvc.dvcfile import FileIsGitIgnored

    tmp_dir.gen({"dir": {"subdir": {"file": "content"}}, ".gitignore": "dir/"})
    with pytest.raises(FileIsGitIgnored) as exc:
        dvc.add(targets=[os.path.join("dir", "subdir")])
    assert str(exc.value) == ("bad DVC file name '{}' is git-ignored.").format(
        os.path.join("dir", "subdir.dvc")
    )


def test_add_on_not_existing_file_should_not_remove_stage_file(tmp_dir, dvc):
    (stage,) = tmp_dir.dvc_gen("foo", "foo")
    (tmp_dir / "foo").unlink()
    dvcfile_contents = (tmp_dir / stage.path).read_text()

    with pytest.raises(OutputDoesNotExistError):
        dvc.add("foo")
    assert (tmp_dir / "foo.dvc").exists()
    assert (tmp_dir / stage.path).read_text() == dvcfile_contents


@pytest.mark.parametrize(
    "target",
    [
        "dvc.repo.index.Index.check_graph",
        "dvc.stage.Stage.add_outs",
    ],
)
def test_add_does_not_remove_stage_file_on_failure(tmp_dir, dvc, mocker, target):
    (stage,) = tmp_dir.dvc_gen("foo", "foo")
    tmp_dir.gen("foo", "foobar")  # update file
    dvcfile_contents = (tmp_dir / stage.path).read_text()

    exc_msg = f"raising error from mocked '{target}'"
    mocker.patch(
        target,
        side_effect=DvcException(exc_msg),
    )

    with pytest.raises(DvcException, match=exc_msg):
        dvc.add("foo")
    assert (tmp_dir / "foo.dvc").exists()
    assert (tmp_dir / stage.path).read_text() == dvcfile_contents


def test_add_updates_to_cloud_versioning_dir(tmp_dir, dvc):
    data_dvc = tmp_dir / "data.dvc"
    data_dvc.dump(
        {
            "outs": [
                {
                    "path": "data",
                    "files": [
                        {
                            "size": 3,
                            "version_id": "WYRG4BglP7pD.gEoJP6a4AqOhl.FRA.h",
                            "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
                            "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
                            "relpath": "bar",
                        },
                        {
                            "size": 3,
                            "version_id": "0vL53tFVY5vVAoJ4HG2jCS1mEcohDPE0",
                            "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
                            "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
                            "relpath": "foo",
                        },
                    ],
                }
            ]
        }
    )

    data = tmp_dir / "data"
    data.mkdir()
    (data / "foo").write_text("foo")
    (data / "bar").write_text("bar2")

    dvc.add("data")

    assert (tmp_dir / "data.dvc").parse() == {
        "outs": [
            {
                "path": "data",
                "files": [
                    {
                        "size": 4,
                        "md5": "224e2539f52203eb33728acd228b4432",
                        "relpath": "bar",
                    },
                    {
                        "size": 3,
                        "version_id": "0vL53tFVY5vVAoJ4HG2jCS1mEcohDPE0",
                        "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
                        "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
                        "relpath": "foo",
                    },
                ],
            }
        ]
    }




tests/func/test_analytics.py
import os
from unittest.mock import call

import pytest

from dvc.analytics import _scm_in_use, collect_and_send_report
from dvc.cli import main
from dvc.repo import Repo
from tests.utils import ANY


def test_daemon_analytics(mocker, tmp_path):
    mock_send = mocker.patch("dvc.analytics.send")
    report = os.fspath(tmp_path)
    assert main(["daemon", "analytics", report]) == 0

    mock_send.assert_called_with(report)


def test_main_analytics(mocker, tmp_dir, dvc):
    mock_is_enabled = mocker.patch("dvc.analytics.collect_and_send_report")
    mock_report = mocker.patch("dvc.analytics.is_enabled", return_value=True)
    tmp_dir.gen("foo", "text")
    assert main(["add", "foo"]) == 0
    assert mock_is_enabled.called
    assert mock_report.called


@pytest.fixture
def mock_daemon(mocker):
    def func(argv):
        return main(["daemon", *argv])

    return mocker.patch("dvc.daemon.daemon", mocker.MagicMock(side_effect=func))


def test_collect_and_send_report(mocker, dvc, mock_daemon):
    mock_post = mocker.patch("requests.post")
    collect_and_send_report()

    assert mock_daemon.call_count == 1
    assert mock_post.call_count == 1
    assert mock_post.call_args == call(
        "https://analytics.dvc.org",
        json=ANY(dict),
        headers={"content-type": "application/json"},
        timeout=5,
    )


def test_scm_dvc_only(tmp_dir, dvc):
    scm = _scm_in_use()
    assert scm == "NoSCM"


def test_scm_git(tmp_dir, scm, dvc):
    scm = _scm_in_use()
    assert scm == "Git"


def test_scm_subrepo(tmp_dir, scm):
    subdir = tmp_dir / "subdir"
    subdir.mkdir()

    with subdir.chdir():
        Repo.init(subdir=True)
        scm = _scm_in_use()

    assert scm == "Git"




tests/func/test_check_ignore.py
import os

import pytest

from dvc.cli import main
from dvc.ignore import DvcIgnore


@pytest.mark.parametrize(
    "file,ret,output", [("ignored", 0, True), ("not_ignored", 1, False)]
)
def test_check_ignore(tmp_dir, dvc, file, ret, output, caplog, capsys):
    tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "ignored")

    assert main(["check-ignore", file]) == ret

    out, _ = capsys.readouterr()
    assert (file in out) is output
    assert "Having any troubles?" not in caplog.text


@pytest.mark.parametrize(
    "file,ret,output",
    [
        ("file", 0, f"{DvcIgnore.DVCIGNORE_FILE}:1:f*\tfile\n"),
        ("foo", 0, f"{DvcIgnore.DVCIGNORE_FILE}:2:!foo\tfoo\n"),
        (
            os.path.join("dir", "foobar"),
            0,
            "{}:1:foobar\t{}\n".format(
                os.path.join("dir", DvcIgnore.DVCIGNORE_FILE),
                os.path.join("dir", "foobar"),
            ),
        ),
    ],
)
def test_check_ignore_details(tmp_dir, dvc, file, ret, output, capsys):
    tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "f*\n!foo")
    tmp_dir.gen({"dir": {DvcIgnore.DVCIGNORE_FILE: "foobar"}})

    assert main(["check-ignore", "-d", file]) == ret
    assert (output, "") == capsys.readouterr()


@pytest.mark.parametrize("non_matching", [True, False])
def test_check_ignore_non_matching(tmp_dir, dvc, non_matching, caplog, capsys):
    tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "other")
    if non_matching:
        assert main(["check-ignore", "-d", "-n", "file"]) == 1
    else:
        assert main(["check-ignore", "-d", "file"]) == 1

    out, _ = capsys.readouterr()
    assert ("::\tfile\n" in out) is non_matching


@pytest.mark.parametrize(
    "args",
    [
        ["-n", "file"],
        ["-a", "file"],
        ["-q", "-d", "file"],
        ["--stdin", "file"],
        [],
    ],
)
def test_check_ignore_error_args_cases(tmp_dir, dvc, args, caplog):
    assert main(["check-ignore", *args]) == 255
    assert "Having any troubles?" not in caplog.text


@pytest.mark.parametrize("path,ret", [({"dir": {}}, 0), ({"dir": "files"}, 1)])
def test_check_ignore_dir(tmp_dir, dvc, path, ret):
    tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "dir/")
    tmp_dir.gen(path)

    assert main(["check-ignore", "-q", "dir"]) == ret


def test_check_ignore_default_dir(tmp_dir, dvc):
    assert main(["check-ignore", "-q", ".dvc"]) == 1


def test_check_ignore_out_side_repo(tmp_dir, dvc):
    tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "file")
    assert main(["check-ignore", "-q", "../file"]) == 1


def test_check_ignore_sub_repo(tmp_dir, dvc, capsys):
    tmp_dir.gen({DvcIgnore.DVCIGNORE_FILE: "other", "dir": {".dvc": {}, "foo": "bar"}})

    assert main(["check-ignore", "-d", os.path.join("dir", "foo")]) == 0
    out, _ = capsys.readouterr()
    assert "in sub_repo:{}\t{}".format("dir", os.path.join("dir", "foo")) in out


def test_check_sub_dir_ignore_file(tmp_dir, dvc, capsys):
    tmp_dir.gen(
        {
            DvcIgnore.DVCIGNORE_FILE: "other",
            "dir": {DvcIgnore.DVCIGNORE_FILE: "bar\nfoo", "foo": "bar"},
        }
    )

    assert main(["check-ignore", "-d", os.path.join("dir", "foo")]) == 0

    out, _ = capsys.readouterr()
    assert (
        "{}:2:foo\t{}".format(
            os.path.join("dir", DvcIgnore.DVCIGNORE_FILE),
            os.path.join("dir", "foo"),
        )
        in out
    )

    sub_dir = tmp_dir / "dir"
    with sub_dir.chdir():
        assert main(["check-ignore", "-d", "foo"]) == 0
        out, _ = capsys.readouterr()
        assert ".dvcignore:2:foo\tfoo" in out


def test_check_ignore_details_all(tmp_dir, dvc, capsys):
    tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "f*\n!foo")

    assert main(["check-ignore", "-d", "-a", "foo"]) == 0
    out, _ = capsys.readouterr()
    assert f"{DvcIgnore.DVCIGNORE_FILE}:1:f*\tfoo\n" in out
    assert f"{DvcIgnore.DVCIGNORE_FILE}:2:!foo\tfoo\n" in out


@pytest.mark.parametrize(
    "file,ret,output", [("ignored", 0, True), ("not_ignored", 1, False)]
)
def test_check_ignore_stdin_mode(tmp_dir, dvc, file, ret, output, capsys, mocker):
    tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "ignored")
    mocker.patch("builtins.input", side_effect=[file, ""])

    assert main(["check-ignore", "--stdin"]) == ret
    out, _ = capsys.readouterr()
    assert (file in out) is output




tests/func/test_checkout.py
import logging
import os
import shutil
import stat
import textwrap

import pytest

from dvc.cli import main
from dvc.dvcfile import PROJECT_FILE, load_file
from dvc.exceptions import (
    CheckoutError,
    CheckoutErrorSuggestGit,
    ConfirmRemoveError,
    NoOutputOrStageError,
)
from dvc.fs import LocalFileSystem, system
from dvc.stage import Stage
from dvc.stage.exceptions import StageFileDoesNotExistError
from dvc.utils import relpath
from dvc.utils.fs import remove
from dvc.utils.serialize import dump_yaml, load_yaml
from tests.utils import get_gitignore_content

logger = logging.getLogger("dvc")


def walk_files(directory):
    for root, _, files in os.walk(directory):
        for f in files:
            yield os.path.join(root, f)


def test_checkout(tmp_dir, dvc, copy_script):
    tmp_dir.dvc_gen({"foo": "foo", "data": {"file": "file"}})
    dvc.run(
        fname="file1.dvc",
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        single_stage=True,
    )
    remove(tmp_dir / "foo")
    remove("data")

    dvc.checkout(force=True)
    assert (tmp_dir / "foo").read_text() == "foo"
    assert (tmp_dir / "data").read_text() == {"file": "file"}


def test_checkout_cli(tmp_dir, dvc, copy_script):
    tmp_dir.dvc_gen({"foo": "foo", "data": {"file": "file"}})
    dvc.run(
        fname="file1.dvc",
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        single_stage=True,
    )
    remove(tmp_dir / "foo")
    remove("data")

    assert main(["checkout", "--force"]) == 0
    assert (tmp_dir / "foo").read_text() == "foo"
    assert (tmp_dir / "data").read_text() == {"file": "file"}

    remove(tmp_dir / "foo")
    remove("data")

    assert main(["checkout", "--force", "foo.dvc"]) == 0
    assert main(["checkout", "--force", "data.dvc"]) == 0
    assert (tmp_dir / "foo").read_text() == "foo"
    assert (tmp_dir / "data").read_text() == {"file": "file"}


def test_checkout_corrupted_cache_file(tmp_dir, dvc):
    (foo_stage,) = tmp_dir.dvc_gen("foo", "foo")
    cache = foo_stage.outs[0].cache_path

    os.chmod(cache, 0o644)
    with open(cache, "a", encoding="utf-8") as fd:
        fd.write("1")

    with pytest.raises(CheckoutError):
        dvc.checkout(force=True)

    assert not os.path.isfile("foo")
    assert not os.path.isfile(cache)


def test_checkout_corrupted_cache_dir(tmp_dir, dvc):
    from dvc_data.hashfile import load

    tmp_dir.gen("data", {"foo": "foo", "bar": "bar"})
    # NOTE: using 'copy' so that cache and link don't have same inode
    ret = main(["config", "cache.type", "copy"])
    assert ret == 0

    dvc.config.load()

    stages = dvc.add("data")
    assert len(stages) == 1
    assert len(stages[0].outs) == 1
    out = stages[0].outs[0]

    # NOTE: modifying cache file for one of the files inside the directory
    # to check if dvc will detect that the cache is corrupted.
    obj = load(dvc.cache.local, out.hash_info)
    _, _, entry_oid = list(obj)[0]
    cache = dvc.cache.local.oid_to_path(entry_oid.value)

    os.chmod(cache, 0o644)
    with open(cache, "w+", encoding="utf-8") as fobj:
        fobj.write("1")

    with pytest.raises(CheckoutError):
        dvc.checkout(force=True)

    assert not os.path.exists(cache)


def test_remove_files_when_checkout(tmp_dir, dvc, scm):
    # add the file into a separate branch
    scm.checkout("branch", True)
    ret = main(["checkout", "--force"])
    assert ret == 0
    tmp_dir.dvc_gen("file_in_a_branch", "random text", commit="add file")

    # Checkout back in master
    scm.checkout("master")
    assert os.path.exists("file_in_a_branch")

    # Make sure `dvc checkout` removes the file
    # self.dvc.checkout()
    ret = main(["checkout", "--force"])
    assert ret == 0
    assert not os.path.exists("file_in_a_branch")


class TestCheckoutCleanWorkingDir:
    def test(self, mocker, tmp_dir, dvc):
        mock_prompt = mocker.patch("dvc.prompt.confirm", return_value=True)
        (stage,) = tmp_dir.dvc_gen("data", {"foo": "foo"})

        # change working directory
        (tmp_dir / "data").gen("not_cached.txt", "not_cached")
        assert main(["checkout", stage.relpath]) == 0
        mock_prompt.assert_called()
        assert not (tmp_dir / "data" / "not_cached.txt").exists()

    def test_force(self, mocker, tmp_dir, dvc):
        mock_prompt = mocker.patch("dvc.prompt.confirm", return_value=False)
        (stage,) = tmp_dir.dvc_gen("data", {"foo": "foo"})

        # change working directory
        (tmp_dir / "data").gen("not_cached.txt", "not_cached")
        assert main(["checkout", stage.relpath]) != 0
        mock_prompt.assert_called()
        assert (tmp_dir / "data" / "not_cached.txt").exists()


def test_checkout_selective_remove(tmp_dir, dvc):
    # Use copy to test for changes in the inodes
    dvc.cache.local.cache_types = ["copy"]
    tmp_dir.dvc_gen({"data": {"foo": "foo", "bar": "bar"}})

    foo_inode = system.inode(os.path.join("data", "foo"))
    bar_inode = system.inode(os.path.join("data", "bar"))
    # move instead of remove, to lock inode assigned to stage_files[0].path
    # if we were to use remove, we might end up with same inode assigned to
    # newly checked out file
    shutil.move(os.path.join("data", "foo"), "random_name")

    assert main(["checkout", "--force", "data.dvc"]) == 0
    assert (tmp_dir / "data").read_text() == {"foo": "foo", "bar": "bar"}
    assert system.inode(os.path.join("data", "foo")) != foo_inode
    assert system.inode(os.path.join("data", "bar")) == bar_inode


def test_gitignore_basic(tmp_dir, dvc, scm):
    tmp_dir.gen("foo", "foo")
    assert not os.path.exists(scm.GITIGNORE)

    tmp_dir.dvc_gen("file1", "random text1", commit="add file1")
    tmp_dir.dvc_gen("file2", "random text2", commit="add file2")
    dvc.run(
        single_stage=True,
        cmd="cp foo file3",
        deps=["foo"],
        outs_no_cache=["file3"],
    )
    assert get_gitignore_content() == ["/file1", "/file2"]


def test_gitignore_when_checkout(tmp_dir, dvc, scm):
    tmp_dir.dvc_gen("file_in_a_master", "master", commit="master")

    scm.checkout("branch", True)
    ret = main(["checkout", "--force"])
    assert ret == 0
    tmp_dir.dvc_gen("file_in_a_branch", "branch", commit="branch")

    scm.checkout("master")
    ret = main(["checkout", "--force"])
    assert ret == 0

    ignored = get_gitignore_content()

    assert len(ignored) == 1
    assert "/file_in_a_master" in ignored

    scm.checkout("branch")
    ret = main(["checkout", "--force"])
    assert ret == 0
    ignored = get_gitignore_content()
    assert "/file_in_a_branch" in ignored


def test_checkout_missing_md5_in_stage_file(tmp_dir, dvc, copy_script):
    tmp_dir.dvc_gen({"foo": "foo", "data": {"file": "file"}})
    stage = dvc.run(
        fname="file1.dvc",
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        single_stage=True,
    )
    d = load_yaml(stage.relpath)
    del d[Stage.PARAM_OUTS][0][LocalFileSystem.PARAM_CHECKSUM]
    del d[Stage.PARAM_DEPS][0][LocalFileSystem.PARAM_CHECKSUM]
    dump_yaml(stage.relpath, d)

    with pytest.raises(CheckoutError):
        dvc.checkout(force=True)


def test_checkout_empty_dir(tmp_dir, dvc):
    empty_dir = tmp_dir / "empty_dir"
    empty_dir.mkdir()
    (stage,) = dvc.add("empty_dir")

    stage.outs[0].remove()
    assert not empty_dir.exists()

    stats = dvc.checkout(force=True)
    assert stats["added"] == [os.path.join("empty_dir", "")]
    assert empty_dir.is_dir()
    assert not list(empty_dir.iterdir())


def test_checkout_not_cached_file(tmp_dir, dvc):
    tmp_dir.dvc_gen("foo", "foo")
    dvc.run(
        cmd="cp foo bar",
        deps=["foo"],
        outs_no_cache=["bar"],
        single_stage=True,
    )
    stats = dvc.checkout(force=True)
    assert not any(stats.values())


def test_checkout_with_deps_cli(tmp_dir, dvc, copy_script):
    tmp_dir.dvc_gen({"foo": "foo", "data": {"file": "file"}})
    dvc.run(
        fname="file1.dvc",
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        single_stage=True,
    )
    remove("foo")
    remove("file1")

    assert not os.path.exists("foo")
    assert not os.path.exists("file1")

    ret = main(["checkout", "--force", "file1.dvc", "--with-deps"])
    assert ret == 0

    assert os.path.exists("foo")
    assert os.path.exists("file1")


def test_checkout_directory(tmp_dir, dvc):
    (stage,) = tmp_dir.dvc_gen({"data": {"foo": "foo", "bar": "bar"}})

    remove("data")
    assert not os.path.exists("data")

    ret = main(["checkout", stage.path])
    assert ret == 0

    assert os.path.exists("data")


def test_checkout_hook(mocker, tmp_dir, dvc):
    """Test that dvc checkout handles EOFError gracefully, which is what
    it will experience when running in a git hook.
    """
    tmp_dir.dvc_gen({"data": {"foo": "foo"}})
    mocker.patch("sys.stdout.isatty", return_value=True)
    mocker.patch("dvc.prompt.input", side_effect=EOFError)

    (tmp_dir / "data").gen("test", "test")
    with pytest.raises(ConfirmRemoveError):
        dvc.checkout()


def test_checkout_suggest_git(tmp_dir, dvc, scm):
    with pytest.raises(CheckoutErrorSuggestGit) as e:
        dvc.checkout(targets="gitbranch")
    assert isinstance(e.value.__cause__, NoOutputOrStageError)
    assert isinstance(e.value.__cause__.__cause__, StageFileDoesNotExistError)

    with pytest.raises(CheckoutErrorSuggestGit) as e:
        dvc.checkout(targets="foobar")
    assert isinstance(e.value.__cause__, NoOutputOrStageError)
    assert isinstance(e.value.__cause__.__cause__, StageFileDoesNotExistError)

    with pytest.raises(CheckoutErrorSuggestGit) as e:
        dvc.checkout(targets="looks-like-dvcfile.dvc")
    assert isinstance(e.value.__cause__, StageFileDoesNotExistError)
    assert e.value.__cause__.__cause__ is None


def test_checkout_target_recursive_should_not_remove_other_used_files(tmp_dir, dvc):
    tmp_dir.dvc_gen({"foo": "foo", "bar": "bar", "data": {"file": "file"}})
    assert main(["checkout", "-R", "data"]) == 0
    assert (tmp_dir / "foo").exists()
    assert (tmp_dir / "bar").exists()


def test_checkout_recursive_not_directory(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    ret = main(["add", "foo"])
    assert ret == 0

    stats = dvc.checkout(targets=["foo.dvc"], recursive=True)
    assert stats == {"added": [], "modified": [], "deleted": []}


def test_checkout_moved_cache_dir_with_symlinks(tmp_dir, dvc):
    tmp_dir.gen({"foo": "foo", "data": {"file": "file"}})
    ret = main(["config", "cache.type", "symlink"])
    assert ret == 0

    ret = main(["add", "foo"])
    assert ret == 0

    ret = main(["add", "data"])
    assert ret == 0

    assert system.is_symlink("foo")
    old_foo_link = os.path.realpath("foo")

    assert system.is_symlink(os.path.join("data", "file"))
    old_data_link = os.path.realpath(os.path.join("data", "file"))

    old_cache_dir = dvc.cache.local.path
    new_cache_dir = old_cache_dir + "_new"
    os.rename(old_cache_dir, new_cache_dir)

    ret = main(["cache", "dir", new_cache_dir])
    assert ret == 0

    ret = main(["checkout", "-f"])
    assert ret == 0

    assert system.is_symlink("foo")
    new_foo_link = os.path.realpath("foo")

    assert system.is_symlink(os.path.join("data", "file"))
    new_data_link = os.path.realpath(os.path.join("data", "file"))

    assert relpath(old_foo_link, old_cache_dir) == relpath(new_foo_link, new_cache_dir)

    assert relpath(old_data_link, old_cache_dir) == relpath(
        new_data_link, new_cache_dir
    )


def test_checkout_no_checksum(tmp_dir, dvc):
    tmp_dir.gen("file", "file content")
    stage = dvc.run(outs=["file"], no_exec=True, cmd="somecmd", single_stage=True)

    with pytest.raises(CheckoutError):
        dvc.checkout([stage.path], force=True)

    assert not os.path.exists("file")


@pytest.mark.parametrize(
    "link, link_test_func",
    [("hardlink", system.is_hardlink), ("symlink", system.is_symlink)],
)
def test_checkout_relink(tmp_dir, dvc, link, link_test_func):
    dvc.cache.local.cache_types = [link]

    tmp_dir.dvc_gen({"dir": {"data": "text"}})
    dvc.unprotect("dir/data")
    assert not link_test_func("dir/data")

    stats = dvc.checkout(["dir.dvc"], relink=True)
    assert stats == empty_checkout
    assert link_test_func("dir/data")


@pytest.mark.parametrize("link", ["hardlink", "symlink", "copy"])
def test_checkout_relink_protected(tmp_dir, dvc, link):
    dvc.cache.local.cache_types = [link]

    tmp_dir.dvc_gen("foo", "foo")
    dvc.unprotect("foo")
    assert os.access("foo", os.W_OK)

    stats = dvc.checkout(["foo.dvc"], relink=True)
    assert stats == empty_checkout

    # NOTE: Windows symlink perms don't propagate to the target
    if link == "copy" or (link == "symlink" and os.name == "nt"):
        assert os.access("foo", os.W_OK)
    else:
        assert not os.access("foo", os.W_OK)


@pytest.mark.parametrize(
    "target",
    [os.path.join("dir", "subdir"), os.path.join("dir", "subdir", "file")],
)
def test_partial_checkout(tmp_dir, dvc, target):
    tmp_dir.dvc_gen({"dir": {"subdir": {"file": "file"}, "other": "other"}})
    shutil.rmtree("dir")
    stats = dvc.checkout([target])
    assert stats["added"] == ["dir" + os.sep]
    assert list(walk_files("dir")) == [os.path.join("dir", "subdir", "file")]


empty_checkout = {"added": [], "deleted": [], "modified": []}


def test_stats_on_empty_checkout(tmp_dir, dvc, scm):
    assert dvc.checkout() == empty_checkout
    tmp_dir.dvc_gen(
        {"dir": {"subdir": {"file": "file"}, "other": "other"}},
        commit="initial",
    )
    assert dvc.checkout() == empty_checkout


def test_stats_on_checkout(tmp_dir, dvc, scm):
    tmp_dir.dvc_gen(
        {
            "dir": {"subdir": {"file": "file"}, "other": "other"},
            "foo": "foo",
            "bar": "bar",
        },
        commit="initial",
    )
    scm.checkout("HEAD~")
    stats = dvc.checkout()
    assert set(stats["deleted"]) == {"dir" + os.sep, "foo", "bar"}

    scm.checkout("-")
    stats = dvc.checkout()
    assert set(stats["added"]) == {"bar", "dir" + os.sep, "foo"}

    tmp_dir.gen({"lorem": "lorem", "bar": "new bar", "dir2": {"file": "file"}})
    (tmp_dir / "foo").unlink()
    scm.gitpython.repo.git.rm("foo.dvc")
    tmp_dir.dvc_add(["bar", "lorem", "dir2"], commit="second")

    scm.checkout("HEAD~")
    stats = dvc.checkout()
    assert set(stats["modified"]) == {"bar"}
    assert set(stats["added"]) == {"foo"}
    assert set(stats["deleted"]) == {"lorem", "dir2" + os.sep}

    scm.checkout("-")
    stats = dvc.checkout()
    assert set(stats["modified"]) == {"bar"}
    assert set(stats["added"]) == {"dir2" + os.sep, "lorem"}
    assert set(stats["deleted"]) == {"foo"}


@pytest.mark.xfail(reason="values relpath")
def test_checkout_stats_on_failure(tmp_dir, dvc, scm):
    tmp_dir.dvc_gen(
        {"foo": "foo", "dir": {"subdir": {"file": "file"}}, "other": "other"},
        commit="initial",
    )
    stage = load_file(dvc, "foo.dvc").stage
    tmp_dir.dvc_gen({"foo": "foobar", "other": "other other"}, commit="second")

    # corrupt cache
    cache = stage.outs[0].cache_path
    os.chmod(cache, 0o644)
    with open(cache, "a", encoding="utf-8") as fd:
        fd.write("destroy cache")

    scm.checkout("HEAD~")
    with pytest.raises(CheckoutError) as exc:
        dvc.checkout(force=True)

    assert exc.value.stats == {
        **empty_checkout,
        "failed": ["foo"],
        "modified": ["other"],
    }


def test_stats_on_added_file_from_tracked_dir(tmp_dir, dvc, scm):
    tmp_dir.dvc_gen(
        {"dir": {"subdir": {"file": "file"}, "other": "other"}},
        commit="initial",
    )

    tmp_dir.gen("dir/subdir/newfile", "newfile")
    tmp_dir.dvc_add("dir", commit="add newfile")
    scm.checkout("HEAD~")
    assert dvc.checkout() == {**empty_checkout, "modified": ["dir" + os.sep]}
    assert dvc.checkout() == empty_checkout

    scm.checkout("-")
    assert dvc.checkout() == {**empty_checkout, "modified": ["dir" + os.sep]}
    assert dvc.checkout() == empty_checkout


def test_stats_on_updated_file_from_tracked_dir(tmp_dir, dvc, scm):
    tmp_dir.dvc_gen(
        {"dir": {"subdir": {"file": "file"}, "other": "other"}},
        commit="initial",
    )

    tmp_dir.gen("dir/subdir/file", "what file?")
    tmp_dir.dvc_add("dir", commit="update file")
    scm.checkout("HEAD~")
    assert dvc.checkout() == {**empty_checkout, "modified": ["dir" + os.sep]}
    assert dvc.checkout() == empty_checkout

    scm.checkout("-")
    assert dvc.checkout() == {**empty_checkout, "modified": ["dir" + os.sep]}
    assert dvc.checkout() == empty_checkout


def test_stats_on_removed_file_from_tracked_dir(tmp_dir, dvc, scm):
    tmp_dir.dvc_gen(
        {"dir": {"subdir": {"file": "file"}, "other": "other"}},
        commit="initial",
    )

    (tmp_dir / "dir" / "subdir" / "file").unlink()
    tmp_dir.dvc_add("dir", commit="removed file from subdir")
    scm.checkout("HEAD~")
    assert dvc.checkout() == {**empty_checkout, "modified": ["dir" + os.sep]}
    assert dvc.checkout() == empty_checkout

    scm.checkout("-")
    assert dvc.checkout() == {**empty_checkout, "modified": ["dir" + os.sep]}
    assert dvc.checkout() == empty_checkout


def test_stats_on_show_changes_does_not_show_summary(tmp_dir, dvc, scm, capsys):
    tmp_dir.dvc_gen(
        {"dir": {"subdir": {"file": "file"}}, "other": "other"},
        commit="initial",
    )
    scm.checkout("HEAD~")

    assert main(["checkout"]) == 0

    out, _ = capsys.readouterr()
    assert out.splitlines() == [
        f"D\tdir{os.sep}".expandtabs(),
        "D\tother".expandtabs(),
    ]


def test_stats_does_not_show_changes_by_default(tmp_dir, dvc, scm, capsys):
    tmp_dir.dvc_gen(
        {"dir": {"subdir": {"file": "file"}}, "other": "other"},
        commit="initial",
    )
    scm.checkout("HEAD~")

    assert main(["checkout", "--summary"]) == 0

    out, _ = capsys.readouterr()
    assert out.rstrip() == "2 files deleted"


@pytest.mark.parametrize("link", ["hardlink", "symlink", "copy"])
def test_checkout_with_relink_existing(tmp_dir, dvc, link):
    tmp_dir.dvc_gen("foo", "foo")
    (tmp_dir / "foo").unlink()

    tmp_dir.dvc_gen("bar", "bar")
    dvc.cache.local.cache_types = [link]

    stats = dvc.checkout(relink=True)
    assert stats == {**empty_checkout, "added": ["foo"]}


def test_checkout_with_deps(tmp_dir, dvc):
    tmp_dir.dvc_gen({"foo": "foo"})
    dvc.run(
        fname="copy_file.dvc",
        cmd="echo foo > bar",
        outs=["bar"],
        deps=["foo"],
        single_stage=True,
    )

    (tmp_dir / "bar").unlink()
    (tmp_dir / "foo").unlink()

    stats = dvc.checkout(["copy_file.dvc"], with_deps=False)
    assert stats == {**empty_checkout, "added": ["bar"]}

    (tmp_dir / "bar").unlink()
    stats = dvc.checkout(["copy_file.dvc"], with_deps=True)
    assert set(stats["added"]) == {"foo", "bar"}


def test_checkout_recursive(tmp_dir, dvc):
    tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})
    dvc.add("dir/*", glob=True)

    (tmp_dir / "dir" / "foo").unlink()
    (tmp_dir / "dir" / "bar").unlink()

    stats = dvc.checkout(["dir"], recursive=True)
    assert set(stats["added"]) == {
        os.path.join("dir", "foo"),
        os.path.join("dir", "bar"),
    }


def test_checkout_for_external_outputs(tmp_dir, dvc, workspace):
    workspace.gen("foo", "foo")

    file_path = workspace / "foo"
    dvc.add("remote://workspace/foo")

    odb = dvc.cloud.get_remote_odb("workspace")
    odb.fs.remove(str(file_path))
    assert not file_path.exists()

    stats = dvc.checkout(force=True)
    assert stats == {**empty_checkout, "added": ["remote://workspace/foo"]}
    assert file_path.exists()

    workspace.gen("foo", "foo\nfoo")

    stats = dvc.checkout(force=True)
    assert stats == {**empty_checkout, "modified": ["remote://workspace/foo"]}
    assert file_path.read_text() == "foo"


def test_checkouts_with_different_addressing(tmp_dir, dvc, run_copy):
    tmp_dir.gen({"foo": "foo", "lorem": "lorem"})
    run_copy("foo", "bar", name="copy-foo-bar")
    run_copy("lorem", "ipsum", name="copy-lorem-ipsum")

    (tmp_dir / "bar").unlink()
    (tmp_dir / "ipsum").unlink()
    assert set(dvc.checkout(PROJECT_FILE)["added"]) == {"bar", "ipsum"}

    (tmp_dir / "bar").unlink()
    (tmp_dir / "ipsum").unlink()
    assert set(dvc.checkout(":")["added"]) == {"bar", "ipsum"}

    (tmp_dir / "bar").unlink()
    assert dvc.checkout("copy-foo-bar")["added"] == ["bar"]

    (tmp_dir / "bar").unlink()
    assert dvc.checkout("dvc.yaml:copy-foo-bar")["added"] == ["bar"]

    (tmp_dir / "bar").unlink()
    assert dvc.checkout(":copy-foo-bar")["added"] == ["bar"]

    (tmp_dir / "bar").unlink()
    (tmp_dir / "data").mkdir()
    with (tmp_dir / "data").chdir():
        assert dvc.checkout(relpath(tmp_dir / "dvc.yaml") + ":copy-foo-bar")[
            "added"
        ] == [relpath(tmp_dir / "bar")]

    (tmp_dir / "bar").unlink()
    assert dvc.checkout("bar")["added"] == ["bar"]


def test_checkouts_on_same_stage_name_and_output_name(tmp_dir, dvc, run_copy):
    tmp_dir.gen("foo", "foo")
    run_copy("foo", "bar", name="copy-foo-bar")
    run_copy("foo", "copy-foo-bar", name="make_collision")

    (tmp_dir / "bar").unlink()
    (tmp_dir / "copy-foo-bar").unlink()
    assert dvc.checkout("copy-foo-bar")["added"] == ["bar"]
    assert dvc.checkout("./copy-foo-bar")["added"] == ["copy-foo-bar"]


def test_checkouts_for_pipeline_tracked_outs(tmp_dir, dvc, scm, run_copy):
    tmp_dir.gen("foo", "foo")
    stage1 = run_copy("foo", "bar", name="copy-foo-bar")
    tmp_dir.gen("lorem", "lorem")
    stage2 = run_copy("lorem", "ipsum", name="copy-lorem-ipsum")

    for out in ["bar", "ipsum"]:
        (tmp_dir / out).unlink()
    assert dvc.checkout(["bar"])["added"] == ["bar"]

    (tmp_dir / "bar").unlink()
    assert set(dvc.checkout([PROJECT_FILE])["added"]) == {"bar", "ipsum"}

    for out in ["bar", "ipsum"]:
        (tmp_dir / out).unlink()
    assert set(dvc.checkout([stage1.addressing])["added"]) == {"bar"}

    (tmp_dir / "bar").unlink()
    assert set(dvc.checkout([stage2.addressing])["added"]) == {"ipsum"}

    (tmp_dir / "ipsum").unlink()
    assert set(dvc.checkout()["added"]) == {"bar", "ipsum"}


def test_checkout_external_modified_file(tmp_dir, dvc, scm, mocker, workspace):
    # regression: happened when file in external output changed and checkout
    # was attempted without force, dvc checks if it's present in its cache
    # before asking user to remove it.
    workspace.gen("foo", "foo")
    dvc.add("remote://workspace/foo", external=True)
    scm.add(["foo.dvc"])
    scm.commit("add foo")

    workspace.gen("foo", "foobar")  # messing up the external outputs
    mocker.patch("dvc.prompt.confirm", return_value=True)
    dvc.checkout()

    assert (workspace / "foo").read_text() == "foo"


def test_checkout_executable(tmp_dir, dvc):
    tmp_dir.dvc_gen("foo", "foo")

    contents = (tmp_dir / "foo.dvc").parse()
    contents["outs"][0]["isexec"] = True
    (tmp_dir / "foo.dvc").dump(contents)

    dvc.checkout("foo")

    isexec = os.stat("foo").st_mode & stat.S_IEXEC
    if os.name == "nt":
        # NOTE: you can't set exec bits on Windows
        assert not isexec
    else:
        assert isexec


def test_checkout_partial(tmp_dir, dvc):
    tmp_dir.dvc_gen({"data": {"foo": "foo", "bar": "bar", "sub_dir": {"baz": "baz"}}})

    data_dir = tmp_dir / "data"
    shutil.rmtree(data_dir)

    dvc.checkout(str(data_dir / "foo"))
    assert data_dir.read_text() == {"foo": "foo"}

    dvc.checkout(str(data_dir / "sub_dir" / "baz"))
    assert data_dir.read_text() == {"foo": "foo", "sub_dir": {"baz": "baz"}}

    dvc.checkout(str(data_dir / "bar"))
    assert data_dir.read_text() == {
        "foo": "foo",
        "bar": "bar",
        "sub_dir": {"baz": "baz"},
    }


def test_checkout_partial_unchanged(tmp_dir, dvc):
    original_dir_shape = {
        "foo": "foo",
        "bar": "bar",
        "sub_dir": {"baz": "baz"},
        "empty_sub_dir": {},
    }
    tmp_dir.dvc_gen({"data": original_dir_shape})

    data_dir = tmp_dir / "data"
    sub_dir = data_dir / "sub_dir"
    foo = data_dir / "foo"
    bar = data_dir / "bar"
    sub_dir_file = sub_dir / "baz"

    # Nothing changed, nothing added/deleted/modified
    stats = dvc.checkout(str(bar))
    assert not any(stats.values())

    # Irrelevant file changed, still nothing added/deleted/modified
    foo.unlink()
    stats = dvc.checkout(str(bar))
    assert not any(stats.values())

    # Relevant change, one modified
    bar.unlink()
    stats = dvc.checkout(str(bar))
    assert len(stats["modified"]) == 1

    # No changes inside data/sub
    stats = dvc.checkout(str(sub_dir))
    assert not any(stats.values())

    # Relevant change, one modified
    sub_dir_file.unlink()
    stats = dvc.checkout(str(sub_dir))
    assert len(stats["modified"]) == 1

    stats = dvc.checkout(str(data_dir / "empty_sub_dir"))
    assert not any(stats.values())

    dvc.checkout(str(data_dir))

    # Everything is in place, no action taken
    stats = dvc.checkout(str(data_dir))
    assert not any(stats.values())


def test_checkout_partial_subdir(tmp_dir, dvc):
    tmp_dir.dvc_gen({"data": {"foo": "foo", "sub_dir": {"bar": "bar", "baz": "baz"}}})

    data_dir = tmp_dir / "data"
    sub_dir = data_dir / "sub_dir"
    sub_dir_bar = sub_dir / "baz"

    shutil.rmtree(sub_dir)
    dvc.checkout(str(sub_dir))
    assert data_dir.read_text() == {
        "foo": "foo",
        "sub_dir": {"bar": "bar", "baz": "baz"},
    }

    sub_dir_bar.unlink()
    dvc.checkout(str(sub_dir_bar))
    assert data_dir.read_text() == {
        "foo": "foo",
        "sub_dir": {"bar": "bar", "baz": "baz"},
    }


def test_checkout_file(tmp_dir, dvc):
    tmp_dir.dvc_gen("foo", "foo")

    stats = dvc.checkout("foo")
    assert not any(stats.values())

    os.unlink("foo")
    stats = dvc.checkout("foo")
    assert stats["added"] == ["foo"]


def test_checkout_dir_compat(tmp_dir, dvc):
    (stage,) = tmp_dir.dvc_gen({"data": {"foo": "foo"}})
    tmp_dir.gen(
        "data.dvc",
        textwrap.dedent(
            f"""\
        outs:
        - md5: {stage.outs[0].hash_info.value}
          path: data
        """
        ),
    )
    remove("data")
    dvc.checkout()
    assert (tmp_dir / "data").read_text() == {"foo": "foo"}




tests/func/test_cli.py
import os

import pytest

from dvc.cli import DvcParserError, parse_args
from dvc.cli.command import CmdBase
from dvc.commands.add import CmdAdd
from dvc.commands.checkout import CmdCheckout
from dvc.commands.config import CmdConfig
from dvc.commands.data_sync import CmdDataPull, CmdDataPush
from dvc.commands.init import CmdInit
from dvc.commands.remove import CmdRemove
from dvc.commands.repro import CmdRepro
from dvc.commands.status import CmdDataStatus
from dvc.exceptions import NotDvcRepoError


def test_argparse(dvc):
    args = parse_args(["init"])
    assert isinstance(args.func(args), CmdInit)


def test_pull(dvc):
    args = parse_args(["pull"])
    cmd = args.func(args)
    assert isinstance(cmd, CmdDataPull)

    cmd.repo.close()


def test_push(dvc):
    args = parse_args(["push"])
    cmd = args.func(args)
    assert isinstance(cmd, CmdDataPush)

    cmd.repo.close()


def test_status(dvc):
    args = parse_args(["status"])
    cmd = args.func(args)
    assert isinstance(cmd, CmdDataStatus)

    cmd.repo.close()


def test_repro(dvc):
    target1 = "1"
    target2 = "2"

    args = parse_args(
        ["repro", target1, target2, "-f", "--force", "-s", "--single-item"]
    )

    cmd = args.func(args)
    assert isinstance(cmd, CmdRepro)
    assert args.targets == [target1, target2]
    assert args.force
    assert args.single_item

    cmd.repo.close()


def test_remove(dvc):
    target1 = "1"
    target2 = "2"

    args = parse_args(["remove", target1, target2])

    cmd = args.func(args)
    assert isinstance(cmd, CmdRemove)
    assert args.targets == [target1, target2]

    cmd.repo.close()


def test_add(dvc):
    target1 = "1"
    target2 = "2"

    args = parse_args(["add", target1, target2])

    cmd = args.func(args)
    assert isinstance(cmd, CmdAdd)
    assert args.targets == [target1, target2]

    cmd.repo.close()


def test_config_unset(dvc):
    name = "section.option"
    value = "1"

    args = parse_args(["config", "-u", "--unset", name, value])

    cmd = args.func(args)
    assert isinstance(cmd, CmdConfig)
    assert args.unset
    assert args.name == (False, "section", "option")
    assert args.value == value


def test_config_list():
    args = parse_args(["config", "--list"])

    assert args.list
    assert args.name is None
    assert args.value is None


def test_checkout(dvc):
    args = parse_args(["checkout"])
    cmd = args.func(args)
    assert isinstance(cmd, CmdCheckout)

    cmd.repo.close()


def test_find_root(dvc):
    class Cmd(CmdBase):
        def run(self):
            pass

    class A:
        quiet = False
        verbose = True
        cd = os.path.pardir

    args = A()
    with pytest.raises(NotDvcRepoError):
        Cmd(args)


def test_cd(dvc):
    class Cmd(CmdBase):
        def run(self):
            pass

    class A:
        quiet = False
        verbose = True
        cd = os.path.pardir

    parent_dir = os.path.realpath(os.path.pardir)
    args = A()
    with pytest.raises(NotDvcRepoError):
        Cmd(args)
    current_dir = os.path.realpath(os.path.curdir)
    assert parent_dir == current_dir


def test_unknown_command_help(capsys):
    try:
        _ = parse_args(["unknown"])
    except DvcParserError:
        pass
    captured = capsys.readouterr()
    output = captured.out
    try:
        _ = parse_args(["--help"])
    except SystemExit:
        pass
    captured = capsys.readouterr()
    help_output = captured.out
    assert output == help_output


def test_unknown_subcommand_help(capsys):
    sample_subcommand = "push"
    try:
        _ = parse_args([sample_subcommand, "--unknown"])
    except DvcParserError:
        pass
    captured = capsys.readouterr()
    output = captured.out
    try:
        _ = parse_args([sample_subcommand, "--help"])
    except SystemExit:
        pass
    captured = capsys.readouterr()
    help_output = captured.out
    assert output == help_output




tests/func/test_commit.py
import os
import textwrap

import pytest

from dvc.dependency.base import DependencyDoesNotExistError
from dvc.dvcfile import PROJECT_FILE
from dvc.fs import localfs
from dvc.output import OutputDoesNotExistError
from dvc.stage.exceptions import StageCommitError


def test_commit_recursive(tmp_dir, dvc):
    tmp_dir.gen({"dir": {"file": "text1", "subdir": {"file2": "text2"}}})
    stages = dvc.add(localfs.find("dir"), no_commit=True)

    assert len(stages) == 2
    assert dvc.status() != {}

    dvc.commit("dir", recursive=True)
    assert dvc.status() == {}


def test_commit_force(tmp_dir, dvc):
    tmp_dir.gen({"dir": {"file": "text1", "file2": "text2"}})
    (stage,) = dvc.add("dir", no_commit=True)

    assert stage.outs[0].changed_cache()

    tmp_dir.gen("dir/file", "file content modified")

    assert stage.outs[0].changed_cache()

    with pytest.raises(StageCommitError):
        dvc.commit(stage.path)

    assert stage.outs[0].changed_cache()

    dvc.commit(stage.path, force=True)
    assert dvc.status([stage.path]) == {}


def test_commit_preserve_fields(tmp_dir, dvc):
    text = textwrap.dedent(
        """\
        # top comment
        desc: top desc
        outs:
        - path: foo # out comment
          desc: out desc
          type: mytype
          labels:
          - label1
          - label2
          meta:
            key1: value1
            key2: value2
          remote: testremote
        meta: some metadata
    """
    )
    tmp_dir.gen("foo.dvc", text)
    tmp_dir.dvc_gen("foo", "foo", commit=False)
    dvc.commit("foo")
    assert (tmp_dir / "foo.dvc").read_text() == textwrap.dedent(
        """\
        # top comment
        desc: top desc
        outs:
        - path: foo # out comment
          desc: out desc
          type: mytype
          labels:
          - label1
          - label2
          meta:
            key1: value1
            key2: value2
          remote: testremote
          md5: acbd18db4cc2f85cedef654fccc4a4d8
          size: 3
        meta: some metadata
    """
    )


@pytest.mark.parametrize("run_kw", [{"single_stage": True}, {"name": "copy"}])
def test_commit_with_deps(tmp_dir, dvc, run_copy, run_kw):
    tmp_dir.gen("foo", "foo")
    (foo_stage,) = dvc.add("foo", no_commit=True)
    assert foo_stage is not None
    assert len(foo_stage.outs) == 1

    stage = run_copy("foo", "file", no_commit=True, **run_kw)
    assert stage is not None
    assert len(stage.outs) == 1

    assert foo_stage.outs[0].changed_cache()
    assert stage.outs[0].changed_cache()

    dvc.commit(stage.path, with_deps=True)
    assert not foo_stage.outs[0].changed_cache()
    assert not stage.outs[0].changed_cache()


def test_commit_changed_md5(tmp_dir, dvc):
    tmp_dir.gen({"file": "file content"})
    (stage,) = dvc.add("file", no_commit=True)

    stage_file_content = (tmp_dir / stage.path).parse()
    stage_file_content["md5"] = "1111111111"
    (tmp_dir / stage.path).dump(stage_file_content)

    with pytest.raises(StageCommitError):
        dvc.commit(stage.path)

    dvc.commit(stage.path, force=True)
    assert "md5" not in (tmp_dir / stage.path).parse()


def test_commit_no_exec(tmp_dir, dvc):
    tmp_dir.gen({"dep": "dep", "out": "out"})
    stage = dvc.run(name="my", cmd="mycmd", deps=["dep"], outs=["out"], no_exec=True)

    assert dvc.status(stage.path)
    dvc.commit(stage.path, force=True)
    assert dvc.status(stage.path) == {}


def test_commit_granular_output(tmp_dir, dvc):
    dvc.run(
        name="mystage",
        cmd=["echo foo>foo", "echo bar>bar"],
        outs=["foo", "bar"],
        no_commit=True,
    )

    cache = tmp_dir / ".dvc" / "cache"
    assert not list(cache.glob("*/*"))

    dvc.commit("foo")
    assert list(cache.glob("*/*")) == [cache / "d3" / "b07384d113edec49eaa6238ad5ff00"]


def test_commit_granular_output_file(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    dvc.add("foo", no_commit=True)
    dvc.commit("foo")
    assert dvc.status() == {}


def test_commit_granular_output_dir(tmp_dir, dvc):
    tmp_dir.gen(
        {
            "data": {
                "foo": "foo",
                "bar": "bar",
                "subdir": {"subfoo": "subfoo", "subbar": "subbar"},
            }
        }
    )
    dvc.add("data", no_commit=True)
    dvc.commit("data")
    assert dvc.status() == {}


def test_commit_granular_dir(tmp_dir, dvc):
    tmp_dir.gen(
        {
            "data": {
                "foo": "foo",
                "bar": "bar",
                "subdir": {"subfoo": "subfoo", "subbar": "subbar"},
            }
        }
    )
    dvc.add("data", no_commit=True)

    cache = tmp_dir / ".dvc" / "cache"

    assert set(cache.glob("*/*")) == set()

    dvc.commit(os.path.join("data", "foo"))
    assert set(cache.glob("*/*")) == {
        cache / "1a" / "ca2c799df82929bbdd976557975546.dir",
        cache / "ac" / "bd18db4cc2f85cedef654fccc4a4d8",
    }

    dvc.commit(os.path.join("data", "subdir"))
    assert set(cache.glob("*/*")) == {
        cache / "1a" / "ca2c799df82929bbdd976557975546.dir",
        cache / "ac" / "bd18db4cc2f85cedef654fccc4a4d8",
        cache / "4c" / "e8d2a2cf314a52fa7f315ca37ca445",
        cache / "68" / "dde2c3c4e7953c2290f176bbdc9a54",
    }

    dvc.commit(os.path.join("data"))
    assert set(cache.glob("*/*")) == {
        cache / "1a" / "ca2c799df82929bbdd976557975546.dir",
        cache / "ac" / "bd18db4cc2f85cedef654fccc4a4d8",
        cache / "4c" / "e8d2a2cf314a52fa7f315ca37ca445",
        cache / "68" / "dde2c3c4e7953c2290f176bbdc9a54",
        cache / "37" / "b51d194a7513e45b56f6524f2d51f2",
    }


def test_commit_no_exec_missing_dep(tmp_dir, dvc):
    stage = dvc.run(name="my", cmd="mycmd", deps=["dep"], outs=["out"], no_exec=True)
    assert dvc.status(stage.path)

    with pytest.raises(DependencyDoesNotExistError):
        dvc.commit(stage.path, force=True)


def test_commit_no_exec_missing_out(tmp_dir, dvc):
    stage = dvc.run(name="my", cmd="mycmd", outs=["out"], no_exec=True)
    assert dvc.status(stage.path)

    with pytest.raises(OutputDoesNotExistError):
        dvc.commit(stage.path, force=True)


def test_commit_pipeline_stage(tmp_dir, dvc, run_copy):
    tmp_dir.gen("foo", "foo")
    stage = run_copy("foo", "bar", no_commit=True, name="copy-foo-bar")
    assert dvc.status(stage.addressing)
    assert dvc.commit(stage.addressing, force=True) == [stage]
    assert not dvc.status(stage.addressing)

    # just to confirm different variants work
    assert dvc.commit(f":{stage.addressing}") == [stage]
    assert dvc.commit(f"{PROJECT_FILE}:{stage.addressing}") == [stage]
    assert dvc.commit(PROJECT_FILE) == [stage]


def test_imported_entries_unchanged(tmp_dir, dvc, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("file", "file content", "initial commit")

    stage = dvc.imp(os.fspath(erepo_dir), "file")

    assert stage.changed_entries() == ([], [], None)


def test_commit_updates_to_cloud_versioning_dir(tmp_dir, dvc):
    data_dvc = tmp_dir / "data.dvc"
    data_dvc.dump(
        {
            "outs": [
                {
                    "path": "data",
                    "files": [
                        {
                            "size": 3,
                            "version_id": "WYRG4BglP7pD.gEoJP6a4AqOhl.FRA.h",
                            "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
                            "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
                            "relpath": "bar",
                        },
                        {
                            "size": 3,
                            "version_id": "0vL53tFVY5vVAoJ4HG2jCS1mEcohDPE0",
                            "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
                            "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
                            "relpath": "foo",
                        },
                    ],
                }
            ]
        }
    )

    data = tmp_dir / "data"
    data.mkdir()
    (data / "foo").write_text("foo")
    (data / "bar").write_text("bar2")

    dvc.commit("data", force=True)

    assert (tmp_dir / "data.dvc").parse() == {
        "outs": [
            {
                "path": "data",
                "files": [
                    {
                        "size": 4,
                        "md5": "224e2539f52203eb33728acd228b4432",
                        "relpath": "bar",
                    },
                    {
                        "size": 3,
                        "version_id": "0vL53tFVY5vVAoJ4HG2jCS1mEcohDPE0",
                        "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
                        "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
                        "relpath": "foo",
                    },
                ],
            }
        ]
    }




tests/func/test_config.py
import os
import textwrap

import pytest

from dvc.cli import main
from dvc.config import Config, ConfigError


def test_config_set(tmp_dir, dvc):
    assert main(["config", "core.analytics", "false"]) == 0
    assert (tmp_dir / ".dvc" / "config").read_text() == textwrap.dedent(
        """\
        [core]
            no_scm = True
            analytics = false
        """
    )
    assert not (tmp_dir / ".dvc" / "config.local").exists()

    assert main(["config", "core.analytics", "true"]) == 0
    assert (tmp_dir / ".dvc" / "config").read_text() == textwrap.dedent(
        """\
        [core]
            no_scm = True
            analytics = true
        """
    )
    assert not (tmp_dir / ".dvc" / "config.local").exists()

    assert main(["config", "core.analytics", "--unset"]) == 0
    assert (tmp_dir / ".dvc" / "config").read_text() == textwrap.dedent(
        """\
        [core]
            no_scm = True
        """
    )
    assert not (tmp_dir / ".dvc" / "config.local").exists()


def test_config_set_local(tmp_dir, dvc):
    assert main(["config", "core.analytics", "false", "--local"]) == 0
    assert (tmp_dir / ".dvc" / "config").read_text() == textwrap.dedent(
        """\
        [core]
            no_scm = True
        """
    )
    assert (tmp_dir / ".dvc" / "config.local").read_text() == textwrap.dedent(
        """\
        [core]
            analytics = false
        """
    )

    assert main(["config", "core.analytics", "true", "--local"]) == 0
    assert (tmp_dir / ".dvc" / "config").read_text() == textwrap.dedent(
        """\
        [core]
            no_scm = True
        """
    )
    assert (tmp_dir / ".dvc" / "config.local").read_text() == textwrap.dedent(
        """\
        [core]
            analytics = true
        """
    )

    assert main(["config", "core.analytics", "--unset", "--local"]) == 0
    assert (tmp_dir / ".dvc" / "config").read_text() == textwrap.dedent(
        """\
        [core]
            no_scm = True
        """
    )
    assert (tmp_dir / ".dvc" / "config.local").read_text() == "\n"


def test_config_set_in_non_dvc_repo(tmp_dir, caplog):
    assert main(["config", "core.analytics", "true"]) != 0
    out = caplog.text
    assert "Not inside a DVC repo" in out


@pytest.mark.parametrize(
    "args, ret, msg",
    [
        (["core.analytics"], 0, "False"),
        (["core.remote"], 0, "myremote"),
        (["remote.myremote.profile"], 0, "iterative"),
        (["remote.myremote.profile", "--local"], 0, "iterative"),
        (
            ["remote.myremote.profile", "--project"],
            251,
            "option 'profile' doesn't exist",
        ),
        (["remote.other.url"], 0, "gs://bucket/path"),
        (["remote.other.url", "--local"], 0, "gs://bucket/path"),
        (
            ["remote.other.url", "--project"],
            251,
            "remote 'other' doesn't exist",
        ),
    ],
)
def test_config_get(tmp_dir, dvc, capsys, caplog, args, ret, msg):
    (tmp_dir / ".dvc" / "config").write_text(
        textwrap.dedent(
            """\
        [core]
            no_scm = true
            analytics = False
            remote = myremote
        ['remote "myremote"']
            url = s3://bucket/path
            region = us-east-2
        """
        )
    )
    (tmp_dir / ".dvc" / "config.local").write_text(
        textwrap.dedent(
            """\
        ['remote "myremote"']
            profile = iterative
        ['remote "other"']
            url = gs://bucket/path
        """
        )
    )

    assert main(["config", *args]) == ret
    text = caplog.text if ret else capsys.readouterr()[0]
    assert msg in text


@pytest.mark.parametrize(
    "args, ret",
    [
        (["--local", "core.remote"], 251),
        (["--project", "core.remote"], 251),
        (["core.remote"], 0),
    ],
)
def test_config_get_in_non_dvc_repo(tmp_dir, caplog, args, ret):
    assert main(["config", *args]) == ret
    if ret != 0:
        out = caplog.text
        assert "Not inside a DVC repo" in out


def test_config_list(tmp_dir, dvc, capsys):
    (tmp_dir / ".dvc" / "config").write_text(
        textwrap.dedent(
            """\
        [core]
            no_scm = true
            analytics = False
            remote = myremote
        ['remote "myremote"']
            url = s3://bucket/path
            region = us-east-2
        """
        )
    )
    (tmp_dir / ".dvc" / "config.local").write_text(
        textwrap.dedent(
            """\
        ['remote "myremote"']
            profile = iterative
            access_key_id = abcde
            secret_access_key = 123456
        ['remote "other"']
            url = gs://bucket/path
        """
        )
    )

    assert main(["config", "--list"]) == 0

    out, _ = capsys.readouterr()
    assert "remote.myremote.url=s3://bucket/path" in out
    assert "remote.myremote.region=us-east-2" in out
    assert "remote.myremote.profile=iterative" in out
    assert "remote.myremote.access_key_id=abcde" in out
    assert "remote.myremote.secret_access_key=123456" in out
    assert "remote.other.url=gs://bucket/path" in out
    assert "core.analytics=False" in out
    assert "core.no_scm=true" in out
    assert "core.remote=myremote" in out


@pytest.mark.parametrize(
    "args, ret",
    [
        (["--list", "--local"], 251),
        (["--list", "--project"], 251),
        (["--list"], 0),
    ],
)
def test_config_list_in_non_dvc_repo(tmp_dir, caplog, args, ret):
    assert main(["config", *args]) == ret
    if ret != 0:
        out = caplog.text
        assert "Not inside a DVC repo" in out


@pytest.mark.parametrize(
    "args", [["core.analytics"], ["core.analytics", "false"], ["--unset"]]
)
def test_list_bad_args(tmp_dir, dvc, caplog, args):
    caplog.clear()
    assert main(["config", "--list", *args]) == 1
    assert (
        "-l/--list can't be used together with any of these options: "
        "-u/--unset, name, value" in caplog.text
    )


def test_set_invalid_key(dvc):
    with pytest.raises(ConfigError, match=r"extra keys not allowed"):  # noqa: PT012
        with dvc.config.edit() as conf:
            conf["core"]["invalid_key"] = "value"


def test_merging_two_levels(dvc):
    with dvc.config.edit() as conf:
        conf["remote"]["test"] = {"url": "ssh://example.com"}

    with pytest.raises(  # noqa: PT012
        ConfigError, match=r"expected 'url' for dictionary value"
    ):
        with dvc.config.edit("global") as conf:
            conf["remote"]["test"] = {"password": "1"}

    with dvc.config.edit("local") as conf:
        conf["remote"]["test"] = {"password": "1"}

    assert dvc.config["remote"]["test"] == {
        "url": "ssh://example.com",
        "password": "1",
        "verify": False,
    }


def test_config_loads_without_error_for_non_dvc_repo(tmp_dir):
    # regression testing for https://github.com/iterative/dvc/issues/3328
    Config.from_cwd(validate=True)


@pytest.mark.parametrize(
    "field, remote_url",
    [
        ("credentialpath", "s3://mybucket/my/path"),
        ("credentialpath", "gs://my-bucket/path"),
        ("keyfile", "ssh://user@example.com:1234/path/to/dir"),
        ("cert_path", "webdavs://example.com/files/USERNAME/"),
        ("key_path", "webdavs://example.com/files/USERNAME/"),
        ("gdrive_service_account_json_file_path", "gdrive://root/test"),
        ("gdrive_user_credentials_file", "gdrive://root/test"),
    ],
)
def test_load_relative_paths(dvc, field, remote_url):
    # set field to test
    with dvc.config.edit() as conf:
        conf["remote"]["test"] = {"url": remote_url, field: "file.txt"}

    # check if written paths are correct
    dvc_dir = dvc.config.dvc_dir
    assert dvc.config["remote"]["test"][field] == os.path.join(
        dvc_dir, "..", "file.txt"
    )

    # load config and check that it contains what we expect
    # (relative paths are evaluated correctly)
    cfg = Config(dvc_dir)
    assert cfg["remote"]["test"][field] == os.path.join(dvc_dir, "..", "file.txt")


def test_config_gdrive_fields(tmp_dir, dvc):
    with dvc.config.edit() as conf:
        conf["remote"]["test"] = {
            "url": "gdrive://root/test",
            "profile": "myprofile",
        }

    Config.from_cwd(validate=True)


def test_config_remote(tmp_dir, dvc, capsys):
    (tmp_dir / ".dvc" / "config").write_text(
        "['remote \"myremote\"']\n  url = s3://bucket/path\n  region = myregion\n"
    )

    assert main(["config", "remote.myremote.url"]) == 0
    out, _ = capsys.readouterr()
    assert "s3://bucket/path" in out

    assert main(["config", "remote.myremote.region"]) == 0
    out, _ = capsys.readouterr()
    assert "myregion" in out


def test_config_show_origin_single(tmp_dir, dvc, capsys):
    (tmp_dir / ".dvc" / "config").write_text(
        "['remote \"myremote\"']\n  url = s3://bucket/path\n  region = myregion\n"
    )

    assert main(["config", "--show-origin", "--project", "remote.myremote.url"]) == 0
    out, _ = capsys.readouterr()
    assert "{}\t{}\n".format(os.path.join(".dvc", "config"), "s3://bucket/path") in out

    assert main(["config", "--show-origin", "--local", "remote.myremote.url"]) == 251

    assert main(["config", "--list", "--project", "--show-origin"]) == 0
    out, _ = capsys.readouterr()
    assert (
        "{}\t{}\n".format(
            os.path.join(".dvc", "config"),
            "remote.myremote.url=s3://bucket/path",
        )
        in out
    )


def test_config_show_origin_merged(tmp_dir, dvc, capsys):
    (tmp_dir / ".dvc" / "config").write_text(
        "['remote \"myremote\"']\n  url = s3://bucket/path\n  region = myregion\n"
    )

    (tmp_dir / ".dvc" / "config.local").write_text(
        "['remote \"myremote\"']\n  timeout = 100\n"
    )

    assert main(["config", "--list", "--show-origin"]) == 0
    out, _ = capsys.readouterr()
    assert (
        "{}\t{}\n".format(
            os.path.join(".dvc", "config"),
            "remote.myremote.url=s3://bucket/path",
        )
        in out
    )

    assert (
        "{}\t{}\n".format(
            os.path.join(".dvc", "config.local"), "remote.myremote.timeout=100"
        )
        in out
    )




tests/func/test_data_cloud.py
import logging
import os
import shutil

import pytest
from flaky.flaky_decorator import flaky

import dvc_data
from dvc.cli import main
from dvc.exceptions import CheckoutError
from dvc.repo.open_repo import clean_repos
from dvc.stage.exceptions import StageNotFound
from dvc.testing.remote_tests import TestRemote  # noqa, pylint: disable=unused-import
from dvc.utils.fs import remove
from dvc_data.hashfile.db import HashFileDB
from dvc_data.hashfile.db.local import LocalHashFileDB


def test_cloud_cli(tmp_dir, dvc, remote, mocker):  # noqa: PLR0915
    jobs = 2
    args = ["-v", "-j", str(jobs)]

    (stage,) = tmp_dir.dvc_gen("foo", "foo")
    cache = stage.outs[0].cache_path

    (stage_dir,) = tmp_dir.dvc_gen(
        {
            "data_dir": {
                "data_sub_dir": {"data_sub": "data_sub"},
                "data": "data",
                "empty": "",
            }
        }
    )
    assert stage_dir is not None
    cache_dir = stage_dir.outs[0].cache_path

    # FIXME check status output
    oids_exist = mocker.spy(LocalHashFileDB, "oids_exist")

    assert main(["push", *args]) == 0
    assert os.path.exists(cache)
    assert os.path.isfile(cache)
    assert os.path.isfile(cache_dir)
    assert oids_exist.called
    assert all(
        _kwargs["jobs"] == jobs for (_args, _kwargs) in oids_exist.call_args_list
    )

    dvc.cache.local.clear()
    oids_exist.reset_mock()

    assert main(["fetch", *args]) == 0
    assert os.path.exists(cache)
    assert os.path.isfile(cache)
    assert os.path.isfile(cache_dir)
    assert oids_exist.called
    assert all(
        _kwargs["jobs"] == jobs for (_args, _kwargs) in oids_exist.call_args_list
    )

    oids_exist.reset_mock()

    assert main(["pull", *args]) == 0
    assert os.path.exists(cache)
    assert os.path.isfile(cache)
    assert os.path.isfile(cache_dir)
    assert os.path.isfile("foo")
    assert os.path.isdir("data_dir")
    assert oids_exist.called
    assert all(
        _kwargs["jobs"] == jobs for (_args, _kwargs) in oids_exist.call_args_list
    )

    with open(cache, encoding="utf-8") as fd:
        assert fd.read() == "foo"
    assert os.path.isfile(cache_dir)

    # NOTE: http doesn't support gc yet
    if remote.url.startswith("http"):
        return

    oids_exist.reset_mock()

    _list_oids_traverse = mocker.spy(HashFileDB, "_list_oids_traverse")
    # NOTE: check if remote gc works correctly on directories
    assert main(["gc", "-cw", "-f", *args]) == 0
    assert _list_oids_traverse.called
    assert all(_kwargs["jobs"] == 2 for (_args, _kwargs) in oids_exist.call_args_list)
    shutil.move(dvc.cache.local.path, dvc.cache.local.path + ".back")

    assert main(["fetch", *args]) == 0

    assert oids_exist.called
    assert all(
        _kwargs["jobs"] == jobs for (_args, _kwargs) in oids_exist.call_args_list
    )

    oids_exist.reset_mock()
    assert main(["pull", "-f", *args]) == 0
    assert os.path.exists(cache)
    assert os.path.isfile(cache)
    assert os.path.isfile(cache_dir)
    assert os.path.isfile("foo")
    assert os.path.isdir("data_dir")
    assert oids_exist.called
    assert all(
        _kwargs["jobs"] == jobs for (_args, _kwargs) in oids_exist.call_args_list
    )


def test_data_cloud_error_cli(dvc):
    f = "non-existing-file"
    assert main(["status", "-c", f])
    assert main(["push", f])
    assert main(["pull", f])
    assert main(["fetch", f])


def test_warn_on_outdated_stage(tmp_dir, dvc, local_remote, caplog):
    stage = dvc.run(outs=["bar"], cmd="echo bar > bar", single_stage=True)
    assert main(["push"]) == 0

    stage_file_path = stage.relpath
    content = (tmp_dir / stage_file_path).parse()
    del content["outs"][0]["md5"]
    (tmp_dir / stage_file_path).dump(content)

    with caplog.at_level(logging.WARNING, logger="dvc"):
        caplog.clear()
        assert main(["status", "-c"]) == 0
        expected_warning = (
            "Output 'bar'(stage: 'bar.dvc') is missing version info. "
            "Cache for it will not be collected. "
            "Use `dvc repro` to get your pipeline up to date."
        )

        assert expected_warning in caplog.text


def test_hash_recalculation(mocker, dvc, tmp_dir, local_remote):
    tmp_dir.gen({"foo": "foo"})
    test_file_md5 = mocker.spy(dvc_data.hashfile.hash, "file_md5")
    ret = main(["config", "cache.type", "hardlink"])
    assert ret == 0
    ret = main(["add", "foo"])
    assert ret == 0
    ret = main(["push"])
    assert ret == 0
    assert test_file_md5.mock.call_count == 1


def test_missing_cache(tmp_dir, dvc, local_remote, caplog):
    tmp_dir.dvc_gen({"foo": "foo", "bar": "bar"})

    # purge cache
    dvc.cache.local.clear()

    header = (
        "Some of the cache files do not exist "
        "neither locally nor on remote. Missing cache files:\n"
    )
    foo = "name: bar, md5: 37b51d194a7513e45b56f6524f2d51f2\n"
    bar = "name: foo, md5: acbd18db4cc2f85cedef654fccc4a4d8\n"

    caplog.clear()
    dvc.push()
    assert header in caplog.text
    assert foo in caplog.text
    assert bar in caplog.text

    caplog.clear()
    dvc.fetch()
    assert header in caplog.text
    assert foo in caplog.text
    assert bar in caplog.text

    caplog.clear()
    assert dvc.status(cloud=True) == {
        "bar": "missing",
        "foo": "missing",
    }
    assert header not in caplog.text
    assert foo not in caplog.text
    assert bar not in caplog.text


def test_verify_hashes(tmp_dir, scm, dvc, mocker, tmp_path_factory, local_remote):
    tmp_dir.dvc_gen({"file": "file1 content"}, commit="add file")
    tmp_dir.dvc_gen({"dir": {"subfile": "file2 content"}}, commit="add dir")
    dvc.push()

    # remove artifacts and cache to trigger fetching
    remove("file")
    remove("dir")
    dvc.cache.local.clear()

    hash_spy = mocker.spy(dvc_data.hashfile.hash, "file_md5")

    dvc.pull()
    # NOTE: 1 is for index.data_tree building
    assert hash_spy.call_count == 1

    # Removing cache will invalidate existing state entries
    dvc.cache.local.clear()

    dvc.config["remote"]["upstream"]["verify"] = True

    dvc.pull()
    assert hash_spy.call_count == 6


@flaky(max_runs=3, min_passes=1)
@pytest.mark.parametrize(
    "erepo", [pytest.lazy_fixture("git_dir"), pytest.lazy_fixture("erepo_dir")]
)
def test_pull_git_imports(tmp_dir, dvc, scm, erepo):
    with erepo.chdir():
        erepo.scm_gen({"dir": {"bar": "bar"}}, commit="second")
        erepo.scm_gen("foo", "foo", commit="first")

    dvc.imp(os.fspath(erepo), "foo")
    dvc.imp(os.fspath(erepo), "dir", out="new_dir", rev="HEAD~")

    assert dvc.pull()["fetched"] == 0

    for item in ["foo", "new_dir"]:
        remove(item)
    dvc.cache.local.clear()
    os.makedirs(dvc.cache.local.path, exist_ok=True)
    clean_repos()

    assert dvc.pull(force=True)["fetched"] == 2

    assert (tmp_dir / "foo").exists()
    assert (tmp_dir / "foo").read_text() == "foo"

    assert (tmp_dir / "new_dir").exists()
    assert (tmp_dir / "new_dir" / "bar").read_text() == "bar"


def test_pull_external_dvc_imports(tmp_dir, dvc, scm, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen({"dir": {"bar": "bar"}}, commit="second")
        erepo_dir.dvc_gen("foo", "foo", commit="first")

        os.remove("foo")
        shutil.rmtree("dir")

    dvc.imp(os.fspath(erepo_dir), "foo")
    dvc.imp(os.fspath(erepo_dir), "dir", out="new_dir", rev="HEAD~")

    assert dvc.pull()["fetched"] == 0

    clean(["foo", "new_dir"], dvc)

    assert dvc.pull(force=True)["fetched"] == 2

    assert (tmp_dir / "foo").exists()
    assert (tmp_dir / "foo").read_text() == "foo"

    assert (tmp_dir / "new_dir").exists()
    assert (tmp_dir / "new_dir" / "bar").read_text() == "bar"


def test_pull_partial_import(tmp_dir, dvc, local_workspace):
    local_workspace.gen("file", "file content")
    dst = tmp_dir / "file"
    stage = dvc.imp_url("remote://workspace/file", os.fspath(dst), no_download=True)

    result = dvc.pull("file")
    assert result["fetched"] == 1
    assert dst.exists()

    assert stage.outs[0].get_hash().value == "d10b4c3ff123b26dc068d43a8bef2d23"


def test_pull_external_dvc_imports_mixed(tmp_dir, dvc, scm, erepo_dir, local_remote):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("foo", "foo", commit="first")
        os.remove("foo")

    # imported: foo
    dvc.imp(os.fspath(erepo_dir), "foo")

    # local-object: bar
    tmp_dir.dvc_gen("bar", "bar")
    dvc.push("bar")

    clean(["foo", "bar"], dvc)

    assert dvc.pull()["fetched"] == 2
    assert (tmp_dir / "foo").read_text() == "foo"
    assert (tmp_dir / "bar").read_text() == "bar"


def clean(outs, dvc=None):
    if dvc:
        dvc.cache.local.clear()
    for path in outs:
        remove(path)
    if dvc:
        clean_repos()


def recurse_list_dir(d):
    return [
        os.path.join(root, f) for root, _, filenames in os.walk(d) for f in filenames
    ]


def test_dvc_pull_pipeline_stages(tmp_dir, dvc, run_copy, local_remote):
    (stage0,) = tmp_dir.dvc_gen("foo", "foo")
    stage1 = run_copy("foo", "bar", single_stage=True)
    stage2 = run_copy("bar", "foobar", name="copy-bar-foobar")
    dvc.push()

    outs = ["foo", "bar", "foobar"]

    clean(outs, dvc)
    dvc.pull()
    assert all((tmp_dir / file).exists() for file in outs)

    for out, stage in zip(outs, [stage0, stage1, stage2]):
        for target in [stage.addressing, out]:
            clean(outs, dvc)
            stats = dvc.pull([target])
            assert stats["fetched"] == 1
            assert stats["added"] == [out]
            assert os.path.exists(out)
            assert not any(os.path.exists(out) for out in set(outs) - {out})

    clean(outs, dvc)
    stats = dvc.pull([stage2.addressing], with_deps=True)
    assert len(stats["added"]) == 3
    assert set(stats["added"]) == set(outs)

    clean(outs, dvc)
    stats = dvc.pull([os.curdir], recursive=True)
    assert set(stats["added"]) == set(outs)


def test_pipeline_file_target_ops(tmp_dir, dvc, run_copy, local_remote):
    path = local_remote.url
    tmp_dir.dvc_gen("foo", "foo")
    run_copy("foo", "bar", single_stage=True)

    tmp_dir.dvc_gen("lorem", "lorem")
    run_copy("lorem", "lorem2", name="copy-lorem-lorem2")

    tmp_dir.dvc_gen("ipsum", "ipsum")
    run_copy("ipsum", "baz", name="copy-ipsum-baz")

    outs = ["foo", "bar", "lorem", "ipsum", "baz", "lorem2"]

    remove(dvc.stage_cache.cache_dir)

    dvc.push()

    outs = ["foo", "bar", "lorem", "ipsum", "baz", "lorem2"]

    # each one's a copy of other, hence 3
    assert len(recurse_list_dir(path)) == 3

    clean(outs, dvc)
    assert set(dvc.pull(["dvc.yaml"])["added"]) == {"lorem2", "baz"}

    clean(outs, dvc)
    assert set(dvc.pull()["added"]) == set(outs)

    # clean everything in remote and push
    from dvc.testing.tmp_dir import TmpDir

    clean(TmpDir(path).iterdir())
    dvc.push(["dvc.yaml:copy-ipsum-baz"])
    assert len(recurse_list_dir(path)) == 1

    clean(TmpDir(path).iterdir())
    dvc.push(["dvc.yaml"])
    assert len(recurse_list_dir(path)) == 2

    with pytest.raises(StageNotFound):
        dvc.push(["dvc.yaml:StageThatDoesNotExist"])

    with pytest.raises(StageNotFound):
        dvc.pull(["dvc.yaml:StageThatDoesNotExist"])


@pytest.mark.parametrize(
    "fs, msg",
    [
        ({"foo": "foo", "bar": "bar"}, "2 files pushed"),
        ({"foo": "foo"}, "1 file pushed"),
        ({}, "Everything is up to date"),
    ],
)
def test_push_stats(tmp_dir, dvc, fs, msg, capsys, local_remote):
    tmp_dir.dvc_gen(fs)

    main(["push"])
    out, _ = capsys.readouterr()
    assert msg in out


@pytest.mark.parametrize(
    "fs, msg",
    [
        ({"foo": "foo", "bar": "bar"}, "2 files fetched"),
        ({"foo": "foo"}, "1 file fetched"),
        ({}, "Everything is up to date."),
    ],
)
def test_fetch_stats(tmp_dir, dvc, fs, msg, capsys, local_remote):
    tmp_dir.dvc_gen(fs)
    dvc.push()
    clean(list(fs.keys()), dvc)

    main(["fetch"])
    out, _ = capsys.readouterr()
    assert msg in out


def test_pull_stats(tmp_dir, dvc, capsys, local_remote):
    tmp_dir.dvc_gen({"foo": "foo", "bar": "bar"})
    dvc.push()
    clean(["foo", "bar"], dvc)
    (tmp_dir / "bar").write_text("foobar")

    assert main(["pull", "--force"]) == 0

    out, _ = capsys.readouterr()
    assert "M\tbar".expandtabs() in out
    assert "A\tfoo".expandtabs() in out
    assert "2 files fetched" in out
    assert "1 file added" in out
    assert "1 file modified" in out

    main(["pull"])
    out, _ = capsys.readouterr()
    assert "Everything is up to date." in out


@pytest.mark.parametrize(
    "key,expected", [("all_tags", 2), ("all_branches", 3), ("all_commits", 3)]
)
def test_push_pull_all(tmp_dir, scm, dvc, local_remote, key, expected):
    tmp_dir.dvc_gen({"foo": "foo"}, commit="first")
    scm.tag("v1")
    dvc.remove("foo.dvc")
    tmp_dir.dvc_gen({"bar": "bar"}, commit="second")
    scm.tag("v2")
    with tmp_dir.branch("branch", new=True):
        dvc.remove("bar.dvc")
        tmp_dir.dvc_gen({"baz": "baz"}, commit="branch")

    assert dvc.push(**{key: True}) == expected

    clean(["foo", "bar", "baz"], dvc)
    assert dvc.pull(**{key: True})["fetched"] == expected


def test_push_pull_fetch_pipeline_stages(tmp_dir, dvc, run_copy, local_remote):
    tmp_dir.dvc_gen("foo", "foo")
    run_copy("foo", "bar", name="copy-foo-bar")

    dvc.push("copy-foo-bar")
    assert len(recurse_list_dir(local_remote.url)) == 1
    # pushing everything so as we can check pull/fetch only downloads
    # from specified targets
    dvc.push()
    clean(["foo", "bar"], dvc)

    dvc.pull("copy-foo-bar")
    assert (tmp_dir / "bar").exists()
    assert len(recurse_list_dir(dvc.cache.local.path)) == 2
    clean(["bar"], dvc)

    dvc.fetch("copy-foo-bar")
    assert len(recurse_list_dir(dvc.cache.local.path)) == 2


def test_pull_partial(tmp_dir, dvc, local_remote):
    tmp_dir.dvc_gen({"foo": {"bar": {"baz": "baz"}, "spam": "spam"}})
    dvc.push()
    clean(["foo"], dvc)

    stats = dvc.pull(os.path.join("foo", "bar"))
    assert stats["fetched"] == 1
    assert (tmp_dir / "foo").read_text() == {"bar": {"baz": "baz"}}


def test_output_remote(tmp_dir, dvc, make_remote):
    make_remote("default", default=True)
    make_remote("for_foo", default=False)
    make_remote("for_data", default=False)

    tmp_dir.dvc_gen("foo", "foo")
    tmp_dir.dvc_gen("bar", "bar")
    tmp_dir.dvc_gen("data", {"one": "one", "two": "two"})

    with (tmp_dir / "foo.dvc").modify() as d:
        d["outs"][0]["remote"] = "for_foo"

    with (tmp_dir / "data.dvc").modify() as d:
        d["outs"][0]["remote"] = "for_data"

    dvc.push()

    default = dvc.cloud.get_remote_odb("default")
    for_foo = dvc.cloud.get_remote_odb("for_foo")
    for_data = dvc.cloud.get_remote_odb("for_data")

    assert set(default.all()) == {"37b51d194a7513e45b56f6524f2d51f2"}
    assert set(for_foo.all()) == {"acbd18db4cc2f85cedef654fccc4a4d8"}
    assert set(for_data.all()) == {
        "f97c5d29941bfb1b2fdab0874906ab82",
        "6b18131dc289fd37006705affe961ef8.dir",
        "b8a9f715dbb64fd5c56e7783c6820a61",
    }

    clean(["foo", "bar", "data"], dvc)

    dvc.pull()

    assert set(dvc.cache.local.all()) == {
        "37b51d194a7513e45b56f6524f2d51f2",
        "acbd18db4cc2f85cedef654fccc4a4d8",
        "f97c5d29941bfb1b2fdab0874906ab82",
        "6b18131dc289fd37006705affe961ef8.dir",
        "b8a9f715dbb64fd5c56e7783c6820a61",
    }


def test_target_remote(tmp_dir, dvc, make_remote):
    make_remote("default", default=True)
    make_remote("myremote", default=False)

    tmp_dir.dvc_gen("foo", "foo")
    tmp_dir.dvc_gen("data", {"one": "one", "two": "two"})

    dvc.push(remote="myremote")

    default = dvc.cloud.get_remote_odb("default")
    myremote = dvc.cloud.get_remote_odb("myremote")

    assert set(default.all()) == set()
    assert set(myremote.all()) == {
        "acbd18db4cc2f85cedef654fccc4a4d8",
        "f97c5d29941bfb1b2fdab0874906ab82",
        "6b18131dc289fd37006705affe961ef8.dir",
        "b8a9f715dbb64fd5c56e7783c6820a61",
    }

    clean(["foo", "data"], dvc)

    dvc.pull(remote="myremote")

    assert set(dvc.cache.local.all()) == {
        "acbd18db4cc2f85cedef654fccc4a4d8",
        "f97c5d29941bfb1b2fdab0874906ab82",
        "6b18131dc289fd37006705affe961ef8.dir",
        "b8a9f715dbb64fd5c56e7783c6820a61",
    }


def test_pull_allow_missing(tmp_dir, dvc, local_remote):
    dvc.stage.add(name="bar", outs=["bar"], cmd="echo bar > bar")

    with pytest.raises(CheckoutError):
        dvc.pull()

    tmp_dir.dvc_gen("foo", "foo")
    dvc.push()
    clean(["foo"], dvc)

    stats = dvc.pull(allow_missing=True)
    assert stats["fetched"] == 1




tests/func/test_data_status.py
from os import fspath
from os.path import join

import pytest

from dvc.repo import Repo
from dvc.repo.data import _transform_git_paths_to_dvc, posixpath_to_os_path
from dvc.testing.tmp_dir import make_subrepo
from dvc.utils.fs import remove

EMPTY_STATUS = {
    "committed": {},
    "uncommitted": {},
    "git": {},
    "not_in_cache": [],
    "not_in_remote": [],
    "unchanged": [],
    "untracked": [],
}


@pytest.mark.parametrize("path", [None, ("sub", "repo")])
def test_git_to_dvc_path_wdir_transformation(tmp_dir, scm, path):
    struct = {"dir": {"foo": "foo", "bar": "bar"}, "file": "file", "dir2": {}}
    tmp_dir.gen(struct)

    subdir = tmp_dir.joinpath(*path) if path else tmp_dir
    make_subrepo(subdir, scm)
    dvc = subdir.dvc

    with subdir.chdir():
        subdir.gen(struct)
        _, _, untracked = scm.status(untracked_files="all")
        # make order independent of the platforms for easier test assertions
        untracked = sorted(map(posixpath_to_os_path, untracked), reverse=True)
        assert _transform_git_paths_to_dvc(dvc, untracked) == [
            "file",
            join("dir", "foo"),
            join("dir", "bar"),
        ]
        with (subdir / "dir").chdir():
            assert _transform_git_paths_to_dvc(dvc, untracked) == [
                join("..", "file"),
                "foo",
                "bar",
            ]
        with (subdir / "dir2").chdir():
            assert _transform_git_paths_to_dvc(dvc, untracked) == [
                join("..", "file"),
                join("..", "dir", "foo"),
                join("..", "dir", "bar"),
            ]


def test_file(M, tmp_dir, dvc, scm):
    tmp_dir.dvc_gen("foo", "foo", commit="add foo")
    tmp_dir.dvc_gen("foo", "foobar")
    remove(tmp_dir / "foo")

    expected = {
        **EMPTY_STATUS,
        "committed": {"modified": ["foo"]},
        "uncommitted": {"deleted": ["foo"]},
        "git": M.dict(),
    }
    assert dvc.data_status() == expected
    assert dvc.data_status(granular=True) == expected


def test_directory(M, tmp_dir, dvc, scm):
    tmp_dir.dvc_gen({"dir": {"foo": "foo"}}, commit="add dir")
    tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar", "foobar": "foobar"}})
    remove(tmp_dir / "dir")
    (tmp_dir / "dir").gen({"foo": "foo", "bar": "barr", "baz": "baz"})
    tmp_dir.gen("untracked", "untracked")

    assert dvc.data_status() == {
        **EMPTY_STATUS,
        "committed": {"modified": [join("dir", "")]},
        "uncommitted": {"modified": [join("dir", "")]},
        "git": M.dict(),
    }

    assert dvc.data_status(granular=True, untracked_files="all") == {
        **EMPTY_STATUS,
        "committed": {
            "added": M.unordered(
                join("dir", "bar"),
                join("dir", "foobar"),
            ),
            "modified": [join("dir", "")],
        },
        "uncommitted": {
            "added": [join("dir", "baz")],
            "modified": M.unordered(join("dir", ""), join("dir", "bar")),
            "deleted": [join("dir", "foobar")],
        },
        "git": M.dict(),
        "not_in_cache": [],
        "unchanged": [join("dir", "foo")],
        "untracked": ["untracked"],
    }


def test_tracked_directory_deep(M, tmp_dir, dvc, scm):
    """Test for a directory not in cwd, but nested inside other directories."""
    (tmp_dir / "sub").gen({"dir": {"foo": "foo"}})
    dvc.add(fspath(tmp_dir / "sub" / "dir"))
    scm.add_commit(["sub/dir.dvc", "sub/.gitignore"], message="add sub/dir")

    (tmp_dir / "sub" / "dir").gen("bar", "bar")
    dvc.commit(None, force=True)
    (tmp_dir / "sub" / "dir").gen("foobar", "foobar")

    assert dvc.data_status() == {
        **EMPTY_STATUS,
        "committed": {"modified": [join("sub", "dir", "")]},
        "uncommitted": {"modified": [join("sub", "dir", "")]},
        "git": M.dict(),
    }
    assert dvc.data_status(granular=True, untracked_files="all") == {
        **EMPTY_STATUS,
        "committed": {
            "added": [join("sub", "dir", "bar")],
            "modified": [join("sub", "dir", "")],
        },
        "uncommitted": {
            "added": [join("sub", "dir", "foobar")],
            "modified": [join("sub", "dir", "")],
        },
        "git": M.dict(),
        "unchanged": [join("sub", "dir", "foo")],
    }


def test_new_empty_git_repo(M, tmp_dir, scm):
    dvc = Repo.init()
    assert dvc.data_status() == {
        **EMPTY_STATUS,
        "git": M.dict(
            is_empty=True,
            is_dirty=True,
        ),
    }


def test_noscm_repo(dvc):
    assert dvc.data_status() == EMPTY_STATUS


def test_unchanged(M, tmp_dir, dvc, scm):
    tmp_dir.dvc_gen({"dir": {"foo": "foo"}}, commit="add dir")
    tmp_dir.dvc_gen("bar", "bar", commit="add foo")

    assert dvc.data_status() == {
        **EMPTY_STATUS,
        "git": M.dict(),
        "unchanged": M.unordered("bar", join("dir", "")),
    }
    assert dvc.data_status(granular=True) == {
        **EMPTY_STATUS,
        "git": M.dict(),
        "unchanged": M.unordered("bar", join("dir", ""), join("dir", "foo")),
    }


def test_skip_uncached_pipeline_outputs(tmp_dir, dvc, run_copy_metrics):
    tmp_dir.gen({"m_temp.yaml": str(5)})
    run_copy_metrics(
        "m_temp.yaml",
        "m.yaml",
        metrics_no_cache=["m.yaml"],
        name="copy-metrics",
    )
    assert dvc.data_status() == EMPTY_STATUS
    assert dvc.data_status(granular=True, untracked_files="all") == EMPTY_STATUS


def test_outs_with_no_hashes(M, tmp_dir, dvc, scm):
    dvc.stage.add(single_stage=True, outs=["bar"])
    dvc.stage.add(deps=["bar"], outs=["foo"], name="copy", cmd="cp foo bar")

    expected_output = {
        **EMPTY_STATUS,
        "git": M.dict(),
    }
    assert dvc.data_status() == expected_output
    assert dvc.data_status(granular=True) == expected_output


def test_outs_with_no_hashes_and_with_uncommitted_files(M, tmp_dir, dvc, scm):
    tmp_dir.gen({"bar": "bar", "foo": "foo"})
    dvc.stage.add(single_stage=True, outs=["bar"])
    dvc.stage.add(deps=["bar"], outs=["foo"], name="copy", cmd="cp foo bar")

    expected_output = {
        **EMPTY_STATUS,
        "uncommitted": {"added": M.unordered("bar", "foo")},
        "git": M.dict(),
    }
    assert dvc.data_status() == expected_output
    assert dvc.data_status(granular=True) == expected_output


def test_subdir(M, tmp_dir, scm):
    subrepo = tmp_dir / "sub"
    make_subrepo(subrepo, scm)

    with subrepo.chdir():
        subrepo.dvc_gen({"dir": {"foo": "foo"}}, commit="add dir")
        subrepo.dvc_gen("bar", "bar", commit="add foo")
        subrepo.gen("untracked", "untracked")

        dvc = subrepo.dvc
        assert dvc.data_status(granular=True, untracked_files="all") == {
            **EMPTY_STATUS,
            "git": M.dict(),
            "unchanged": M.unordered("bar", join("dir", ""), join("dir", "foo")),
            "untracked": ["untracked"],
        }


def test_untracked_newly_added_files(M, tmp_dir, dvc, scm):
    tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})
    tmp_dir.gen("foobar", "foobar")

    expected = {
        **EMPTY_STATUS,
        "untracked": M.unordered(join("dir", "foo"), join("dir", "bar"), "foobar"),
        "git": M.dict(),
    }
    assert dvc.data_status(untracked_files="all") == expected
    assert dvc.data_status(granular=True, untracked_files="all") == expected


def test_missing_cache_workspace_exists(M, tmp_dir, dvc, scm):
    tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar"}})
    tmp_dir.dvc_gen("foobar", "foobar")
    remove(dvc.cache.repo.path)

    assert dvc.data_status(untracked_files="all") == {
        **EMPTY_STATUS,
        "untracked": M.unordered("foobar.dvc", "dir.dvc", ".gitignore"),
        "committed": {"added": M.unordered("foobar", join("dir", ""))},
        "not_in_cache": M.unordered("foobar", join("dir", "")),
        "git": M.dict(),
    }

    assert dvc.data_status(granular=True, untracked_files="all") == {
        **EMPTY_STATUS,
        "untracked": M.unordered("foobar.dvc", "dir.dvc", ".gitignore"),
        "committed": {"added": M.unordered("foobar", join("dir", ""))},
        "uncommitted": {"unknown": M.unordered(join("dir", "foo"), join("dir", "bar"))},
        "not_in_cache": M.unordered(
            "foobar",
            join("dir", ""),
        ),
        "git": M.dict(),
    }


def test_missing_cache_missing_workspace(M, tmp_dir, dvc, scm):
    tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar"}})
    tmp_dir.dvc_gen("foobar", "foobar")
    for path in [dvc.cache.repo.path, "dir", "foobar"]:
        remove(path)

    assert dvc.data_status(untracked_files="all") == {
        **EMPTY_STATUS,
        "untracked": M.unordered("foobar.dvc", "dir.dvc", ".gitignore"),
        "uncommitted": {"deleted": M.unordered("foobar", join("dir", ""))},
        "committed": {"added": M.unordered("foobar", join("dir", ""))},
        "not_in_cache": M.unordered("foobar", join("dir", "")),
        "git": M.dict(),
    }

    assert dvc.data_status(granular=True, untracked_files="all") == {
        **EMPTY_STATUS,
        "untracked": M.unordered("foobar.dvc", "dir.dvc", ".gitignore"),
        "uncommitted": {"deleted": M.unordered("foobar", join("dir", ""))},
        "committed": {"added": M.unordered("foobar", join("dir", ""))},
        "not_in_cache": M.unordered("foobar", join("dir", "")),
        "git": M.dict(),
    }


def test_git_committed_missing_cache_workspace_exists(M, tmp_dir, dvc, scm):
    tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar"}}, commit="add dir")
    tmp_dir.dvc_gen("foobar", "foobar", commit="add foobar")
    remove(dvc.cache.local.path)

    assert dvc.data_status(untracked_files="all") == {
        **EMPTY_STATUS,
        "not_in_cache": M.unordered("foobar", join("dir", "")),
        "git": M.dict(),
        "unchanged": M.unordered("foobar", join("dir", "")),
    }
    assert dvc.data_status(granular=True) == {
        **EMPTY_STATUS,
        "not_in_cache": M.unordered(
            "foobar",
            join("dir", ""),
        ),
        "uncommitted": {"unknown": M.unordered(join("dir", "foo"), join("dir", "bar"))},
        "git": M.dict(),
        "unchanged": M.unordered("foobar", join("dir", "")),
    }


def test_git_committed_missing_cache_missing_workspace(M, tmp_dir, dvc, scm):
    tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar"}}, commit="add dir")
    tmp_dir.dvc_gen("foobar", "foobar", commit="add foobar")
    for path in [dvc.cache.repo.path, "dir", "foobar"]:
        remove(path)

    assert dvc.data_status(untracked_files="all") == {
        **EMPTY_STATUS,
        "uncommitted": {"deleted": M.unordered(join("dir", ""), "foobar")},
        "not_in_cache": M.unordered(join("dir", ""), "foobar"),
        "git": M.dict(),
    }
    assert dvc.data_status(granular=True, untracked_files="all") == {
        **EMPTY_STATUS,
        "committed": {},
        "uncommitted": {"deleted": M.unordered(join("dir", ""), "foobar")},
        "not_in_cache": M.unordered(join("dir", ""), "foobar"),
        "git": M.dict(),
    }


def test_partial_missing_cache(M, tmp_dir, dvc, scm):
    tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar"}})

    # remove "foo" from cache
    odb = dvc.cache.repo
    odb.fs.rm(odb.oid_to_path("acbd18db4cc2f85cedef654fccc4a4d8"))

    assert dvc.data_status() == {
        **EMPTY_STATUS,
        "committed": {"added": [join("dir", "")]},
        "git": M.dict(),
    }
    assert dvc.data_status(granular=True) == {
        **EMPTY_STATUS,
        "committed": {
            "added": M.unordered(
                join("dir", ""), join("dir", "foo"), join("dir", "bar")
            )
        },
        "not_in_cache": [join("dir", "foo")],
        "git": M.dict(),
    }


def test_missing_dir_object_from_head(M, tmp_dir, dvc, scm):
    (stage,) = tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar"}}, commit="add dir")
    remove("dir")
    tmp_dir.dvc_gen({"dir": {"foobar": "foobar"}})
    odb = dvc.cache.repo
    odb.fs.rm(odb.oid_to_path(stage.outs[0].hash_info.value))

    assert dvc.data_status() == {
        **EMPTY_STATUS,
        "committed": {"modified": [join("dir", "")]},
        "git": M.dict(),
    }
    assert dvc.data_status(granular=True) == {
        **EMPTY_STATUS,
        "committed": {
            "modified": [join("dir", "")],
            "unknown": [join("dir", "foobar")],
        },
        "git": M.dict(),
    }


def test_missing_dir_object_from_index(M, tmp_dir, dvc, scm):
    tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar"}}, commit="add dir")
    remove("dir")
    (stage,) = tmp_dir.dvc_gen({"dir": {"foobar": "foobar"}})
    odb = dvc.cache.repo
    odb.fs.rm(odb.oid_to_path(stage.outs[0].hash_info.value))

    assert dvc.data_status() == {
        **EMPTY_STATUS,
        "committed": {"modified": [join("dir", "")]},
        "not_in_cache": [join("dir", "")],
        "git": M.dict(),
    }
    assert dvc.data_status(granular=True) == {
        **EMPTY_STATUS,
        "committed": {
            "modified": [join("dir", "")],
        },
        "uncommitted": {"unknown": [join("dir", "foobar")]},
        "not_in_cache": [join("dir", "")],
        "git": M.dict(),
    }


def test_missing_remote_cache(M, tmp_dir, dvc, scm, local_remote):
    tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar"}})
    tmp_dir.dvc_gen("foobar", "foobar")

    assert dvc.data_status() == {
        **EMPTY_STATUS,
        "committed": {"added": M.unordered("foobar", join("dir", ""))},
        "git": M.dict(),
    }

    assert dvc.data_status(untracked_files="all", not_in_remote=True) == {
        **EMPTY_STATUS,
        "untracked": M.unordered("foobar.dvc", "dir.dvc", ".gitignore"),
        "committed": {"added": M.unordered("foobar", join("dir", ""))},
        "not_in_remote": M.unordered("foobar", join("dir", "")),
        "git": M.dict(),
    }

    assert dvc.data_status(
        granular=True, untracked_files="all", not_in_remote=True
    ) == {
        **EMPTY_STATUS,
        "untracked": M.unordered("foobar.dvc", "dir.dvc", ".gitignore"),
        "committed": {
            "added": M.unordered(
                "foobar",
                join("dir", ""),
                join("dir", "foo"),
                join("dir", "bar"),
            )
        },
        "uncommitted": {},
        "not_in_remote": M.unordered(
            "foobar",
            join("dir", ""),
            join("dir", "foo"),
            join("dir", "bar"),
        ),
        "git": M.dict(),
    }


def test_root_from_dir_to_file(M, tmp_dir, dvc, scm):
    tmp_dir.dvc_gen({"data": {"foo": "foo", "bar": "bar"}})
    remove("data")
    tmp_dir.gen("data", "file")

    assert dvc.data_status() == {
        **EMPTY_STATUS,
        "committed": {"added": [join("data", "")]},
        "uncommitted": {"modified": ["data"]},
        "git": M.dict(),
    }
    assert dvc.data_status(granular=True) == {
        **EMPTY_STATUS,
        "committed": {
            "added": M.unordered(
                join("data", ""), join("data", "foo"), join("data", "bar")
            )
        },
        "uncommitted": {
            "deleted": M.unordered(join("data", "foo"), join("data", "bar")),
            "modified": ["data"],
        },
        "git": M.dict(),
    }


def test_root_from_file_to_dir(M, tmp_dir, dvc, scm):
    tmp_dir.dvc_gen("data", "file")
    remove("data")
    tmp_dir.gen({"data": {"foo": "foo", "bar": "bar"}})

    assert dvc.data_status() == {
        **EMPTY_STATUS,
        "committed": {"added": ["data"]},
        "uncommitted": {"modified": [join("data", "")]},
        "git": M.dict(),
    }
    assert dvc.data_status(granular=True) == {
        **EMPTY_STATUS,
        "committed": {"added": ["data"]},
        "uncommitted": {
            "modified": [join("data", "")],
            "added": M.unordered(join("data", "foo"), join("data", "bar")),
        },
        "git": M.dict(),
    }


def test_empty_dir(tmp_dir, scm, dvc, M):
    # regression testing for https://github.com/iterative/dvc/issues/8958
    tmp_dir.dvc_gen({"data": {"foo": "foo"}})
    remove("data")

    (tmp_dir / "data").mkdir()

    assert dvc.data_status() == {
        **EMPTY_STATUS,
        "committed": {"added": [join("data", "")]},
        "uncommitted": {"modified": [join("data", "")]},
        "git": M.dict(),
    }




tests/func/test_diff.py
import hashlib
import os

import pytest
from funcy import first

from dvc.exceptions import DvcException
from dvc.utils.fs import remove


def digest(text):
    return hashlib.md5(bytes(text, "utf-8")).hexdigest()


def test_no_scm(tmp_dir, dvc):
    from dvc.scm import NoSCMError

    tmp_dir.dvc_gen("file", "text")

    with pytest.raises(NoSCMError):
        dvc.diff()


def test_added(tmp_dir, scm, dvc):
    tmp_dir.dvc_gen("file", "text")

    assert dvc.diff() == {
        "added": [{"path": "file", "hash": digest("text")}],
        "deleted": [],
        "modified": [],
        "not in cache": [],
        "renamed": [],
    }


def test_added_deep(tmp_dir, scm, dvc):
    tmp_dir.gen({"datas": {"data": {"file": "text"}}})
    dvc.add(os.path.join("datas", "data"))

    assert dvc.diff() == {
        "added": [
            {
                "path": os.path.join("datas", "data" + os.sep),
                "hash": "0dab3fae569586d4c33272e5011605bf.dir",
            },
            {
                "path": os.path.join("datas", "data", "file"),
                "hash": "1cb251ec0d568de6a929b520c4aed8d1",
            },
        ],
        "deleted": [],
        "modified": [],
        "not in cache": [],
        "renamed": [],
    }


def test_no_cache_entry(tmp_dir, scm, dvc):
    tmp_dir.dvc_gen("file", "first", commit="add a file")

    tmp_dir.dvc_gen({"dir": {"1": "1", "2": "2"}})
    tmp_dir.dvc_gen("file", "second")

    remove(tmp_dir / ".dvc" / "cache")

    dir_checksum = "5fb6b29836c388e093ca0715c872fe2a.dir"

    assert dvc.diff() == {
        "added": [
            {"path": os.path.join("dir", ""), "hash": dir_checksum},
            {"path": os.path.join("dir", "1"), "hash": digest("1")},
            {"path": os.path.join("dir", "2"), "hash": digest("2")},
        ],
        "deleted": [],
        "modified": [
            {
                "path": "file",
                "hash": {"old": digest("first"), "new": digest("second")},
            }
        ],
        "not in cache": [
            {
                "path": "file",
                "hash": digest("first"),
            }
        ],
        "renamed": [],
    }


@pytest.mark.parametrize("delete_data", [True, False])
def test_deleted(tmp_dir, scm, dvc, delete_data):
    tmp_dir.dvc_gen("file", "text", commit="add file")
    (tmp_dir / "file.dvc").unlink()
    if delete_data:
        (tmp_dir / "file").unlink()

    assert dvc.diff() == {
        "added": [],
        "deleted": [{"path": "file", "hash": digest("text")}],
        "modified": [],
        "not in cache": [],
        "renamed": [],
    }


def test_modified(tmp_dir, scm, dvc):
    tmp_dir.dvc_gen("file", "first", commit="first version")
    tmp_dir.dvc_gen("file", "second")

    assert dvc.diff() == {
        "added": [],
        "deleted": [],
        "modified": [
            {
                "path": "file",
                "hash": {"old": digest("first"), "new": digest("second")},
            }
        ],
        "not in cache": [],
        "renamed": [],
    }


def test_modified_subrepo(tmp_dir, scm, dvc):
    from dvc.repo import Repo

    tmp_dir.gen({"subdir": {"file": "first"}})
    subrepo_dir = tmp_dir / "subdir"

    with subrepo_dir.chdir():
        subrepo = Repo.init(subdir=True)
        subrepo.add("file")

    scm.add(os.path.join("subdir", "file.dvc"))
    scm.commit("init")

    (subrepo_dir / "file").write_text("second")

    with subrepo_dir.chdir():
        subrepo = Repo()
        assert subrepo.diff() == {
            "added": [],
            "deleted": [],
            "modified": [
                {
                    "path": "file",
                    "hash": {"old": digest("first"), "new": digest("second")},
                }
            ],
            "not in cache": [],
            "renamed": [],
        }


def test_refs(tmp_dir, scm, dvc):
    tmp_dir.dvc_gen("file", "first", commit="first version")
    tmp_dir.dvc_gen("file", "second", commit="second version")
    tmp_dir.dvc_gen("file", "third", commit="third version")

    HEAD_2 = digest("first")  # noqa: N806
    HEAD_1 = digest("second")  # noqa: N806
    HEAD = digest("third")  # noqa: N806

    assert dvc.diff("HEAD~1") == {
        "added": [],
        "deleted": [],
        "modified": [{"path": "file", "hash": {"old": HEAD_1, "new": HEAD}}],
        "not in cache": [],
        "renamed": [],
    }

    assert dvc.diff("HEAD~2", "HEAD~1") == {
        "added": [],
        "deleted": [],
        "modified": [{"path": "file", "hash": {"old": HEAD_2, "new": HEAD_1}}],
        "not in cache": [],
        "renamed": [],
    }

    with pytest.raises(DvcException, match=r"unknown Git revision 'missing'"):
        dvc.diff("missing")


def test_directories(tmp_dir, scm, dvc):
    tmp_dir.dvc_gen({"dir": {"1": "1", "2": "2"}}, commit="add a directory")
    tmp_dir.dvc_gen({"dir": {"3": "3"}}, commit="add a file")
    tmp_dir.dvc_gen({"dir": {"2": "two"}}, commit="modify a file")

    (tmp_dir / "dir" / "2").unlink()
    assert dvc.status() != {}  # sanity check
    dvc.add("dir")
    scm.add(["dir.dvc"])
    scm.commit("delete a file")

    # The ":/<text>" format is a way to specify revisions by commit message:
    #       https://git-scm.com/docs/revisions
    #
    assert dvc.diff(":/init", ":/directory") == {
        "added": [
            {
                "path": os.path.join("dir", ""),
                "hash": "5fb6b29836c388e093ca0715c872fe2a.dir",
            },
            {"path": os.path.join("dir", "1"), "hash": digest("1")},
            {"path": os.path.join("dir", "2"), "hash": digest("2")},
        ],
        "deleted": [],
        "modified": [],
        "not in cache": [],
        "renamed": [],
    }

    assert dvc.diff(":/directory", ":/modify") == {
        "added": [{"path": os.path.join("dir", "3"), "hash": digest("3")}],
        "deleted": [],
        "modified": [
            {
                "path": os.path.join("dir", ""),
                "hash": {
                    "old": "5fb6b29836c388e093ca0715c872fe2a.dir",
                    "new": "9b5faf37366b3370fd98e3e60ca439c1.dir",
                },
            },
            {
                "path": os.path.join("dir", "2"),
                "hash": {"old": digest("2"), "new": digest("two")},
            },
        ],
        "not in cache": [],
        "renamed": [],
    }

    assert dvc.diff(":/modify", ":/delete") == {
        "added": [],
        "deleted": [{"path": os.path.join("dir", "2"), "hash": digest("two")}],
        "modified": [
            {
                "path": os.path.join("dir", ""),
                "hash": {
                    "old": "9b5faf37366b3370fd98e3e60ca439c1.dir",
                    "new": "83ae82fb367ac9926455870773ff09e6.dir",
                },
            }
        ],
        "not in cache": [],
        "renamed": [],
    }


def test_diff_no_cache(tmp_dir, scm, dvc):
    tmp_dir.dvc_gen({"dir": {"file": "file content"}}, commit="first")
    scm.tag("v1")

    tmp_dir.dvc_gen({"dir": {"file": "modified file content"}}, commit="second")
    scm.tag("v2")

    remove(dvc.cache.local.path)

    # invalidate_dir_info to force cache loading
    dvc.cache.local._dir_info = {}

    diff = dvc.diff("v1", "v2")
    assert diff["added"] == []
    assert diff["deleted"] == []
    assert first(diff["modified"])["path"] == os.path.join("dir", "")
    assert diff["not in cache"] == []

    (tmp_dir / "dir" / "file").unlink()
    remove(str(tmp_dir / "dir"))
    diff = dvc.diff()
    assert diff["added"] == []
    assert diff["deleted"] == [
        {
            "path": os.path.join("dir", ""),
            "hash": "f0f7a307d223921557c929f944bf5303.dir",
        }
    ]
    assert diff["renamed"] == []
    assert diff["modified"] == []
    assert diff["not in cache"] == [
        {
            "path": os.path.join("dir", ""),
            "hash": "f0f7a307d223921557c929f944bf5303.dir",
        }
    ]


def test_diff_dirty(tmp_dir, scm, dvc):
    tmp_dir.dvc_gen(
        {"file": "file_content", "dir": {"dir_file1": "dir file content"}},
        commit="initial",
    )

    (tmp_dir / "file").unlink()
    tmp_dir.gen({"dir": {"dir_file2": "dir file 2 content"}})
    tmp_dir.dvc_gen("new_file", "new_file_content")

    result = dvc.diff()

    assert result == {
        "added": [
            {
                "hash": digest("dir file 2 content"),
                "path": os.path.join("dir", "dir_file2"),
            },
            {"hash": "86d049de17c76ac44cdcac146042ec9b", "path": "new_file"},
        ],
        "deleted": [{"hash": "7f0b6bb0b7e951b7fd2b2a4a326297e1", "path": "file"}],
        "modified": [
            {
                "hash": {
                    "new": "38175ad60f0e58ac94e0e2b7688afd81.dir",
                    "old": "92daf39af116ca2fb245acaeb2ae65f7.dir",
                },
                "path": os.path.join("dir", ""),
            }
        ],
        "not in cache": [],
        "renamed": [],
    }


def test_no_changes(tmp_dir, scm, dvc):
    tmp_dir.dvc_gen("file", "first", commit="add a file")
    assert dvc.diff() == {}


def test_no_commits(tmp_dir):
    from dvc.repo import Repo
    from dvc.scm import Git

    git = Git.init(tmp_dir.fs_path)
    assert git.no_commits

    assert Repo.init().diff() == {}


def test_abs_target(tmp_dir, scm, dvc):
    tmp_dir.dvc_gen("file", "text")

    assert dvc.diff(targets=(tmp_dir / "file").fs_path) == {
        "added": [{"path": "file", "hash": digest("text")}],
        "deleted": [],
        "modified": [],
        "not in cache": [],
        "renamed": [],
    }


def setup_targets_test(tmp_dir):
    tmp_dir.dvc_gen("file", "first", commit="add a file")

    tmp_dir.dvc_gen({"dir": {"1": "1", "2": "2"}})
    tmp_dir.dvc_gen("file", "second")

    tmp_dir.dvc_gen(os.path.join("dir_with", "file.txt"), "first")


def test_targets_missing_path(tmp_dir, scm, dvc):
    setup_targets_test(tmp_dir)

    with pytest.raises(FileNotFoundError):
        dvc.diff(targets=["missing"])


def test_targets_single_file(tmp_dir, scm, dvc):
    setup_targets_test(tmp_dir)

    assert dvc.diff(targets=["file"]) == {
        "added": [],
        "deleted": [],
        "modified": [
            {
                "path": "file",
                "hash": {"old": digest("first"), "new": digest("second")},
            }
        ],
        "not in cache": [],
        "renamed": [],
    }


def test_targets_single_dir(tmp_dir, scm, dvc):
    setup_targets_test(tmp_dir)

    dir_checksum = "5fb6b29836c388e093ca0715c872fe2a.dir"

    expected_result = {
        "added": [
            {"path": os.path.join("dir", ""), "hash": dir_checksum},
            {"path": os.path.join("dir", "1"), "hash": digest("1")},
            {"path": os.path.join("dir", "2"), "hash": digest("2")},
        ],
        "deleted": [],
        "modified": [],
        "not in cache": [],
        "renamed": [],
    }

    assert dvc.diff(targets=["dir"]) == expected_result
    assert dvc.diff(targets=["dir" + os.path.sep]) == expected_result


def test_targets_single_file_in_dir(tmp_dir, scm, dvc):
    setup_targets_test(tmp_dir)

    assert dvc.diff(targets=[os.path.join("dir", "1")]) == {
        "added": [{"path": os.path.join("dir", "1"), "hash": digest("1")}],
        "deleted": [],
        "modified": [],
        "not in cache": [],
        "renamed": [],
    }


def test_targets_two_files_in_dir(tmp_dir, scm, dvc):
    setup_targets_test(tmp_dir)

    assert dvc.diff(targets=[os.path.join("dir", "1"), os.path.join("dir", "2")]) == {
        "added": [
            {"path": os.path.join("dir", "1"), "hash": digest("1")},
            {"path": os.path.join("dir", "2"), "hash": digest("2")},
        ],
        "deleted": [],
        "modified": [],
        "not in cache": [],
        "renamed": [],
    }


def test_targets_file_and_dir(tmp_dir, scm, dvc):
    setup_targets_test(tmp_dir)

    dir_checksum = "5fb6b29836c388e093ca0715c872fe2a.dir"

    assert dvc.diff(targets=["file", "dir"]) == {
        "added": [
            {"path": os.path.join("dir", ""), "hash": dir_checksum},
            {"path": os.path.join("dir", "1"), "hash": digest("1")},
            {"path": os.path.join("dir", "2"), "hash": digest("2")},
        ],
        "deleted": [],
        "modified": [
            {
                "path": "file",
                "hash": {"old": digest("first"), "new": digest("second")},
            }
        ],
        "not in cache": [],
        "renamed": [],
    }


def test_targets_single_dir_with_file(tmp_dir, scm, dvc):
    setup_targets_test(tmp_dir)

    expected_result = {
        "added": [
            {
                "path": os.path.join("dir_with", "file.txt"),
                "hash": digest("first"),
            }
        ],
        "deleted": [],
        "modified": [],
        "not in cache": [],
        "renamed": [],
    }

    assert dvc.diff(targets=["dir_with"], recursive=True) == expected_result
    assert (
        dvc.diff(targets=["dir_with" + os.path.sep], recursive=True) == expected_result
    )


def test_targets_single_file_in_dir_with_file(tmp_dir, scm, dvc):
    setup_targets_test(tmp_dir)

    assert dvc.diff(targets=[os.path.join("dir_with", "file.txt")]) == {
        "added": [
            {
                "path": os.path.join("dir_with", "file.txt"),
                "hash": digest("first"),
            }
        ],
        "deleted": [],
        "modified": [],
        "not in cache": [],
        "renamed": [],
    }


@pytest.mark.parametrize("commit_last", [True, False])
def test_diff_add_similar_files(tmp_dir, scm, dvc, commit_last):
    if commit_last:
        last_commit_msg = "commit #2"
        a_rev = "HEAD~1"
    else:
        last_commit_msg = None
        a_rev = "HEAD"

    tmp_dir.dvc_gen(
        {"dir": {"file": "text1", "subdir": {"file2": "text2"}}},
        commit="commit #1",
    )
    tmp_dir.dvc_gen(
        {"dir2": {"file": "text1", "subdir": {"file2": "text2"}}},
        commit=last_commit_msg,
    )

    assert dvc.diff(a_rev) == {
        "added": [
            {
                "path": os.path.join("dir2", ""),
                "hash": "cb58ee07cb01044db229e4d6121a0dfc.dir",
            },
            {
                "path": os.path.join("dir2", "file"),
                "hash": "cef7ccd89dacf1ced6f5ec91d759953f",
            },
            {
                "path": os.path.join("dir2", "subdir", "file2"),
                "hash": "fe6123a759017e4a2af4a2d19961ed71",
            },
        ],
        "deleted": [],
        "modified": [],
        "renamed": [],
        "not in cache": [],
    }


@pytest.mark.parametrize("commit_last", [True, False])
def test_diff_rename_folder(tmp_dir, scm, dvc, commit_last):
    if commit_last:
        last_commit_msg = "commit #2"
        a_rev = "HEAD~1"
    else:
        last_commit_msg = None
        a_rev = "HEAD"

    tmp_dir.dvc_gen(
        {"dir": {"file": "text1", "subdir": {"file2": "text2"}}},
        commit="commit #1",
    )
    (tmp_dir / "dir").replace(tmp_dir / "dir2")
    tmp_dir.dvc_add("dir2", commit=last_commit_msg)
    assert dvc.diff(a_rev) == {
        "added": [],
        "deleted": [],
        "modified": [],
        "renamed": [
            {
                "path": {
                    "old": os.path.join("dir", ""),
                    "new": os.path.join("dir2", ""),
                },
                "hash": "cb58ee07cb01044db229e4d6121a0dfc.dir",
            },
            {
                "path": {
                    "old": os.path.join("dir", "file"),
                    "new": os.path.join("dir2", "file"),
                },
                "hash": "cef7ccd89dacf1ced6f5ec91d759953f",
            },
            {
                "path": {
                    "old": os.path.join("dir", "subdir", "file2"),
                    "new": os.path.join("dir2", "subdir", "file2"),
                },
                "hash": "fe6123a759017e4a2af4a2d19961ed71",
            },
        ],
        "not in cache": [],
    }


@pytest.mark.parametrize("commit_last", [True, False])
def test_diff_rename_file(tmp_dir, scm, dvc, commit_last):
    if commit_last:
        last_commit_msg = "commit #2"
        a_rev = "HEAD~1"
    else:
        last_commit_msg = None
        a_rev = "HEAD"

    paths = tmp_dir.gen({"dir": {"file": "text1", "subdir": {"file2": "text2"}}})
    tmp_dir.dvc_add(paths, commit="commit #1")
    (tmp_dir / "dir" / "file").replace(tmp_dir / "dir" / "subdir" / "file3")

    tmp_dir.dvc_add(paths, commit=last_commit_msg)
    assert dvc.diff(a_rev) == {
        "added": [],
        "deleted": [],
        "modified": [
            {
                "path": os.path.join("dir", ""),
                "hash": {
                    "old": "cb58ee07cb01044db229e4d6121a0dfc.dir",
                    "new": "a4ac9c339aacc60b6a3152e362c319c8.dir",
                },
            }
        ],
        "renamed": [
            {
                "path": {
                    "old": os.path.join("dir", "file"),
                    "new": os.path.join("dir", "subdir", "file3"),
                },
                "hash": "cef7ccd89dacf1ced6f5ec91d759953f",
            }
        ],
        "not in cache": [],
    }


def test_rename_multiple_files_same_hashes(tmp_dir, scm, dvc):
    """Test diff by renaming >=2 instances of file with same hashes.

    DVC should be able to detect that they are renames, and should not include
    them in either of the `added` or the `deleted` section.
    """
    tmp_dir.dvc_gen(
        {"dir": {"foo": "foo", "subdir": {"foo": "foo"}}}, commit="commit #1"
    )
    remove(tmp_dir / "dir")
    # changing foo and subdir/foo to bar and subdir/bar respectively
    tmp_dir.dvc_gen(
        {"dir": {"bar": "foo", "subdir": {"bar": "foo"}}}, commit="commit #2"
    )
    assert dvc.diff("HEAD~") == {
        "added": [],
        "deleted": [],
        "modified": [
            {
                "hash": {
                    "new": "31b36b3ea5f4485e27f10578c47183b0.dir",
                    "old": "c7684c8b3b0d28cf80d5305e2d856bfc.dir",
                },
                "path": os.path.join("dir", ""),
            }
        ],
        "not in cache": [],
        "renamed": [
            {
                "hash": "acbd18db4cc2f85cedef654fccc4a4d8",
                "path": {
                    "new": os.path.join("dir", "bar"),
                    "old": os.path.join("dir", "foo"),
                },
            },
            {
                "hash": "acbd18db4cc2f85cedef654fccc4a4d8",
                "path": {
                    "new": os.path.join("dir", "subdir", "bar"),
                    "old": os.path.join("dir", "subdir", "foo"),
                },
            },
        ],
    }




tests/func/test_dvcfile.py
import textwrap

import pytest

from dvc.annotations import Annotation
from dvc.dvcfile import (
    LOCK_FILE,
    PROJECT_FILE,
    ParametrizedDumpError,
    SingleStageFile,
    load_file,
)
from dvc.stage.exceptions import StageFileDoesNotExistError
from dvc.stage.loader import StageNotFound
from dvc.utils.strictyaml import YAMLValidationError

STAGE_EXAMPLE = {
    "stage1": {
        "cmd": "cp foo bar",
        "desc": "stage desc",
        "meta": {"key1": "value1", "key2": "value2"},
        "deps": ["foo"],
        "outs": [{"bar": {"desc": "bar desc", "meta": {"key": "value"}}}],
    }
}


def test_run_load_one_for_multistage(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    stage1 = dvc.run(
        cmd="cp foo foo2",
        deps=["foo"],
        name="copy-foo-foo2",
        outs_persist_no_cache=["foo2"],
        always_changed=True,
    )
    stage2 = load_file(dvc, PROJECT_FILE).stages["copy-foo-foo2"]
    assert stage1 == stage2
    foo_out = stage2.outs[0]
    assert stage2.cmd == "cp foo foo2"
    assert stage2.name == "copy-foo-foo2"
    assert foo_out.def_path == "foo2"
    assert foo_out.persist
    assert not foo_out.use_cache
    assert stage2.deps[0].def_path == "foo"
    assert dvc.reproduce(":copy-foo-foo2")


def test_run_load_one_for_multistage_non_existing(tmp_dir, dvc):
    with pytest.raises(StageFileDoesNotExistError):
        assert load_file(dvc, PROJECT_FILE).stages.get("copy-foo-foo2")


def test_run_load_one_for_multistage_non_existing_stage_name(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    stage = dvc.run(
        cmd="cp foo foo2",
        deps=["foo"],
        name="copy-foo-foo2",
        metrics=["foo2"],
        always_changed=True,
    )
    with pytest.raises(StageNotFound):
        assert load_file(dvc, stage.path).stages["random-name"]


def test_run_load_one_on_single_stage(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    stage = dvc.run(
        cmd="cp foo foo2",
        deps=["foo"],
        metrics=["foo2"],
        always_changed=True,
        single_stage=True,
    )
    assert isinstance(load_file(dvc, stage.path), SingleStageFile)
    assert load_file(dvc, stage.path).stages.get("random-name") == stage
    assert load_file(dvc, stage.path).stage == stage


def test_has_stage_with_name(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    dvc.run(
        cmd="cp foo foo2",
        deps=["foo"],
        name="copy-foo-foo2",
        metrics=["foo2"],
        always_changed=True,
    )
    dvcfile = load_file(dvc, PROJECT_FILE)
    assert "copy-foo-foo2" in dvcfile.stages
    assert "copy" not in dvcfile.stages


def test_load_all_multistage(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    stage1 = dvc.run(
        cmd="cp foo foo2",
        deps=["foo"],
        name="copy-foo-foo2",
        metrics=["foo2"],
        always_changed=True,
    )
    stages = load_file(dvc, PROJECT_FILE).stages.values()
    assert len(stages) == 1
    assert list(stages) == [stage1]

    tmp_dir.gen("bar", "bar")
    stage2 = dvc.run(
        cmd="cp bar bar2",
        deps=["bar"],
        name="copy-bar-bar2",
        metrics=["bar2"],
        always_changed=True,
    )
    assert set(load_file(dvc, PROJECT_FILE).stages.values()) == {
        stage2,
        stage1,
    }


def test_load_all_singlestage(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    stage1 = dvc.run(
        cmd="cp foo foo2",
        deps=["foo"],
        metrics=["foo2"],
        always_changed=True,
        single_stage=True,
    )
    dvcfile = load_file(dvc, "foo2.dvc")
    assert isinstance(dvcfile, SingleStageFile)
    assert len(dvcfile.stages) == 1
    stages = dvcfile.stages.values()
    assert len(stages) == 1
    assert list(stages) == [stage1]


def test_try_get_single_stage_from_pipeline_file(tmp_dir, dvc):
    from dvc.dvcfile import DvcException

    tmp_dir.gen("foo", "foo")
    dvc.run(
        cmd="cp foo foo2",
        deps=["foo"],
        name="copy-foo-foo2",
        metrics=["foo2"],
        always_changed=True,
    )
    with pytest.raises(DvcException):
        assert load_file(dvc, PROJECT_FILE).stage


def test_stage_collection(tmp_dir, dvc):
    tmp_dir.gen(
        {
            "dir": {"file1": "file1", "file2": "file2"},
            "foo": "foo",
            "bar": "bar",
        }
    )
    (stage1,) = dvc.add("dir")
    stage2 = dvc.run(
        cmd="cp foo foo2",
        deps=["foo"],
        name="copy-foo-foo2",
        metrics=["foo2"],
        always_changed=True,
    )
    stage3 = dvc.run(
        cmd="cp bar bar2",
        deps=["bar"],
        metrics=["bar2"],
        always_changed=True,
        single_stage=True,
    )
    assert set(dvc.index.stages) == {stage1, stage3, stage2}


def test_remove_stage(tmp_dir, dvc, run_copy):
    tmp_dir.gen("foo", "foo")
    stage = run_copy("foo", "bar", name="copy-foo-bar")
    stage2 = run_copy("bar", "foobar", name="copy-bar-foobar")

    dvc_file = load_file(dvc, PROJECT_FILE)
    assert dvc_file.exists()
    assert {"copy-bar-foobar", "copy-foo-bar"} == set(
        dvc_file._load()[0]["stages"].keys()
    )

    dvc_file.remove_stage(stage)

    assert ["copy-bar-foobar"] == list(dvc_file._load()[0]["stages"].keys())

    # sanity check
    stage2.reload()

    # re-check to see if it fails if there's no stage entry
    dvc_file.remove_stage(stage)
    dvc_file.remove(force=True)
    # should not fail when there's no file at all.
    dvc_file.remove_stage(stage)


def test_remove_stage_lockfile(tmp_dir, dvc, run_copy):
    tmp_dir.gen("foo", "foo")
    stage = run_copy("foo", "bar", name="copy-foo-bar")
    stage2 = run_copy("bar", "foobar", name="copy-bar-foobar")

    dvc_file = load_file(dvc, PROJECT_FILE)
    lock_file = dvc_file._lockfile
    assert dvc_file.exists()
    assert lock_file.exists()
    assert {"copy-bar-foobar", "copy-foo-bar"} == set(lock_file.load()["stages"].keys())
    lock_file.remove_stage(stage)

    assert ["copy-bar-foobar"] == list(lock_file.load()["stages"].keys())

    # sanity check
    stage2.reload()

    # re-check to see if it fails if there's no stage entry
    lock_file.remove_stage(stage)
    lock_file.remove()
    # should not fail when there's no file at all.
    lock_file.remove_stage(stage)


def test_remove_stage_dvcfiles(tmp_dir, dvc, run_copy):
    tmp_dir.gen("foo", "foo")
    stage = run_copy("foo", "bar", single_stage=True)

    dvc_file = load_file(dvc, stage.path)
    assert dvc_file.exists()
    dvc_file.remove_stage(stage)
    assert not dvc_file.exists()

    # re-check to see if it fails if there's no stage entry
    dvc_file.remove_stage(stage)
    dvc_file.remove(force=True)

    # should not fail when there's no file at all.
    dvc_file.remove_stage(stage)


def test_remove_stage_on_lockfile_format_error(tmp_dir, dvc, run_copy):
    tmp_dir.gen("foo", "foo")
    stage = run_copy("foo", "bar", name="copy-foo-bar")
    dvc_file = load_file(dvc, stage.path)
    lock_file = dvc_file._lockfile

    data = dvc_file._load()[0]
    lock_data = lock_file.load()
    lock_data["gibberish"] = True
    data["gibberish"] = True
    (tmp_dir / lock_file.relpath).dump(lock_data)
    with pytest.raises(YAMLValidationError):
        dvc_file.remove_stage(stage)

    lock_file.remove()
    dvc_file.dump(stage, update_pipeline=False)

    (tmp_dir / dvc_file.relpath).dump(data)
    with pytest.raises(YAMLValidationError):
        dvc_file.remove_stage(stage)


def test_remove_stage_preserves_comment(tmp_dir, dvc, run_copy):
    tmp_dir.gen(
        "dvc.yaml",
        textwrap.dedent(
            """\
            stages:
                generate-foo:
                    cmd: "echo foo > foo"
                    # This copies 'foo' text to 'foo' file.
                    outs:
                    - foo
                copy-foo-bar:
                    cmd: "python copy.py foo bar"
                    deps:
                    - foo
                    outs:
                    - bar"""
        ),
    )

    dvc.reproduce(PROJECT_FILE)

    dvc_file = load_file(dvc, PROJECT_FILE)

    assert dvc_file.exists()
    assert (tmp_dir / LOCK_FILE).exists()
    assert (tmp_dir / "foo").exists()
    assert (tmp_dir / "bar").exists()

    dvc_file.remove_stage(dvc_file.stages["copy-foo-bar"])
    assert (
        "# This copies 'foo' text to 'foo' file."
        in (tmp_dir / PROJECT_FILE).read_text()
    )


def test_remove_stage_removes_dvcfiles_if_no_stages_left(tmp_dir, dvc, run_copy):
    tmp_dir.gen("foo", "foo")
    run_copy("foo", "bar", name="run_copy")

    dvc_file = load_file(dvc, PROJECT_FILE)

    assert dvc_file.exists()
    assert (tmp_dir / LOCK_FILE).exists()
    assert (tmp_dir / "foo").exists()

    dvc_file.remove_stage(dvc_file.stages["run_copy"])
    assert not dvc_file.exists()
    assert not (tmp_dir / LOCK_FILE).exists()


def test_dvcfile_dump_preserves_meta(tmp_dir, dvc, run_copy):
    tmp_dir.gen("foo", "foo")
    stage = run_copy("foo", "bar", name="run_copy")
    dvcfile = stage.dvcfile

    data = dvcfile._load()[0]
    metadata = {"name": "copy-file"}
    stage.meta = metadata
    data["stages"]["run_copy"]["meta"] = metadata

    dvcfile.dump(stage)
    assert dvcfile._load()[0] == data
    assert dvcfile._load()[0]["stages"]["run_copy"]["meta"] == metadata


def test_dvcfile_dump_preserves_desc(tmp_dir, dvc, run_copy):
    tmp_dir.gen("foo", "foo")
    stage_desc = "test stage description"
    out_desc = "test out description"

    stage = run_copy("foo", "bar", name="run_copy", desc=stage_desc)
    dvcfile = stage.dvcfile

    data = dvcfile._load()[0]
    data["stages"]["run_copy"]["outs"][0] = {"bar": {"desc": out_desc}}
    (tmp_dir / dvcfile.path).dump(data)

    assert stage.desc == stage_desc
    stage.outs[0].annot.desc = out_desc
    dvcfile.dump(stage)
    loaded = dvcfile._load()[0]
    assert loaded == data
    assert loaded["stages"]["run_copy"]["desc"] == stage_desc
    assert loaded["stages"]["run_copy"]["outs"][0]["bar"]["desc"] == out_desc


def test_dvcfile_dump_preserves_comments(tmp_dir, dvc):
    text = textwrap.dedent(
        """\
        stages:
          generate-foo:
            cmd: echo foo > foo
            # This copies 'foo' text to 'foo' file.
            outs:
            - foo"""
    )
    tmp_dir.gen("dvc.yaml", text)
    stage = dvc.stage.load_one(name="generate-foo")
    stage.outs[0].use_cache = False
    dvcfile = stage.dvcfile

    dvcfile.dump(stage)
    assert dvcfile._load()[1] == (text + ":\n\tcache: false\n".expandtabs())


@pytest.mark.parametrize(
    "data, name",
    [
        ({"build-us": {"cmd": "echo ${foo}"}}, "build-us"),
        (
            {"build": {"foreach": ["us", "gb"], "do": {"cmd": "echo ${foo}"}}},
            "build@us",
        ),
    ],
)
def test_dvcfile_try_dumping_parametrized_stage(tmp_dir, dvc, data, name):
    (tmp_dir / "dvc.yaml").dump({"stages": data, "vars": [{"foo": "foobar"}]})

    stage = dvc.stage.load_one(name=name)
    dvcfile = stage.dvcfile

    with pytest.raises(ParametrizedDumpError) as exc:
        dvcfile.dump(stage)

    assert str(exc.value) == f"cannot dump a parametrized stage: '{name}'"


def test_dvcfile_load_dump_stage_with_desc_meta(tmp_dir, dvc):
    data = {"stages": STAGE_EXAMPLE}
    (tmp_dir / "dvc.yaml").dump(data)

    stage = dvc.stage.load_one(name="stage1")
    assert stage.meta == {"key1": "value1", "key2": "value2"}
    assert stage.desc == "stage desc"
    assert stage.outs[0].annot == Annotation(desc="bar desc", meta={"key": "value"})

    # sanity check
    stage.dump()
    assert (tmp_dir / "dvc.yaml").parse() == data


def test_dvcfile_load_with_plots(tmp_dir, dvc):
    (tmp_dir / "dvc.yaml").dump(
        {
            "plots": [
                {"path/to/plot": {"x": "value", "y": "value"}},
                {"path/to/another/plot": {"x": "value", "y": "value"}},
                {"path/to/empty/plot": None},
                "path/to/plot/str",
            ],
            "stages": STAGE_EXAMPLE,
        },
    )
    plots = list(dvc.plots.collect())
    top_level_plots = plots[0]["workspace"]["definitions"]["data"]["dvc.yaml"]["data"]
    assert all(
        name in top_level_plots for name in ("path/to/plot", "path/to/another/plot")
    )




tests/func/test_external_repo.py
import os
from unittest.mock import ANY

from dvc.repo.open_repo import CLONES
from dvc.repo.open_repo import _external_repo as external_repo
from dvc.scm import Git
from dvc.testing.tmp_dir import make_subrepo
from dvc.utils import relpath
from dvc.utils.fs import remove
from dvc_data.hashfile.build import build
from dvc_data.hashfile.transfer import transfer


def test_external_repo(erepo_dir, mocker):
    with erepo_dir.chdir():
        with erepo_dir.branch("branch", new=True):
            erepo_dir.dvc_gen("file", "branch", commit="create file on branch")
        erepo_dir.dvc_gen("file", "master", commit="create file on master")

    url = os.fspath(erepo_dir)

    clone_spy = mocker.spy(Git, "clone")

    with external_repo(url) as repo:
        with repo.dvcfs.open("file") as fd:
            assert fd.read() == "master"

    with external_repo(url, rev="branch") as repo:
        with repo.dvcfs.open("file") as fd:
            assert fd.read() == "branch"

    assert clone_spy.call_count == 1


def test_source_change(erepo_dir):
    url = os.fspath(erepo_dir)
    with external_repo(url) as repo:
        old_rev = repo.scm.get_rev()

    erepo_dir.scm_gen("file", "text", commit="a change")

    with external_repo(url) as repo:
        new_rev = repo.scm.get_rev()

    assert old_rev != new_rev


def test_cache_reused(erepo_dir, mocker, local_cloud):
    from dvc_objects.fs import generic

    erepo_dir.add_remote(config=local_cloud.config)
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("file", "text", commit="add file")
    erepo_dir.dvc.push()

    download_spy = mocker.spy(generic, "transfer")

    # Use URL to prevent any fishy optimizations
    url = f"file://{erepo_dir.as_posix()}"
    with external_repo(url) as repo:
        repo.fetch()
        assert download_spy.mock.call_count == 1

    # Should not download second time
    erepo_dir.scm.branch("branch")
    with external_repo(url, "branch") as repo:
        repo.fetch()
        assert download_spy.mock.call_count == 1


def test_known_sha(erepo_dir):
    erepo_dir.scm.commit("init")

    url = f"file://{erepo_dir.as_posix()}"
    with external_repo(url) as repo:
        rev = repo.scm.get_rev()
        prev_rev = repo.scm.resolve_rev("HEAD^")

    # Hits cache
    with external_repo(url, rev) as repo:
        pass

    # No clone, no pull, copies a repo, checks out the known sha
    with external_repo(url, prev_rev) as repo:
        pass


def test_pull_subdir_file(tmp_dir, erepo_dir):
    with erepo_dir.chdir():
        subdir = erepo_dir / "subdir"
        subdir.mkdir()
        (subdir / "file").write_text("contents")
        erepo_dir.dvc_add(subdir / "file", commit="create file")

    dest = tmp_dir / "file"
    with external_repo(os.fspath(erepo_dir)) as repo:
        repo.dvcfs.get(
            "subdir/file",
            os.fspath(dest),
        )

    assert dest.is_file()
    assert dest.read_text() == "contents"


def test_relative_remote(erepo_dir, tmp_dir):
    # these steps reproduce the script on this issue:
    # https://github.com/iterative/dvc/issues/2756
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("file", "contents", commit="create file")

    upstream_dir = tmp_dir
    upstream_url = relpath(upstream_dir, erepo_dir)
    erepo_dir.add_remote(url=upstream_url)

    erepo_dir.dvc.push()

    (erepo_dir / "file").unlink()
    remove(erepo_dir.dvc.cache.local.path)

    url = os.fspath(erepo_dir)

    with external_repo(url) as repo:
        assert os.path.isabs(repo.config["remote"]["upstream"]["url"])
        assert os.path.isdir(repo.config["remote"]["upstream"]["url"])
        with repo.dvcfs.open("file") as fd:
            assert fd.read() == "contents"


def test_shallow_clone_branch(erepo_dir, mocker):
    with erepo_dir.chdir():
        with erepo_dir.branch("branch", new=True):
            erepo_dir.dvc_gen("file", "branch", commit="create file on branch")
        erepo_dir.dvc_gen("file", "master", commit="create file on master")

    url = os.fspath(erepo_dir)
    clone_spy = mocker.spy(Git, "clone")

    with external_repo(url, rev="branch") as repo:
        with repo.dvcfs.open("file") as fd:
            assert fd.read() == "branch"

    clone_spy.assert_called_with(url, ANY, shallow_branch="branch", progress=ANY)

    path, _ = CLONES[url]
    CLONES[url] = (path, True)

    mock_fetch = mocker.patch.object(Git, "fetch")
    with external_repo(url) as repo:
        with repo.dvcfs.open("file") as fd:
            assert fd.read() == "master"
    mock_fetch.assert_called_with(unshallow=True)


def test_shallow_clone_tag(erepo_dir, mocker):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("file", "foo", commit="init")
        erepo_dir.scm.tag("v1")
        erepo_dir.dvc_gen("file", "bar", commit="update file")

    url = os.fspath(erepo_dir)

    clone_spy = mocker.spy(Git, "clone")
    with external_repo(url, rev="v1") as repo:
        with repo.dvcfs.open("file") as fd:
            assert fd.read() == "foo"

    clone_spy.assert_called_with(url, ANY, shallow_branch="v1", progress=ANY)

    path, _ = CLONES[url]
    CLONES[url] = (path, True)

    mock_fetch = mocker.patch.object(Git, "fetch")
    with external_repo(url, rev="master") as repo:
        with repo.dvcfs.open("file") as fd:
            assert fd.read() == "bar"
    mock_fetch.assert_called_with(unshallow=True)


def test_subrepos_are_ignored(tmp_dir, erepo_dir):
    subrepo = erepo_dir / "dir" / "subrepo"
    make_subrepo(subrepo, erepo_dir.scm)
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("dir/foo", "foo", commit="foo")
        erepo_dir.scm_gen("dir/bar", "bar", commit="bar")

    with subrepo.chdir():
        subrepo.dvc_gen({"file": "file"}, commit="add files on subrepo")

    with external_repo(os.fspath(erepo_dir)) as repo:
        repo.dvcfs.get(
            "dir",
            os.fspath(tmp_dir / "out"),
        )
        expected_files = {"foo": "foo", "bar": "bar", ".gitignore": "/foo\n"}
        assert (tmp_dir / "out").read_text() == expected_files

        # clear cache to test saving to cache
        cache_dir = tmp_dir / repo.cache.local.path
        remove(cache_dir)
        os.makedirs(cache_dir)

        staging, _, obj = build(
            repo.cache.local,
            "dir",
            repo.dvcfs,
            "md5",
            ignore=repo.dvcignore,
        )
        transfer(
            staging,
            repo.cache.local,
            {obj.hash_info},
            shallow=False,
            hardlink=True,
        )
        assert set(cache_dir.glob("??/*")) == {
            cache_dir / "e1" / "d9e8eae5374860ae025ec84cfd85c7.dir",
            cache_dir / "37" / "b51d194a7513e45b56f6524f2d51f2",
            cache_dir / "94" / "7d2b84e5aa88170e80dff467a5bfb6",
            cache_dir / "ac" / "bd18db4cc2f85cedef654fccc4a4d8",
        }


def test_subrepos_are_ignored_for_git_tracked_dirs(tmp_dir, erepo_dir):
    subrepo = erepo_dir / "dir" / "subrepo"
    make_subrepo(subrepo, erepo_dir.scm)
    with erepo_dir.chdir():
        scm_files = {"foo": "foo", "bar": "bar", "subdir": {"lorem": "lorem"}}
        erepo_dir.scm_gen({"dir": scm_files}, commit="add scm dir")

    with subrepo.chdir():
        subrepo.dvc_gen({"file": "file"}, commit="add files on subrepo")

    with external_repo(os.fspath(erepo_dir)) as repo:
        repo.dvcfs.get(
            "dir",
            os.fspath(tmp_dir / "out"),
        )
        # subrepo files should not be here
        assert (tmp_dir / "out").read_text() == scm_files




tests/func/test_fs.py
import os
from operator import itemgetter

from dvc.repo import Repo


def test_cleanfs_subrepo(tmp_dir, dvc, scm, monkeypatch):
    tmp_dir.gen({"subdir": {}})
    subrepo_dir = tmp_dir / "subdir"
    with subrepo_dir.chdir():
        subrepo = Repo.init(subdir=True)
        subrepo_dir.gen({"foo": "foo", "dir": {"bar": "bar"}})

    path = subrepo_dir.fs_path

    assert dvc.fs.exists(dvc.fs.path.join(path, "foo"))
    assert dvc.fs.isfile(dvc.fs.path.join(path, "foo"))
    assert dvc.fs.exists(dvc.fs.path.join(path, "dir"))
    assert dvc.fs.isdir(dvc.fs.path.join(path, "dir"))

    assert subrepo.fs.exists(subrepo.fs.path.join(path, "foo"))
    assert subrepo.fs.isfile(subrepo.fs.path.join(path, "foo"))
    assert subrepo.fs.exists(subrepo.fs.path.join(path, "dir"))
    assert subrepo.fs.isdir(subrepo.fs.path.join(path, "dir"))


def test_walk_dont_ignore_subrepos(tmp_dir, scm, dvc):
    tmp_dir.dvc_gen({"foo": "foo"}, commit="add foo")
    subrepo_dir = tmp_dir / "subdir"
    subrepo_dir.mkdir()
    with subrepo_dir.chdir():
        Repo.init(subdir=True)
    scm.add(["subdir"])
    scm.commit("Add subrepo")

    dvc_fs = dvc.fs
    dvc._reset()
    scm_fs = scm.get_fs("HEAD")
    path = os.fspath(tmp_dir)
    get_dirs = itemgetter(1)

    assert set(get_dirs(next(dvc_fs.walk(path)))) == {".dvc", "subdir", ".git"}
    assert set(get_dirs(next(scm_fs.walk("/")))) == {".dvc", "subdir"}




tests/func/test_gc.py
import datetime
import logging
import os
import shutil
import textwrap

import pytest

from dvc.cli import main
from dvc.exceptions import CollectCacheError, InvalidArgumentError
from dvc.fs import LocalFileSystem
from dvc.utils.fs import remove
from dvc_data.hashfile.db.local import LocalHashFileDB


@pytest.fixture
def good_and_bad_cache(tmp_dir, dvc):
    tmp_dir.dvc_gen("foo", "foo")
    (stage,) = tmp_dir.dvc_gen(
        "data",
        {"sub": {"data_sub": "data_sub", "data": "data", "—Ç–µ—Å—Ç": "–ø—Ä–æ–≤–µ—Ä–∫–∞"}},
    )
    raw_dir_hash = stage.outs[0].hash_info.as_raw().value
    odb = dvc.cache.local

    bad_cache = {raw_dir_hash}
    for i in ["123", "234", "345"]:
        odb.add_bytes(i, i.encode("utf8"))
        bad_cache.add(i)

    good_cache = {md5 for md5 in odb.all() if md5 not in bad_cache}
    return good_cache, bad_cache


def test_gc_api(dvc, good_and_bad_cache):
    dvc.gc(workspace=True)
    odb = dvc.cache.local
    good_cache, bad_cache = good_and_bad_cache
    assert set(odb.oids_exist([*good_cache, *bad_cache])) == good_cache


def test_gc_cli(dvc, good_and_bad_cache):
    assert main(["gc", "-wf"]) == 0
    odb = dvc.cache.local
    good_cache, bad_cache = good_and_bad_cache
    assert set(odb.oids_exist([*good_cache, *bad_cache])) == good_cache


def test_gc_branches_tags(tmp_dir, dvc, scm):
    tmp_dir.dvc_gen("file", "v1.0", commit="v1.0")
    scm.tag("v1.0")

    with tmp_dir.branch("test", new=True):
        dvc.remove("file.dvc")
        tmp_dir.dvc_gen("file", "test", commit="test")

    dvc.remove("file.dvc")
    tmp_dir.dvc_gen("file", "trash", commit="trash")

    dvc.remove("file.dvc")
    tmp_dir.dvc_gen("file", "master", commit="trash")

    odb = dvc.cache.local
    assert len(list(odb.all())) == 4

    dvc.gc(all_tags=True, all_branches=True)
    assert len(list(odb.all())) == 3

    dvc.gc(all_tags=False, all_branches=True)
    assert len(list(odb.all())) == 2

    dvc.gc(all_tags=True, all_branches=False)
    assert len(list(odb.all())) == 1


def test_gc_multiple_dvc_repos(tmp_dir, scm, dvc, erepo_dir):
    tmp_dir.dvc_gen("only_in_first", "only in main repo")
    tmp_dir.dvc_gen("in_both", "in both repos")

    erepo_dir.dvc.cache.local.path = dvc.cache.local.path
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("in_both", "in both repos")
        erepo_dir.dvc_gen("only_in_second", "only in additional repo")

    odb = dvc.cache.local
    assert len(list(odb.all())) == 3

    dvc.gc(repos=[erepo_dir], workspace=True)
    assert len(list(odb.all())) == 3

    dvc.gc(workspace=True)
    assert len(list(odb.all())) == 2


def test_all_commits(tmp_dir, scm, dvc):
    tmp_dir.dvc_gen("testfile", "uncommitted")
    tmp_dir.dvc_gen("testfile", "committed", commit="committed")
    tmp_dir.dvc_gen("testfile", "modified", commit="modified")
    tmp_dir.dvc_gen("testfile", "workspace")

    n = _count_files(dvc.cache.local.path)
    dvc.gc(all_commits=True)

    # Only one uncommitted file should go away
    assert _count_files(dvc.cache.local.path) == n - 1


def test_gc_no_dir_cache(tmp_dir, dvc):
    tmp_dir.dvc_gen({"foo": "foo", "bar": "bar"})
    (dir_stage,) = tmp_dir.dvc_gen({"dir": {"x": "x", "subdir": {"y": "y"}}})

    remove(dir_stage.outs[0].cache_path)

    with pytest.raises(CollectCacheError):
        dvc.gc(workspace=True)

    assert _count_files(dvc.cache.local.path) == 4
    dvc.gc(force=True, workspace=True)
    assert _count_files(dvc.cache.local.path) == 2


def _count_files(path):
    return sum(len(files) for _, _, files in os.walk(path))


def test_gc_no_unpacked_dir(tmp_dir, dvc):
    dir_stages = tmp_dir.dvc_gen({"dir": {"file": "text"}})
    dvc.status()

    os.remove("dir.dvc")
    unpackeddir = dir_stages[0].outs[0].cache_path + LocalHashFileDB.UNPACKED_DIR_SUFFIX

    # older (pre 1.0) versions of dvc used to generate this dir
    shutil.copytree("dir", unpackeddir)
    assert os.path.exists(unpackeddir)

    dvc.gc(force=True, workspace=True)
    assert not os.path.exists(unpackeddir)


def test_gc_without_workspace_raises_error(tmp_dir, dvc):
    dvc.gc(force=True, workspace=True)  # works without error

    from dvc.exceptions import InvalidArgumentError

    with pytest.raises(InvalidArgumentError):
        dvc.gc(force=True)

    with pytest.raises(InvalidArgumentError):
        dvc.gc(force=True, workspace=False)


def test_gc_cloud_with_or_without_specifier(tmp_dir, erepo_dir, local_cloud):
    erepo_dir.add_remote(config=local_cloud.config)
    dvc = erepo_dir.dvc
    from dvc.exceptions import InvalidArgumentError

    with pytest.raises(InvalidArgumentError):
        dvc.gc(force=True, cloud=True)

    dvc.gc(cloud=True, all_tags=True)
    dvc.gc(cloud=True, all_commits=True)
    dvc.gc(cloud=True, all_branches=True)
    dvc.gc(cloud=True, all_commits=False, all_branches=True, all_tags=True)


def test_gc_without_workspace_on_tags_branches_commits(tmp_dir, dvc):
    dvc.gc(force=True, all_tags=True)
    dvc.gc(force=True, all_commits=True)
    dvc.gc(force=False, all_branches=True)

    # even if workspace is disabled, and others are enabled, assume as if
    # workspace is enabled.
    dvc.gc(force=False, all_branches=True, all_commits=False, workspace=False)


@pytest.mark.parametrize("cloud", ["c", ""])
def test_gc_without_workspace(tmp_dir, dvc, caplog, cloud):
    with caplog.at_level(logging.WARNING, logger="dvc"):
        assert main(["gc", f"-{cloud}vf"]) == 255

    assert (
        "Either of `-w|--workspace`, `-a|--all-branches`, `-T|--all-tags` "
        "`--all-experiments`, `--all-commits`, `--date` or `--rev` "
        "needs to be set." in caplog.text
    )


def test_gc_with_possible_args_positive(tmp_dir, dvc):
    for flag in ["-w", "-a", "-T", "--all-commits", "-aT", "-wa", "-waT"]:
        assert main(["gc", "-vf", flag]) == 0


def test_gc_cloud_positive(tmp_dir, dvc, tmp_path_factory, local_remote):
    for flag in ["-cw", "-ca", "-cT", "-caT", "-cwT"]:
        assert main(["gc", "-vf", flag]) == 0


def test_gc_cloud_remove_order(tmp_dir, scm, dvc, tmp_path_factory, mocker):
    storage = os.fspath(tmp_path_factory.mktemp("test_remote_base"))
    dvc.config["remote"]["local_remote"] = {"url": storage}
    dvc.config["core"]["remote"] = "local_remote"

    (standalone, dir1, dir2) = tmp_dir.dvc_gen(
        {
            "file1": "standalone",
            "dir1": {"file2": "file2"},
            "dir2": {"file3": "file3", "file4": "file4"},
        }
    )
    dvc.push()
    dvc.remove(standalone.relpath)
    dvc.remove(dir1.relpath)
    dvc.remove(dir2.relpath)
    dvc.gc(workspace=True)

    mocked_remove = mocker.patch.object(LocalFileSystem, "remove", autospec=True)
    dvc.gc(workspace=True, cloud=True)
    assert len(mocked_remove.mock_calls) == 4
    # Unpacked dir should be the first removed
    for args in mocked_remove.call_args_list[:2]:
        checksum = str(args[0][1])
        assert checksum.endswith(".dir.unpacked")
    # Then, bulk remove should be applied

    # First to `.dir`
    checksums = mocked_remove.call_args_list[2][0][1]
    assert isinstance(checksums, list)
    assert all(x.endswith(".dir") for x in checksums)
    # And later to individual files
    checksums = mocked_remove.call_args_list[3][0][1]
    assert isinstance(checksums, list)
    assert not any(x.endswith(".dir") for x in checksums)


def test_gc_not_collect_pipeline_tracked_files(tmp_dir, dvc, run_copy):
    from dvc.dvcfile import PROJECT_FILE, load_file

    tmp_dir.gen("foo", "foo")
    tmp_dir.gen("bar", "bar")

    run_copy("foo", "foo2", name="copy")
    shutil.rmtree(dvc.stage_cache.cache_dir)
    assert _count_files(dvc.cache.local.path) == 1
    dvc.gc(workspace=True, force=True)
    assert _count_files(dvc.cache.local.path) == 1

    # remove pipeline file and lockfile and check
    load_file(dvc, PROJECT_FILE).remove(force=True)
    dvc.gc(workspace=True, force=True)
    assert _count_files(dvc.cache.local.path) == 0


def test_gc_external_output(tmp_dir, dvc, workspace):
    workspace.gen({"foo": "foo", "bar": "bar"})

    (foo_stage,) = dvc.add("remote://workspace/foo")
    (bar_stage,) = dvc.add("remote://workspace/bar")

    foo_hash = foo_stage.outs[0].hash_info.value
    bar_hash = bar_stage.outs[0].hash_info.value

    assert (workspace / "cache" / foo_hash[:2] / foo_hash[2:]).read_text() == "foo"
    assert (workspace / "cache" / bar_hash[:2] / bar_hash[2:]).read_text() == "bar"

    (tmp_dir / "foo.dvc").unlink()

    dvc.gc(workspace=True)

    assert not (workspace / "cache" / foo_hash[:2] / foo_hash[2:]).exists()
    assert (workspace / "cache" / bar_hash[:2] / bar_hash[2:]).read_text() == "bar"


def test_gc_all_experiments(tmp_dir, scm, dvc):
    from dvc.repo.experiments.refs import ExpRefInfo

    (foo,) = tmp_dir.dvc_gen("foo", "foo", commit="foo")
    foo_hash = foo.outs[0].hash_info.value

    tmp_dir.dvc_gen("foo", "bar", commit="bar")
    baseline = scm.get_rev()

    (baz,) = tmp_dir.dvc_gen("foo", "baz", commit="baz")
    baz_hash = baz.outs[0].hash_info.value

    ref = ExpRefInfo(baseline, "exp")
    scm.set_ref(str(ref), scm.get_rev())

    dvc.gc(all_experiments=True, force=True)

    assert not (tmp_dir / ".dvc" / "cache" / foo_hash[:2] / foo_hash[2:]).exists()
    assert (
        tmp_dir / ".dvc" / "cache" / baz_hash[:2] / baz_hash[2:]
    ).read_text() == "baz"


def test_gc_rev_num(tmp_dir, scm, dvc):
    num = 2

    hashes = {}
    for i in range(4):
        i_str = str(i)
        f = tmp_dir.dvc_gen("foo", i_str, commit=i_str)
        hashes[i] = f[0].outs[0].hash_info.value

    dvc.gc(rev="HEAD", num=num, force=True)

    for n, i in enumerate(reversed(range(4))):
        cache = tmp_dir / ".dvc" / "cache" / hashes[i][:2] / hashes[i][2:]
        if n >= num:
            assert not cache.exists()
        else:
            assert cache.read_text() == str(i)


def test_date(tmp_dir, scm, dvc):
    tmp_dir.dvc_gen("testfile", "content", commit="add testfile")

    now = datetime.datetime.now()
    datestamp = (now.date() + datetime.timedelta(days=1)).isoformat()

    tmp_dir.dvc_gen("testfile", "modified", commit="modified")

    dvc.gc(commit_date=datestamp)

    assert _count_files(dvc.cache.local.path) == 1
    assert dvc.cache.local.exists("9ae73c65f418e6f79ceb4f0e4a4b98d5")  # "modified"

    tmp_dir.dvc_gen("testfile", "modified, again", commit="modify")

    datestamp = (now.date() - datetime.timedelta(days=1)).isoformat()
    dvc.gc(commit_date=datestamp)
    assert _count_files(dvc.cache.local.path) == 2
    assert dvc.cache.local.exists("9ae73c65f418e6f79ceb4f0e4a4b98d5")
    assert dvc.cache.local.exists(
        "3bcf3b1be3e794a97a5a6b93a005784c"
    )  # "modified, again"


def test_gc_not_in_remote(tmp_dir, scm, dvc, tmp_path_factory, mocker):
    storage = os.fspath(tmp_path_factory.mktemp("test_remote_base"))
    dvc.config["remote"]["local_remote"] = {"url": storage}
    dvc.config["core"]["remote"] = "local_remote"

    (standalone, dir1, dir2) = tmp_dir.dvc_gen(
        {
            "file1": "standalone",
            "dir1": {"file2": "file2"},
            "dir2": {"file3": "file3", "file4": "file4"},
        }
    )
    mocked_remove = mocker.spy(LocalFileSystem, "remove")
    dvc.gc(workspace=True)
    assert not mocked_remove.call_args_list

    dvc.push(["file1", "dir1"])

    dvc.gc(workspace=True, not_in_remote=True)

    assert len(mocked_remove.mock_calls) == 3

    arg_list = mocked_remove.call_args_list

    standalone_hash = standalone.outs[0].hash_info.value
    dir1_hash = dir1.outs[0].hash_info.value
    assert f"{dir1_hash[2:]}.unpacked" in arg_list[0][0][1]
    assert f"{dir1_hash[2:]}" in arg_list[1][0][1][0]
    # We expect 2 calls: standalone_hash and dir1/file2/file2
    assert len(arg_list[2][0][1]) == 2
    # Order is not guaranteed here.
    assert (
        f"{standalone_hash[2:]}" in arg_list[2][0][1][0]
        or f"{standalone_hash[2:]}" in arg_list[2][0][1][1]
    )


def test_gc_not_in_remote_remote_arg(tmp_dir, scm, dvc, tmp_path_factory, mocker):
    storage = os.fspath(tmp_path_factory.mktemp("test_remote_base"))
    dvc.config["remote"]["local_remote"] = {"url": storage}
    dvc.config["core"]["remote"] = "local_remote"
    other_storage = os.fspath(tmp_path_factory.mktemp("test_remote_other"))
    dvc.config["remote"]["other_remote"] = {"url": other_storage}

    tmp_dir.dvc_gen(
        {
            "file1": "standalone",
            "dir1": {"file2": "file2"},
            "dir2": {"file3": "file3", "file4": "file4"},
        }
    )
    mocked_remove = mocker.spy(LocalFileSystem, "remove")

    dvc.push(["file1", "dir1"], remote="other_remote")

    dvc.gc(workspace=True, not_in_remote=True)

    assert not mocked_remove.mock_calls

    dvc.gc(workspace=True, not_in_remote=True, remote="other_remote")

    assert len(mocked_remove.mock_calls) == 3


def test_gc_not_in_remote_with_remote_field(
    tmp_dir, scm, dvc, tmp_path_factory, mocker
):
    storage = os.fspath(tmp_path_factory.mktemp("test_remote_base"))
    dvc.config["remote"]["local_remote"] = {"url": storage}
    dvc.config["core"]["remote"] = "local_remote"

    other_storage = os.fspath(tmp_path_factory.mktemp("test_remote_other"))
    dvc.config["remote"]["other_remote"] = {"url": other_storage}

    text = textwrap.dedent(
        """\
        outs:
        - path: foo
          remote: other_remote
    """
    )
    tmp_dir.gen("foo.dvc", text)
    tmp_dir.dvc_gen("foo", "foo")
    dvc.push()

    mocked_remove = mocker.spy(LocalFileSystem, "remove")
    dvc.gc(workspace=True, not_in_remote=True)
    assert len(mocked_remove.mock_calls) == 1


def test_gc_not_in_remote_cloud(tmp_dir, scm, dvc):
    with pytest.raises(
        InvalidArgumentError,
        match="`--not-in-remote` and `--cloud` are mutually exclusive",
    ):
        dvc.gc(workspace=True, not_in_remote=True, cloud=True)


def test_gc_cloud_remote_field(tmp_dir, scm, dvc, tmp_path_factory, mocker):
    storage = os.fspath(tmp_path_factory.mktemp("test_remote_base"))
    dvc.config["remote"]["local_remote"] = {"url": storage}
    dvc.config["core"]["remote"] = "local_remote"
    other_storage = os.fspath(tmp_path_factory.mktemp("test_remote_other"))
    dvc.config["remote"]["other_remote"] = {"url": other_storage}

    text = textwrap.dedent(
        """\
        outs:
        - path: foo
          remote: other_remote
    """
    )
    tmp_dir.gen("foo.dvc", text)
    tmp_dir.dvc_gen("foo", "foo")
    dvc.push()
    tmp_dir.dvc_gen("foo", "bar")

    mocked_remove = mocker.spy(LocalFileSystem, "remove")
    dvc.gc(workspace=True, cloud=True)
    assert len(mocked_remove.mock_calls) == 2  # local and other_remote




tests/func/test_get.py
import errno
import logging
import os

import pytest

from dvc.cachemgr import CacheManager
from dvc.cli import main
from dvc.exceptions import FileExistsLocallyError
from dvc.fs import system
from dvc.repo import Repo
from dvc.repo.get import GetDVCFileError
from dvc.testing.tmp_dir import make_subrepo


def test_get_repo_file(tmp_dir, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("file", "contents", commit="create file")

    Repo.get(os.fspath(erepo_dir), "file", "file_imported")

    assert os.path.isfile("file_imported")
    assert (tmp_dir / "file_imported").read_text() == "contents"


def test_get_repo_file_no_override(tmp_dir, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("file1", "file1 contents", commit="create file")
        erepo_dir.dvc_gen("file2", "file2 contents", commit="create file2")

    Repo.get(os.fspath(erepo_dir), "file1", "file_imported")
    # getting another file with a name that already exists in Repo.
    with pytest.raises(FileExistsLocallyError) as exc_info:
        Repo.get(os.fspath(erepo_dir), "file2", "file_imported")

    # Make sure it's a functional FileExistsError with errno
    assert isinstance(exc_info.value, FileExistsError)
    assert exc_info.value.errno == errno.EEXIST

    assert os.path.isfile("file_imported")
    assert (tmp_dir / "file_imported").read_text() == "file1 contents"


def test_get_repo_file_with_override(tmp_dir, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("file1", "file1 contents", commit="create file")
        erepo_dir.dvc_gen("file2", "file2 contents", commit="create file2")

    Repo.get(os.fspath(erepo_dir), "file1", "file_imported")

    # override with the 2nd file
    Repo.get(os.fspath(erepo_dir), "file2", "file_imported", force=True)

    assert os.path.isfile("file_imported")
    assert (tmp_dir / "file_imported").read_text() == "file2 contents"


def test_get_repo_dir(tmp_dir, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen({"dir": {"file": "contents"}}, commit="create dir")

    Repo.get(os.fspath(erepo_dir), "dir", "dir_imported")

    assert (tmp_dir / "dir_imported").read_text() == {"file": "contents"}


@pytest.mark.parametrize(
    "erepo", [pytest.lazy_fixture("git_dir"), pytest.lazy_fixture("erepo_dir")]
)
def test_get_git_file(tmp_dir, erepo):
    src = "some_file"
    dst = "some_file_imported"

    erepo.scm_gen({src: "hello"}, commit="add a regular file")

    Repo.get(os.fspath(erepo), src, dst)

    assert (tmp_dir / dst).read_text() == "hello"


@pytest.mark.parametrize(
    "erepo", [pytest.lazy_fixture("git_dir"), pytest.lazy_fixture("erepo_dir")]
)
def test_get_git_dir(tmp_dir, erepo):
    src = "some_directory"
    dst = "some_directory_imported"

    erepo.scm_gen({src: {"dir": {"file.txt": "hello"}}}, commit="add a regular dir")

    Repo.get(os.fspath(erepo), src, dst)

    assert (tmp_dir / dst).read_text() == {"dir": {"file.txt": "hello"}}


def test_cache_type_is_properly_overridden(tmp_dir, erepo_dir):
    with erepo_dir.chdir():
        with erepo_dir.dvc.config.edit() as conf:
            conf["cache"]["type"] = "symlink"
        erepo_dir.dvc.cache = CacheManager(erepo_dir.dvc)
        erepo_dir.scm_add(
            [erepo_dir.dvc.config.files["repo"]], "set cache type to symlinks"
        )
        erepo_dir.dvc_gen("file", "contents", "create file")
    assert system.is_symlink(erepo_dir / "file")

    Repo.get(os.fspath(erepo_dir), "file", "file_imported")

    assert not system.is_symlink("file_imported")
    assert (tmp_dir / "file_imported").read_text() == "contents"


def test_get_repo_rev(tmp_dir, erepo_dir):
    with erepo_dir.chdir(), erepo_dir.branch("branch", new=True):
        erepo_dir.dvc_gen("file", "contents", commit="create file on branch")

    Repo.get(os.fspath(erepo_dir), "file", "file_imported", rev="branch")
    assert (tmp_dir / "file_imported").read_text() == "contents"


def test_get_from_non_dvc_repo(tmp_dir, git_dir):
    git_dir.scm_gen({"some_file": "contents"}, commit="create file")

    Repo.get(os.fspath(git_dir), "some_file", "file_imported")
    assert (tmp_dir / "file_imported").read_text() == "contents"


def test_get_a_dvc_file(tmp_dir, erepo_dir):
    with pytest.raises(GetDVCFileError):
        Repo.get(os.fspath(erepo_dir), "some_file.dvc")


# https://github.com/iterative/dvc/pull/2837#discussion_r352123053
def test_get_full_dvc_path(tmp_dir, erepo_dir, tmp_path_factory):
    path = tmp_path_factory.mktemp("ext")
    external_data = path / "ext_data"
    external_data.write_text("ext_data")

    with erepo_dir.chdir():
        erepo_dir.dvc.add(os.fspath(external_data), external=True)
        erepo_dir.scm_add("ext_data.dvc", commit="add external data")

    Repo.get(os.fspath(erepo_dir), os.fspath(external_data), "ext_data_imported")
    assert (tmp_dir / "ext_data_imported").read_text() == "ext_data"


def test_non_cached_output(tmp_dir, erepo_dir):
    src = "non_cached_file"
    dst = src + "_imported"

    with erepo_dir.chdir():
        erepo_dir.dvc.run(
            outs_no_cache=[src],
            cmd="echo hello > non_cached_file",
            single_stage=True,
        )
        erepo_dir.scm_add([src, src + ".dvc"], commit="add non-cached output")

    Repo.get(os.fspath(erepo_dir), src, dst)

    assert (tmp_dir / dst).is_file()
    # NOTE: using strip() to account for `echo` differences on win and *nix
    assert (tmp_dir / dst).read_text().strip() == "hello"


# https://github.com/iterative/dvc/pull/2837#discussion_r352123053
def test_absolute_file_outside_repo(tmp_dir, erepo_dir):
    with pytest.raises(FileNotFoundError):
        Repo.get(os.fspath(erepo_dir), "/root/")


def test_absolute_file_outside_git_repo(tmp_dir, git_dir):
    with pytest.raises(FileNotFoundError):
        Repo.get(os.fspath(git_dir), "/root/")


def test_unknown_path(tmp_dir, erepo_dir):
    with pytest.raises(FileNotFoundError):
        Repo.get(os.fspath(erepo_dir), "a_non_existing_file")


@pytest.mark.parametrize("dname", [".", "dir", "dir/subdir"])
def test_get_to_dir(tmp_dir, erepo_dir, dname):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("file", "contents", commit="create file")

    os.makedirs(dname, exist_ok=True)

    Repo.get(os.fspath(erepo_dir), "file", dname)

    assert (tmp_dir / dname).is_dir()
    assert (tmp_dir / dname / "file").read_text() == "contents"


def test_get_from_non_dvc_master(tmp_dir, git_dir):
    with git_dir.chdir(), git_dir.branch("branch", new=True):
        git_dir.init(dvc=True)
        git_dir.dvc_gen("some_file", "some text", commit="create some file")

    Repo.get(os.fspath(git_dir), "some_file", out="some_dst", rev="branch")

    assert (tmp_dir / "some_dst").read_text() == "some text"


def test_get_file_from_dir(tmp_dir, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen(
            {
                "dir": {
                    "1": "1",
                    "2": "2",
                    "subdir": {"foo": "foo", "bar": "bar"},
                }
            },
            commit="create dir",
        )

    Repo.get(os.fspath(erepo_dir), os.path.join("dir", "1"))
    assert (tmp_dir / "1").read_text() == "1"

    Repo.get(os.fspath(erepo_dir), os.path.join("dir", "2"), out="file")
    assert (tmp_dir / "file").read_text() == "2"

    Repo.get(os.fspath(erepo_dir), os.path.join("dir", "subdir"))
    assert (tmp_dir / "subdir" / "foo").read_text() == "foo"
    assert (tmp_dir / "subdir" / "bar").read_text() == "bar"

    Repo.get(os.fspath(erepo_dir), os.path.join("dir", "subdir", "foo"), out="X")
    assert (tmp_dir / "X").read_text() == "foo"


def test_get_url_positive(tmp_dir, erepo_dir, caplog, local_cloud):
    erepo_dir.add_remote(config=local_cloud.config)
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("foo", "foo")
    erepo_dir.dvc.push()

    caplog.clear()
    with caplog.at_level(logging.ERROR, logger="dvc"):
        assert main(["get", os.fspath(erepo_dir), "foo", "--show-url"]) == 0
        assert not caplog.text


def test_get_url_not_existing(tmp_dir, erepo_dir, caplog):
    with caplog.at_level(logging.ERROR, logger="dvc"):
        assert (
            main(
                [
                    "get",
                    os.fspath(erepo_dir),
                    "not-existing-file",
                    "--show-url",
                ]
            )
            != 0
        )


def test_get_url_git_only_repo(tmp_dir, scm, caplog):
    tmp_dir.scm_gen({"foo": "foo"}, commit="initial")

    with caplog.at_level(logging.ERROR):
        assert main(["get", os.fspath(tmp_dir), "foo", "--show-url"]) != 0


def test_get_pipeline_tracked_outs(tmp_dir, dvc, scm, git_dir, run_copy, local_remote):
    from dvc.dvcfile import LOCK_FILE, PROJECT_FILE

    tmp_dir.gen("foo", "foo")
    run_copy("foo", "bar", name="copy-foo-bar")
    dvc.push()

    dvc.scm.add([PROJECT_FILE, LOCK_FILE])
    dvc.scm.commit("add pipeline stage")

    with git_dir.chdir():
        Repo.get(f"file://{tmp_dir.as_posix()}", "bar", out="baz")
        assert (git_dir / "baz").read_text() == "foo"


def test_get_mixed_dir(tmp_dir, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen(os.path.join("dir", "foo"), "foo", commit="foo")
        erepo_dir.scm_gen(os.path.join("dir", "bar"), "bar", commit="bar")

    Repo.get(os.fspath(erepo_dir), "dir")
    assert (tmp_dir / "dir").read_text() == {
        ".gitignore": "/foo\n",
        "foo": "foo",
        "bar": "bar",
    }


@pytest.mark.parametrize("is_dvc", [True, False])
@pytest.mark.parametrize("files", [{"foo": "foo"}, {"dir": {"bar": "bar"}}])
def test_get_from_subrepos(tmp_dir, erepo_dir, is_dvc, files):
    subrepo = erepo_dir / "subrepo"
    make_subrepo(subrepo, erepo_dir.scm)
    gen = subrepo.dvc_gen if is_dvc else subrepo.scm_gen
    with subrepo.chdir():
        gen(files, commit="add files in subrepo")

    key = next(iter(files))
    Repo.get(os.fspath(erepo_dir), f"subrepo/{key}", out="out")

    assert (tmp_dir / "out").read_text() == files[key]


def test_granular_get_from_subrepos(tmp_dir, erepo_dir):
    subrepo = erepo_dir / "subrepo"
    make_subrepo(subrepo, erepo_dir.scm)
    with subrepo.chdir():
        subrepo.dvc_gen({"dir": {"bar": "bar"}}, commit="files in subrepo")

    path = os.path.join("subrepo", "dir", "bar")
    Repo.get(os.fspath(erepo_dir), path, out="out")
    assert (tmp_dir / "out").read_text() == "bar"


def test_get_complete_repo(tmp_dir, dvc, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen({"foo": "foo"}, commit="add foo")

    subrepo = erepo_dir / "subrepo"
    make_subrepo(subrepo, erepo_dir.scm)
    with subrepo.chdir():
        subrepo.dvc_gen({"dir": {"bar": "bar"}}, commit="files in subrepo")

    Repo.get(os.fspath(erepo_dir), "subrepo", out="out_sub")
    assert (tmp_dir / "out_sub").read_text() == {
        ".gitignore": "/dir\n",
        "dir": {"bar": "bar"},
    }

    Repo.get(os.fspath(erepo_dir), ".", out="out")
    assert (tmp_dir / "out").read_text() == {
        ".gitignore": "/foo\n",
        "foo": "foo",
    }




tests/func/test_get_url.py
import errno
import os

import pytest

from dvc.exceptions import FileExistsLocallyError, URLMissingError
from dvc.repo import Repo
from dvc.testing.workspace_tests import TestGetUrl as _TestGetUrl


def test_get_file(tmp_dir):
    tmp_dir.gen({"foo": "foo contents"})

    Repo.get_url("foo", "foo_imported")

    assert (tmp_dir / "foo_imported").is_file()
    assert (tmp_dir / "foo_imported").read_text() == "foo contents"


def test_get_file_conflict_and_override(tmp_dir):
    tmp_dir.gen({"foo": "foo contents"})
    tmp_dir.gen({"bar": "bar contents"})

    with pytest.raises(FileExistsLocallyError) as exc_info:
        Repo.get_url("foo", "bar")

    # verify no override
    assert (tmp_dir / "bar").is_file()
    assert (tmp_dir / "bar").read_text() == "bar contents"

    # verify meaningful/BC exception type/errno
    assert isinstance(exc_info.value, FileExistsError)
    assert exc_info.value.errno == errno.EEXIST

    # now, override
    Repo.get_url("foo", "bar", force=True)

    assert (tmp_dir / "bar").is_file()
    assert (tmp_dir / "bar").read_text() == "foo contents"


def test_get_dir(tmp_dir):
    tmp_dir.gen({"foo": {"foo": "foo contents"}})

    Repo.get_url("foo", "foo_imported")

    assert (tmp_dir / "foo_imported").is_dir()
    assert (tmp_dir / "foo_imported" / "foo").is_file()
    assert (tmp_dir / "foo_imported" / "foo").read_text() == "foo contents"


@pytest.mark.parametrize("dname", [".", "dir", "dir/subdir"])
def test_get_url_to_dir(tmp_dir, dname):
    tmp_dir.gen({"src": {"foo": "foo contents"}, "dir": {"subdir": {}}})

    Repo.get_url(os.path.join("src", "foo"), dname)

    assert (tmp_dir / dname).is_dir()
    assert (tmp_dir / dname / "foo").read_text() == "foo contents"


def test_get_url_nonexistent(tmp_dir):
    with pytest.raises(URLMissingError):
        Repo.get_url("nonexistent")


class TestGetUrl(_TestGetUrl):
    pass




tests/func/test_ignore.py
import os
import shutil
from pathlib import Path
from typing import List

import pytest

from dvc.ignore import DvcIgnore, DvcIgnorePatterns
from dvc.output import OutputIsIgnoredError
from dvc.pathspec_math import PatternInfo, merge_patterns
from dvc.repo import Repo
from dvc.testing.tmp_dir import TmpDir
from dvc_data.hashfile.build import IgnoreInCollectedDirError
from dvc_data.hashfile.utils import get_mtime_and_size


def _to_pattern_info_list(str_list: List):
    return [PatternInfo(a, "") for a in str_list]


@pytest.mark.parametrize("filename", ["ignored", "—Ç–µ—Å—Ç"])
def test_ignore(tmp_dir, dvc, filename):
    tmp_dir.gen({"dir": {filename: filename, "other": "text2"}})
    tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, f"dir/{filename}")

    dvc._reset()

    result = dvc.dvcignore.find(dvc.fs, tmp_dir)
    assert set(result) == {
        (tmp_dir / DvcIgnore.DVCIGNORE_FILE).fs_path,
        (tmp_dir / "dir" / "other").fs_path,
    }


def test_walk(tmp_dir, dvc):
    tmp_dir.gen(
        {
            "foo": "foo",
            "bar": "bar",
            "dir": {
                "foo": "foo",
                "bar": "bar",
                "baz": "baz",
                "subdir": {"foo": "foo", "qux": "qux"},
            },
        }
    )
    tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "dir/bar\nfoo")

    dvc._reset()

    result = list(dvc.dvcignore.walk(dvc.fs, tmp_dir))
    assert result[0][0] == str(tmp_dir)
    assert result[0][1] == ["dir"]
    assert set(result[0][2]) == {"bar", ".dvcignore"}
    assert result[1][0] == str(tmp_dir / "dir")
    assert result[1][1] == ["subdir"]
    assert result[1][2] == ["baz"]
    assert result[2][0] == str(tmp_dir / "dir" / "subdir")
    assert result[2][1] == []
    assert result[2][2] == ["qux"]

    result = list(dvc.dvcignore.walk(dvc.fs, tmp_dir, detail=True))
    assert result == [
        (
            str(tmp_dir),
            {"dir": dvc.fs.info(str(tmp_dir / "dir"))},
            {
                "bar": dvc.fs.info(str(tmp_dir / "bar")),
                ".dvcignore": dvc.fs.info(str(tmp_dir / ".dvcignore")),
            },
        ),
        (
            str(tmp_dir / "dir"),
            {
                "subdir": dvc.fs.info(str(tmp_dir / "dir" / "subdir")),
            },
            {
                "baz": dvc.fs.info(str(tmp_dir / "dir" / "baz")),
            },
        ),
        (
            str(tmp_dir / "dir" / "subdir"),
            {},
            {"qux": dvc.fs.info(str(tmp_dir / "dir" / "subdir" / "qux"))},
        ),
    ]


def test_rename_ignored_file(tmp_dir, dvc):
    tmp_dir.gen({"dir": {"ignored": "...", "other": "text"}})

    tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "ignored*")
    dvc._reset()

    mtime, size = get_mtime_and_size("dir", dvc.fs, dvc.dvcignore)

    shutil.move("dir/ignored", "dir/ignored_new")
    new_mtime, new_size = get_mtime_and_size("dir", dvc.fs, dvc.dvcignore)

    assert new_mtime == mtime
    assert new_size == size


def test_rename_file(tmp_dir, dvc):
    tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})
    mtime, size = get_mtime_and_size("dir", dvc.fs, dvc.dvcignore)

    shutil.move("dir/foo", "dir/foo_new")
    new_mtime, new_size = get_mtime_and_size("dir", dvc.fs, dvc.dvcignore)

    assert new_mtime != mtime
    assert new_size == size


def test_remove_ignored_file(tmp_dir, dvc):
    tmp_dir.gen({"dir": {"ignored": "...", "other": "text"}})
    tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "dir/ignored")
    dvc._reset()

    mtime, size = get_mtime_and_size("dir", dvc.fs, dvc.dvcignore)

    os.remove("dir/ignored")
    assert get_mtime_and_size("dir", dvc.fs, dvc.dvcignore) == (mtime, size)


def test_remove_file(tmp_dir, dvc):
    tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})
    mtime, size = get_mtime_and_size("dir", dvc.fs, dvc.dvcignore)

    os.remove("dir/foo")
    new_mtime, new_size = get_mtime_and_size("dir", dvc.fs, dvc.dvcignore)

    assert new_mtime != mtime
    assert new_size != size


def test_dvcignore_in_out_dir(tmp_dir, dvc):
    tmp_dir.gen({"dir": {"foo": "foo", DvcIgnore.DVCIGNORE_FILE: ""}})

    with pytest.raises(IgnoreInCollectedDirError):
        dvc.add("dir")


@pytest.mark.parametrize("dname", ["dir", "dir/subdir"])
def test_ignore_collecting_dvcignores(tmp_dir, dvc, dname):
    tmp_dir.gen({"dir": {"subdir": {}}})

    top_ignore_file = (tmp_dir / dname).with_name(DvcIgnore.DVCIGNORE_FILE)
    top_ignore_file.write_text(os.path.basename(dname))
    dvc._reset()

    ignore_file = tmp_dir / dname / DvcIgnore.DVCIGNORE_FILE
    ignore_file.write_text("foo")

    dvcignore = dvc.dvcignore

    top_ignore_path = os.path.dirname(os.fspath(top_ignore_file))

    sub_dir_path = os.path.dirname(os.fspath(ignore_file))

    assert (
        DvcIgnorePatterns(
            *merge_patterns(
                os.path,
                _to_pattern_info_list([".hg/", ".git/", ".git", ".dvc/"]),
                os.fspath(tmp_dir),
                _to_pattern_info_list([os.path.basename(dname)]),
                top_ignore_path,
            ),
            os.sep,
        )
        == dvcignore._get_trie_pattern(top_ignore_path)
        == dvcignore._get_trie_pattern(sub_dir_path)
    )


def test_ignore_on_branch(tmp_dir, scm, dvc):
    from dvc.fs import GitFileSystem

    tmp_dir.scm_gen({"foo": "foo", "bar": "bar"}, commit="add files")

    with tmp_dir.branch("branch", new=True):
        tmp_dir.scm_gen(DvcIgnore.DVCIGNORE_FILE, "foo", commit="add ignore")

    dvc._reset()

    result = dvc.dvcignore.find(dvc.fs, tmp_dir)
    assert set(result) == {
        (tmp_dir / "foo").fs_path,
        (tmp_dir / "bar").fs_path,
        (tmp_dir / DvcIgnore.DVCIGNORE_FILE).fs_path,
    }

    dvc.fs = GitFileSystem(scm=scm, rev="branch")
    dvc.root_dir = "/"
    assert dvc.dvcignore.is_ignored_file("/foo")


def test_match_nested(tmp_dir, dvc):
    tmp_dir.gen(
        {
            ".dvcignore": "*.backup\ntmp",
            "foo": "foo",
            "tmp": "...",
            "dir": {"x.backup": "x backup", "tmp": "content"},
        }
    )
    dvc._reset()
    result = dvc.dvcignore.find(dvc.fs, tmp_dir)
    assert set(result) == {
        (tmp_dir / DvcIgnore.DVCIGNORE_FILE).fs_path,
        (tmp_dir / "foo").fs_path,
    }


def test_ignore_external(tmp_dir, scm, dvc, tmp_path_factory):
    tmp_dir.gen(".dvcignore", "*.backup\ntmp")
    ext_dir = TmpDir(os.fspath(tmp_path_factory.mktemp("external_dir")))
    ext_dir.gen({"y.backup": "y", "tmp": {"file": "ext tmp"}})

    result = dvc.dvcignore.find(dvc.fs, ext_dir)
    assert set(result) == {
        (ext_dir / "y.backup").fs_path,
        (ext_dir / "tmp" / "file").fs_path,
    }
    assert dvc.dvcignore.is_ignored_dir(os.fspath(ext_dir / "tmp")) is False
    assert dvc.dvcignore.is_ignored_file(os.fspath(ext_dir / "y.backup")) is False


def test_ignore_resurface_subrepo(tmp_dir, scm, dvc):
    tmp_dir.dvc_gen({"foo": "foo"}, commit="add foo")
    subrepo_dir = tmp_dir / "subdir"
    subrepo_dir.mkdir()
    with subrepo_dir.chdir():
        Repo.init(subdir=True)
        subrepo_dir.gen({"bar": {"bar": "bar"}})

    dvc._reset()

    files = ["foo"]
    dirs = ["bar"]
    root = os.fspath(subrepo_dir)
    assert dvc.dvcignore(root, dirs, files, ignore_subrepos=False) == (
        dirs,
        files,
    )
    assert dvc.dvcignore(root, dirs, files) == ([], [])

    assert dvc.dvcignore.is_ignored_dir(os.fspath(subrepo_dir / "bar"))
    assert not dvc.dvcignore.is_ignored_dir(
        os.fspath(subrepo_dir / "bar"), ignore_subrepos=False
    )


def test_ignore_blank_line(tmp_dir, dvc):
    tmp_dir.gen({"dir": {"ignored": "text", "other": "text2"}})
    tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "foo\n\ndir/ignored")
    dvc._reset()
    result = dvc.dvcignore.find(dvc.fs, tmp_dir / "dir")
    assert set(result) == {(tmp_dir / "dir" / "other").fs_path}


# It is not possible to re-include a file if a parent directory of
# that file is excluded.
# Git doesn't list excluded directories for performance reasons,
# so any patterns on contained files have no effect,
# no matter where they are defined.
@pytest.mark.parametrize(
    "data_struct, pattern_list, result_set",
    [
        (
            {"dir": {"subdir": {"not_ignore": "121"}}},
            ["subdir/*", "!not_ignore"],
            {os.path.join("dir", "subdir", "not_ignore")},
        ),
        (
            {"dir": {"subdir": {"should_ignore": "121"}}},
            ["subdir", "!should_ignore"],
            set(),
        ),
        (
            {"dir": {"subdir": {"should_ignore": "121"}}},
            ["subdir/", "!should_ignore"],
            set(),
        ),
    ],
)
def test_ignore_file_in_parent_path(
    tmp_dir, dvc, data_struct, pattern_list, result_set
):
    tmp_dir.gen(data_struct)
    tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "\n".join(pattern_list))
    dvc._reset()
    result = dvc.dvcignore.find(dvc.fs, tmp_dir / "dir")
    assert set(result) == {(tmp_dir / relpath).fs_path for relpath in result_set}


# If there is a separator at the end of the pattern then the pattern
# will only match directories,
# otherwise the pattern can match both files and directories.
# For example, a pattern doc/frotz/ matches doc/frotz directory,
# but not a/doc/frotz directory;
def test_ignore_sub_directory(tmp_dir, dvc):
    tmp_dir.gen(
        {
            "dir": {
                "doc": {"fortz": {"b": "b"}},
                "a": {"doc": {"fortz": {"a": "a"}}},
            }
        }
    )
    tmp_dir.gen({"dir": {DvcIgnore.DVCIGNORE_FILE: "doc/fortz"}})

    dvc._reset()
    result = dvc.dvcignore.find(dvc.fs, tmp_dir / "dir")
    assert set(result) == {
        (tmp_dir / "dir" / "a" / "doc" / "fortz" / "a").fs_path,
        (tmp_dir / "dir" / DvcIgnore.DVCIGNORE_FILE).fs_path,
    }


# however frotz/ matches frotz and a/frotz that is a directory
def test_ignore_directory(tmp_dir, dvc):
    tmp_dir.gen({"dir": {"fortz": {}, "a": {"fortz": {}}}})
    tmp_dir.gen({"dir": {DvcIgnore.DVCIGNORE_FILE: "fortz"}})
    dvc._reset()
    result = dvc.dvcignore.find(dvc.fs, tmp_dir / "dir")
    assert set(result) == {(tmp_dir / "dir" / DvcIgnore.DVCIGNORE_FILE).fs_path}


def test_multi_ignore_file(tmp_dir, dvc, monkeypatch):
    tmp_dir.gen({"dir": {"subdir": {"should_ignore": "1", "not_ignore": "1"}}})
    tmp_dir.gen(DvcIgnore.DVCIGNORE_FILE, "dir/subdir/*_ignore")
    tmp_dir.gen({"dir": {DvcIgnore.DVCIGNORE_FILE: "!subdir/not_ignore"}})
    dvc._reset()
    result = dvc.dvcignore.find(dvc.fs, tmp_dir / "dir")
    assert set(result) == {
        (tmp_dir / "dir" / "subdir" / "not_ignore").fs_path,
        (tmp_dir / "dir" / DvcIgnore.DVCIGNORE_FILE).fs_path,
    }


def test_pattern_trie_fs(tmp_dir, dvc):
    tmp_dir.gen(
        {
            "top": {
                "first": {
                    DvcIgnore.DVCIGNORE_FILE: "a\nb\nc",
                    "middle": {
                        "second": {
                            DvcIgnore.DVCIGNORE_FILE: "d\ne\nf",
                            "bottom": {},
                        }
                    },
                }
            },
            "other": {DvcIgnore.DVCIGNORE_FILE: "1\n2\n3"},
        }
    )
    dvc._reset()
    dvcignore = dvc.dvcignore

    ignore_pattern_top = dvcignore._get_trie_pattern(os.fspath(tmp_dir / "top"))
    ignore_pattern_other = dvcignore._get_trie_pattern(os.fspath(tmp_dir / "other"))
    ignore_pattern_first = dvcignore._get_trie_pattern(
        os.fspath(tmp_dir / "top" / "first")
    )
    ignore_pattern_middle = dvcignore._get_trie_pattern(
        os.fspath(tmp_dir / "top" / "first" / "middle")
    )
    ignore_pattern_second = dvcignore._get_trie_pattern(
        os.fspath(tmp_dir / "top" / "first" / "middle" / "second")
    )
    ignore_pattern_bottom = dvcignore._get_trie_pattern(
        os.fspath(tmp_dir / "top" / "first" / "middle" / "second" / "bottom")
    )

    base_pattern = (
        _to_pattern_info_list([".hg/", ".git/", ".git", ".dvc/"]),
        os.fspath(tmp_dir),
    )
    first_pattern = merge_patterns(
        os.path,
        *base_pattern,
        _to_pattern_info_list(["a", "b", "c"]),
        os.fspath(tmp_dir / "top" / "first"),
    )
    second_pattern = merge_patterns(
        os.path,
        *first_pattern,
        _to_pattern_info_list(["d", "e", "f"]),
        os.fspath(tmp_dir / "top" / "first" / "middle" / "second"),
    )
    other_pattern = merge_patterns(
        os.path,
        *base_pattern,
        _to_pattern_info_list(["1", "2", "3"]),
        os.fspath(tmp_dir / "other"),
    )

    assert DvcIgnorePatterns(*base_pattern, os.sep) == ignore_pattern_top
    assert DvcIgnorePatterns(*other_pattern, os.sep) == ignore_pattern_other
    assert (
        DvcIgnorePatterns(*first_pattern, os.sep)
        == ignore_pattern_first
        == ignore_pattern_middle
    )
    assert (
        DvcIgnorePatterns(*second_pattern, os.sep)
        == ignore_pattern_second
        == ignore_pattern_bottom
    )


def test_ignore_in_added_dir(tmp_dir, dvc):
    tmp_dir.gen(
        {
            "dir": {
                "sub": {
                    "ignored": {"content": "ignored content"},
                    "not_ignored": "not ignored content",
                }
            },
            ".dvcignore": "**/ignored",
        }
    )
    dvc._reset()

    ignored_path = tmp_dir / "dir" / "sub" / "ignored"
    result = dvc.dvcignore.find(dvc.fs, ignored_path)
    assert set(result) == set()
    assert ignored_path.exists()

    dvc.add("dir")
    shutil.rmtree(ignored_path)
    dvc.checkout()

    assert not ignored_path.exists()


def test_ignored_output(tmp_dir, scm, dvc, run_copy):
    tmp_dir.gen({".dvcignore": "*.log\n!foo.log", "foo": "foo content"})

    with pytest.raises(OutputIsIgnoredError):
        run_copy("foo", "abc.log", name="copy")

    run_copy("foo", "foo.log", name="copy")


def test_ignored_output_nested(tmp_dir, scm, dvc, run_copy):
    tmp_dir.gen({".dvcignore": "/*.log", "copy": {"foo": "foo content"}})

    run_copy("foo", "foo.log", name="copy", wdir="copy")

    assert Path("copy/foo.log").exists()


def test_run_dvcignored_dep(tmp_dir, dvc, run_copy):
    tmp_dir.gen({".dvcignore": "dir\n", "dir": {"foo": "foo"}})
    run_copy(os.path.join("dir", "foo"), "bar", name="copy-foo-to-bar")
    assert (tmp_dir / "bar").read_text() == "foo"


def test_pull_ignore(tmp_dir, dvc, local_cloud):
    tmp_dir.dvc_gen(
        {
            ".dvcignore": "data/processed/",
            "data": {"foo": "foo", "processed": {"bar": "bar"}},
        }
    )
    tmp_dir.add_remote(config=local_cloud.config)
    dvc.add("data")
    dvc.push()

    foo_path = tmp_dir / "data" / "foo"
    foo_path.unlink()
    assert not foo_path.exists()

    dvc.cache.local.clear()
    dvc.pull()

    assert foo_path.exists()
    assert foo_path.read_text() == "foo"




tests/func/test_import.py
import filecmp
import os

import pytest
from funcy import first

from dvc.cachemgr import CacheManager
from dvc.config import NoRemoteError
from dvc.dvcfile import load_file
from dvc.fs import system
from dvc.scm import Git
from dvc.stage.exceptions import StagePathNotFoundError
from dvc.testing.tmp_dir import make_subrepo
from dvc.utils.fs import remove


def test_import(tmp_dir, scm, dvc, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("foo", "foo content", commit="create foo")

    stage = dvc.imp(os.fspath(erepo_dir), "foo", "foo_imported")

    assert os.path.isfile("foo_imported")
    assert (tmp_dir / "foo_imported").read_text() == "foo content"
    assert scm.is_ignored("foo_imported")
    assert stage.deps[0].def_repo == {
        "url": os.fspath(erepo_dir),
        "rev_lock": erepo_dir.scm.get_rev(),
    }
    assert stage.deps[0].fs.repo.cache.local.path == dvc.cache.local.path


@pytest.mark.parametrize("src_is_dvc", [True, False])
def test_import_git_file(tmp_dir, scm, dvc, git_dir, src_is_dvc):
    if src_is_dvc:
        git_dir.init(dvc=True)

    git_dir.scm_gen("src", "hello", commit="add a git file")

    stage = tmp_dir.dvc.imp(os.fspath(git_dir), "src", "dst")

    assert (tmp_dir / "dst").read_text() == "hello"
    assert tmp_dir.scm.is_ignored(os.fspath(tmp_dir / "dst"))
    assert stage.deps[0].def_repo == {
        "url": os.fspath(git_dir),
        "rev_lock": git_dir.scm.get_rev(),
    }


def test_import_cached_file(mocker, erepo_dir, tmp_dir, dvc, scm, monkeypatch):
    src = "some_file"
    dst = "some_file_imported"

    with erepo_dir.chdir():
        erepo_dir.dvc_gen({src: "hello"}, commit="add a regular file")

    tmp_dir.dvc_gen({dst: "hello"})
    (tmp_dir / dst).unlink()

    remote_exception = NoRemoteError("dvc import")
    mocker.patch.object(dvc.cloud, "get_remote_odb", side_effect=remote_exception)
    tmp_dir.dvc.imp(os.fspath(erepo_dir), src, dst)

    assert (tmp_dir / dst).is_file()
    assert filecmp.cmp(erepo_dir / src, tmp_dir / dst, shallow=False)


@pytest.mark.parametrize("src_is_dvc", [True, False])
def test_import_git_dir(tmp_dir, scm, dvc, git_dir, src_is_dvc):
    if src_is_dvc:
        git_dir.init(dvc=True)

    git_dir.scm_gen({"src": {"file.txt": "hello"}}, commit="add a dir")

    stage = dvc.imp(os.fspath(git_dir), "src", "dst")

    assert (tmp_dir / "dst").read_text() == {"file.txt": "hello"}
    assert tmp_dir.scm.is_ignored(os.fspath(tmp_dir / "dst"))
    assert stage.deps[0].def_repo == {
        "url": os.fspath(git_dir),
        "rev_lock": git_dir.scm.get_rev(),
    }


def test_import_dir(tmp_dir, scm, dvc, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen({"dir": {"foo": "foo content"}}, commit="create dir")

    stage = dvc.imp(os.fspath(erepo_dir), "dir", "dir_imported")

    assert (tmp_dir / "dir_imported").read_text() == {"foo": "foo content"}
    assert scm.is_ignored("dir_imported")
    assert stage.deps[0].def_repo == {
        "url": os.fspath(erepo_dir),
        "rev_lock": erepo_dir.scm.get_rev(),
    }


def test_import_file_from_dir(tmp_dir, scm, dvc, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen(
            {
                "dir": {
                    "1": "1",
                    "2": "2",
                    "subdir": {"foo": "foo", "bar": "bar"},
                }
            },
            commit="create dir",
        )

    stage = dvc.imp(os.fspath(erepo_dir), os.path.join("dir", "1"))

    assert (tmp_dir / "1").read_text() == "1"
    assert scm.is_ignored("1")
    assert stage.deps[0].def_repo == {
        "url": os.fspath(erepo_dir),
        "rev_lock": erepo_dir.scm.get_rev(),
    }

    dvc.imp(os.fspath(erepo_dir), os.path.join("dir", "2"), out="file")
    assert (tmp_dir / "file").read_text() == "2"
    assert (tmp_dir / "file.dvc").exists()

    dvc.imp(os.fspath(erepo_dir), os.path.join("dir", "subdir"))
    assert (tmp_dir / "subdir" / "foo").read_text() == "foo"
    assert (tmp_dir / "subdir" / "bar").read_text() == "bar"
    assert (tmp_dir / "subdir.dvc").exists()

    dvc.imp(os.fspath(erepo_dir), os.path.join("dir", "subdir", "foo"), out="X")
    assert (tmp_dir / "X").read_text() == "foo"
    assert (tmp_dir / "X.dvc").exists()


def test_import_file_from_dir_to_dir(tmp_dir, scm, dvc, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen({"dir": {"foo": "foo"}}, commit="create dir")

    with pytest.raises(StagePathNotFoundError):
        dvc.imp(
            os.fspath(erepo_dir),
            os.path.join("dir", "foo"),
            out=os.path.join("dir", "foo"),
        )

    tmp_dir.gen({"dir": {}})
    dvc.imp(
        os.fspath(erepo_dir),
        os.path.join("dir", "foo"),
        out=os.path.join("dir", "foo"),
    )
    assert not (tmp_dir / "foo.dvc").exists()
    assert (tmp_dir / "dir" / "foo").read_text() == "foo"
    assert (tmp_dir / "dir" / "foo.dvc").exists()


def test_import_non_cached(erepo_dir, tmp_dir, dvc, scm):
    src = "non_cached_output"
    dst = src + "_imported"

    with erepo_dir.chdir():
        erepo_dir.dvc.run(
            cmd=f"echo hello > {src}", outs_no_cache=[src], single_stage=True
        )

    erepo_dir.scm_add([os.fspath(erepo_dir / src)], commit="add a non-cached out")

    stage = tmp_dir.dvc.imp(os.fspath(erepo_dir), src, dst)

    assert (tmp_dir / dst).is_file()
    assert filecmp.cmp(erepo_dir / src, tmp_dir / dst, shallow=False)
    assert tmp_dir.scm.is_ignored(dst)
    assert stage.deps[0].def_repo == {
        "url": os.fspath(erepo_dir),
        "rev_lock": erepo_dir.scm.get_rev(),
    }


def test_import_rev(tmp_dir, scm, dvc, erepo_dir):
    rev = None
    with erepo_dir.chdir(), erepo_dir.branch("branch", new=True):
        erepo_dir.dvc_gen("foo", "foo content", commit="create foo on branch")
        rev = erepo_dir.scm.get_rev()

    stage = dvc.imp(os.fspath(erepo_dir), "foo", "foo_imported", rev="branch")

    assert (tmp_dir / "foo_imported").read_text() == "foo content"
    assert scm.is_ignored("foo_imported")
    assert stage.deps[0].def_repo == {
        "url": os.fspath(erepo_dir),
        "rev": "branch",
        "rev_lock": rev,
    }


def test_pull_imported_stage(tmp_dir, dvc, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("foo", "foo content", commit="create foo")
    dvc.imp(os.fspath(erepo_dir), "foo", "foo_imported")

    dst_stage = load_file(dvc, "foo_imported.dvc").stage
    dst_cache = dst_stage.outs[0].cache_path

    remove("foo_imported")
    remove(dst_cache)
    dvc.pull(["foo_imported.dvc"])

    assert os.path.isfile("foo_imported")
    assert os.path.isfile(dst_cache)


def test_import_no_download(tmp_dir, scm, dvc, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("foo", "foo content", commit="create foo")

    dvc.imp(os.fspath(erepo_dir), "foo", "foo_imported", no_download=True)

    assert not os.path.exists("foo_imported")

    dst_stage = load_file(dvc, "foo_imported.dvc").stage

    assert dst_stage.deps[0].def_repo == {
        "url": os.fspath(erepo_dir),
        "rev_lock": erepo_dir.scm.get_rev(),
    }
    assert scm.is_ignored("foo_imported")


def test_pull_import_no_download(tmp_dir, scm, dvc, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.scm_gen(os.path.join("foo", "bar"), b"bar", commit="add bar")
        erepo_dir.dvc_gen(os.path.join("foo", "baz"), b"baz contents", commit="add baz")
        size = (
            len(b"bar")
            + len(b"baz contents")
            + len((erepo_dir / "foo" / ".gitignore").read_bytes())
        )

    dvc.imp(os.fspath(erepo_dir), "foo", "foo_imported", no_download=True)

    dvc.pull(["foo_imported.dvc"])
    assert (tmp_dir / "foo_imported").exists
    assert (tmp_dir / "foo_imported" / "bar").read_bytes() == b"bar"
    assert (tmp_dir / "foo_imported" / "baz").read_bytes() == b"baz contents"

    stage = load_file(dvc, "foo_imported.dvc").stage

    assert stage.outs[0].hash_info.value == "bdb8641831d8fcb03939637e09011c21.dir"

    assert stage.outs[0].meta.size == size
    assert stage.outs[0].meta.nfiles == 3
    assert stage.outs[0].meta.isdir


def test_pull_import_no_download_rev_lock(
    tmp_dir,
    dvc,
    erepo_dir,
):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("foo", "foo content", commit="add")

    dvc.imp(os.fspath(erepo_dir), "foo", "foo_imported", no_download=True)

    with erepo_dir.chdir():
        erepo_dir.dvc_gen("foo", "modified foo content", commit="modify foo")

    dvc.pull(["foo_imported.dvc"])
    assert (tmp_dir / "foo_imported").read_text() == "foo content"


def test_cache_type_is_properly_overridden(tmp_dir, scm, dvc, erepo_dir):
    with erepo_dir.chdir():
        with erepo_dir.dvc.config.edit() as conf:
            conf["cache"]["type"] = "symlink"
        erepo_dir.dvc.cache = CacheManager(erepo_dir.dvc)
        erepo_dir.scm_add(
            [erepo_dir.dvc.config.files["repo"]],
            "set source repo cache type to symlink",
        )
        erepo_dir.dvc_gen("foo", "foo content", "create foo")
    assert system.is_symlink(erepo_dir / "foo")

    dvc.imp(os.fspath(erepo_dir), "foo", "foo_imported")

    assert not system.is_symlink("foo_imported")
    assert (tmp_dir / "foo_imported").read_text() == "foo content"
    assert scm.is_ignored("foo_imported")


def test_pull_imported_directory_stage(tmp_dir, dvc, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen({"dir": {"foo": "foo content"}}, commit="create dir")

    dvc.imp(os.fspath(erepo_dir), "dir", "dir_imported")

    remove("dir_imported")
    dvc.cache.local.clear()

    dvc.pull(["dir_imported.dvc"])

    assert (tmp_dir / "dir_imported").read_text() == {"foo": "foo content"}


def test_pull_wildcard_imported_directory_stage(tmp_dir, dvc, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen({"dir123": {"foo": "foo content"}}, commit="create dir")

    dvc.imp(os.fspath(erepo_dir), "dir123", "dir_imported123")

    remove("dir_imported123")
    dvc.cache.local.clear()

    dvc.pull(["dir_imported*.dvc"], glob=True)

    assert (tmp_dir / "dir_imported123").read_text() == {"foo": "foo content"}


def test_push_wildcard_from_bare_git_repo(
    tmp_dir, make_tmp_dir, erepo_dir, local_cloud
):
    Git.init(tmp_dir.fs_path, bare=True).close()

    erepo_dir.add_remote(config=local_cloud.config)
    with erepo_dir.chdir():
        erepo_dir.dvc_gen(
            {
                "dir123": {"foo": "foo content"},
                "dirextra": {"extrafoo": "extra foo content"},
            },
            commit="initial",
        )
    erepo_dir.dvc.push([os.path.join(os.fspath(erepo_dir), "dire*")], glob=True)

    erepo_dir.scm.gitpython.repo.create_remote("origin", os.fspath(tmp_dir))
    erepo_dir.scm.gitpython.repo.remote("origin").push("master")

    dvc_repo = make_tmp_dir("dvc-repo", scm=True, dvc=True)
    with dvc_repo.chdir():
        dvc_repo.dvc.imp(os.fspath(tmp_dir), "dirextra")

        dvc_repo.dvc.imp(os.fspath(tmp_dir), "dir123")


@pytest.mark.parametrize("dname", [".", "dir", "dir/subdir"])
def test_import_to_dir(dname, tmp_dir, dvc, erepo_dir):
    os.makedirs(dname, exist_ok=True)

    with erepo_dir.chdir():
        erepo_dir.dvc_gen("foo", "foo content", commit="create foo")

    stage = dvc.imp(os.fspath(erepo_dir), "foo", dname)

    dst = os.path.join(dname, "foo")

    assert stage.outs[0].fspath == os.path.abspath(dst)
    assert os.path.isdir(dname)
    assert (tmp_dir / dst).read_text() == "foo content"


def test_pull_non_workspace(tmp_dir, scm, dvc, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("foo", "master content", commit="create foo")

        with erepo_dir.branch("branch", new=True):
            erepo_dir.dvc_gen("foo", "branch content", commit="modify foo")

    stage = dvc.imp(os.fspath(erepo_dir), "foo", "foo_imported", rev="branch")
    tmp_dir.scm_add([stage.relpath], commit="imported branch")
    scm.tag("ref-to-branch")

    # Overwrite via import
    (tmp_dir / "foo_imported").unlink()
    dvc.imp(os.fspath(erepo_dir), "foo", "foo_imported", rev="master")

    remove(stage.outs[0].cache_path)
    dvc.fetch(all_tags=True)
    assert os.path.exists(stage.outs[0].cache_path)


def test_import_non_existing(erepo_dir, tmp_dir, dvc):
    with pytest.raises(FileNotFoundError):
        tmp_dir.dvc.imp(os.fspath(erepo_dir), "invalid_output")

    # https://github.com/iterative/dvc/pull/2837#discussion_r352123053
    with pytest.raises(FileNotFoundError):
        tmp_dir.dvc.imp(os.fspath(erepo_dir), "/root/", "root")


def test_pull_no_rev_lock(erepo_dir, tmp_dir, dvc):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("foo", "contents", commit="create foo")

    stage = dvc.imp(os.fspath(erepo_dir), "foo", "foo_imported")
    assert "rev" not in stage.deps[0].def_repo
    stage.deps[0].def_repo.pop("rev_lock")

    load_file(dvc, stage.path).dump(stage)

    remove(stage.outs[0].cache_path)
    (tmp_dir / "foo_imported").unlink()

    dvc.pull([stage.path])

    assert (tmp_dir / "foo_imported").is_file()
    assert (tmp_dir / "foo_imported").read_text() == "contents"


def test_import_from_bare_git_repo(tmp_dir, make_tmp_dir, erepo_dir, local_cloud):
    Git.init(tmp_dir.fs_path, bare=True).close()

    erepo_dir.add_remote(config=local_cloud.config)
    with erepo_dir.chdir():
        erepo_dir.dvc_gen({"foo": "foo"}, commit="initial")
    erepo_dir.dvc.push()

    erepo_dir.scm.gitpython.repo.create_remote("origin", os.fspath(tmp_dir))
    erepo_dir.scm.gitpython.repo.remote("origin").push("master")

    dvc_repo = make_tmp_dir("dvc-repo", scm=True, dvc=True)
    with dvc_repo.chdir():
        dvc_repo.dvc.imp(os.fspath(tmp_dir), "foo")


def test_import_pipeline_tracked_outs(
    tmp_dir, dvc, scm, erepo_dir, run_copy, local_remote
):
    from dvc.dvcfile import LOCK_FILE, PROJECT_FILE

    tmp_dir.gen("foo", "foo")
    run_copy("foo", "bar", name="copy-foo-bar")
    dvc.push()

    dvc.scm.add([PROJECT_FILE, LOCK_FILE])
    dvc.scm.commit("add pipeline stage")

    with erepo_dir.chdir():
        erepo_dir.dvc.imp(f"file://{tmp_dir.as_posix()}", "bar", out="baz")
        assert (erepo_dir / "baz").read_text() == "foo"


def test_local_import(tmp_dir, dvc, scm):
    tmp_dir.dvc_gen("foo", "foo", commit="init")
    (tmp_dir / "outdir").mkdir()
    dvc.imp(".", "foo", out="outdir")


def test_import_mixed_dir(tmp_dir, dvc, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen(os.path.join("dir", "foo"), "foo", commit="foo")
        erepo_dir.scm_gen(os.path.join("dir", "bar"), "bar", commit="bar")

    dvc.imp(os.fspath(erepo_dir), "dir")
    assert (tmp_dir / "dir").read_text() == {
        ".gitignore": "/foo\n",
        "foo": "foo",
        "bar": "bar",
    }


@pytest.mark.parametrize("is_dvc", [True, False])
@pytest.mark.parametrize("files", [{"foo": "foo"}, {"dir": {"bar": "bar"}}])
def test_import_subrepos(tmp_dir, erepo_dir, dvc, scm, is_dvc, files):
    subrepo = erepo_dir / "subrepo"
    make_subrepo(subrepo, erepo_dir.scm)
    gen = subrepo.dvc_gen if is_dvc else subrepo.scm_gen
    with subrepo.chdir():
        gen(files, commit="add files in subrepo")

    key = next(iter(files))
    path = str((subrepo / key).relative_to(erepo_dir))

    stage = dvc.imp(os.fspath(erepo_dir), path, out="out")

    assert (tmp_dir / "out").read_text() == files[key]
    assert stage.deps[0].def_path == path
    assert stage.deps[0].def_repo == {
        "url": os.fspath(erepo_dir),
        "rev_lock": erepo_dir.scm.get_rev(),
    }


def test_granular_import_from_subrepos(tmp_dir, dvc, erepo_dir):
    subrepo = erepo_dir / "subrepo"
    make_subrepo(subrepo, erepo_dir.scm)
    with subrepo.chdir():
        subrepo.dvc_gen({"dir": {"bar": "bar"}}, commit="files in subrepo")

    path = os.path.join("subrepo", "dir", "bar")
    stage = dvc.imp(os.fspath(erepo_dir), path, out="out")
    assert (tmp_dir / "out").read_text() == "bar"
    assert stage.deps[0].def_path == path
    assert stage.deps[0].def_repo == {
        "url": os.fspath(erepo_dir),
        "rev_lock": erepo_dir.scm.get_rev(),
    }


@pytest.mark.parametrize("is_dvc", [True, False])
@pytest.mark.parametrize("files", [{"foo": "foo"}, {"dir": {"bar": "bar"}}])
def test_pull_imported_stage_from_subrepos(tmp_dir, dvc, erepo_dir, is_dvc, files):
    subrepo = erepo_dir / "subrepo"
    make_subrepo(subrepo, erepo_dir.scm)
    gen = subrepo.dvc_gen if is_dvc else subrepo.scm_gen
    with subrepo.chdir():
        gen(files, commit="files in subrepo")

    key = first(files)
    path = os.path.join("subrepo", key)
    dvc.imp(os.fspath(erepo_dir), path, out="out")

    # clean everything
    dvc.cache.local.clear()
    remove("out")

    stats = dvc.pull(["out.dvc"])

    expected = [f"out{os.sep}"] if isinstance(files[key], dict) else ["out"]
    assert stats["added"] == expected
    assert (tmp_dir / "out").read_text() == files[key]


def test_import_complete_repo(tmp_dir, dvc, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen({"foo": "foo"}, commit="add foo")

    subrepo = erepo_dir / "subrepo"
    make_subrepo(subrepo, erepo_dir.scm)
    with subrepo.chdir():
        subrepo.dvc_gen({"dir": {"bar": "bar"}}, commit="files in subrepo")

    dvc.imp(os.fspath(erepo_dir), "subrepo", out="out_sub")
    assert (tmp_dir / "out_sub").read_text() == {
        ".gitignore": "/dir\n",
        "dir": {"bar": "bar"},
    }

    dvc.imp(os.fspath(erepo_dir), os.curdir, out="out")
    assert (tmp_dir / "out").read_text() == {
        ".gitignore": "/foo\n",
        "foo": "foo",
    }


def test_import_with_no_exec(tmp_dir, dvc, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("foo", "foo content", commit="create foo")

    dvc.imp(os.fspath(erepo_dir), "foo", out="foo_imported", no_exec=True)

    dst = tmp_dir / "foo_imported"
    assert not dst.exists()


def test_import_with_jobs(mocker, dvc, erepo_dir):
    import dvc_data.hashfile.transfer as otransfer

    with erepo_dir.chdir():
        erepo_dir.dvc_gen(
            {
                "dir1": {
                    "file1": "file1",
                    "file2": "file2",
                    "file3": "file3",
                    "file4": "file4",
                }
            },
            commit="init",
        )

    spy = mocker.spy(otransfer, "transfer")
    dvc.imp(os.fspath(erepo_dir), "dir1", jobs=3)
    # the first call will be retrieving dir cache for "dir1" w/jobs None
    for _args, kwargs in spy.call_args_list[1:]:
        assert kwargs.get("jobs") == 3


def test_chained_import(tmp_dir, dvc, make_tmp_dir, erepo_dir, local_cloud):
    erepo_dir.add_remote(config=local_cloud.config)
    with erepo_dir.chdir():
        erepo_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar"}}, commit="init")
    erepo_dir.dvc.push()
    remove(erepo_dir.dvc.cache.local.path)
    remove(os.fspath(erepo_dir / "dir"))

    erepo2 = make_tmp_dir("erepo2", scm=True, dvc=True)
    with erepo2.chdir():
        erepo2.dvc.imp(os.fspath(erepo_dir), "dir")
        erepo2.scm.add("dir.dvc")
        erepo2.scm.commit("import")
    remove(erepo2.dvc.cache.local.path)
    remove(os.fspath(erepo2 / "dir"))

    dvc.imp(os.fspath(erepo2), "dir", "dir_imported")
    dst = tmp_dir / "dir_imported"
    assert (dst / "foo").read_text() == "foo"
    assert (dst / "bar").read_text() == "bar"

    dvc.cache.local.clear()
    remove("dir_imported")

    # pulled objects should come from the original upstream repo's remote,
    # no cache or remote should be needed from the intermediate repo
    dvc.pull(["dir_imported.dvc"])
    assert not os.path.exists(erepo_dir.dvc.cache.local.path)
    assert not os.path.exists(erepo2.dvc.cache.local.path)
    assert (dst / "foo").read_text() == "foo"
    assert (dst / "bar").read_text() == "bar"


@pytest.mark.parametrize("paths", ([], ["dir"]))
def test_parameterized_repo(tmp_dir, dvc, scm, erepo_dir, paths):
    path = erepo_dir.joinpath(*paths)
    path.mkdir(parents=True, exist_ok=True)
    (path / "params.yaml").dump({"out": "foo"})
    (path / "dvc.yaml").dump(
        {
            "stages": {
                "train": {"cmd": "echo ${out} > ${out}", "outs": ["${out}"]},
            }
        }
    )
    path.gen({"foo": "foo"})
    with path.chdir():
        erepo_dir.dvc.commit(None, force=True)
        erepo_dir.scm.add_commit(
            ["params.yaml", "dvc.yaml", "dvc.lock", ".gitignore"],
            message="init",
        )

    to_import = os.path.join(*paths, "foo")
    stage = dvc.imp(os.fspath(erepo_dir), to_import, "foo_imported")

    assert (tmp_dir / "foo_imported").read_text() == "foo"
    assert stage.deps[0].def_repo == {
        "url": os.fspath(erepo_dir),
        "rev_lock": erepo_dir.scm.get_rev(),
    }




tests/func/test_import_url.py
import os
import textwrap
from uuid import uuid4

import pytest

from dvc.cli import main
from dvc.dependency.base import Dependency, DependencyDoesNotExistError
from dvc.dvcfile import load_file
from dvc.exceptions import InvalidArgumentError
from dvc.stage import Stage
from dvc.testing.workspace_tests import TestImport as _TestImport


def test_cmd_import(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    ret = main(["import-url", "foo", "import"])
    assert ret == 0
    assert os.path.exists("import.dvc")

    ret = main(["import-url", "non-existing-file", "import"])
    assert ret != 0


def test_cmd_unsupported_scheme(dvc):
    ret = main(["import-url", "unsupported://path", "import_unsupported"])
    assert ret != 0


def test_default_output(tmp_dir, dvc, cloud):
    filename = str(uuid4())
    tmpfile = cloud / filename
    tmpfile.write_bytes(b"content")
    cloud.gen(filename, "content")

    ret = main(["import-url", tmpfile.fs_path])
    assert ret == 0
    assert (tmp_dir / filename).read_bytes() == b"content"


def test_should_remove_outs_before_import(tmp_dir, dvc, mocker, erepo_dir):
    erepo_dir.gen({"foo": "foo"})

    remove_outs_call_counter = mocker.spy(Stage, "remove_outs")
    ret = main(["import-url", os.fspath(erepo_dir / "foo")])

    assert ret == 0
    assert remove_outs_call_counter.mock.call_count == 1


def test_import_conflict_and_override(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    tmp_dir.gen("bar", "bar")

    # bar exists, fail
    ret = main(["import-url", "foo", "bar"])
    assert ret != 0
    assert not os.path.exists("bar.dvc")

    # force override
    ret = main(["import-url", "foo", "bar", "--force"])
    assert ret == 0
    assert os.path.exists("bar.dvc")


def test_import_filename(tmp_dir, dvc, cloud):
    external_source = cloud / "file"
    (cloud / "file").write_text("content", encoding="utf-8")
    ret = main(["import-url", "--file", "bar.dvc", external_source.fs_path])
    assert ret == 0
    assert (tmp_dir / "bar.dvc").exists()

    (tmp_dir / "bar.dvc").unlink()
    (tmp_dir / "sub").mkdir()

    path = tmp_dir / "sub" / "bar.dvc"
    ret = main(["import-url", "--file", path.fs_path, external_source.fs_path, "out"])
    assert ret == 0
    assert path.exists()


@pytest.mark.parametrize("dname", [".", "dir", "dir/subdir"])
def test_import_url_to_dir(dname, tmp_dir, dvc):
    tmp_dir.gen({"data_dir": {"file": "file content"}})
    src = os.path.join("data_dir", "file")

    os.makedirs(dname, exist_ok=True)

    stage = dvc.imp_url(src, dname)

    dst = tmp_dir / dname / "file"

    assert stage.outs[0].fs_path == os.fspath(dst)
    assert os.path.isdir(dname)
    assert dst.read_text() == "file content"


def test_import_stage_accompanies_target(tmp_dir, dvc, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("file1", "file1 content", commit="commit file")

    tmp_dir.gen({"dir": {}})
    erepo = {"url": os.fspath(erepo_dir)}
    dvc.imp_url("file1", out=os.path.join("dir", "imported_file"), erepo=erepo)

    assert (tmp_dir / "dir" / "imported_file").exists()
    assert (tmp_dir / "dir" / "imported_file.dvc").exists()


def test_import_url_nonexistent(dvc, erepo_dir):
    with pytest.raises(DependencyDoesNotExistError):
        dvc.imp_url(os.fspath(erepo_dir / "non-existent"))


def test_import_url_with_no_exec(tmp_dir, dvc, erepo_dir):
    tmp_dir.gen({"data_dir": {"file": "file content"}})
    src = os.path.join("data_dir", "file")

    dvc.imp_url(src, ".", no_exec=True)
    dst = tmp_dir / "file"
    assert not dst.exists()


class TestImport(_TestImport):
    @pytest.fixture
    def stage_md5(self):
        return "dc24e1271084ee317ac3c2656fb8812b"

    @pytest.fixture
    def dir_md5(self):
        return "b6dcab6ccd17ca0a8bf4a215a37d14cc.dir"

    @pytest.fixture
    def is_object_storage(self):
        return False


def test_import_url_preserve_fields(tmp_dir, dvc):
    text = textwrap.dedent(
        """\
        # top comment
        desc: top desc
        deps:
        - path: foo # dep comment
        outs:
        - path: bar # out comment
          desc: out desc
          type: mytype
          labels:
          - label1
          - label2
          meta:
            key: value
        meta: some metadata
    """
    )
    tmp_dir.gen("bar.dvc", text)

    tmp_dir.gen("foo", "foo")
    dvc.imp_url("foo", out="bar")
    assert (tmp_dir / "bar.dvc").read_text() == textwrap.dedent(
        """\
        # top comment
        desc: top desc
        deps:
        - path: foo # dep comment
          md5: acbd18db4cc2f85cedef654fccc4a4d8
          size: 3
        outs:
        - path: bar # out comment
          desc: out desc
          type: mytype
          labels:
          - label1
          - label2
          meta:
            key: value
          md5: acbd18db4cc2f85cedef654fccc4a4d8
          size: 3
        meta: some metadata
        md5: be7ade0aa89cc8d56e320867a9de9740
        frozen: true
    """
    )


def test_import_url_to_remote_absolute(tmp_dir, make_tmp_dir, dvc, local_remote):
    tmp_abs_dir = make_tmp_dir("abs")
    tmp_foo = tmp_abs_dir / "foo"
    tmp_foo.write_text("foo")

    stage = dvc.imp_url(str(tmp_foo), to_remote=True)

    foo = tmp_dir / "foo"
    assert stage.deps[0].fspath == str(tmp_foo)
    assert stage.outs[0].fspath == os.fspath(foo)
    assert foo.with_suffix(".dvc").exists()


def test_import_url_to_remote_invalid_combinations(dvc):
    with pytest.raises(InvalidArgumentError, match="--no-exec"):
        dvc.imp_url("s3://bucket/foo", no_exec=True, to_remote=True)


def test_import_url_to_remote_status(tmp_dir, dvc, local_cloud, local_remote):
    local_cloud.gen("foo", "foo")

    stage = dvc.imp_url(str(local_cloud / "foo"), to_remote=True)
    assert stage.md5 is not None

    status = dvc.status()
    assert status["foo.dvc"] == [{"changed outs": {"foo": "not in cache"}}]

    dvc.pull()

    status = dvc.status()
    assert len(status) == 0


def test_import_url_no_download(tmp_dir, scm, dvc, local_workspace):
    local_workspace.gen("file", "file content")
    dst = tmp_dir / "file"
    stage = dvc.imp_url("remote://workspace/file", os.fspath(dst), no_download=True)

    assert stage.deps[0].hash_info.value == "d10b4c3ff123b26dc068d43a8bef2d23"

    assert not dst.exists()
    assert scm.is_ignored(dst)

    out = stage.outs[0]
    assert not out.hash_info
    assert out.meta.size is None

    status = dvc.status()
    assert status["file.dvc"] == [
        {"changed outs": {"file": "deleted"}},
    ]


def test_partial_import_pull(tmp_dir, scm, dvc, local_workspace):
    local_workspace.gen("file", "file content")
    dst = tmp_dir / "file"
    dvc.imp_url("remote://workspace/file", os.fspath(dst), no_download=True)

    dvc.pull(["file.dvc"])

    assert dst.exists()

    stage = load_file(dvc, "file.dvc").stage

    assert stage.outs[0].hash_info.value == "d10b4c3ff123b26dc068d43a8bef2d23"
    assert stage.outs[0].meta.size == 12


def test_import_url_fs_config(tmp_dir, dvc, workspace, mocker):
    import dvc.fs as dvc_fs

    workspace.gen("foo", "foo")

    url = "remote://workspace/foo"
    get_fs_config = mocker.spy(dvc_fs, "get_fs_config")
    dep_init = mocker.spy(Dependency, "__init__")
    dvc.imp_url(url, fs_config={"jobs": 42})

    dep_init_kwargs = dep_init.call_args[1]
    assert dep_init_kwargs.get("fs_config") == {"jobs": 42}

    assert get_fs_config.call_args_list[0][1] == {"url": "foo"}
    assert get_fs_config.call_args_list[1][1] == {"url": url, "jobs": 42}
    assert get_fs_config.call_args_list[2][1] == {"name": "workspace"}




tests/func/test_init.py
import logging
import os

import pytest

from dvc.cli import main
from dvc.config import Config
from dvc.exceptions import InitError
from dvc.repo import Repo as DvcRepo


def test_api_init(scm):
    DvcRepo.init().close()
    assert os.path.isdir(DvcRepo.DVC_DIR)


def test_cli_init(scm):
    ret = main(["init"])
    assert ret == 0
    assert os.path.isdir(DvcRepo.DVC_DIR)


def test_double_init(scm):
    ret = main(["init"])
    assert ret == 0
    assert os.path.isdir(DvcRepo.DVC_DIR)

    ret = main(["init"])
    assert ret != 0
    assert os.path.isdir(DvcRepo.DVC_DIR)

    ret = main(["init", "--force"])
    assert ret == 0
    assert os.path.isdir(DvcRepo.DVC_DIR)


def test_init_no_scm_fail_api(tmp_dir):
    with pytest.raises(InitError):
        DvcRepo.init()


def test_init_no_scm_fail_cli(tmp_dir):
    ret = main(["init"])
    assert ret != 0


def test_init_no_scm_api(tmp_dir):
    repo = DvcRepo.init(no_scm=True)

    assert (tmp_dir / DvcRepo.DVC_DIR).is_dir()
    assert repo.config["core"]["no_scm"]


def test_init_no_scm_cli(tmp_dir):
    ret = main(["init", "--no-scm"])
    assert ret == 0

    dvc_path = tmp_dir / DvcRepo.DVC_DIR
    assert dvc_path.is_dir()
    assert Config(os.fspath(dvc_path))["core"]["no_scm"]


def test_init_quiet_should_not_display_welcome_screen(tmp_dir, scm, caplog):
    with caplog.at_level(logging.INFO, logger="dvc"):
        ret = main(["init", "--quiet"])

        assert ret == 0
        assert not caplog.text


def test_allow_init_dvc_subdir(tmp_dir, scm, monkeypatch):
    tmp_dir.gen({"subdir": {}})

    with monkeypatch.context() as m:
        m.chdir("subdir")
        assert main(["init", "--subdir"]) == 0

    repo = DvcRepo("subdir")
    assert repo.root_dir == os.fspath(tmp_dir / "subdir")
    assert repo.scm.root_dir == os.fspath(tmp_dir)


def test_subdir_init_no_option(tmp_dir, scm, monkeypatch, caplog):
    tmp_dir.gen({"subdir": {}})

    caplog.clear()
    with monkeypatch.context() as m:
        m.chdir("subdir")
        with caplog.at_level(logging.ERROR, logger="dvc"):
            assert main(["init"]) == 1

    assert (
        "{} is not tracked by any supported SCM tool (e.g. Git). "
        "Use `--no-scm` if you don't want to use any SCM or "
        "`--subdir` if initializing inside a subdirectory of a parent SCM "
        "repository.".format(os.fspath(tmp_dir / "subdir")) in caplog.text
    )


def test_gen_dvcignore(tmp_dir):
    DvcRepo.init(no_scm=True)
    text = (
        "# Add patterns of files dvc should ignore, which could improve\n"
        "# the performance. Learn more at\n"
        "# https://dvc.org/doc/user-guide/dvcignore\n"
    )
    assert text == (tmp_dir / ".dvcignore").read_text()


def test_init_when_ignored_by_git(tmp_dir, scm, caplog):
    # https://github.com/iterative/dvc/issues/3738
    tmp_dir.gen({".gitignore": ".*"})
    with caplog.at_level(logging.ERROR, logger="dvc"):
        assert main(["init"]) == 1
    assert (
        "{dvc_dir} is ignored by your SCM tool. \n"
        "Make sure that it's tracked, "
        "for example, by adding '!.dvc' to .gitignore.".format(
            dvc_dir=tmp_dir / DvcRepo.DVC_DIR
        )
        in caplog.text
    )




tests/func/test_install.py
import os
import pathlib
import sys

import pytest
from git import GitCommandError

from dvc.exceptions import DvcException
from dvc_data.hashfile.hash import file_md5
from tests.func.parsing.test_errors import escape_ansi


@pytest.mark.skipif(
    sys.platform == "win32", reason="Git hooks aren't supported on Windows"
)
class TestInstall:
    def _hook(self, name):
        return pathlib.Path(".git") / "hooks" / name

    def test_create_hooks(self, scm, dvc):
        dvc.install()

        hooks_with_commands = [
            ("post-checkout", "exec dvc git-hook post-checkout"),
            ("pre-commit", "exec dvc git-hook pre-commit"),
            ("pre-push", "exec dvc git-hook pre-push"),
        ]

        for fname, command in hooks_with_commands:
            hook_path = self._hook(fname)
            assert hook_path.is_file()
            assert command in hook_path.read_text()

    def test_install_pre_commit_tool(self, scm, dvc):
        dvc.install(use_pre_commit_tool=True)

        precommit_path = pathlib.Path(".") / ".pre-commit-config.yaml"
        assert precommit_path.is_file()

    def test_fail_if_hook_exists(self, scm, dvc):
        self._hook("post-checkout").write_text("hook content")

        with pytest.raises(DvcException) as exc_info:  # noqa: PT011
            dvc.install()

        assert (
            escape_ansi(str(exc_info.value)) == "Hook 'post-checkout' already exists. "
            "Please refer to <https://man.dvc.org/install> for more info."
        )

    def test_pre_commit_hook(self, tmp_dir, scm, dvc, caplog):
        tmp_dir.dvc_gen("file", "file content", commit="create foo")
        tmp_dir.gen("file", "file modified")
        dvc.install()

        # scm.commit bypasses hooks
        with pytest.raises(GitCommandError, match=r"modified:\s*file"):
            scm.gitpython.repo.git.commit(m="file modified")

    def test_post_checkout(self, tmp_dir, scm, dvc):
        tmp_dir.dvc_gen({"file": "file content"}, commit="add")
        os.unlink("file")
        dvc.install()

        scm.gitpython.git.checkout("-b", "new_branch")

        assert os.path.isfile("file")

    def test_pre_push_hook(self, tmp_dir, scm, dvc, tmp_path_factory):
        temp = tmp_path_factory.mktemp("external")
        git_remote = temp / "project.git"
        storage_path = temp / "dvc_storage"

        with dvc.config.edit() as conf:
            conf["remote"]["store"] = {"url": os.fspath(storage_path)}
            conf["core"]["remote"] = "store"
        tmp_dir.dvc_gen("file", "file_content", "commit message")

        file_checksum = file_md5("file", dvc.fs)
        expected_storage_path = storage_path / file_checksum[:2] / file_checksum[2:]

        scm.gitpython.repo.clone(os.fspath(git_remote))
        scm.gitpython.repo.create_remote("origin", os.fspath(git_remote))

        dvc.install()

        assert not expected_storage_path.is_file()
        scm.gitpython.repo.git.push("origin", "master")
        assert expected_storage_path.is_file()
        assert expected_storage_path.read_text() == "file_content"


@pytest.mark.skipif(
    sys.platform == "win32", reason="Git hooks aren't supported on Windows"
)
def test_merge_driver_no_ancestor(tmp_dir, scm, dvc):
    with tmp_dir.branch("one", new=True):
        tmp_dir.dvc_gen({"data": {"foo": "foo"}}, commit="one: add data")

    scm.checkout("two", create_new=True)
    dvc.checkout()  # keep things in sync

    tmp_dir.dvc_gen({"data": {"bar": "bar"}}, commit="two: add data")

    # installing hook only before merge, as it runs `dvc` commands which makes
    # `checkouts` and `commits` above slower
    dvc.install()
    (tmp_dir / ".gitattributes").write_text("*.dvc merge=dvc")

    scm.gitpython.repo.git.merge("one", m="merged", no_gpg_sign=True, no_signoff=True)

    # NOTE: dvc shouldn't checkout automatically as it might take a long time
    assert (tmp_dir / "data").read_text() == {"bar": "bar"}
    assert (tmp_dir / "data.dvc").read_text() == (
        "outs:\n"
        "- md5: 5ea40360f5b4ec688df672a4db9c17d1.dir\n"
        "  size: 6\n"
        "  nfiles: 2\n"
        "  path: data\n"
    )

    dvc.checkout("data.dvc")
    assert (tmp_dir / "data").read_text() == {"foo": "foo", "bar": "bar"}


@pytest.mark.skipif(
    sys.platform == "win32", reason="Git hooks aren't supported on Windows"
)
def test_merge_driver(tmp_dir, scm, dvc):
    tmp_dir.dvc_gen({"data": {"master": "master"}}, commit="master: add data")

    with tmp_dir.branch("one", new=True):
        tmp_dir.dvc_gen({"data": {"one": "one"}}, commit="one: add data")

    scm.checkout("two", create_new=True)
    dvc.checkout()  # keep things in sync

    tmp_dir.dvc_gen({"data": {"two": "two"}}, commit="two: add data")

    # installing hook only before merge, as it runs `dvc` commands on
    # `checkouts` and `commits` which slows tests down
    dvc.install()
    (tmp_dir / ".gitattributes").write_text("*.dvc merge=dvc")

    scm.gitpython.repo.git.merge("one", m="merged", no_gpg_sign=True, no_signoff=True)

    # NOTE: dvc shouldn't checkout automatically as it might take a long time
    assert (tmp_dir / "data").read_text() == {"master": "master", "two": "two"}
    assert (tmp_dir / "data.dvc").read_text() == (
        "outs:\n"
        "- md5: 839ef9371606817569c1ee0e5f4ed233.dir\n"
        "  size: 12\n"
        "  nfiles: 3\n"
        "  path: data\n"
    )

    dvc.checkout("data.dvc")
    assert (tmp_dir / "data").read_text() == {
        "master": "master",
        "one": "one",
        "two": "two",
    }




tests/func/test_lock.py
import pytest

from dvc.cli import main
from dvc.exceptions import DvcException
from dvc.lock import Lock, LockError


def test_with(tmp_dir, dvc, mocker):
    # patching to speedup tests
    mocker.patch("dvc.lock.DEFAULT_TIMEOUT", 0.01)

    lockfile = tmp_dir / dvc.tmp_dir / "lock"
    with Lock(lockfile):
        with pytest.raises(LockError), Lock(lockfile):
            pass


def test_unlock_lock_failed(tmp_dir, dvc, request, mocker):
    # patching to speedup tests
    mocker.patch("dvc.lock.DEFAULT_TIMEOUT", 0.01)

    lockfile = tmp_dir / dvc.tmp_dir / "lock"
    lock = Lock(lockfile)
    lock_ext = Lock(lockfile)

    # It's a common scenario now to have lock unlocked and locked back (e.g. in
    # repro of a stage) in with. We should see LockError exception here.
    with lock:
        lock.unlock()
        lock_ext.lock()  # imitate an external process had time to lock it
        request.addfinalizer(lock_ext.unlock)
        with pytest.raises(LockError):
            lock.lock()


def test_unlock_unlocked_raises():
    lock = Lock("lock")
    with pytest.raises(DvcException, match="Unlock called on an unlocked lock"):
        lock.unlock()


def test_cli(tmp_dir, dvc, mocker, caplog):
    # patching to speedup tests
    mocker.patch("dvc.lock.DEFAULT_TIMEOUT", 0.01)

    expected_error_msg = (
        "Unable to acquire lock. Most likely another DVC process is "
        "running or was terminated abruptly. Check the page "
        "<https://dvc.org/doc/user-guide/troubleshooting#lock-issue> "
        "for other possible reasons and to learn how to resolve this."
    )
    with Lock(tmp_dir / dvc.tmp_dir / "lock"):
        assert main(["add", "foo"]) == 1
    assert expected_error_msg in caplog.text




tests/func/test_lockfile.py
from collections import OrderedDict
from operator import itemgetter

from dvc.dvcfile import LOCK_FILE
from dvc.stage.utils import split_params_deps
from dvc.utils.fs import remove
from dvc.utils.serialize import dumps_yaml, parse_yaml_for_update
from tests.func.test_run_multistage import supported_params

FS_STRUCTURE = {
    "foo": "bar\nfoobar",
    "bar": "foo\nfoobar",
    "foobar": "foobar\nbar",
    "params.yaml": dumps_yaml(supported_params),
    "params2.yaml": dumps_yaml(supported_params),
}


def read_lock_file(file=LOCK_FILE):
    with open(file, encoding="utf-8") as f:
        data = parse_yaml_for_update(f.read(), file)
    assert isinstance(data, OrderedDict)
    return data


def assert_eq_lockfile(previous, new):
    for content in (previous, new):
        assert isinstance(content, OrderedDict)

    # if they both are OrderedDict, then `==` will also check for order
    assert previous == new


def test_deps_outs_are_sorted_by_path(tmp_dir, dvc, run_head):
    tmp_dir.gen(FS_STRUCTURE)
    deps = ["foo", "bar", "foobar"]
    run_head(*deps, name="copy-first-line")

    initial_content = read_lock_file()
    lock = initial_content["stages"]["copy-first-line"]

    # lock stage key order:
    assert list(lock.keys()) == ["cmd", "deps", "outs"]

    # `path` key appear first and then the `md5`
    assert all(list(dep.keys()) == ["path", "md5", "size"] for dep in lock["deps"])
    assert all(list(out.keys()) == ["path", "md5", "size"] for out in lock["outs"])

    # deps are always sorted by the file path naming
    assert list(map(itemgetter("path"), lock["deps"])) == sorted(deps)

    # outs are too
    assert list(map(itemgetter("path"), lock["outs"])) == [
        d + "-1" for d in sorted(deps)
    ]


def test_order_is_preserved_when_pipeline_order_changes(tmp_dir, dvc, run_head):
    tmp_dir.gen(FS_STRUCTURE)
    deps = ["foo", "bar", "foobar"]
    stage = run_head(*deps, name="copy-first-line")

    initial_content = read_lock_file()
    # reverse order of stage.outs and dump to the pipeline file
    # then, again change stage.deps and dump to the pipeline file
    reversal = stage.outs.reverse, stage.deps.reverse
    for reverse_items in reversal:
        reverse_items()
        stage.dvcfile._dump_pipeline_file(stage)

        # we only changed the order, should not reproduce
        assert not dvc.reproduce(stage.addressing)

        new_lock_content = read_lock_file()
        assert_eq_lockfile(new_lock_content, initial_content)

        (tmp_dir / LOCK_FILE).unlink()
        assert dvc.reproduce(stage.addressing) == [stage]
        new_lock_content = read_lock_file()
        assert_eq_lockfile(new_lock_content, initial_content)


def test_cmd_changes_other_orders_are_preserved(tmp_dir, dvc, run_head):
    tmp_dir.gen(FS_STRUCTURE)
    deps = ["foo", "bar", "foobar"]
    stage = run_head(*deps, name="copy-first-line")

    initial_content = read_lock_file()
    # let's change cmd in pipeline file
    # it should only change "cmd", otherwise it should be
    # structurally same as cmd
    new_cmd = "python head.py foo bar foobar"
    assert stage.cmd != new_cmd  # sanity check
    stage.cmd = new_cmd
    stage.dvcfile._dump_pipeline_file(stage)

    initial_content["stages"]["copy-first-line"]["cmd"] = stage.cmd

    assert dvc.reproduce(stage.addressing) == [stage]

    new_lock_content = read_lock_file()
    assert_eq_lockfile(new_lock_content, initial_content)


def test_params_dump(tmp_dir, dvc, run_head):
    tmp_dir.gen(FS_STRUCTURE)

    stage = run_head(
        "foo",
        "bar",
        "foobar",
        name="copy-first-line",
        params=[
            "params2.yaml:answer,lists,name",
            "params.yaml:lists,floats,nested.nested1,nested.nested1.nested2",
        ],
    )

    initial_content = read_lock_file()
    lock = initial_content["stages"]["copy-first-line"]

    # lock stage key order:
    assert list(lock.keys()) == ["cmd", "deps", "params", "outs"]
    assert list(lock["params"].keys()) == ["params.yaml", "params2.yaml"]

    # # params keys are always sorted by the name
    assert list(lock["params"]["params.yaml"].keys()) == [
        "floats",
        "lists",
        "nested.nested1",
        "nested.nested1.nested2",
    ]
    assert list(lock["params"]["params2.yaml"]) == ["answer", "lists", "name"]

    assert not dvc.reproduce(stage.addressing)

    # let's change the order of params and dump them in pipeline file
    params, _ = split_params_deps(stage)
    for param in params:
        param.params.reverse()

    stage.dvcfile._dump_pipeline_file(stage)
    assert not dvc.reproduce(stage.addressing)

    (tmp_dir / LOCK_FILE).unlink()
    assert dvc.reproduce(stage.addressing) == [stage]
    assert_eq_lockfile(initial_content, read_lock_file())

    # remove build-cache and check if the same structure is built
    for item in [dvc.stage_cache.cache_dir, LOCK_FILE]:
        remove(item)
    assert dvc.reproduce(stage.addressing) == [stage]
    assert_eq_lockfile(initial_content, read_lock_file())




tests/func/test_ls.py
import os
import shutil
import textwrap
from operator import itemgetter

import pytest

from dvc.repo import Repo
from dvc.scm import CloneError

FS_STRUCTURE = {
    "README.md": "content",
    "model/script.py": "content",
    "model/train.py": "content",
    ".gitignore": "content",
}

DVC_STRUCTURE = {
    "structure.xml": "content",
    "data/subcontent/data.xml": "content",
    "data/subcontent/statistics/data.csv": "content",
    "model/people.csv": "content",
}


def match_files(files, expected_files):
    left = {(f["path"], f["isout"]) for f in files}
    right = {(os.path.join(*args), isout) for (args, isout) in expected_files}
    assert left == right


def create_dvc_pipeline(tmp_dir, dvc):
    script = textwrap.dedent(
        """\
        import os, sys
        f = sys.argv[1]
        os.makedirs(os.path.dirname(f))
        open(f, "w+").close()
    """
    )
    tmp_dir.scm_gen({"script.py": script}, commit="init")
    tmp_dir.dvc_gen({"dep": "content"}, commit="init dvc")
    dvc.run(
        cmd="python script.py {}".format(os.path.join("out", "file")),
        outs=[os.path.join("out", "file")],
        deps=["dep"],
        fname="out.dvc",
        single_stage=True,
    )
    tmp_dir.scm_add(["out.dvc"], commit="run")
    shutil.rmtree("out")


def test_ls_repo(tmp_dir, dvc, scm):
    tmp_dir.scm_gen(FS_STRUCTURE, commit="init")
    tmp_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")

    files = Repo.ls(os.fspath(tmp_dir))
    match_files(
        files,
        (
            ((".dvcignore",), False),
            ((".gitignore",), False),
            (("README.md",), False),
            (("structure.xml.dvc",), False),
            (("model",), False),
            (("data",), False),
            (("structure.xml",), True),
        ),
    )


def test_ls_repo_recursive(tmp_dir, dvc, scm):
    tmp_dir.scm_gen(FS_STRUCTURE, commit="init")
    tmp_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")

    files = Repo.ls(os.fspath(tmp_dir), recursive=True)
    match_files(
        files,
        (
            ((".dvcignore",), False),
            ((".gitignore",), False),
            (("README.md",), False),
            (("structure.xml.dvc",), False),
            (("model", "script.py"), False),
            (("model", "train.py"), False),
            (("model", "people.csv.dvc"), False),
            (("data", "subcontent", "data.xml.dvc"), False),
            (("data", "subcontent", "statistics", "data.csv.dvc"), False),
            (("data", "subcontent", "statistics", "data.csv"), True),
            (("data", "subcontent", "statistics", ".gitignore"), False),
            (("data", "subcontent", "data.xml"), True),
            (("data", "subcontent", ".gitignore"), False),
            (("model", "people.csv"), True),
            (("model", ".gitignore"), False),
            (("structure.xml",), True),
        ),
    )


def test_ls_repo_dvc_only_recursive(tmp_dir, dvc, scm):
    tmp_dir.scm_gen(FS_STRUCTURE, commit="init")
    tmp_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")

    files = Repo.ls(os.fspath(tmp_dir), recursive=True, dvc_only=True)
    match_files(
        files,
        (
            (("data", "subcontent", "statistics", "data.csv"), True),
            (("data", "subcontent", "data.xml"), True),
            (("model", "people.csv"), True),
            (("structure.xml",), True),
        ),
    )


def test_ls_repo_with_new_path_dir(tmp_dir, dvc, scm):
    tmp_dir.scm_gen(FS_STRUCTURE, commit="init")
    tmp_dir.dvc_gen({"mysub": {}}, commit="dvc")
    tmp_dir.gen({"mysub/sub": {"foo": "content"}})

    files = Repo.ls(os.fspath(tmp_dir), path="mysub/sub")
    match_files(
        files,
        ((("foo",), False),),
    )


def test_ls_repo_with_path_dir(tmp_dir, dvc, scm):
    tmp_dir.scm_gen(FS_STRUCTURE, commit="init")
    tmp_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")

    files = Repo.ls(os.fspath(tmp_dir), path="model")
    match_files(
        files,
        (
            (("script.py",), False),
            (("train.py",), False),
            (("people.csv",), True),
            (("people.csv.dvc",), False),
            ((".gitignore",), False),
        ),
    )


def test_ls_repo_with_path_dir_dvc_only_empty(tmp_dir, dvc, scm):
    tmp_dir.scm_gen(FS_STRUCTURE, commit="init")
    tmp_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")
    tmp_dir.scm_gen({"folder/.keep": "content"}, commit="add .keep")
    tmp_dir.scm_gen({"empty_scm_folder/": {}}, commit="add scm empty")
    tmp_dir.dvc_gen({"empty_dvc_folder": {}}, commit="empty dvc folder")

    with pytest.raises(FileNotFoundError):
        Repo.ls(os.fspath(tmp_dir), path="not_exist_folder")

    assert Repo.ls(os.fspath(tmp_dir), path="empty_scm_folder") == []

    assert Repo.ls(os.fspath(tmp_dir), path="folder", dvc_only=True) == []

    assert Repo.ls(os.fspath(tmp_dir), path="empty_dvc_folder", dvc_only=True) == []


def test_ls_repo_with_path_subdir(tmp_dir, dvc, scm):
    tmp_dir.scm_gen(FS_STRUCTURE, commit="init")
    tmp_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")

    path = os.path.join("data", "subcontent")
    files = Repo.ls(os.fspath(tmp_dir), path)
    match_files(
        files,
        (
            (("data.xml",), True),
            (("data.xml.dvc",), False),
            (("statistics",), False),
            ((".gitignore",), False),
        ),
    )


def test_ls_repo_with_path_subdir_dvc_only(tmp_dir, dvc, scm):
    tmp_dir.scm_gen(FS_STRUCTURE, commit="init")
    tmp_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")

    path = os.path.join("data", "subcontent")
    files = Repo.ls(os.fspath(tmp_dir), path, dvc_only=True)
    match_files(files, ((("data.xml",), True), (("statistics",), False)))


def test_ls_repo_with_path_subdir_dvc_only_recursive(tmp_dir, dvc, scm):
    tmp_dir.scm_gen(FS_STRUCTURE, commit="init")
    tmp_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")

    path = os.path.join("data", "subcontent")
    files = Repo.ls(os.fspath(tmp_dir), path, dvc_only=True, recursive=True)
    match_files(files, ((("data.xml",), True), (("statistics", "data.csv"), True)))


def test_ls_repo_with_path_file_out(tmp_dir, dvc, scm):
    tmp_dir.scm_gen(FS_STRUCTURE, commit="init")
    tmp_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")

    path = os.path.join("data", "subcontent", "data.xml")
    files = Repo.ls(os.fspath(tmp_dir), path)
    match_files(files, ((("data.xml",), True),))


def test_ls_repo_with_file_path_fs(tmp_dir, dvc, scm):
    tmp_dir.scm_gen(FS_STRUCTURE, commit="init")
    tmp_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")

    path = "README.md"
    files = Repo.ls(os.fspath(tmp_dir), path, recursive=True)
    match_files(files, ((("README.md",), False),))


def test_ls_repo_with_missed_path(tmp_dir, dvc, scm):
    tmp_dir.scm_gen(FS_STRUCTURE, commit="init")
    tmp_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")

    with pytest.raises(FileNotFoundError):
        Repo.ls(os.fspath(tmp_dir), path="missed_path")


def test_ls_repo_with_missed_path_dvc_only(tmp_dir, dvc, scm):
    tmp_dir.scm_gen(FS_STRUCTURE, commit="init")
    tmp_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")

    with pytest.raises(FileNotFoundError):
        Repo.ls(
            os.fspath(tmp_dir),
            path="missed_path",
            recursive=True,
            dvc_only=True,
        )


def test_ls_repo_with_removed_dvc_dir(tmp_dir, dvc, scm):
    create_dvc_pipeline(tmp_dir, dvc)

    files = Repo.ls(os.fspath(tmp_dir))
    match_files(
        files,
        (
            (("script.py",), False),
            (("dep.dvc",), False),
            (("out.dvc",), False),
            (("dep",), True),
            (("out",), False),
            ((".dvcignore",), False),
            ((".gitignore",), False),
        ),
    )


def test_ls_repo_with_removed_dvc_dir_recursive(tmp_dir, dvc, scm):
    create_dvc_pipeline(tmp_dir, dvc)

    files = Repo.ls(os.fspath(tmp_dir), recursive=True)
    match_files(
        files,
        (
            (("script.py",), False),
            (("dep.dvc",), False),
            (("out.dvc",), False),
            (("dep",), True),
            (("out", "file"), True),
            ((".dvcignore",), False),
            ((".gitignore",), False),
        ),
    )


def test_ls_repo_with_removed_dvc_dir_with_path_dir(tmp_dir, dvc, scm):
    create_dvc_pipeline(tmp_dir, dvc)

    path = "out"
    files = Repo.ls(os.fspath(tmp_dir), path)
    match_files(files, ((("file",), True),))


def test_ls_repo_with_removed_dvc_dir_with_path_file(tmp_dir, dvc, scm):
    create_dvc_pipeline(tmp_dir, dvc)

    path = os.path.join("out", "file")
    files = Repo.ls(os.fspath(tmp_dir), path)
    match_files(files, ((("file",), True),))


def test_ls_repo_with_rev(erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.scm_gen(FS_STRUCTURE, commit="init")
        erepo_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")

    rev = erepo_dir.scm.list_all_commits()[1]
    files = Repo.ls(os.fspath(erepo_dir), rev=rev)
    match_files(
        files,
        (
            ((".dvcignore",), False),
            ((".gitignore",), False),
            (("README.md",), False),
            (("model",), False),
        ),
    )


def test_ls_remote_repo(erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.scm_gen(FS_STRUCTURE, commit="init")
        erepo_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")

    url = f"file://{erepo_dir.as_posix()}"
    files = Repo.ls(url)
    match_files(
        files,
        (
            ((".dvcignore",), False),
            ((".gitignore",), False),
            (("README.md",), False),
            (("structure.xml.dvc",), False),
            (("model",), False),
            (("data",), False),
            (("structure.xml",), True),
        ),
    )


def test_ls_remote_repo_recursive(erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.scm_gen(FS_STRUCTURE, commit="init")
        erepo_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")

    url = f"file://{erepo_dir.as_posix()}"
    files = Repo.ls(url, recursive=True)
    match_files(
        files,
        (
            ((".dvcignore",), False),
            ((".gitignore",), False),
            (("README.md",), False),
            (("structure.xml.dvc",), False),
            (("model", "script.py"), False),
            (("model", "train.py"), False),
            (("model", "people.csv.dvc"), False),
            (("data", "subcontent", "data.xml.dvc"), False),
            (("data", "subcontent", "statistics", "data.csv.dvc"), False),
            (("data", "subcontent", "statistics", "data.csv"), True),
            (("data", "subcontent", "statistics", ".gitignore"), False),
            (("data", "subcontent", "data.xml"), True),
            (("data", "subcontent", ".gitignore"), False),
            (("model", "people.csv"), True),
            (("model", ".gitignore"), False),
            (("structure.xml",), True),
        ),
    )


def test_ls_remote_git_only_repo_recursive(git_dir):
    with git_dir.chdir():
        git_dir.scm_gen(FS_STRUCTURE, commit="init")

    url = f"file://{git_dir.as_posix()}"
    files = Repo.ls(url, recursive=True)
    match_files(
        files,
        (
            ((".gitignore",), False),
            (("README.md",), False),
            (("model", "script.py"), False),
            (("model", "train.py"), False),
        ),
    )


def test_ls_remote_repo_with_path_dir(erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.scm_gen(FS_STRUCTURE, commit="init")
        erepo_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")

    url = f"file://{erepo_dir.as_posix()}"
    path = "model"
    files = Repo.ls(url, path)
    match_files(
        files,
        (
            (("script.py",), False),
            (("train.py",), False),
            (("people.csv",), True),
            (("people.csv.dvc",), False),
            ((".gitignore",), False),
        ),
    )


def test_ls_remote_repo_with_rev(erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.scm_gen(FS_STRUCTURE, commit="init")
        erepo_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")

    rev = erepo_dir.scm.list_all_commits()[1]
    url = f"file://{erepo_dir.as_posix()}"
    files = Repo.ls(url, rev=rev)
    match_files(
        files,
        (
            ((".dvcignore",), False),
            ((".gitignore",), False),
            (("README.md",), False),
            (("model",), False),
        ),
    )


def test_ls_remote_repo_with_rev_recursive(erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen(DVC_STRUCTURE, commit="dvc")
        erepo_dir.scm_gen(FS_STRUCTURE, commit="init")

    rev = erepo_dir.scm.list_all_commits()[1]
    url = f"file://{erepo_dir.as_posix()}"
    files = Repo.ls(url, rev=rev, recursive=True)
    match_files(
        files,
        (
            (("structure.xml.dvc",), False),
            (("model", "people.csv.dvc"), False),
            (("data", "subcontent", "data.xml.dvc"), False),
            (("data", "subcontent", "statistics", "data.csv.dvc"), False),
            (("data", "subcontent", "statistics", "data.csv"), True),
            (("data", "subcontent", "statistics", ".gitignore"), False),
            (("data", "subcontent", "data.xml"), True),
            (("data", "subcontent", ".gitignore"), False),
            (("model", "people.csv"), True),
            (("model", ".gitignore"), False),
            (("structure.xml",), True),
            ((".dvcignore",), False),
            ((".gitignore",), False),
        ),
    )


def test_ls_not_existed_url():
    from time import time

    dirname = "__{}_{}".format("not_existed", time())
    with pytest.raises(CloneError):
        Repo.ls(dirname, recursive=True)


def test_ls_shows_pipeline_tracked_outs(tmp_dir, dvc, scm, run_copy):
    from dvc.dvcfile import LOCK_FILE, PROJECT_FILE

    tmp_dir.gen("foo", "foo")
    run_copy("foo", "bar", name="copy-foo-bar")
    dvc.scm.add([PROJECT_FILE, LOCK_FILE])
    dvc.scm.commit("add pipeline stage")

    files = Repo.ls(os.curdir, dvc_only=True)
    match_files(files, ((("bar",), True),))


def test_ls_granular(erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen(
            {
                "dir": {
                    "1": "1",
                    "2": "2",
                    "subdir": {"foo": "foo", "bar": "bar"},
                }
            },
            commit="create dir",
        )

    entries = Repo.ls(os.fspath(erepo_dir), os.path.join("dir", "subdir"))
    assert entries == [
        {"isout": True, "isdir": False, "isexec": False, "path": "bar"},
        {"isout": True, "isdir": False, "isexec": False, "path": "foo"},
    ]

    entries = Repo.ls(os.fspath(erepo_dir), "dir")
    assert entries == [
        {"isout": True, "isdir": False, "isexec": False, "path": "1"},
        {"isout": True, "isdir": False, "isexec": False, "path": "2"},
        {"isout": True, "isdir": True, "isexec": False, "path": "subdir"},
    ]


@pytest.mark.parametrize("use_scm", [True, False])
def test_ls_target(erepo_dir, use_scm):
    with erepo_dir.chdir():
        gen = erepo_dir.scm_gen if use_scm else erepo_dir.dvc_gen
        gen(
            {
                "dir": {
                    "1": "1",
                    "2": "2",
                    "subdir": {"foo": "foo", "bar": "bar"},
                }
            },
            commit="create dir",
        )

    isout = not use_scm

    def _ls(path):
        return Repo.ls(os.fspath(erepo_dir), path)

    assert _ls(os.path.join("dir", "1")) == [
        {"isout": isout, "isdir": False, "isexec": False, "path": "1"}
    ]
    assert _ls(os.path.join("dir", "subdir", "foo")) == [
        {"isout": isout, "isdir": False, "isexec": False, "path": "foo"}
    ]
    assert _ls(os.path.join("dir", "subdir")) == [
        {"isdir": False, "isexec": 0, "isout": isout, "path": "bar"},
        {"isdir": False, "isexec": 0, "isout": isout, "path": "foo"},
    ]


@pytest.mark.parametrize(
    "dvc_top_level, erepo",
    [
        (True, pytest.lazy_fixture("erepo_dir")),
        (False, pytest.lazy_fixture("git_dir")),
    ],
)
def test_subrepo(dvc_top_level, erepo):
    from tests.func.test_get import make_subrepo

    dvc_files = {"foo.txt": "foo.txt", "dvc_dir": {"lorem": "lorem"}}
    scm_files = {"bar.txt": "bar.txt", "scm_dir": {"ipsum": "ipsum"}}
    subrepo = erepo / "subrepo"
    make_subrepo(subrepo, erepo.scm)

    for repo in [erepo, subrepo]:
        with repo.chdir():
            repo.scm_gen(scm_files, commit=f"scm track for top {repo}")
            if hasattr(repo, "dvc"):
                repo.dvc_gen(dvc_files, commit=f"dvc track for {repo}")

    def _list_files(repo, path=None):
        return set(map(itemgetter("path"), Repo.ls(os.fspath(repo), path)))

    extras = {".dvcignore", ".gitignore"}
    git_tracked_outputs = {"bar.txt", "scm_dir"}
    dvc_files = {"dvc_dir", "foo.txt", "foo.txt.dvc", "dvc_dir.dvc"}
    common_outputs = git_tracked_outputs | extras | dvc_files

    top_level_outputs = common_outputs if dvc_top_level else git_tracked_outputs
    assert _list_files(erepo) == top_level_outputs
    assert _list_files(erepo, "scm_dir") == {"ipsum"}
    if dvc_top_level:
        assert _list_files(erepo, "dvc_dir") == {"lorem"}

    assert _list_files(subrepo, ".") == common_outputs
    assert _list_files(subrepo, "scm_dir") == {"ipsum"}
    assert _list_files(subrepo, "dvc_dir") == {"lorem"}


def test_broken_symlink(tmp_dir, dvc):
    from dvc.fs import system

    tmp_dir.gen("file", "content")
    system.symlink("file", "link")

    os.remove("file")

    entries = Repo.ls(os.fspath(tmp_dir))

    assert entries == [
        {
            "isout": False,
            "isdir": False,
            "isexec": False,
            "path": ".dvcignore",
        },
        {
            "isout": False,
            "isdir": False,
            "isexec": False,
            "path": "link",
        },
    ]




tests/func/test_ls_url.py
from dvc.testing.workspace_tests import TestLsUrl as _TestLsUrl


class TestLsUrl(_TestLsUrl):
    pass




tests/func/test_merge_driver.py
import os

import pytest

from dvc.cli import main
from dvc.utils.fs import remove


def _gen(tmp_dir, struct, name):
    remove(tmp_dir / "data")
    if struct is None:
        (tmp_dir / name).touch()
    else:
        (stage,) = tmp_dir.dvc_gen({"data": struct})
        os.rename(stage.path, name)


@pytest.mark.parametrize(
    "ancestor, our, their, merged",
    [
        (
            {"foo": "foo"},
            {"foo": "foo", "bar": "bar"},
            {"foo": "foo", "baz": "baz"},
            {"foo": "foo", "bar": "bar", "baz": "baz"},
        ),
        (
            {"common": "common", "subdir": {"foo": "foo"}},
            {"common": "common", "subdir": {"foo": "foo", "bar": "bar"}},
            {"common": "common", "subdir": {"foo": "foo", "baz": "baz"}},
            {
                "common": "common",
                "subdir": {"foo": "foo", "bar": "bar", "baz": "baz"},
            },
        ),
        ({}, {"foo": "foo"}, {"bar": "bar"}, {"foo": "foo", "bar": "bar"}),
        ({}, {}, {"bar": "bar"}, {"bar": "bar"}),
        ({}, {"foo": "foo"}, {}, {"foo": "foo"}),
        (None, {"foo": "foo"}, {"bar": "bar"}, {"foo": "foo", "bar": "bar"}),
        (None, None, {"bar": "bar"}, {"bar": "bar"}),
        (None, {"foo": "foo"}, None, {"foo": "foo"}),
        (
            {"foo": "foo"},
            {"foo": "bar"},
            {"foo": "foo", "baz": "baz"},
            {"foo": "bar", "baz": "baz"},
        ),
        ({"foo": "foo"}, {}, {"foo": "foo", "bar": "bar"}, {"bar": "bar"}),
        (
            {"common": "common", "subdir": {"foo": "foo", "bar": "bar"}},
            {"common": "common", "subdir": {"foo": "foo", "bar": "baz"}},
            {"common": "common", "subdir": {"bar": "bar", "bizz": "bizz"}},
            {
                "common": "common",
                "subdir": {"bar": "baz", "bizz": "bizz"},
            },
        ),
    ],
)
def test_merge(tmp_dir, dvc, ancestor, our, their, merged):
    _gen(tmp_dir, ancestor, "ancestor")
    _gen(tmp_dir, our, "our")
    _gen(tmp_dir, their, "their")

    assert (
        main(
            [
                "git-hook",
                "merge-driver",
                "--ancestor",
                "ancestor",
                "--our",
                "our",
                "--their",
                "their",
            ]
        )
        == 0
    )

    _gen(tmp_dir, merged, "merged")

    assert (tmp_dir / "our").read_text() == (tmp_dir / "merged").read_text()


@pytest.mark.parametrize(
    "ancestor, our, their, error",
    [
        (
            {"foo": "foo"},
            {"foo": "bar"},
            {"foo": "baz"},
            "unable to auto-merge the following paths:\nfoo",
        ),
        (
            {"common": "common", "foo": "foo"},
            {"common": "common", "bar": "bar"},
            {"baz": "baz"},
            "unable to auto-merge the following paths:\nboth deleted: ('foo',)",
        ),
    ],
)
def test_merge_conflict(tmp_dir, dvc, ancestor, our, their, error, caplog):
    _gen(tmp_dir, ancestor, "ancestor")
    _gen(tmp_dir, our, "our")
    _gen(tmp_dir, their, "their")

    assert (
        main(
            [
                "git-hook",
                "merge-driver",
                "--ancestor",
                "ancestor",
                "--our",
                "our",
                "--their",
                "their",
            ]
        )
        != 0
    )

    assert error in caplog.text


def test_merge_different_output_options(tmp_dir, dvc, caplog):
    (tmp_dir / "ancestor").touch()

    (tmp_dir / "our").write_text("outs:\n- md5: f123456789.dir\n  path: path\n")

    (tmp_dir / "their").write_text(
        "outs:\n- md5: f987654321.dir\n  path: path\n  cache: false\n"
    )

    assert (
        main(
            [
                "git-hook",
                "merge-driver",
                "--ancestor",
                "ancestor",
                "--our",
                "our",
                "--their",
                "their",
            ]
        )
        != 0
    )

    error = "unable to auto-merge outputs with different options"
    assert error in caplog.text


def test_merge_file(tmp_dir, dvc, caplog):
    (tmp_dir / "ancestor").touch()

    (tmp_dir / "our").write_text("outs:\n- md5: f123456789.dir\n  path: path\n")

    (tmp_dir / "their").write_text("outs:\n- md5: f987654321\n  path: path\n")

    assert (
        main(
            [
                "git-hook",
                "merge-driver",
                "--ancestor",
                "ancestor",
                "--our",
                "our",
                "--their",
                "their",
            ]
        )
        != 0
    )

    err = "unable to auto-merge outputs that are not directories"
    assert err in caplog.text


def test_merge_non_dvc_add(tmp_dir, dvc, caplog):
    (tmp_dir / "ancestor").touch()

    (tmp_dir / "our").write_text(
        "outs:\n"
        "- md5: f123456789.dir\n"
        "  path: path\n"
        "- md5: ff123456789.dir\n"
        "  path: another\n"
    )

    (tmp_dir / "their").write_text("outs:\n- md5: f987654321\n  path: path\n")

    assert (
        main(
            [
                "git-hook",
                "merge-driver",
                "--ancestor",
                "ancestor",
                "--our",
                "our",
                "--their",
                "their",
            ]
        )
        != 0
    )

    error = "unable to auto-merge DVC files that weren't created by `dvc add`"
    assert error in caplog.text




tests/func/test_move.py
import os
import textwrap

import pytest

from dvc.cli import main
from dvc.exceptions import MoveNotDataSourceError, OutputNotFoundError


def test_move(tmp_dir, dvc):
    tmp_dir.dvc_gen("foo", "foo")
    dvc.move("foo", "foo1")

    assert not (tmp_dir / "foo").is_file()
    assert (tmp_dir / "foo1").is_file()


def test_move_non_existent_file(dvc):
    with pytest.raises(OutputNotFoundError):
        dvc.move("non_existent_file", "dst")


def test_move_directory(tmp_dir, dvc):
    tmp_dir.dvc_gen("data", {"foo": "foo", "bar": "bar"})
    dvc.move("data", "dst")
    assert not (tmp_dir / "data").is_dir()
    assert (tmp_dir / "dst").is_dir()


def test_cmd_move(tmp_dir, dvc):
    tmp_dir.dvc_gen("foo", "foo")
    assert main(["move", "foo", "foo1"]) == 0
    assert main(["move", "non-existing-file", "dst"]) != 0


def test_move_not_data_source(tmp_dir, dvc):
    tmp_dir.dvc_gen("foo", "foo")
    dvc.run(
        cmd="cp foo file1",
        outs=["file1"],
        deps=["foo"],
        single_stage=True,
    )

    with pytest.raises(MoveNotDataSourceError):
        dvc.move("file1", "dst")

    assert main(["move", "file1", "dst"]) != 0
    assert (tmp_dir / "file1").exists()


def test_move_file_with_extension(tmp_dir, dvc):
    tmp_dir.dvc_gen("file.csv", "1,2,3\n")

    assert main(["move", "file.csv", "other_name.csv"]) == 0
    assert not (tmp_dir / "file.csv").exists()
    assert not (tmp_dir / "file.csv.dvc").exists()
    assert (tmp_dir / "other_name.csv").exists()
    assert (tmp_dir / "other_name.csv.dvc").exists()


def test_move_file_to_directory(tmp_dir, dvc):
    tmp_dir.dvc_gen("foo", "foo")
    tmp_dir.gen({"data": {"bar": "bar"}})

    assert main(["move", "foo", os.path.join("data", "foo")]) == 0
    assert not (tmp_dir / "foo").exists()
    assert not (tmp_dir / "foo.dvc").exists()
    assert (tmp_dir / "data" / "foo").exists()
    assert (tmp_dir / "data" / "foo.dvc").exists()


def test_move_file_to_directory_without_specified_target_name(tmp_dir, dvc):
    tmp_dir.dvc_gen("foo", "foo")
    tmp_dir.gen({"data": {"bar": "bar"}})

    assert main(["move", "foo", "data"]) == 0
    assert not (tmp_dir / "foo").exists()
    assert not (tmp_dir / "foo.dvc").exists()
    assert (tmp_dir / "data" / "foo").exists()
    assert (tmp_dir / "data" / "foo.dvc").exists()

    new_stage = (tmp_dir / "data" / "foo.dvc").load_yaml()
    assert new_stage["outs"][0]["path"] == "foo"


def test_move_directory_should_not_overwrite_existing(tmp_dir, dvc, scm):
    tmp_dir.dvc_gen({"data": {"foo": "foo"}})
    new_dir = tmp_dir / "dir"
    new_dir.mkdir()

    dvc.move("data", "dir")
    assert not (tmp_dir / "data").exists()
    assert not (tmp_dir / "data.dvc").exists()
    assert set(new_dir.iterdir()) == {
        new_dir / ".gitignore",
        new_dir / "data.dvc",
        new_dir / "data",
    }
    assert set((new_dir / "data").iterdir()) == {new_dir / "data" / "foo"}


def test_move_file_between_directories(tmp_dir, dvc):
    tmp_dir.gen({"data": {"foo": "foo"}})
    dvc.add(os.path.join("data", "foo"))

    (tmp_dir / "data2").mkdir()

    assert main(["move", os.path.join("data", "foo"), "data2"]) == 0
    assert not (tmp_dir / "data" / "foo").exists()
    assert not (tmp_dir / "data" / "foo.dvc").exists()
    assert (tmp_dir / "data2" / "foo").exists()
    assert (tmp_dir / "data2" / "foo.dvc").exists()

    d = (tmp_dir / "data2" / "foo.dvc").load_yaml()
    assert d["outs"][0]["path"] == "foo"


def test_move_file_inside_directory(tmp_dir, dvc):
    tmp_dir.gen({"data": {"foo": "foo"}})
    file = tmp_dir / "data" / "foo"
    dvc.add(file.fs_path)

    with (tmp_dir / "data").chdir():
        assert main(["move", "foo", "data.txt"]) == 0

    assert not file.exists()
    assert (tmp_dir / "data" / "data.txt").exists()
    assert (tmp_dir / "data" / "data.txt.dvc").exists()


def test_move_should_save_stage_info(tmp_dir, dvc):
    tmp_dir.dvc_gen({"old_name": {"file1": "file1"}})

    dvc.move("old_name", "new_name")

    assert dvc.status() == {}


def test_should_move_to_dir_on_non_default_stage_file(tmp_dir, dvc):
    stage_file_name = "stage.dvc"
    tmp_dir.gen({"file": "file_content"})

    dvc.add("file", file=stage_file_name)
    os.mkdir("directory")

    dvc.move("file", "directory")

    assert os.path.exists(os.path.join("directory", "file"))


def test_move_gitignored(tmp_dir, scm, dvc):
    from dvc.dvcfile import FileIsGitIgnored

    tmp_dir.dvc_gen({"foo": "foo"})

    os.mkdir("dir")
    (tmp_dir / "dir").gen(".gitignore", "*")

    with pytest.raises(FileIsGitIgnored):
        dvc.move("foo", "dir")

    assert (tmp_dir / "foo").read_text() == "foo"
    assert (tmp_dir / "foo.dvc").exists()
    assert not (tmp_dir / "dir" / "foo").exists()
    assert not (tmp_dir / "dir" / "foo.dvc").exists()


def test_move_output_overlap(tmp_dir, dvc):
    from dvc.exceptions import OverlappingOutputPathsError

    tmp_dir.dvc_gen({"foo": "foo", "dir": {"bar": "bar"}})

    with pytest.raises(OverlappingOutputPathsError):
        dvc.move("foo", "dir")

    assert (tmp_dir / "foo").read_text() == "foo"
    assert (tmp_dir / "foo.dvc").exists()
    assert not (tmp_dir / "dir" / "foo").exists()
    assert not (tmp_dir / "dir" / "foo.dvc").exists()


def test_move_meta(tmp_dir, dvc):
    (stage,) = tmp_dir.dvc_gen("foo", "foo")
    data = (tmp_dir / stage.path).parse()
    data["meta"] = {"custom_key": 42}
    (tmp_dir / stage.path).dump(data)

    dvc.move("foo", "bar")
    res = (tmp_dir / "bar.dvc").read_text()
    assert res == textwrap.dedent(
        """\
        outs:
        - md5: acbd18db4cc2f85cedef654fccc4a4d8
          size: 3
          path: bar
        meta:
          custom_key: 42
    """
    )




tests/func/test_odb.py
import os
import stat

import configobj
import pytest

from dvc.cachemgr import CacheManager
from dvc.cli import main
from dvc.utils import relpath
from dvc_data.hashfile.hash_info import HashInfo
from dvc_objects.errors import ObjectFormatError


def test_cache(tmp_dir, dvc):
    cache1_md5 = "123"
    cache2_md5 = "234"
    cache1 = os.path.join(
        dvc.cache.local.path,
        cache1_md5[0:2],
        cache1_md5[2:],
    )
    cache2 = os.path.join(
        dvc.cache.local.path,
        cache2_md5[0:2],
        cache2_md5[2:],
    )
    tmp_dir.gen({cache1: "1", cache2: "2"})

    assert os.path.exists(cache1)
    assert os.path.exists(cache2)

    odb = CacheManager(dvc)

    md5_list = list(odb.local.all())
    assert len(md5_list) == 2
    assert cache1_md5 in md5_list
    assert cache2_md5 in md5_list

    odb_cache1 = odb.local.oid_to_path(cache1_md5)
    odb_cache2 = odb.local.oid_to_path(cache2_md5)
    assert os.fspath(odb_cache1) == cache1
    assert os.fspath(odb_cache2) == cache2


def test_cache_load_bad_dir_cache(tmp_dir, dvc):
    from dvc_data.hashfile import load

    dir_hash = "123.dir"
    fname = os.fspath(dvc.cache.local.oid_to_path(dir_hash))
    tmp_dir.gen({fname: "<clearly>not,json"})
    with pytest.raises(ObjectFormatError):
        load(dvc.cache.local, HashInfo("md5", dir_hash))

    dir_hash = "234.dir"
    fname = os.fspath(dvc.cache.local.oid_to_path(dir_hash))
    tmp_dir.gen({fname: '{"a": "b"}'})
    with pytest.raises(ObjectFormatError):
        load(dvc.cache.local, HashInfo("md5", dir_hash))


def test_external_cache_dir(tmp_dir, dvc, make_tmp_dir):
    cache_dir = make_tmp_dir("cache")

    with dvc.config.edit() as conf:
        conf["cache"]["dir"] = cache_dir.fs_path
    assert not os.path.exists(dvc.cache.local.path)
    dvc.cache = CacheManager(dvc)

    tmp_dir.dvc_gen({"foo": "foo"})

    tmp_dir.dvc_gen(
        {
            "data_dir": {
                "data": "data_dir/data",
                "data_sub_dir": {"data_sub": "data_dir/data_sub_dir/data_sub"},
            }
        }
    )

    assert not os.path.exists(".dvc/cache")
    assert len(os.listdir(cache_dir)) != 0


def test_remote_cache_references(tmp_dir, dvc):
    with dvc.config.edit() as conf:
        conf["remote"]["storage"] = {"url": "ssh://user@localhost:23"}
        conf["remote"]["cache"] = {"url": "remote://storage/tmp"}
        conf["cache"]["ssh"] = "cache"

    dvc.cache = CacheManager(dvc)

    assert dvc.cache.ssh.path == "/tmp"


def test_shared_cache_dir(tmp_dir):
    cache_dir = os.path.abspath(os.path.join(os.curdir, "cache"))
    for d in ["dir1", "dir2"]:
        os.mkdir(d)
        with (tmp_dir / d).chdir():
            ret = main(["init", "--no-scm"])
            assert ret == 0

            ret = main(["config", "cache.dir", cache_dir])
            assert ret == 0

            assert not os.path.exists(os.path.join(".dvc", "cache"))

            (tmp_dir / d).gen({"common": "common", "unique": d})

            ret = main(["add", "common", "unique"])
            assert ret == 0

    assert not os.path.exists(os.path.join("dir1", ".dvc", "cache"))
    assert not os.path.exists(os.path.join("dir2", ".dvc", "cache"))
    assert os.path.exists(
        os.path.join(cache_dir, "dc", "f6c2fa538b445a3a095255c3641dfc")
    )
    assert os.path.exists(
        os.path.join(cache_dir, "b4", "333c8cfa2ebba7ef20ec6c3265902b")
    )
    assert os.path.exists(
        os.path.join(cache_dir, "9e", "fab2399c7c560b34de477b9aa0a465")
    )


def test_cache_link_type(tmp_dir, scm, dvc):
    with dvc.config.edit() as conf:
        conf["cache"]["type"] = "reflink,copy"
    dvc.cache = CacheManager(dvc)

    stages = tmp_dir.dvc_gen({"foo": "foo"})
    assert len(stages) == 1
    assert (tmp_dir / "foo").read_text().strip() == "foo"


def test_cmd_cache_dir(tmp_dir, scm, dvc):
    ret = main(["cache", "dir"])
    assert ret == 0


def test_cmd_cache_abs_path(tmp_dir, scm, dvc, make_tmp_dir):
    cache_dir = make_tmp_dir("cache")
    ret = main(["cache", "dir", cache_dir.fs_path])
    assert ret == 0

    config = configobj.ConfigObj(dvc.config.files["repo"])
    assert config["cache"]["dir"] == cache_dir.fs_path


def test_cmd_cache_relative_path(tmp_dir, scm, dvc, make_tmp_dir):
    cache_dir = make_tmp_dir("cache")
    dname = relpath(cache_dir)
    ret = main(["cache", "dir", dname])
    assert ret == 0

    dvc.config.load()
    dvc.cache = CacheManager(dvc)

    # NOTE: we are in the repo's root and config is in .dvc/, so
    # dir path written to config should be just one level above.
    rel = os.path.join("..", dname)
    config = configobj.ConfigObj(dvc.config.files["repo"])
    assert config["cache"]["dir"] == rel.replace("\\", "/")

    tmp_dir.dvc_gen({"foo": "foo"})

    assert os.path.exists(
        os.path.join(cache_dir, "ac", "bd18db4cc2f85cedef654fccc4a4d8")
    )


def test_default_cache_type(dvc):
    assert dvc.cache.local.cache_types == ["reflink", "copy"]


@pytest.mark.skipif(os.name == "nt", reason="Not supported for Windows.")
@pytest.mark.parametrize("group", [False, True])
def test_shared_cache(tmp_dir, dvc, group):
    from dvc.fs import system

    if group:
        with dvc.config.edit() as conf:
            conf["cache"].update({"shared": "group"})
    dvc.cache = CacheManager(dvc)
    cache_dir = dvc.cache.local.path

    assert not os.path.exists(cache_dir)

    tmp_dir.dvc_gen({"file": "file content", "dir": {"file2": "file 2 content"}})

    file_mode = oct(0o444)
    dir_mode = oct(0o2775 if group else (0o777 & ~system.umask))
    for root, dnames, fnames in os.walk(cache_dir):
        for dname in dnames:
            path = os.path.join(root, dname)
            assert oct(stat.S_IMODE(os.stat(path).st_mode)) == dir_mode

        for fname in fnames:
            path = os.path.join(root, fname)
            assert oct(stat.S_IMODE(os.stat(path).st_mode)) == file_mode




tests/func/test_remote.py
import errno
import itertools
import os
import stat
from unittest.mock import patch

import configobj
import pytest

from dvc.cli import main
from dvc.config import Config
from dvc.exceptions import DownloadError, RemoteCacheRequiredError, UploadError
from dvc.utils.fs import remove


def test_remote(dvc):
    remotes = ["a", "b", "c"]

    assert main(["remote", "list"]) == 0
    assert main(["remote", "remove", remotes[0]]) != 0

    for r in remotes:
        assert main(["remote", "add", "--default", r, "s3://bucket/name"]) == 0

    assert main(["remote", "list"]) == 0

    assert main(["remote", "modify", remotes[0], "checksum_jobs", "1"]) == 0
    assert main(["remote", "remove", remotes[0]]) == 0

    assert main(["remote", "list"]) == 0


def test_remote_add_relative_path(dvc):
    dname = os.path.join("..", "path", "to", "dir")
    ret = main(["remote", "add", "mylocal", dname])
    assert ret == 0

    # NOTE: we are in the repo's root and config is in .dvc/, so
    # dir path written to config should be just one level above.
    rel = os.path.join("..", dname)
    config = configobj.ConfigObj(dvc.config.files["repo"])
    assert config['remote "mylocal"']["url"] == rel.replace("\\", "/")


def test_remote_overwrite(dvc):
    remote_name = "a"
    remote_url = "s3://bucket/name"
    assert main(["remote", "add", remote_name, remote_url]) == 0
    assert main(["remote", "add", remote_name, remote_url]) == 251
    assert main(["remote", "add", "-f", remote_name, remote_url]) == 0


def test_referencing_other_remotes(dvc):
    assert main(["remote", "add", "foo", "ssh://localhost/"]) == 0
    assert main(["remote", "add", "bar", "remote://foo/dvc-storage"]) == 0

    config = configobj.ConfigObj(dvc.config.files["repo"])
    assert config['remote "bar"']["url"] == "remote://foo/dvc-storage"


def test_remove_default(tmp_dir, dvc):
    remote = "mys3"
    assert main(["remote", "add", "--default", remote, "s3://bucket/name"]) == 0
    assert main(["remote", "modify", remote, "profile", "default"]) == 0
    assert main(["config", "--local", "core.remote", remote]) == 0

    config = configobj.ConfigObj(dvc.config.files["repo"])
    local_config = configobj.ConfigObj(dvc.config.files["local"])
    assert config["core"]["remote"] == remote
    assert local_config["core"]["remote"] == remote

    assert main(["remote", "remove", remote]) == 0

    config = configobj.ConfigObj(dvc.config.files["repo"])
    local_config = configobj.ConfigObj(dvc.config.files["local"])
    assert config.get("core", {}).get("remote") is None
    assert local_config.get("core", {}).get("remote") is None


def test_remote_remove(dvc):
    ret = main(["config", "core.checksum_jobs", "1"])
    assert ret == 0

    remote = "mys3"
    ret = main(["remote", "add", remote, "s3://bucket/name"])
    assert ret == 0

    ret = main(["remote", "remove", remote])
    assert ret == 0


def test_remote_default_cmd(dvc):
    remote = "mys3"
    ret = main(["remote", "add", "mys3", "s3://bucket/path"])
    assert ret == 0

    ret = main(["remote", "default", "mys3"])
    assert ret == 0
    config_file = os.path.join(dvc.dvc_dir, Config.CONFIG)
    config = configobj.ConfigObj(config_file)
    default = config["core"]["remote"]
    assert default == remote

    ret = main(["remote", "default", "--unset"])
    assert ret == 0
    config = configobj.ConfigObj(config_file)
    default = config.get("core", {}).get("remote")
    assert default is None


def test_show_default(dvc, capsys):
    assert main(["remote", "add", "foo", "s3://bucket/name"]) == 0
    assert main(["remote", "default", "foo"]) == 0
    assert main(["remote", "default"]) == 0
    out, _ = capsys.readouterr()
    assert out == "foo\n"


def test_upper_case_remote(tmp_dir, dvc, local_cloud):
    remote_name = "UPPERCASEREMOTE"

    tmp_dir.gen("foo", "foo")

    ret = main(["remote", "add", remote_name, local_cloud.url])
    assert ret == 0

    ret = main(["push", "-r", remote_name])
    assert ret == 0


def test_dir_hash_should_be_key_order_agnostic(tmp_dir, dvc):
    from dvc_data.hashfile.build import build
    from dvc_data.hashfile.tree import Tree

    tmp_dir.gen({"data": {"1": "1 content", "2": "2 content"}})

    path = (tmp_dir / "data").fs_path

    tree = Tree.from_list([{"relpath": "1", "md5": "1"}, {"relpath": "2", "md5": "2"}])
    tree.digest()
    with patch("dvc_data.hashfile.build._build_tree", return_value=(None, tree)):
        _, _, obj = build(dvc.cache.local, path, dvc.cache.local.fs, "md5")
        hash1 = obj.hash_info

    # remove the raw dir obj to force building the tree on the next build call
    dvc.cache.local.fs.remove(dvc.cache.local.oid_to_path(hash1.as_raw().value))

    tree = Tree.from_list([{"md5": "1", "relpath": "1"}, {"md5": "2", "relpath": "2"}])
    tree.digest()
    with patch("dvc_data.hashfile.build._build_tree", return_value=(None, tree)):
        _, _, obj = build(dvc.cache.local, path, dvc.cache.local.fs, "md5")
        hash2 = obj.hash_info

    assert hash1 == hash2


def test_partial_push_n_pull(  # noqa: C901
    tmp_dir, dvc, tmp_path_factory, local_remote
):
    from dvc_objects.fs import generic

    foo = tmp_dir.dvc_gen({"foo": "foo content"})[0].outs[0]
    bar = tmp_dir.dvc_gen({"bar": "bar content"})[0].outs[0]
    baz = tmp_dir.dvc_gen({"baz": {"foo": "foo content"}})[0].outs[0]

    # Faulty upload version, failing on foo
    original = generic.transfer
    odb = dvc.cloud.get_remote_odb("upstream")

    def unreliable_upload(from_fs, from_info, to_fs, to_info, **kwargs):
        on_error = kwargs["on_error"]
        assert on_error
        if isinstance(from_info, str):
            from_info = [from_info]
        else:
            from_info = list(from_info)
        if isinstance(to_info, str):
            to_info = [to_info]
        else:
            to_info = list(to_info)
        for i in range(len(from_info) - 1, -1, -1):
            from_i = from_info[i]
            to_i = to_info[i]
            if os.path.abspath(to_i) == os.path.abspath(
                odb.get(foo.hash_info.value).path
            ):
                if on_error:
                    on_error(from_i, to_i, Exception("stop foo"))
                del from_info[i]
                del to_info[i]
        return original(from_fs, from_info, to_fs, to_info, **kwargs)

    with patch.object(generic, "transfer", unreliable_upload):
        with pytest.raises(UploadError) as upload_error_info:
            dvc.push()
        assert upload_error_info.value.amount == 2

        assert not odb.exists(foo.hash_info.value)
        assert odb.exists(bar.hash_info.value)
        assert not odb.exists(baz.hash_info.value)

    # Push everything and delete local cache
    dvc.push()
    dvc.cache.local.clear()

    baz._collect_used_dir_cache()

    def unreliable_download(_from_fs, from_info, _to_fs, to_info, **kwargs):
        on_error = kwargs["on_error"]
        assert on_error
        if isinstance(from_info, str):
            from_info = [from_info]
        if isinstance(to_info, str):
            to_info = [to_info]
        for from_i, to_i in zip(from_info, to_info):
            on_error(from_i, to_i, Exception())

    with patch.object(generic, "transfer", unreliable_download):
        with pytest.raises(DownloadError) as download_error_info:
            dvc.pull()
        # error count should be len(.dir + standalone file checksums)
        # since files inside dir are ignored if dir cache entry is missing
        assert download_error_info.value.amount == 2


def test_raise_on_too_many_open_files(
    tmp_dir, dvc, tmp_path_factory, mocker, local_remote
):
    tmp_dir.dvc_gen({"file": "file content"})

    mocker.patch(
        "dvc_objects.fs.generic.transfer",
        side_effect=OSError(errno.EMFILE, "Too many open files"),
    )

    with pytest.raises(OSError, match="Too many open files") as e:
        dvc.push()
    assert e.value.errno == errno.EMFILE


def test_modify_missing_remote(tmp_dir, dvc):
    assert main(["remote", "modify", "myremote", "user", "xxx"]) == 251


def test_remote_modify_local_on_repo_config(tmp_dir, dvc):
    assert main(["remote", "add", "myremote", "http://example.com/path"]) == 0
    assert main(["remote", "modify", "myremote", "user", "xxx", "--local"]) == 0
    assert dvc.config.load_one("local")["remote"]["myremote"] == {"user": "xxx"}
    assert dvc.config.load_one("repo")["remote"]["myremote"] == {
        "url": "http://example.com/path"
    }
    dvc.config.load()
    assert dvc.config["remote"]["myremote"] == {
        "url": "http://example.com/path",
        "user": "xxx",
        "verify": False,
    }


def test_external_dir_resource_on_no_cache(tmp_dir, dvc, tmp_path_factory):
    # https://github.com/iterative/dvc/issues/2647, is some situations
    # (external dir dependency) cache is required to calculate dir md5
    external_dir = tmp_path_factory.mktemp("external_dir")
    file = external_dir / "file"

    dvc.cache.local = None
    with pytest.raises(RemoteCacheRequiredError):
        dvc.run(
            cmd=f"echo content > {file}",
            outs=[os.fspath(file)],
            name="echo",
            external=True,
        )


def test_push_order(tmp_dir, dvc, tmp_path_factory, mocker, local_remote):
    from dvc_objects.fs import generic

    foo = tmp_dir.dvc_gen({"foo": {"bar": "bar content"}})[0].outs[0]
    tmp_dir.dvc_gen({"baz": "baz content"})

    mocked_upload = mocker.spy(generic, "transfer")
    dvc.push()

    # foo .dir file should be uploaded after bar
    odb = dvc.cloud.get_remote_odb("upstream")
    foo_path = odb.oid_to_path(foo.hash_info.value)
    bar_path = odb.oid_to_path(foo.obj._trie[("bar",)][1].value)
    paths = list(
        itertools.chain.from_iterable(
            args[3] for args, _ in mocked_upload.call_args_list
        )
    )
    assert paths.index(foo_path) > paths.index(bar_path)


def test_remote_modify_validation(dvc):
    remote_name = "drive"
    unsupported_config = "unsupported_config"
    assert main(["remote", "add", "-d", remote_name, "gdrive://test/test"]) == 0
    assert (
        main(["remote", "modify", remote_name, unsupported_config, "something"]) == 251
    )
    config = configobj.ConfigObj(dvc.config.files["repo"])
    assert unsupported_config not in config[f'remote "{remote_name}"']


def test_remote_modify_unset(dvc):
    assert main(["remote", "add", "-d", "myremote", "gdrive://test/test"]) == 0
    config = configobj.ConfigObj(dvc.config.files["repo"])
    assert config['remote "myremote"'] == {"url": "gdrive://test/test"}

    assert main(["remote", "modify", "myremote", "gdrive_client_id", "something"]) == 0
    config = configobj.ConfigObj(dvc.config.files["repo"])
    assert config['remote "myremote"'] == {
        "url": "gdrive://test/test",
        "gdrive_client_id": "something",
    }

    assert main(["remote", "modify", "myremote", "gdrive_client_id", "--unset"]) == 0
    config = configobj.ConfigObj(dvc.config.files["repo"])
    assert config['remote "myremote"'] == {"url": "gdrive://test/test"}


def test_remote_modify_default(dvc):
    remote_repo = "repo_level"
    remote_local = "local_level"
    wrong_name = "anything"
    assert main(["remote", "add", remote_repo, "s3://bucket/repo"]) == 0
    assert main(["remote", "add", remote_local, "s3://bucket/local"]) == 0

    assert main(["remote", "default", wrong_name]) == 251
    assert main(["remote", "default", remote_repo]) == 0
    assert main(["remote", "default", "--local", remote_local]) == 0

    repo_config = configobj.ConfigObj(dvc.config.files["repo"])
    local_config = configobj.ConfigObj(dvc.config.files["local"])

    assert repo_config["core"]["remote"] == remote_repo
    assert local_config["core"]["remote"] == remote_local


def test_remote_rename(dvc):
    remote_name = "drive"
    remote_url = "gdrive://test/test"
    new_name = "new"
    other_name = "other"
    # prepare
    assert main(["remote", "add", remote_name, remote_url]) == 0
    config = dvc.config.load_one("repo")
    assert config["remote"][remote_name]["url"] == remote_url
    assert new_name not in config.get("remote", {})

    # rename failed
    assert main(["remote", "rename", remote_name]) == 254
    assert main(["remote", "rename", new_name, other_name]) == 251
    config = dvc.config.load_one("repo")
    assert config["remote"][remote_name]["url"] == remote_url
    assert new_name not in config.get("remote", {})

    # rename success
    assert main(["remote", "rename", remote_name, new_name]) == 0
    config = dvc.config.load_one("repo")
    assert remote_name not in config.get("remote", {})
    assert config["remote"][new_name]["url"] == remote_url


def test_remote_duplicated(dvc):
    remote_name = "drive"
    remote_url = "gdrive://test/test"
    used_name = "overlap"
    another_url = "gdrive://test/test1"
    # prepare
    assert main(["remote", "add", remote_name, remote_url]) == 0
    assert main(["remote", "add", "--local", used_name, another_url]) == 0
    config = dvc.config.load_one("repo")
    assert config["remote"][remote_name]["url"] == remote_url
    local_config = dvc.config.load_one("local")
    assert local_config["remote"][used_name]["url"] == another_url

    # rename duplicated
    assert main(["remote", "rename", remote_name, used_name]) == 251
    config = dvc.config.load_one("repo")
    assert config["remote"][remote_name]["url"] == remote_url
    local_config = dvc.config.load_one("local")
    assert local_config["remote"][used_name]["url"] == another_url


def test_remote_default(dvc):
    remote_name = "drive"
    remote_url = "gdrive://test/test"
    new_name = "new"
    # prepare
    assert main(["remote", "add", "-d", remote_name, remote_url]) == 0
    assert main(["remote", "default", "--local", remote_name]) == 0
    config = dvc.config.load_one("repo")
    assert config["core"]["remote"] == remote_name
    assert config["remote"][remote_name]["url"] == remote_url
    assert new_name not in config.get("remote", {})
    local_config = dvc.config.load_one("local")
    assert local_config["core"]["remote"] == remote_name

    # rename success
    assert main(["remote", "rename", remote_name, new_name]) == 0
    config = dvc.config.load_one("repo")
    assert remote_name not in config.get("remote", {})
    assert config["core"]["remote"] == new_name
    assert config["remote"][new_name]["url"] == remote_url
    assert remote_name not in config.get("remote", {})
    local_config = dvc.config.load_one("local")
    assert local_config["core"]["remote"] == new_name


def test_protect_local_remote(tmp_dir, dvc, local_remote):
    (stage,) = tmp_dir.dvc_gen("file", "file content")

    dvc.push()
    odb = dvc.cloud.get_remote_odb("upstream")
    remote_cache_file = odb.oid_to_path(stage.outs[0].hash_info.value)

    assert os.path.exists(remote_cache_file)
    assert stat.S_IMODE(os.stat(remote_cache_file).st_mode) == 0o444


def test_push_incomplete_dir(tmp_dir, dvc, mocker, local_remote):
    (stage,) = tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar"}})
    remote_odb = dvc.cloud.get_remote_odb("upstream")

    odb = dvc.cache.local
    out = stage.outs[0]
    file_objs = [entry_obj for _, _, entry_obj in out.obj]

    # remove one of the cache files for directory
    remove(odb.oid_to_path(file_objs[0].value))

    dvc.push()
    assert not remote_odb.exists(out.hash_info.value)
    assert not remote_odb.exists(file_objs[0].value)
    assert remote_odb.exists(file_objs[1].value)




tests/func/test_remove.py
import os

import pytest

from dvc.cli import main
from dvc.fs import system
from dvc.stage.exceptions import StageFileDoesNotExistError, StageFileIsNotDvcFileError
from dvc.utils.fs import remove
from dvc_objects.errors import ObjectDBError
from tests.utils import get_gitignore_content


@pytest.mark.parametrize("remove_outs", [True, False])
def test_remove(tmp_dir, scm, dvc, run_copy, remove_outs):
    (stage1,) = tmp_dir.dvc_gen("foo", "foo")
    stage2 = run_copy("foo", "bar", single_stage=True)
    stage3 = run_copy("bar", "foobar", name="copy-bar-foobar")

    assert "/foo" in get_gitignore_content()
    assert "/bar" in get_gitignore_content()
    assert "/foobar" in get_gitignore_content()

    for stage in [stage1, stage2, stage3]:
        dvc.remove(stage.addressing, outs=remove_outs)
        out_exists = (out.exists for out in stage.outs)
        assert stage not in dvc.index.stages
        if remove_outs:
            assert not any(out_exists)
        else:
            assert all(out_exists)

    assert not (tmp_dir / ".gitignore").exists()


def test_remove_file_target(tmp_dir, dvc):
    tmp_dir.dvc_gen("foo", "foo")

    with pytest.raises(
        StageFileIsNotDvcFileError,
        match="'foo' is not a .dvc file. Do you mean 'foo.dvc'?",
    ):
        dvc.remove("foo")

    dvc.remove("foo.dvc")


def test_remove_non_existent_file(tmp_dir, dvc):
    with pytest.raises(StageFileDoesNotExistError):
        dvc.remove("non_existent_dvc_file.dvc")
    with pytest.raises(StageFileDoesNotExistError):
        dvc.remove("non_existent_stage_name")


def test_remove_broken_symlink(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    dvc.cache.local.cache_types = ["symlink"]

    (stage,) = dvc.add("foo")
    remove(dvc.cache.local.path)
    assert system.is_symlink("foo")

    with pytest.raises(ObjectDBError):
        dvc.remove(stage.addressing)
    assert os.path.lexists("foo")
    assert (tmp_dir / stage.relpath).exists()

    dvc.remove(stage.addressing, outs=True)
    assert not os.path.lexists("foo")
    assert not (tmp_dir / stage.relpath).exists()


def test_cmd_remove(tmp_dir, dvc):
    assert main(["remove", "non-existing-dvc-file"]) == 1

    (stage,) = tmp_dir.dvc_gen("foo", "foo")
    assert main(["remove", stage.addressing]) == 0
    assert not (tmp_dir / stage.relpath).exists()
    assert (tmp_dir / "foo").exists()

    (stage,) = tmp_dir.dvc_gen("foo", "foo")
    assert main(["remove", stage.addressing, "--outs"]) == 0
    assert not (tmp_dir / stage.relpath).exists()
    assert not (tmp_dir / "foo").exists()


def test_cmd_remove_gitignore_single_stage(tmp_dir, scm, dvc, run_copy):
    stage = dvc.run(name="my", cmd='echo "hello" > out', deps=[], outs=["out"])

    assert (tmp_dir / ".gitignore").exists()

    assert main(["remove", stage.addressing]) == 0
    assert not (tmp_dir / stage.relpath).exists()
    assert not (stage.dvcfile._lockfile).exists()
    assert not (tmp_dir / ".gitignore").exists()


def test_cmd_remove_gitignore_multistage(tmp_dir, scm, dvc, run_copy):
    (stage,) = tmp_dir.dvc_gen("foo", "foo")
    stage1 = run_copy("foo", "foo1", single_stage=True)
    stage2 = run_copy("foo1", "foo2", name="copy-foo1-foo2")

    assert (tmp_dir / ".gitignore").exists()

    assert main(["remove", stage2.addressing]) == 0
    assert main(["remove", stage1.addressing]) == 0
    assert main(["remove", stage.addressing]) == 0
    assert not (tmp_dir / ".gitignore").exists()




tests/func/test_repo.py
import os

from dvc.cachemgr import CacheManager
from dvc.dvcfile import LOCK_FILE, PROJECT_FILE
from dvc.fs import system
from dvc.repo import Repo
from dvc.scm import Git


def test_open_bare(tmp_dir, scm, dvc, tmp_path_factory):
    tmp_dir.dvc_gen(
        {
            "dir123": {"foo": "foo content"},
            "dirextra": {"extrafoo": "extra foo content"},
        },
        commit="initial",
    )

    url = os.fspath(tmp_path_factory.mktemp("bare"))
    Git.init(url, bare=True).close()

    scm.gitpython.repo.create_remote("origin", url)
    scm.gitpython.repo.remote("origin").push("master")

    with Repo.open(url) as repo:
        assert repo.scm.root_dir != url

    with Repo.open(url, uninitialized=True) as repo:
        assert repo.scm.root_dir != url


def test_destroy(tmp_dir, dvc, run_copy):
    dvc.config["cache"]["type"] = ["symlink"]
    dvc.cache = CacheManager(dvc)

    tmp_dir.dvc_gen("file", "text")
    tmp_dir.dvc_gen({"dir": {"file": "lorem", "subdir/file": "ipsum"}})

    run_copy("file", "file2", single_stage=True)
    run_copy("file2", "file3", name="copy-file2-file3")
    run_copy("file3", "file4", name="copy-file3-file4")

    dvc.destroy()

    # Remove all the files related to DVC
    for path in [
        ".dvc",
        ".dvcignore",
        "file.dvc",
        "file2.dvc",
        "dir.dvc",
        PROJECT_FILE,
        LOCK_FILE,
    ]:
        assert not (tmp_dir / path).exists()

    # Leave the rest of the files
    for path in [
        "file",
        "file2",
        "file3",
        "file4",
        "dir/file",
        "dir/subdir/file",
    ]:
        assert (tmp_dir / path).is_file()

    # Make sure that data was unprotected after `destroy`
    for path in [
        "file",
        "file2",
        "file3",
        "file4",
        "dir",
        "dir/file",
        "dir/subdir",
        "dir/subdir/file",
    ]:
        assert not system.is_symlink(tmp_dir / path)




tests/func/test_repo_index.py
from itertools import chain

import pytest
from pygtrie import Trie

from dvc.repo.index import Index
from dvc.stage import Stage


def test_index(tmp_dir, scm, dvc, run_copy):
    (stage1,) = tmp_dir.dvc_gen("foo", "foo")
    stage2 = run_copy("foo", "bar", name="copy-foo-bar")
    tmp_dir.commit([s.outs[0].fspath for s in (stage1, stage2)], msg="add")

    index = Index.from_repo(dvc)

    assert set(index.stages) == {stage1, stage2}
    assert index.outs_graph
    assert index.graph
    assert isinstance(index.outs_trie, Trie)
    index.check_graph()


def test_repr(tmp_dir, scm, dvc):
    tmp_dir.dvc_gen("foo", "foo", commit="add foo")

    brancher = dvc.brancher([scm.get_rev()])
    rev = next(brancher)
    assert rev == "workspace"
    assert repr(Index(dvc)) == f"Index({dvc}, fs@{rev})"

    rev = next(brancher)
    assert rev == scm.get_rev()
    assert repr(Index(dvc)) == f"Index({dvc}, fs@{rev[:7]})"


def outputs_equal(actual, expected):
    actual, expected = list(actual), list(expected)

    def sort_fn(output):
        return output.fspath

    assert len(actual) == len(expected)
    pairs = zip(sorted(actual, key=sort_fn), sorted(expected, key=sort_fn))
    assert all(actual.fspath == expected.fspath for actual, expected in pairs)
    return True


def test_deps_outs_getters(tmp_dir, dvc, run_copy_metrics):
    (foo_stage,) = tmp_dir.dvc_gen({"foo": "foo"})
    tmp_dir.gen({"params.yaml": "param: 100\n"})
    tmp_dir.gen({"m_temp.yaml": str(5)})

    run_stage1 = run_copy_metrics(
        "m_temp.yaml",
        "m.yaml",
        metrics=["m.yaml"],
        params=["param"],
        name="copy-metrics",
    )
    (tmp_dir / "metric_t.json").dump_json(
        [{"a": 1, "b": 2}, {"a": 2, "b": 3}], sort_keys=True
    )
    run_stage2 = run_copy_metrics(
        "metric_t.json",
        "metric.json",
        plots_no_cache=["metric.json"],
        name="copy-metrics2",
    )

    index = Index.from_repo(dvc)

    stages = [foo_stage, run_stage1, run_stage2]
    (metrics,) = run_stage1.outs
    _, params = run_stage1.deps
    (plots,) = run_stage2.outs

    expected_outs = chain.from_iterable([stage.outs for stage in stages])
    expected_deps = chain.from_iterable([stage.deps for stage in stages])

    assert outputs_equal(index.outs, expected_outs)
    assert outputs_equal(index.deps, expected_deps)
    assert outputs_equal(index.decorated_outs, [metrics, plots])
    assert outputs_equal(index.metrics, [metrics])
    assert outputs_equal(index.plots, [plots])
    assert outputs_equal(index.params, [params])


def test_update(dvc):
    """Test that update overwrites existing stages with the new ones.

    The old stages and the new ones might have same hash, so we are
    making sure that the old stages were removed and replaced by new ones
    using `id`/`is` checks.
    """
    index = Index.from_repo(dvc)
    new_stage = Stage(dvc, path="path1")
    new_index = index.update({new_stage})

    assert not index.stages
    assert new_index.stages == [new_stage]

    dup_stage1 = Stage(dvc, path="path1")
    dup_stage2 = Stage(dvc, path="path2")
    dup_index = index.update([dup_stage1, dup_stage2])
    assert not index.stages
    assert len(new_index.stages) == 1
    assert new_index.stages[0] is new_stage
    assert set(map(id, dup_index.stages)) == {id(dup_stage1), id(dup_stage2)}


def assert_index_equal(first, second, strict=True, ordered=True):
    assert len(first) == len(second), "Index have different no. of stages"
    assert set(first) == set(second), "Index does not have same stages"
    if ordered:
        assert list(first) == list(
            second
        ), "Index does not have same sequence of stages"
    if strict:
        assert set(map(id, first)) == set(
            map(id, second)
        ), "Index is not strictly equal"


def test_skip_graph_checks(dvc, mocker):
    # See https://github.com/iterative/dvc/issues/2671 for more info
    mock_build_graph = mocker.spy(Index.graph, "fget")

    # sanity check
    Index(dvc).check_graph()
    assert mock_build_graph.called
    mock_build_graph.reset_mock()

    # check that our hack can be enabled
    dvc._skip_graph_checks = True
    Index(dvc).check_graph()
    assert not mock_build_graph.called
    mock_build_graph.reset_mock()

    # check that our hack can be disabled
    dvc._skip_graph_checks = False
    Index(dvc).check_graph()
    assert mock_build_graph.called


def get_index(dvc, rev):
    if rev:
        brancher = dvc.brancher(revs=[rev])
        if rev != "workspace":
            assert next(brancher) == "workspace"
        next(brancher)
    return Index.from_repo(dvc)


@pytest.mark.parametrize("rev", ["workspace", "HEAD"])
def test_used_objs(tmp_dir, scm, dvc, run_copy, rev):
    from dvc_data.hashfile.hash_info import HashInfo

    dvc.scm_context.autostage = True
    tmp_dir.dvc_gen({"dir": {"subdir": {"file": "file"}}, "foo": "foo"})
    run_copy("foo", "bar", name="copy-foo-bar")
    scm.commit("commit")

    index = get_index(dvc, rev)

    expected_objs = [
        HashInfo(
            name="md5",
            value="acbd18db4cc2f85cedef654fccc4a4d8",
            obj_name="bar",
        ),
        HashInfo(
            name="md5",
            value="8c7dd922ad47494fc02c388e12c00eac",
            obj_name="dir/subdir/file",
        ),
        HashInfo(
            name="md5",
            value="d28c9e28591aeb7e303dc6772ffa6f6b.dir",
            obj_name="dir",
        ),
    ]

    assert index.used_objs() == {None: set(expected_objs)}
    assert index.used_objs("dir") == {None: set(expected_objs[1:])}
    assert index.used_objs(".", recursive=True) == {None: set(expected_objs)}
    assert index.used_objs("copy-foo-bar", with_deps=True) == {None: {expected_objs[0]}}


def test_view_granular_dir(tmp_dir, scm, dvc, run_copy):
    tmp_dir.dvc_gen(
        {"dir": {"subdir": {"in-subdir": "in-subdir"}, "in-dir": "in-dir"}},
        commit="init",
    )
    index = Index.from_repo(dvc)

    # view should include the specific target, parent dirs, and children
    # view should exclude any siblings of the target
    view = index.targets_view("dir/subdir")

    assert view.data_keys == {
        "repo": {
            ("dir", "subdir"),
        }
    }

    data_index = view.data["repo"]
    assert ("dir",) in data_index
    assert (
        "dir",
        "subdir",
    ) in data_index
    assert ("dir", "subdir", "in-subdir") in data_index
    assert (
        "dir",
        "in-dir",
    ) not in data_index


def test_view_onerror(tmp_dir, scm, dvc):
    from dvc.exceptions import NoOutputOrStageError

    tmp_dir.dvc_gen({"foo": "foo"}, commit="init")
    index = Index.from_repo(dvc)

    with pytest.raises(NoOutputOrStageError):
        index.targets_view(["foo", "missing"])

    failed = []

    def onerror(target, exc):
        failed.append((target, exc))

    view = index.targets_view(["foo", "missing"], onerror=onerror)
    data = view.data["repo"]

    assert len(failed) == 1
    target, exc = failed[0]
    assert target == "missing"
    assert isinstance(exc, NoOutputOrStageError)
    assert len(data) == 1
    assert data[("foo",)]


def test_view_stage_filter(tmp_dir, scm, dvc, run_copy):
    (stage1,) = tmp_dir.dvc_gen("foo", "foo")
    stage2 = run_copy("foo", "bar", name="copy-foo-bar")
    tmp_dir.commit([s.outs[0].fspath for s in (stage1, stage2)], msg="add")
    index = Index.from_repo(dvc)

    view = index.targets_view(None)
    assert set(view.stages) == {stage1, stage2}
    assert {out.fs_path for out in view.outs} == {
        out.fs_path for out in (stage1.outs + stage2.outs)
    }

    view = index.targets_view(
        None, stage_filter=lambda s: getattr(s, "name", "").startswith("copy")
    )
    assert set(view.stages) == {stage2}
    assert {out.fs_path for out in view.outs} == {out.fs_path for out in stage2.outs}


def test_view_outs_filter(tmp_dir, scm, dvc, run_copy):
    (stage1,) = tmp_dir.dvc_gen("foo", "foo")
    stage2 = run_copy("foo", "bar", name="copy-foo-bar")
    tmp_dir.commit([s.outs[0].fspath for s in (stage1, stage2)], msg="add")
    index = Index.from_repo(dvc)

    view = index.targets_view(None, outs_filter=lambda o: o.def_path == "foo")
    assert set(view.stages) == {stage1, stage2}
    assert {out.fs_path for out in view.outs} == {out.fs_path for out in stage1.outs}


def test_view_combined_filter(tmp_dir, scm, dvc, run_copy):
    (stage1,) = tmp_dir.dvc_gen("foo", "foo")
    stage2 = run_copy("foo", "bar", name="copy-foo-bar")
    tmp_dir.commit([s.outs[0].fspath for s in (stage1, stage2)], msg="add")
    index = Index.from_repo(dvc)

    view = index.targets_view(
        None,
        stage_filter=lambda s: getattr(s, "name", "").startswith("copy"),
        outs_filter=lambda o: o.def_path == "foo",
    )
    assert set(view.stages) == {stage2}
    assert set(view.outs) == set()

    view = index.targets_view(
        None,
        stage_filter=lambda s: getattr(s, "name", "").startswith("copy"),
        outs_filter=lambda o: o.def_path == "bar",
    )
    assert set(view.stages) == {stage2}
    assert {out.fs_path for out in view.outs} == {out.fs_path for out in stage2.outs}


def test_view_brancher(tmp_dir, scm, dvc):
    tmp_dir.dvc_gen({"foo": "foo"}, commit="init")
    index = Index.from_repo(dvc)

    for _ in dvc.brancher(revs=["HEAD"]):
        view = index.targets_view("foo")
        data = view.data["repo"]
        assert data[("foo",)]


def test_with_gitignore(tmp_dir, dvc, scm):
    (stage,) = tmp_dir.dvc_gen({"data": {"foo": "foo", "bar": "bar"}})

    index = Index.from_repo(dvc)
    assert index.stages == [stage]

    scm.ignore(stage.path)
    scm._reset()

    index = Index.from_repo(dvc)
    assert not index.stages


def test_ignored_dir_unignored_pattern(tmp_dir, dvc, scm):
    tmp_dir.gen({".gitignore": "data/**\n!data/**/\n!data/**/*.dvc"})
    scm.add([".gitignore"])
    (stage,) = tmp_dir.dvc_gen({"data/raw/tracked.csv": "5,6,7,8"})
    index = Index.from_repo(dvc)
    assert index.stages == [stage]




tests/func/test_repro.py
import filecmp
import os
import re
import shutil
from pathlib import Path

import pytest

from dvc.cli import main
from dvc.dvcfile import DVC_FILE, load_file
from dvc.exceptions import CyclicGraphError, ReproductionError, StagePathAsOutputError
from dvc.fs import LocalFileSystem, system
from dvc.output import Output
from dvc.stage import Stage
from dvc.stage.exceptions import StageFileDoesNotExistError
from dvc.utils import relpath
from dvc.utils.fs import remove
from dvc.utils.serialize import dump_yaml, load_yaml
from dvc_data.hashfile.hash import file_md5


@pytest.fixture(
    params=(
        {"single_stage": True},
        {"single_stage": False},
    ),
    ids=["single_stage", "multi_stage"],
)
def run_stage(dvc, request):
    def inner(*args, name=None, **kwargs):
        assert name
        # these should not be passed
        assert "single_stage" not in kwargs
        assert "fname" not in kwargs

        if request.param["single_stage"]:
            kwargs.update(
                {
                    "fname": name + ".dvc",
                    "single_stage": True,
                }
            )
        else:
            kwargs["name"] = name
        return dvc.run(*args, **kwargs)

    return inner


def test_repro_fail(tmp_dir, run_stage, copy_script):
    tmp_dir.dvc_gen("foo", "foo")
    stage = run_stage(
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        name="run1",
    )
    os.unlink("copy.py")
    assert main(["repro", stage.addressing]) != 0


def test_repro_cyclic_graph(tmp_dir, dvc, run_stage):
    tmp_dir.gen("foo", "foo")
    run_stage(
        deps=["foo"],
        outs=["bar.txt"],
        cmd="echo bar > bar.txt",
        name="copybarbar-txt",
    )
    run_stage(
        deps=["bar.txt"],
        outs=["baz.txt"],
        cmd="echo baz > baz.txt",
        name="copybazbaz-txt",
    )

    stage_dump = {
        "cmd": "echo baz > foo",
        "deps": [{"path": "baz.txt"}],
        "outs": [{"path": "foo"}],
    }
    dump_yaml("cycle.dvc", stage_dump)

    with pytest.raises(CyclicGraphError):
        dvc.reproduce("cycle.dvc")


class TestReproWorkingDirectoryAsOutput:
    """
    |  stage.cwd  |  out.path | cwd as output |
    |:-----------:|:---------:|:-------------:|
    |     dir     |    dir    |      True     |
    | dir/subdir/ |    dir    |      True     |
    |     dir     |   dir-1   |     False     |
    |      .      | something |     False     |
    """

    def test(self, dvc):
        # File structure:
        #       .
        #       |-- dir1
        #       |  |__ dir2.dvc         (out.path == ../dir2)
        #       |__ dir2
        #           |__ something.dvc    (stage.cwd == ./dir2)

        os.mkdir(os.path.join(dvc.root_dir, "dir1"))

        dvc.run(
            fname=os.path.join("dir1", "dir2.dvc"),
            wdir="dir1",
            outs=[os.path.join("..", "dir2")],
            cmd="mkdir {path}".format(path=os.path.join("..", "dir2")),
            single_stage=True,
        )

        faulty_stage_path = os.path.join("dir2", "something.dvc")

        output = os.path.join("..", "something")
        stage_dump = {
            "cmd": f"echo something > {output}",
            "outs": [{"path": output}],
        }
        dump_yaml(faulty_stage_path, stage_dump)

        with pytest.raises(StagePathAsOutputError):
            dvc.reproduce(faulty_stage_path)

    def test_nested(self, mocker, dvc):
        #       .
        #       |-- a
        #       |  |__ nested
        #       |     |__ dir
        #       |       |__ error.dvc     (stage.cwd == 'a/nested/dir')
        #       |__ b
        #          |__ nested.dvc         (stage.out == 'a/nested')
        dir1 = "b"
        dir2 = "a"

        os.mkdir(dir1)
        os.mkdir(dir2)

        nested_dir = os.path.join(dir2, "nested")
        out_dir = relpath(nested_dir, dir1)

        nested_stage = dvc.run(
            fname=os.path.join(dir1, "b.dvc"),
            wdir=dir1,
            outs=[out_dir],  # ../a/nested
            cmd=f"mkdir {out_dir}",
            single_stage=True,
        )

        os.mkdir(os.path.join(nested_dir, "dir"))

        error_stage_path = os.path.join(nested_dir, "dir", "error.dvc")

        output = os.path.join("..", "..", "something")
        stage_dump = {
            "cmd": f"echo something > {output}",
            "outs": [{"path": output}],
        }
        dump_yaml(error_stage_path, stage_dump)

        # NOTE: os.walk() walks in a sorted order and we need dir2 subdirs to
        # be processed before dir1 to load error.dvc first.
        dvc.index = dvc.index.update(
            [
                nested_stage,
                load_file(dvc, error_stage_path).stage,
            ]
        )

        mocker.patch.object(dvc, "_reset")  # to prevent `stages` resetting
        with pytest.raises(StagePathAsOutputError):
            dvc.reproduce(error_stage_path)

    def test_similar_paths(self, dvc):
        # File structure:
        #
        #       .
        #       |-- something.dvc   (out.path == something)
        #       |-- something
        #       |__ something-1
        #          |-- a
        #          |__ a.dvc        (stage.cwd == something-1)

        dvc.run(outs=["something"], cmd="mkdir something", single_stage=True)

        os.mkdir("something-1")

        stage = os.path.join("something-1", "a.dvc")

        stage_dump = {"cmd": "echo a > a", "outs": [{"path": "a"}]}
        dump_yaml(stage, stage_dump)

        dvc.reproduce(stage)


def test_repro_dep_under_dir(tmp_dir, dvc, copy_script, run_stage):
    tmp_dir.gen("foo", "foo")
    tmp_dir.dvc_gen("data", {"file": "file", "sub": {"foo": "foo"}})

    stage = run_stage(
        outs=["file1"],
        deps=["data/file", "copy.py"],
        cmd="python copy.py data/file file1",
        name="copy-data-file1",
    )

    assert filecmp.cmp("file1", "data/file", shallow=False)

    os.unlink("data/file")
    shutil.copyfile("foo", "data/file")

    stages = dvc.reproduce(stage.addressing)
    assert len(stages) == 2
    assert filecmp.cmp("file1", "foo", shallow=False)


def test_repro_dep_dir_with_outputs_under_it(tmp_dir, dvc, copy_script, run_stage):
    tmp_dir.gen("foo", "foo")
    file_stage, _ = tmp_dir.dvc_gen(
        {"data/file": "file", "data/sub": {"foo": "foo", "bar": "bar"}}
    )
    run_stage(
        cmd="ls data/file data/sub",
        deps=["data/file", "data/sub"],
        name="list-files",
    )
    copy_stage = run_stage(
        deps=["data"],
        outs=["file1"],
        cmd="python copy.py data file1",
        name="copy-data-file1",
    )
    os.unlink("data/file")
    shutil.copyfile("foo", "data/file")
    assert dvc.reproduce(copy_stage.addressing) == [file_stage, copy_stage]


def test_repro_force(tmp_dir, dvc, run_stage, copy_script):
    tmp_dir.dvc_gen("foo", "foo")
    stage = run_stage(
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        name="run1",
    )
    stages = dvc.reproduce(stage.addressing, force=True)
    assert len(stages) == 2


def test_repro_changed_code(tmp_dir, dvc, copy_script, run_stage):
    tmp_dir.gen("bar", "bar")
    tmp_dir.dvc_gen("foo", "foo")
    stage = run_stage(
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        name="run1",
    )
    with (tmp_dir / "copy.py").open("a+", encoding="utf8") as f:
        f.write("\nshutil.copyfile('bar', sys.argv[2])")
    stages = dvc.reproduce(stage.addressing)

    assert filecmp.cmp("file1", "bar", shallow=False)
    assert len(stages) == 1


def test_repro_changed_data(tmp_dir, dvc, copy_script, run_stage):
    tmp_dir.gen("bar", "bar")
    tmp_dir.dvc_gen("foo", "foo")
    stage = run_stage(
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        name="run1",
    )
    shutil.copyfile("bar", "foo")

    stages = dvc.reproduce(stage.addressing)

    assert filecmp.cmp("file1", "bar", shallow=False)
    assert len(stages) == 2


def test_repro_dry(tmp_dir, dvc, copy_script, run_stage):
    tmp_dir.gen("bar", "bar")
    tmp_dir.dvc_gen("foo", "foo")
    stage = run_stage(
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        name="run1",
    )
    shutil.copyfile("bar", "foo")

    stages = dvc.reproduce(stage.addressing, dry=True)

    assert len(stages), 2
    assert not filecmp.cmp("file1", "bar", shallow=False)

    ret = main(["repro", "--dry", stage.addressing])
    assert ret == 0
    assert not filecmp.cmp("file1", "bar", shallow=False)


def test_repro_up_to_date(tmp_dir, copy_script, run_stage):
    tmp_dir.gen("bar", "bar")
    tmp_dir.dvc_gen("foo", "foo")
    stage = run_stage(
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        name="run1",
    )
    ret = main(["repro", stage.addressing])
    assert ret == 0


def test_repro_dry_no_exec(tmp_dir, dvc):
    deps = []
    for d in range(3):
        idir = f"idir{d}"
        odir = f"odir{d}"

        deps.append("-d")
        deps.append(odir)

        os.mkdir(idir)

        f = os.path.join(idir, "file")
        with open(f, "w+", encoding="utf-8") as fobj:
            fobj.write(str(d))

        ret = main(
            [
                "stage",
                "add",
                "-n",
                f"copy-{idir}-{odir}",
                "-d",
                idir,
                "-o",
                odir,
                'python -c \'import shutil; shutil.copytree("{}", "{}")\''.format(
                    idir, odir
                ),
            ]
        )
        assert ret == 0

    ret = main(
        [
            "stage",
            "add",
            "-n",
            "ls",
            *deps,
            "ls {}".format(" ".join(dep for i, dep in enumerate(deps) if i % 2)),
        ]
    )
    assert ret == 0

    ret = main(["repro", "--dry", "ls"])
    assert ret == 0


def test_repro_changed_deep_data(tmp_dir, dvc, copy_script, run_stage):
    tmp_dir.gen("bar", "bar")
    tmp_dir.dvc_gen("foo", "foo")
    run_stage(
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        name="run1",
    )
    file2_stage = run_stage(
        outs=["file2"],
        deps=["file1", "copy.py"],
        cmd="python copy.py file1 file2",
        name="copy-file-file2",
    )
    shutil.copyfile("bar", "foo")
    stages = dvc.reproduce(file2_stage.addressing)
    assert filecmp.cmp("file1", "bar", shallow=False)
    assert filecmp.cmp("file2", "bar", shallow=False)
    assert len(stages) == 3


def test_repro_force_downstream(tmp_dir, dvc, copy_script):
    tmp_dir.gen("foo", "foo")
    stages = dvc.add("foo")
    assert len(stages) == 1
    foo_stage = stages[0]
    assert foo_stage is not None

    shutil.copyfile("copy.py", "copy1.py")
    file1 = "file1"
    file1_stage = dvc.run(
        outs=[file1],
        deps=["foo", "copy1.py"],
        cmd=f"python copy1.py foo {file1}",
        single_stage=True,
    )
    assert file1_stage is not None

    shutil.copyfile("copy.py", "copy2.py")
    file2 = "file2"
    file2_stage = dvc.run(
        outs=[file2],
        deps=[file1, "copy2.py"],
        cmd=f"python copy2.py {file1} {file2}",
        single_stage=True,
    )
    assert file2_stage is not None

    shutil.copyfile("copy.py", "copy3.py")
    file3 = "file3"
    file3_stage = dvc.run(
        outs=[file3],
        deps=[file2, "copy3.py"],
        cmd=f"python copy3.py {file2} {file3}",
        single_stage=True,
    )
    assert file3_stage is not None

    with open("copy2.py", "a", encoding="utf-8") as fobj:
        fobj.write("\n\n")

    stages = dvc.reproduce(file3_stage.path, force_downstream=True)
    assert len(stages) == 2
    assert stages[0].path == file2_stage.path
    assert stages[1].path == file3_stage.path


def test_repro_pipeline(tmp_dir, dvc, copy_script, run_stage):
    tmp_dir.gen("bar", "bar")
    tmp_dir.dvc_gen("foo", "foo")
    run_stage(
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        name="run1",
    )
    stage = run_stage(
        outs=["file2"],
        deps=["file1", "copy.py"],
        cmd="python copy.py file1 file2",
        name="copy-file-file2",
    )
    stages = dvc.reproduce(stage.addressing, force=True, pipeline=True)
    assert len(stages) == 3


def test_repro_pipeline_cli(tmp_dir, dvc, copy_script, run_stage):
    tmp_dir.gen("bar", "bar")
    tmp_dir.dvc_gen("foo", "foo")
    stage = run_stage(
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        name="run1",
    )
    ret = main(["repro", "--pipeline", "-f", stage.addressing])
    assert ret == 0


def test_repro_pipelines(tmp_dir, dvc, copy_script, run_stage):
    foo_stage, bar_stage = tmp_dir.dvc_gen({"foo": "foo", "bar": "bar"})
    file1_stage = run_stage(
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        name="copy-FOO-file1",
    )
    file2_stage = run_stage(
        outs=["file2"],
        deps=["bar", "copy.py"],
        cmd="python copy.py bar file2",
        name="copy-BAR-file2",
    )
    assert set(dvc.reproduce(all_pipelines=True, force=True)) == {
        foo_stage,
        bar_stage,
        file1_stage,
        file2_stage,
    }


def test_repro_pipelines_cli(tmp_dir, dvc, copy_script, run_stage):
    tmp_dir.dvc_gen({"foo": "foo", "bar": "bar"})
    run_stage(
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        name="copy-FOO-file1",
    )
    run_stage(
        outs=["file2"],
        deps=["bar", "copy.py"],
        cmd="python copy.py bar file2",
        name="copy-BAR-file2",
    )
    assert main(["repro", "-f", "-P"]) == 0


def test_repro_frozen(tmp_dir, dvc, copy_script, run_stage):
    tmp_dir.gen("bar", "bar")
    tmp_dir.dvc_gen("foo", "foo")
    run_stage(
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        name="run1",
    )
    file2_stage = run_stage(
        outs=["file2"],
        deps=["file1", "copy.py"],
        cmd="python copy.py file1 file2",
        name="copy-file1-file2",
    )

    shutil.copyfile("bar", "foo")

    ret = main(["freeze", file2_stage.addressing])
    assert ret == 0
    stages = dvc.reproduce(file2_stage.addressing)
    assert len(stages) == 0

    ret = main(["unfreeze", file2_stage.addressing])
    assert ret == 0
    stages = dvc.reproduce(file2_stage.addressing)
    assert filecmp.cmp("file1", "bar", shallow=False)
    assert filecmp.cmp("file2", "bar", shallow=False)
    assert len(stages) == 3


@pytest.mark.parametrize(
    "target",
    [
        "Dvcfile",
        "pipelines.yaml",
        "pipelines.yaml:name",
        "Dvcfile:name",
        "stage.dvc",
        "stage.dvc:name",
        "not-existing-stage.json",
    ],
)
def test_freeze_non_existing(dvc, target):
    with pytest.raises(StageFileDoesNotExistError):
        dvc.freeze(target)

    ret = main(["freeze", target])
    assert ret != 0


def test_repro_frozen_callback(tmp_dir, dvc, copy_script, run_stage):
    tmp_dir.gen("foo", "foo")
    # NOTE: purposefully not specifying deps or outs
    # to create a callback stage.
    stage = run_stage(
        cmd="python copy.py foo file1",
        name="copy-FOO-file1",
    )

    stages = dvc.reproduce(stage.addressing)
    assert len(stages) == 1

    dvc.freeze(stage.addressing)
    stages = dvc.reproduce(stage.addressing)
    assert len(stages) == 0

    dvc.unfreeze(stage.addressing)
    stages = dvc.reproduce(stage.addressing)
    assert len(stages) == 1


def test_repro_frozen_unchanged(tmp_dir, dvc, copy_script, run_stage):
    """
    Check that freezing/unfreezing doesn't affect stage state
    """
    tmp_dir.gen("bar", "bar")
    tmp_dir.dvc_gen("foo", "foo")
    stage = run_stage(
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        name="run1",
    )
    target = stage.addressing
    dvc.freeze(target)
    stages = dvc.reproduce(target)
    assert len(stages) == 0

    dvc.unfreeze(target)
    stages = dvc.reproduce(target)
    assert len(stages) == 0


def test_repro_metrics_add_unchanged(tmp_dir, dvc, copy_script):
    """
    Check that adding/removing metrics doesn't affect stage state
    """
    tmp_dir.gen("foo", "foo")
    stages = dvc.add("foo")
    assert len(stages) == 1
    assert stages[0] is not None

    file1 = "file1"
    file1_stage = file1 + ".dvc"
    dvc.run(
        fname=file1_stage,
        outs_no_cache=[file1],
        deps=["foo", "copy.py"],
        cmd=f"python copy.py foo {file1}",
        single_stage=True,
    )

    stages = dvc.reproduce(file1_stage)
    assert len(stages) == 0

    d = load_yaml(file1_stage)
    d["outs"][0]["metric"] = True
    dump_yaml(file1_stage, d)

    stages = dvc.reproduce(file1_stage)
    assert len(stages) == 0

    d = load_yaml(file1_stage)
    d["outs"][0]["metric"] = False
    dump_yaml(file1_stage, d)

    stages = dvc.reproduce(file1_stage)
    assert len(stages) == 0


def test_repro_phony(tmp_dir, dvc, copy_script, run_stage):
    tmp_dir.gen("bar", "bar")
    tmp_dir.dvc_gen("foo", "foo")
    run_stage(
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        name="run1",
    )
    stage = run_stage(cmd="cat file1", deps=["file1"], name="cat")
    shutil.copyfile("bar", "foo")

    dvc.reproduce(stage.addressing)

    assert filecmp.cmp("file1", "bar", shallow=False)


def test_non_existing_output(tmp_dir, dvc, copy_script, run_stage):
    tmp_dir.gen("bar", "bar")
    tmp_dir.dvc_gen("foo", "foo")
    stage = run_stage(
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        name="run1",
    )
    os.unlink("foo")

    with pytest.raises(ReproductionError):
        dvc.reproduce(stage.addressing)


def test_repro_data_source(tmp_dir, dvc, copy_script, run_stage):
    tmp_dir.gen("bar", "bar")
    tmp_dir.dvc_gen("foo", "foo")
    stage = run_stage(
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        name="run1",
    )
    shutil.copyfile("bar", "foo")

    stages = dvc.reproduce(stage.addressing)

    assert filecmp.cmp("foo", "bar", shallow=False)
    assert stages[0].outs[0].hash_info.value == file_md5("bar")


def test_repro_changed_dir(tmp_dir, dvc, copy_script, run_stage):
    tmp_dir.gen({"foo": "foo", "bar": "bar"})
    shutil.copyfile("foo", "file")

    stage = run_stage(
        outs=["dir"],
        deps=["file", "copy.py"],
        cmd="mkdir dir && python copy.py foo dir/foo",
        name="copy-in-dir",
    )

    stages = dvc.reproduce(stage.addressing)
    assert len(stages) == 0

    os.unlink("file")
    shutil.copyfile("bar", "file")

    stages = dvc.reproduce(stage.addressing)
    assert len(stages) == 1


def test_repro_changed_dir_data(tmp_dir, dvc, copy_script, run_stage):
    tmp_dir.gen({"data": {"foo": "foo"}, "bar": "bar"})
    stage = run_stage(
        outs=["dir"],
        deps=["data", "copy.py"],
        cmd="python copy.py data dir",
        name="copy-dir",
    )

    assert not dvc.reproduce(stage.addressing)

    with (tmp_dir / "data" / "foo").open("a", encoding="utf-8") as f:
        f.write("add")

    stages = dvc.reproduce(stage.addressing)
    assert len(stages) == 1

    # Check that dvc indeed registers changed output dir
    shutil.move("bar", "dir")
    stages = dvc.reproduce(stage.addressing)
    assert len(stages) == 1

    file = os.path.join("data", "foo")
    # Check that dvc registers mtime change for the directory.
    system.hardlink(file, file + ".lnk")
    stages = dvc.reproduce(stage.addressing)
    assert len(stages) == 1


def test_repro_missing_md5_in_stage_file(tmp_dir, dvc, copy_script):
    tmp_dir.dvc_gen("foo", "foo")
    stage = dvc.run(
        fname="file1.dvc",
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        single_stage=True,
    )
    d = load_yaml(stage.relpath)
    del d[Stage.PARAM_OUTS][0][LocalFileSystem.PARAM_CHECKSUM]
    del d[Stage.PARAM_DEPS][0][LocalFileSystem.PARAM_CHECKSUM]
    dump_yaml(stage.relpath, d)

    stages = dvc.reproduce(stage.addressing)
    assert len(stages) == 1


def test_cmd_repro(tmp_dir, copy_script, run_stage):
    tmp_dir.gen("bar", "bar")
    tmp_dir.dvc_gen("foo", "foo")
    stage = run_stage(
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        name="run1",
    )
    shutil.copyfile("bar", "foo")

    ret = main(["status"])
    assert ret == 0

    ret = main(["repro", stage.addressing])
    assert ret == 0

    ret = main(["repro", "non-existing-file"])
    assert ret != 0


@pytest.mark.skipif(os.name == "nt", reason="not on nt")
def test_repro_shell(tmp_dir, monkeypatch, dvc):
    monkeypatch.setenv("SHELL", "/bin/sh")
    dvc.run(
        fname="shell.txt.dvc",
        outs=["shell.txt"],
        cmd="echo $SHELL > shell.txt",
        single_stage=True,
    )
    shell = os.getenv("SHELL")

    assert (tmp_dir / "shell.txt").read_text().rstrip() == shell
    (tmp_dir / "shell.txt").unlink()

    dvc.reproduce("shell.txt.dvc")
    assert (tmp_dir / "shell.txt").read_text().rstrip() == shell


def test_repro_all_pipelines(mocker, dvc, run_stage):
    stages = [
        run_stage(
            outs=["start.txt"],
            cmd="echo start > start.txt",
            name="start",
        ),
        run_stage(
            deps=["start.txt"],
            outs=["middle.txt"],
            cmd="echo middle > middle.txt",
            name="middle",
        ),
        run_stage(
            deps=["middle.txt"],
            outs=["final.txt"],
            cmd="echo final > final.txt",
            name="final",
        ),
        run_stage(
            outs=["disconnected.txt"],
            cmd="echo other > disconnected.txt",
            name="disconnected",
        ),
    ]

    from dvc_data.hashfile.state import StateNoop

    dvc.state = StateNoop()

    mock_reproduce = mocker.patch.object(Stage, "reproduce", side_effect=stages)
    ret = main(["repro", "--all-pipelines"])
    assert ret == 0
    assert mock_reproduce.call_count == 4


def test_repro_no_commit(tmp_dir, dvc, copy_script, run_stage):
    tmp_dir.gen("bar", "bar")
    tmp_dir.dvc_gen("foo", "foo")
    stage = run_stage(
        outs=["file1"],
        deps=["foo", "copy.py"],
        cmd="python copy.py foo file1",
        name="run1",
    )
    remove(dvc.cache.local.path)
    ret = main(["repro", stage.addressing, "--no-commit"])
    assert ret == 0
    # run-cache should be skipped if `-no-commit`.
    assert not os.path.isdir(dvc.cache.local.path)


class TestReproAlreadyCached:
    def test(self, dvc, run_stage):
        stage = run_stage(
            always_changed=True,
            deps=[],
            outs=["datetime.txt"],
            cmd='python -c "import time; print(time.time())" > datetime.txt',
            name="datetime",
        )
        run_out = stage.outs[0]
        repro_out = dvc.reproduce(stage.addressing)[0].outs[0]

        assert run_out.hash_info != repro_out.hash_info

    def test_force_with_dependencies(self, tmp_dir, dvc, run_stage):
        tmp_dir.dvc_gen("foo", "foo")
        stage = run_stage(
            name="datetime",
            deps=["foo"],
            outs=["datetime.txt"],
            cmd='python -c "import time; print(time.time())" > datetime.txt',
        )

        ret = main(["repro", "--force", stage.addressing])
        assert ret == 0

        saved_stage = dvc.stage.get_target(stage.addressing)
        assert stage.outs[0].hash_info != saved_stage.outs[0].hash_info

    def test_force_import(self, mocker, tmp_dir, dvc):
        tmp_dir.dvc_gen("foo", "foo")

        ret = main(["import-url", "foo", "bar"])
        assert ret == 0

        spy_get = mocker.spy(LocalFileSystem, "get")
        spy_checkout = mocker.spy(Output, "checkout")

        assert main(["unfreeze", "bar.dvc"]) == 0
        ret = main(["repro", "--force", "bar.dvc"])
        assert ret == 0
        assert spy_get.call_count == 1
        assert spy_checkout.call_count == 0


@pytest.fixture
def repro_dir(tmp_dir, dvc, run_copy):
    # Creates repo with following structure:
    #    data_dir/dir_file              origin_data
    #         |       |                   |
    #         |       |              origin_copy.dvc
    # unrelated2.dvc  |               |       |
    #                 |               |    unrelated1.dvc
    #    dir/subdir/dir_file_copy.dvc |
    #                  |              |
    #                  |        dir/origin_copy_2.dvc
    #                  |            |
    #                   \          /
    #                    \        /
    #                   dir/Dvcfile
    tmp_dir.gen(
        {
            "origin_data": "origin data content",
            "data_dir": {"dir_file": "dir file content"},
            "dir": {"subdir": {}},
        }
    )

    stages = {}

    origin_copy = tmp_dir / "origin_copy"
    stage = run_copy("origin_data", os.fspath(origin_copy), single_stage=True)
    assert stage is not None
    assert origin_copy.read_text() == "origin data content"
    stages["origin_copy"] = stage

    origin_copy_2 = tmp_dir / "dir" / "origin_copy_2"
    stage = run_copy(
        os.fspath(origin_copy),
        os.fspath(origin_copy_2),
        fname=os.fspath(origin_copy_2) + ".dvc",
        single_stage=True,
    )
    assert stage is not None
    assert origin_copy_2.read_text() == "origin data content"
    stages["origin_copy_2"] = stage

    dir_file_path = tmp_dir / "data_dir" / "dir_file"
    dir_file_copy = tmp_dir / "dir" / "subdir" / "dir_file_copy"
    stage = run_copy(
        os.fspath(dir_file_path),
        os.fspath(dir_file_copy),
        fname=os.fspath(dir_file_copy) + ".dvc",
        single_stage=True,
    )
    assert stage is not None
    assert dir_file_copy.read_text() == "dir file content"
    stages["dir_file_copy"] = stage

    last_stage = tmp_dir / "dir" / DVC_FILE
    deps = [os.fspath(origin_copy_2), os.fspath(dir_file_copy)]
    stage = dvc.run(
        cmd="echo {}".format(" ".join(deps)),
        fname=os.fspath(last_stage),
        deps=deps,
        single_stage=True,
    )
    assert stage is not None
    stages["last_stage"] = stage

    # Unrelated are to verify that reproducing `dir` will not trigger them too
    assert run_copy(os.fspath(origin_copy), "unrelated1", single_stage=True) is not None
    assert (
        run_copy(os.fspath(dir_file_path), "unrelated2", single_stage=True) is not None
    )

    return stages


def _rewrite_file(path_elements, new_content):
    if isinstance(path_elements, str):
        path_elements = [path_elements]
    file = Path(os.sep.join(path_elements))
    file.unlink()
    file.write_text(new_content, encoding="utf-8")


def _read_out(stage):
    return Path(stage.outs[0].fspath).read_text(encoding="utf-8")


def test_recursive_repro_default(dvc, repro_dir):
    """
    Test recursive repro on dir after a dep outside this dir has changed.
    """
    _rewrite_file("origin_data", "new origin data content")

    stages = dvc.reproduce("dir", recursive=True)

    # Check that the dependency ("origin_copy") and the dependent stages
    # inside the folder have been reproduced ("origin_copy_2", "last_stage")
    assert stages == [
        repro_dir["origin_copy"],
        repro_dir["origin_copy_2"],
        repro_dir["last_stage"],
    ]
    assert _read_out(repro_dir["origin_copy"]) == "new origin data content"
    assert _read_out(repro_dir["origin_copy_2"]) == "new origin data content"


def test_recursive_repro_single(dvc, repro_dir):
    """
    Test recursive single-item repro on dir
    after a dep outside this dir has changed.
    """
    _rewrite_file("origin_data", "new origin content")
    _rewrite_file(["data_dir", "dir_file"], "new dir file content")

    stages = dvc.reproduce("dir", recursive=True, single_item=True)
    # Check that just stages inside given dir
    # with changed direct deps have been reproduced.
    # This means that "origin_copy_2" stage should not be reproduced
    # since it depends on "origin_copy".
    # Also check that "dir_file_copy" stage was reproduced before "last_stage"
    assert stages == [repro_dir["dir_file_copy"], repro_dir["last_stage"]]
    assert _read_out(repro_dir["dir_file_copy"]) == "new dir file content"


def test_recursive_repro_single_force(dvc, repro_dir):
    """
    Test recursive single-item force repro on dir
    without any dependencies changing.
    """
    stages = dvc.reproduce("dir", recursive=True, single_item=True, force=True)
    # Check that all stages inside given dir have been reproduced
    # Also check that "dir_file_copy" stage was reproduced before "last_stage"
    # and that "origin_copy" stage was reproduced before "last_stage" stage
    assert len(stages) == 3
    assert set(stages) == {
        repro_dir["origin_copy_2"],
        repro_dir["dir_file_copy"],
        repro_dir["last_stage"],
    }
    assert stages.index(repro_dir["origin_copy_2"]) < stages.index(
        repro_dir["last_stage"]
    )
    assert stages.index(repro_dir["dir_file_copy"]) < stages.index(
        repro_dir["last_stage"]
    )


def test_recursive_repro_empty_dir(tmp_dir, dvc):
    """
    Test recursive repro on an empty directory
    """
    (tmp_dir / "emptydir").mkdir()

    stages = dvc.reproduce("emptydir", recursive=True, force=True)
    assert stages == []


def test_recursive_repro_recursive_missing_file(dvc):
    """
    Test recursive repro on a missing file
    """
    with pytest.raises(StageFileDoesNotExistError):
        dvc.reproduce("notExistingStage.dvc", recursive=True)
    with pytest.raises(StageFileDoesNotExistError):
        dvc.reproduce("notExistingDir/", recursive=True)


def test_recursive_repro_on_stage_file(dvc, repro_dir):
    """
    Test recursive repro on a stage file instead of directory
    """
    stages = dvc.reproduce(
        repro_dir["origin_copy_2"].relpath, recursive=True, force=True
    )
    assert stages == [repro_dir["origin_copy"], repro_dir["origin_copy_2"]]


def test_dvc_formatting_retained(tmp_dir, dvc, run_copy):
    tmp_dir.dvc_gen("foo", "foo content")
    stage = run_copy("foo", "foo_copy", fname="foo_copy.dvc", single_stage=True)
    stage_path = tmp_dir / stage.relpath

    # Add comments and custom formatting to DVC-file
    lines = list(map(_format_dvc_line, stage_path.read_text().splitlines()))
    lines.insert(0, "# Starting comment")
    stage_text = "".join(line + "\n" for line in lines)
    stage_path.write_text(stage_text)

    # Rewrite data source and repro
    (tmp_dir / "foo").write_text("new foo")
    dvc.reproduce("foo_copy.dvc", force=True)

    def _hide_md5(text):
        return re.sub(r"\b[a-f0-9]{32}\b", "<md5>", text)

    def _hide_size(text):
        return re.sub(r"size: [0-9]*\b", "size: <size>", text)

    def _mask(text):
        return _hide_size(_hide_md5(text))

    assert _mask(stage_text) == _mask(stage_path.read_text())


def _format_dvc_line(line):
    # Add line comment for all cache and md5 keys
    if "cache:" in line or "md5:" in line:
        return line + " # line comment"
    # Format command as one word per line
    if line.startswith("cmd: "):
        pre, command = line.split(None, 1)
        return pre + " >\n" + "\n".join("  " + s for s in command.split())
    return line


def test_downstream(dvc):
    # The dependency graph should look like this:
    #
    #       E
    #      / \
    #     D   F
    #    / \   \
    #   B   C   G
    #    \ /
    #     A
    #
    assert main(["stage", "add", "-n", "stage-A", "--run", "-o", "A", "echo A>A"]) == 0
    assert (
        main(
            ["stage", "add", "-n", "stage-B", "--run", "-d", "A", "-o", "B", "echo B>B"]
        )
        == 0
    )
    assert (
        main(
            ["stage", "add", "-n", "stage-C", "--run", "-d", "A", "-o", "C", "echo C>C"]
        )
        == 0
    )
    assert (
        main(
            [
                "stage",
                "add",
                "-n",
                "stage-D",
                "--run",
                "-d",
                "B",
                "-d",
                "C",
                "-o",
                "D",
                "echo D>D",
            ]
        )
        == 0
    )
    assert main(["stage", "add", "-n", "stage-G", "--run", "-o", "G", "echo G>G"]) == 0
    assert (
        main(
            ["stage", "add", "-n", "stage-F", "--run", "-d", "G", "-o", "F", "echo F>F"]
        )
        == 0
    )
    assert (
        main(
            [
                "stage",
                "add",
                "-n",
                "stage-E",
                "--run",
                "-d",
                "D",
                "-d",
                "F",
                "-o",
                "E",
                "echo E>E",
            ]
        )
        == 0
    )

    # We want the evaluation to move from B to E
    #
    #       E
    #      /
    #     D
    #    /
    #   B
    #
    evaluation = dvc.reproduce("stage-B", downstream=True, force=True)

    assert len(evaluation) == 3
    assert evaluation[0].addressing == "stage-B"
    assert evaluation[1].addressing == "stage-D"
    assert evaluation[2].addressing == "stage-E"

    # B, C should be run (in any order) before D
    # See https://github.com/iterative/dvc/issues/3602
    evaluation = dvc.reproduce("stage-A", downstream=True, force=True)

    assert len(evaluation) == 5
    assert evaluation[0].addressing == "stage-A"
    assert {evaluation[1].addressing, evaluation[2].addressing} == {
        "stage-B",
        "stage-C",
    }
    assert evaluation[3].addressing == "stage-D"
    assert evaluation[4].addressing == "stage-E"


def test_repro_when_cmd_changes(tmp_dir, dvc, run_copy, mocker):
    from dvc.dvcfile import SingleStageFile

    tmp_dir.gen("foo", "foo")
    stage = run_copy("foo", "bar", single_stage=True)
    assert not dvc.reproduce(stage.addressing)

    from dvc.stage.run import cmd_run

    m = mocker.patch("dvc.stage.run.cmd_run", wraps=cmd_run)

    data = SingleStageFile(dvc, stage.path)._load()[0]
    data["cmd"] = "  ".join(stage.cmd.split())  # change cmd spacing by two
    (tmp_dir / stage.path).dump(data)

    assert dvc.status([stage.addressing]) == {stage.addressing: ["changed checksum"]}
    assert dvc.reproduce(stage.addressing)[0] == stage
    m.assert_called_once_with(stage, dry=False, run_env=None)




tests/func/test_repro_multistage.py
import os
from copy import deepcopy
from textwrap import dedent

import pytest
from funcy import lsplit

from dvc.cli import main
from dvc.dvcfile import LOCK_FILE, PROJECT_FILE
from dvc.exceptions import CyclicGraphError, ReproductionError
from dvc.stage import PipelineStage
from dvc.stage.cache import RunCacheNotSupported
from dvc.stage.exceptions import StageNotFound
from dvc.utils.fs import remove


def test_non_existing_stage_name(tmp_dir, dvc, run_copy):
    tmp_dir.gen("file1", "file1")
    run_copy("file1", "file2", name="copy-file1-file2")

    with pytest.raises(StageNotFound):
        dvc.freeze(":copy-file1-file3")

    assert main(["freeze", ":copy-file1-file3"]) != 0


def test_repro_frozen(tmp_dir, dvc, run_copy):
    (data_stage,) = tmp_dir.dvc_gen("data", "foo")
    stage0 = run_copy("data", "stage0", name="copy-data-stage0")
    run_copy("stage0", "stage1", name="copy-data-stage1")
    run_copy("stage1", "stage2", name="copy-data-stage2")

    dvc.freeze("copy-data-stage1")

    tmp_dir.gen("data", "bar")
    stages = dvc.reproduce()
    assert stages == [data_stage, stage0]


def test_downstream(M, tmp_dir, dvc):
    # The dependency graph should look like this:
    #
    #       E
    #      / \
    #     D   F
    #    / \   \
    #   B   C   G
    #    \ /
    #     A
    #
    assert main(["stage", "add", "--run", "-n", "A-gen", "-o", "A", "echo A>A"]) == 0
    assert (
        main(["stage", "add", "--run", "-n", "B-gen", "-d", "A", "-o", "B", "echo B>B"])
        == 0
    )
    assert (
        main(
            [
                "stage",
                "add",
                "--run",
                "-n",
                "C-gen",
                "-d",
                "A",
                "-o",
                "C",
                "echo C>C",
            ]
        )
        == 0
    )
    assert (
        main(
            [
                "stage",
                "add",
                "--run",
                "-n",
                "D-gen",
                "-d",
                "B",
                "-d",
                "C",
                "-o",
                "D",
                "echo D>D",
            ]
        )
        == 0
    )
    assert main(["stage", "add", "--run", "-n", "G-gen", "-o", "G", "echo G>G"]) == 0
    assert (
        main(["stage", "add", "--run", "-n", "F-gen", "-d", "G", "-o", "F", "echo F>F"])
        == 0
    )
    assert (
        main(
            [
                "stage",
                "add",
                "--run",
                "-n",
                "E-gen",
                "-d",
                "D",
                "-d",
                "F",
                "-o",
                "E",
                "echo E>E",
            ]
        )
        == 0
    )

    # We want the evaluation to move from B to E
    #
    #       E
    #      /
    #     D
    #    /
    #   B
    #
    evaluation = dvc.reproduce(PROJECT_FILE + ":B-gen", downstream=True, force=True)

    assert len(evaluation) == 3
    assert all(isinstance(stage, PipelineStage) for stage in evaluation)
    assert all(stage.relpath == PROJECT_FILE for stage in evaluation)
    assert [stage.name for stage in evaluation] == ["B-gen", "D-gen", "E-gen"]

    # B, C should be run (in any order) before D
    # See https://github.com/iterative/dvc/issues/3602
    evaluation = dvc.reproduce(PROJECT_FILE + ":A-gen", downstream=True, force=True)

    assert len(evaluation) == 5
    assert all(isinstance(stage, PipelineStage) for stage in evaluation)
    assert all(stage.relpath == PROJECT_FILE for stage in evaluation)
    assert [stage.name for stage in evaluation] == [
        "A-gen",
        M.any_of("B-gen", "C-gen"),
        M.any_of("B-gen", "C-gen"),
        "D-gen",
        "E-gen",
    ]


def test_repro_when_cmd_changes(tmp_dir, dvc, run_copy, mocker):
    from dvc.dvcfile import ProjectFile

    tmp_dir.gen("foo", "foo")
    stage = run_copy("foo", "bar", name="copy-file")
    target = "copy-file"
    assert not dvc.reproduce(target)

    from dvc.stage.run import cmd_run

    m = mocker.patch("dvc.stage.run.cmd_run", wraps=cmd_run)
    stage.cmd = "  ".join(stage.cmd.split())  # change cmd spacing by two
    ProjectFile(dvc, PROJECT_FILE)._dump_pipeline_file(stage)

    assert dvc.status([target]) == {target: ["changed command"]}
    assert dvc.reproduce(target)[0] == stage
    m.assert_called_once_with(stage, dry=False, run_env=None)


def test_repro_when_new_deps_is_added_in_dvcfile(tmp_dir, dvc, run_copy, copy_script):
    from dvc.dvcfile import load_file

    tmp_dir.gen({"foo": "foo", "bar": "bar"})
    stage = dvc.run(
        cmd="python copy.py {} {}".format("foo", "foobar"),
        outs=["foobar"],
        deps=["foo"],
        name="copy-file",
    )
    target = PROJECT_FILE + ":copy-file"
    assert not dvc.reproduce(target)

    dvcfile = load_file(dvc, stage.path)
    data, _ = dvcfile._load()
    data["stages"]["copy-file"]["deps"] += ["copy.py"]
    (tmp_dir / stage.path).dump(data)

    assert dvc.reproduce(target)[0] == stage


def test_repro_when_new_outs_is_added_in_dvcfile(tmp_dir, dvc, copy_script):
    from dvc.dvcfile import load_file

    tmp_dir.gen({"foo": "foo", "bar": "bar"})
    stage = dvc.run(
        cmd="python copy.py {} {}".format("foo", "foobar"),
        outs=[],  # scenario where user forgot to add
        deps=["foo"],
        name="copy-file",
    )
    target = ":copy-file"
    assert not dvc.reproduce(target)

    dvcfile = load_file(dvc, stage.path)
    data, _ = dvcfile._load()
    data["stages"]["copy-file"]["outs"] = ["foobar"]
    (tmp_dir / stage.path).dump(data)

    assert dvc.reproduce(target)[0] == stage


def test_repro_when_new_deps_is_moved(tmp_dir, dvc, copy_script):
    from dvc.dvcfile import load_file

    tmp_dir.gen({"foo": "foo", "bar": "foo"})
    stage = dvc.run(
        cmd="python copy.py {} {}".format("foo", "foobar"),
        outs=["foobar"],
        deps=["foo"],
        name="copy-file",
    )
    target = ":copy-file"
    assert not dvc.reproduce(target)

    # hardcode values in source code, ignore sys.argv
    tmp_dir.gen(
        "copy.py",
        """
import shutil

shutil.copyfile('bar', 'foobar')
""",
    )
    from shutil import move

    move("foo", "bar")

    dvcfile = load_file(dvc, stage.path)
    data, _ = dvcfile._load()
    data["stages"]["copy-file"]["deps"] = ["bar"]
    (tmp_dir / stage.path).dump(data)

    assert dvc.reproduce(target)[0] == stage


def test_repro_when_new_out_overlaps_others_stage_outs(tmp_dir, dvc):
    from dvc.exceptions import OverlappingOutputPathsError

    tmp_dir.gen({"dir": {"file1": "file1"}, "foo": "foo"})
    dvc.add("dir")
    (tmp_dir / PROJECT_FILE).dump(
        {
            "stages": {
                "run-copy": {
                    "cmd": "python copy {} {}".format("foo", "dir/foo"),
                    "deps": ["foo"],
                    "outs": ["dir/foo"],
                }
            }
        },
    )
    with pytest.raises(OverlappingOutputPathsError):
        dvc.reproduce(":run-copy")


def test_repro_when_new_deps_added_does_not_exist(tmp_dir, dvc, copy_script):
    tmp_dir.gen("foo", "foo")
    (tmp_dir / PROJECT_FILE).dump(
        {
            "stages": {
                "run-copy": {
                    "cmd": "python copy.py {} {}".format("foo", "foobar"),
                    "deps": ["foo", "bar"],
                    "outs": ["foobar"],
                }
            }
        },
    )
    with pytest.raises(ReproductionError):
        dvc.reproduce(":run-copy")


def test_repro_when_new_outs_added_does_not_exist(tmp_dir, dvc, copy_script):
    tmp_dir.gen("foo", "foo")
    (tmp_dir / PROJECT_FILE).dump(
        {
            "stages": {
                "run-copy": {
                    "cmd": "python copy.py {} {}".format("foo", "foobar"),
                    "deps": ["foo"],
                    "outs": ["foobar", "bar"],
                }
            }
        },
    )
    with pytest.raises(ReproductionError):
        dvc.reproduce(":run-copy")


def test_repro_when_lockfile_gets_deleted(tmp_dir, dvc, copy_script):
    tmp_dir.gen("foo", "foo")
    (tmp_dir / PROJECT_FILE).dump(
        {
            "stages": {
                "run-copy": {
                    "cmd": "python copy.py {} {}".format("foo", "foobar"),
                    "deps": ["foo"],
                    "outs": ["foobar"],
                }
            }
        },
    )
    assert dvc.reproduce(":run-copy")
    assert os.path.exists(LOCK_FILE)

    assert not dvc.reproduce(":run-copy")
    os.unlink(LOCK_FILE)
    stages = dvc.reproduce(":run-copy")
    assert stages
    assert stages[0].relpath == PROJECT_FILE
    assert stages[0].name == "run-copy"


def test_cyclic_graph_error(tmp_dir, dvc, run_copy):
    tmp_dir.gen("foo", "foo")
    run_copy("foo", "bar", name="copy-foo-bar")
    run_copy("bar", "baz", name="copy-bar-baz")
    run_copy("baz", "foobar", name="copy-baz-foobar")

    data = (tmp_dir / PROJECT_FILE).parse()
    data["stages"]["copy-baz-foo"] = {
        "cmd": "echo baz > foo",
        "deps": ["baz"],
        "outs": ["foo"],
    }
    (tmp_dir / PROJECT_FILE).dump(data)
    with pytest.raises(CyclicGraphError):
        dvc.reproduce(":copy-baz-foo")


def test_repro_multiple_params(tmp_dir, dvc):
    from dvc.stage.utils import split_params_deps
    from tests.func.test_run_multistage import supported_params

    (tmp_dir / "params2.yaml").dump(supported_params)
    (tmp_dir / "params.yaml").dump(supported_params)

    (tmp_dir / "foo").write_text("foo")
    stage = dvc.run(
        name="read_params",
        deps=["foo"],
        outs=["bar"],
        params=[
            "params2.yaml:lists,floats,name",
            "answer,floats,nested.nested1",
        ],
        cmd="cat params2.yaml params.yaml > bar",
    )

    params, deps = split_params_deps(stage)
    assert len(params) == 2
    assert len(deps) == 1
    assert len(stage.outs) == 1

    lockfile = stage.dvcfile._lockfile
    assert lockfile.load()["stages"]["read_params"]["params"] == {
        "params2.yaml": {
            "lists": [42, 42.0, "42"],
            "floats": 42.0,
            "name": "Answer",
        },
        "params.yaml": {
            "answer": 42,
            "floats": 42.0,
            "nested.nested1": {"nested2": "42", "nested2-2": 41.99999},
        },
    }
    data, _ = stage.dvcfile._load()
    params = data["stages"]["read_params"]["params"]

    custom, defaults = lsplit(lambda v: isinstance(v, dict), params)
    assert set(custom[0]["params2.yaml"]) == {"name", "lists", "floats"}
    assert set(defaults) == {"answer", "floats", "nested.nested1"}

    assert not dvc.reproduce(stage.addressing)
    params = deepcopy(supported_params)
    params["answer"] = 43
    (tmp_dir / "params.yaml").dump(params)

    assert dvc.reproduce(stage.addressing) == [stage]


@pytest.mark.parametrize("multiline", [True, False])
def test_repro_list_of_commands_in_order(tmp_dir, dvc, multiline):
    cmd = ["echo foo>foo", "echo bar>bar"]
    if multiline:
        cmd = "\n".join(cmd)

    (tmp_dir / "dvc.yaml").dump({"stages": {"multi": {"cmd": cmd}}})

    (tmp_dir / "dvc.yaml").write_text(
        dedent(
            """\
            stages:
              multi:
                cmd:
                - echo foo>foo
                - echo bar>bar
        """
        )
    )
    dvc.reproduce(targets=["multi"])
    assert (tmp_dir / "foo").read_text() == "foo\n"
    assert (tmp_dir / "bar").read_text() == "bar\n"


@pytest.mark.parametrize("multiline", [True, False])
def test_repro_list_of_commands_raise_and_stops_after_failure(tmp_dir, dvc, multiline):
    cmd = ["echo foo>foo", "failed_command", "echo baz>bar"]
    if multiline:
        cmd = "\n".join(cmd)

    (tmp_dir / "dvc.yaml").dump({"stages": {"multi": {"cmd": cmd}}})

    with pytest.raises(ReproductionError):
        dvc.reproduce(targets=["multi"])
    assert (tmp_dir / "foo").read_text() == "foo\n"
    assert not (tmp_dir / "bar").exists()


def test_repro_pulls_mising_data_source(tmp_dir, dvc, mocker, local_remote):
    (foo,) = tmp_dir.dvc_gen("foo", "foo")

    dvc.push()

    dvc.stage.add(name="copy-foo", cmd="cp foo bar", deps=["foo"], outs=["bar"])
    remove("foo")
    remove(foo.outs[0].cache_path)

    assert dvc.reproduce(pull=True)


def test_repro_pulls_mising_import(tmp_dir, dvc, mocker, erepo_dir, local_remote):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("foo", "foo", commit="first")

    foo_import = dvc.imp(os.fspath(erepo_dir), "foo")

    dvc.push()

    dvc.stage.add(name="copy-foo", cmd="cp foo bar", deps=["foo"], outs=["bar"])
    remove("foo")
    remove(foo_import.outs[0].cache_path)

    assert dvc.reproduce(pull=True)


def test_repro_allow_missing(tmp_dir, dvc):
    tmp_dir.gen("fixed", "fixed")
    dvc.stage.add(name="create-foo", cmd="echo foo > foo", deps=["fixed"], outs=["foo"])
    dvc.stage.add(name="copy-foo", cmd="cp foo bar", deps=["foo"], outs=["bar"])
    (create_foo, copy_foo) = dvc.reproduce()

    remove("foo")
    remove(create_foo.outs[0].cache_path)
    remove(dvc.stage_cache.cache_dir)

    ret = dvc.reproduce(allow_missing=True)
    # both stages are skipped
    assert not ret


def test_repro_allow_missing_and_pull(tmp_dir, dvc, mocker, local_remote):
    tmp_dir.gen("fixed", "fixed")
    dvc.stage.add(name="create-foo", cmd="echo foo > foo", deps=["fixed"], outs=["foo"])
    dvc.stage.add(name="copy-foo", cmd="cp foo bar", deps=["foo"], outs=["bar"])
    (create_foo,) = dvc.reproduce("create-foo")

    dvc.push()

    remove("foo")
    remove(create_foo.outs[0].cache_path)
    remove(dvc.stage_cache.cache_dir)

    ret = dvc.reproduce(pull=True, allow_missing=True)
    # create-foo is skipped ; copy-foo pulls missing dep
    assert len(ret) == 1


def test_repro_pulls_continue_without_run_cache(tmp_dir, dvc, mocker, local_remote):
    (foo,) = tmp_dir.dvc_gen("foo", "foo")

    dvc.push()
    mocker.patch.object(
        dvc.stage_cache, "pull", side_effect=RunCacheNotSupported("foo")
    )
    dvc.stage.add(name="copy-foo", cmd="cp foo bar", deps=["foo"], outs=["bar"])
    remove("foo")
    remove(foo.outs[0].cache_path)

    assert dvc.reproduce(pull=True)


def test_repro_skip_pull_if_no_run_cache_is_passed(tmp_dir, dvc, mocker, local_remote):
    (foo,) = tmp_dir.dvc_gen("foo", "foo")

    dvc.push()
    spy_pull = mocker.spy(dvc.stage_cache, "pull")
    dvc.stage.add(name="copy-foo", cmd="cp foo bar", deps=["foo"], outs=["bar"])
    remove("foo")
    remove(foo.outs[0].cache_path)

    assert dvc.reproduce(pull=True, run_cache=False)
    assert not spy_pull.called




tests/func/test_root.py
from dvc.cli import main


def test_root(tmp_dir, dvc, capsys):
    assert main(["root"]) == 0
    assert "." in capsys.readouterr()[0]


def test_root_locked(tmp_dir, dvc, capsys):
    # NOTE: check that `dvc root` is not blocked with dvc lock
    with dvc.lock:
        assert main(["root"]) == 0
    assert "." in capsys.readouterr()[0]




tests/func/test_run_cache.py
import os

import pytest

from dvc.dvcfile import LOCK_FILE
from dvc.utils.fs import remove


def _recurse_count_files(path):
    return len([os.path.join(r, f) for r, _, fs in os.walk(path) for f in fs])


def test_push_pull(tmp_dir, dvc, erepo_dir, run_copy, local_remote):
    tmp_dir.gen("foo", "foo")
    run_copy("foo", "bar", name="copy-foo-bar")
    assert dvc.push(run_cache=True) == 2
    erepo_dir.add_remote(config=local_remote.config)
    with erepo_dir.chdir():
        assert not os.path.exists(erepo_dir.dvc.stage_cache.cache_dir)
        assert erepo_dir.dvc.pull(run_cache=True)["fetched"] == 0
        assert os.listdir(erepo_dir.dvc.stage_cache.cache_dir)


def test_restore(tmp_dir, dvc, run_copy, mocker):
    tmp_dir.gen("foo", "foo")
    run_copy("foo", "bar", name="copy-foo-bar")

    mock_restore = mocker.spy(dvc.stage_cache, "restore")
    mock_run = mocker.patch("dvc.stage.run.cmd_run")

    # removing any information that `dvc` could use to re-generate from
    (tmp_dir / "bar").unlink()
    (tmp_dir / LOCK_FILE).unlink()

    (stage,) = dvc.reproduce("copy-foo-bar")

    mock_restore.assert_called_once_with(stage, dry=False)
    mock_run.assert_not_called()
    assert (tmp_dir / "bar").exists()
    assert not (tmp_dir / "foo").unlink()
    assert (tmp_dir / LOCK_FILE).exists()


def test_save(tmp_dir, dvc, run_copy):
    run_cache_dir = dvc.stage_cache.cache_dir
    assert not os.path.exists(run_cache_dir)

    tmp_dir.gen("foo", "foo")
    stage = run_copy("foo", "bar", name="copy-foo-bar")
    assert _recurse_count_files(run_cache_dir) == 1
    assert dvc.stage_cache._load(stage)


def test_do_not_save_on_no_exec_and_dry(tmp_dir, dvc, run_copy):
    run_cache_dir = dvc.stage_cache.cache_dir
    assert not os.path.exists(run_cache_dir)

    tmp_dir.gen("foo", "foo")
    stage = run_copy("foo", "bar", name="copy-foo-bar", no_exec=True)

    assert _recurse_count_files(run_cache_dir) == 0
    assert not dvc.stage_cache._load(stage)

    (stage,) = dvc.reproduce("copy-foo-bar", dry=True)

    assert _recurse_count_files(run_cache_dir) == 0
    assert not dvc.stage_cache._load(stage)


@pytest.mark.parametrize(
    "out_type,run_cache",
    [
        ("metrics_no_cache", True),
        ("plots_no_cache", True),
        ("outs_no_cache", False),
    ],
)
def test_outs_no_cache_deactivate_run_cache(tmp_dir, dvc, out_type, run_cache):
    tmp_dir.gen("foo", "foo")
    dvc.run(
        deps=["foo"],
        cmd="cp foo bar && cp foo goo",
        outs=["goo"],
        name="copy-foo-bar",
        **{out_type: ["bar"]},
    )
    assert os.path.isdir(dvc.stage_cache.cache_dir) == run_cache


def test_memory_for_multiple_runs_of_same_stage(tmp_dir, dvc, run_copy, mocker):
    tmp_dir.gen("foo", "foo")
    assert not os.path.exists(dvc.stage_cache.cache_dir)
    run_copy("foo", "bar", name="copy-foo-bar")
    assert _recurse_count_files(dvc.stage_cache.cache_dir) == 1
    tmp_dir.gen("foo", "foobar")
    run_copy("foo", "bar", name="copy-foo-bar")
    assert _recurse_count_files(dvc.stage_cache.cache_dir) == 2

    from dvc.stage import run as _run

    mock_restore = mocker.spy(dvc.stage_cache, "restore")
    mock_run = mocker.spy(_run, "cmd_run")

    (tmp_dir / "bar").unlink()
    (tmp_dir / LOCK_FILE).unlink()
    (stage,) = dvc.reproduce("copy-foo-bar")

    assert (tmp_dir / LOCK_FILE).exists()
    assert (tmp_dir / "bar").read_text() == "foobar"
    mock_run.assert_not_called()
    mock_restore.assert_called_once_with(stage, dry=False)
    mock_restore.reset_mock()

    (tmp_dir / LOCK_FILE).unlink()
    tmp_dir.gen("foo", "foo")
    dvc.reproduce("copy-foo-bar")

    assert (tmp_dir / "bar").read_text() == "foo"
    mock_run.assert_not_called()
    mock_restore.assert_called_once_with(stage, dry=False)
    assert (tmp_dir / "bar").exists()
    assert not (tmp_dir / "foo").unlink()
    assert (tmp_dir / LOCK_FILE).exists()


def test_memory_runs_of_multiple_stages(tmp_dir, dvc, run_copy, mocker):
    tmp_dir.gen("foo", "foo")
    assert not os.path.exists(dvc.stage_cache.cache_dir)

    run_copy("foo", "foo.bak", name="backup-foo")
    assert _recurse_count_files(dvc.stage_cache.cache_dir) == 1

    tmp_dir.gen("bar", "bar")
    run_copy("bar", "bar.bak", name="backup-bar")
    assert _recurse_count_files(dvc.stage_cache.cache_dir) == 2

    from dvc.stage import run as _run

    mock_restore = mocker.spy(dvc.stage_cache, "restore")
    mock_run = mocker.spy(_run, "cmd_run")

    (tmp_dir / "foo.bak").unlink()
    (tmp_dir / "bar.bak").unlink()
    (tmp_dir / LOCK_FILE).unlink()
    (stage,) = dvc.reproduce("backup-foo")

    assert (tmp_dir / "foo.bak").read_text() == "foo"
    assert (tmp_dir / LOCK_FILE).exists()
    mock_run.assert_not_called()
    mock_restore.assert_called_once_with(stage, dry=False)
    mock_restore.reset_mock()

    (stage,) = dvc.reproduce("backup-bar")

    assert (tmp_dir / "bar.bak").read_text() == "bar"
    assert (tmp_dir / LOCK_FILE).exists()
    mock_run.assert_not_called()
    mock_restore.assert_called_once_with(stage, dry=False)


def test_restore_pull(tmp_dir, dvc, run_copy, mocker, local_remote):
    import dvc.output as dvc_output

    tmp_dir.gen("foo", "foo")
    stage = run_copy("foo", "bar", name="copy-foo-bar")

    dvc.push(run_cache=True)

    mock_restore = mocker.spy(dvc.stage_cache, "restore")
    mock_run = mocker.patch("dvc.stage.run.cmd_run")
    mock_checkout = mocker.spy(dvc_output, "checkout")

    # removing any information that `dvc` could use to re-generate from
    (tmp_dir / "bar").unlink()
    (tmp_dir / LOCK_FILE).unlink()
    remove(stage.outs[0].cache_path)

    # removing local run cache
    remove(dvc.stage_cache.cache_dir)

    (stage,) = dvc.reproduce("copy-foo-bar", pull=True)

    mock_restore.assert_called_once_with(stage, pull=True, dry=False)
    mock_run.assert_not_called()
    assert mock_checkout.call_count == 2
    assert (tmp_dir / "bar").exists()
    assert not (tmp_dir / "foo").unlink()
    assert (tmp_dir / LOCK_FILE).exists()




tests/func/test_run_multistage.py
import os
import textwrap

import pytest

from dvc.exceptions import InvalidArgumentError
from dvc.stage.exceptions import DuplicateStageName, InvalidStageName


def test_run_with_name(tmp_dir, dvc, run_copy):
    from dvc.dvcfile import LOCK_FILE, PROJECT_FILE
    from dvc.stage import PipelineStage

    tmp_dir.dvc_gen("foo", "foo")
    assert not os.path.exists(PROJECT_FILE)
    stage = run_copy("foo", "bar", name="copy-foo-to-bar")
    assert isinstance(stage, PipelineStage)
    assert stage.name == "copy-foo-to-bar"
    assert os.path.exists(PROJECT_FILE)
    assert os.path.exists(LOCK_FILE)


def test_run_no_exec(tmp_dir, dvc, run_copy):
    from dvc.dvcfile import LOCK_FILE, PROJECT_FILE
    from dvc.stage import PipelineStage

    tmp_dir.dvc_gen("foo", "foo")
    assert not os.path.exists(PROJECT_FILE)
    stage = run_copy("foo", "bar", name="copy-foo-to-bar", no_exec=True)
    assert isinstance(stage, PipelineStage)
    assert stage.name == "copy-foo-to-bar"
    assert os.path.exists(PROJECT_FILE)
    assert not os.path.exists(LOCK_FILE)

    data, _ = stage.dvcfile._load()
    assert data["stages"]["copy-foo-to-bar"] == {
        "cmd": "python copy.py foo bar",
        "deps": ["copy.py", "foo"],
        "outs": ["bar"],
    }


def test_run_with_multistage_and_single_stage(tmp_dir, dvc, run_copy):
    from dvc.stage import PipelineStage, Stage

    tmp_dir.dvc_gen("foo", "foo")
    stage1 = run_copy("foo", "foo1", single_stage=True)
    stage2 = run_copy("foo1", "foo2", name="copy-foo1-foo2")
    stage3 = run_copy("foo2", "foo3", single_stage=True)

    assert isinstance(stage2, PipelineStage)
    assert isinstance(stage1, Stage)
    assert isinstance(stage3, Stage)
    assert stage2.name == "copy-foo1-foo2"


def test_run_multi_stage_repeat(tmp_dir, dvc, run_copy):
    from dvc.dvcfile import PROJECT_FILE, load_file
    from dvc.stage import PipelineStage

    tmp_dir.dvc_gen("foo", "foo")
    run_copy("foo", "foo1", name="copy-foo-foo1")
    run_copy("foo1", "foo2", name="copy-foo1-foo2")
    run_copy("foo2", "foo3", single_stage=True)

    stages = list(load_file(dvc, PROJECT_FILE).stages.values())
    assert len(stages) == 2
    assert all(isinstance(stage, PipelineStage) for stage in stages)
    assert {stage.name for stage in stages} == {
        "copy-foo-foo1",
        "copy-foo1-foo2",
    }


def test_multi_stage_run_cached(tmp_dir, dvc, run_copy, mocker):
    from dvc.stage.run import subprocess

    tmp_dir.dvc_gen("foo", "foo")

    run_copy("foo", "foo2", name="copy-foo1-foo2")
    spy = mocker.spy(subprocess, "Popen")
    run_copy("foo", "foo2", name="copy-foo1-foo2")
    assert not spy.called


def test_multistage_dump_on_non_cached_outputs(tmp_dir, dvc):
    tmp_dir.dvc_gen("foo")
    dvc.run(
        cmd="cp foo foo1",
        deps=["foo"],
        name="copy-foo1-foo2",
        outs_no_cache=["foo1"],
    )


def test_multistage_with_wdir(tmp_dir, dvc):
    from dvc.dvcfile import load_file

    tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})
    stage = dvc.run(
        cmd="cp foo foo1",
        deps=["foo"],
        name="copy-foo1-foo2",
        outs=["foo1"],
        wdir="dir",
    )

    data, _ = load_file(dvc, stage.path)._load()
    assert data["stages"]["copy-foo1-foo2"]["wdir"] == "dir"


def test_multistage_always_changed(tmp_dir, dvc):
    from dvc.dvcfile import load_file

    tmp_dir.gen({"foo": "foo", "bar": "bar"})
    stage = dvc.run(
        cmd="cp foo foo1",
        deps=["foo"],
        name="copy-foo1-foo2",
        outs=["foo1"],
        always_changed=True,
    )

    data, _ = load_file(dvc, stage.path)._load()
    assert data["stages"]["copy-foo1-foo2"]["always_changed"]


def test_graph(tmp_dir, dvc):
    from dvc.exceptions import CyclicGraphError

    tmp_dir.gen({"foo": "foo", "bar": "bar"})

    dvc.run(deps=["foo"], outs=["bar"], cmd="echo foo > bar", name="1")

    dvc.run(deps=["bar"], outs=["baz"], cmd="echo bar > baz", name="2")

    with pytest.raises(CyclicGraphError):
        dvc.run(deps=["baz"], outs=["foo"], cmd="echo baz > foo", name="3")


def test_run_dump_on_multistage(tmp_dir, dvc, run_head):
    from dvc.dvcfile import PROJECT_FILE, load_file

    tmp_dir.gen(
        {
            "dir": {
                "foo": "foo\nfoo",
                "bar": "bar\nbar",
                "foobar": "foobar\foobar",
            }
        }
    )

    dvc.run(
        cmd="cp foo foo2",
        deps=["foo"],
        name="copy-foo-foo2",
        wdir="dir",
        outs_persist=["foo2"],
        always_changed=True,
    )
    data = load_file(dvc, PROJECT_FILE)._load()[0]
    assert data == {
        "stages": {
            "copy-foo-foo2": {
                "cmd": "cp foo foo2",
                "deps": ["foo"],
                "outs": [{"foo2": {"persist": True}}],
                "always_changed": True,
                "wdir": "dir",
            }
        }
    }

    run_head(
        "foo",
        "bar",
        "foobar",
        name="head-files",
        outs=["bar-1"],
        outs_persist=["foo-1"],
        metrics_no_cache=["foobar-1"],
        wdir="dir",
    )
    assert load_file(dvc, PROJECT_FILE)._load()[0] == {
        "stages": {
            "head-files": {
                "cmd": "python {} foo bar foobar".format(
                    (tmp_dir / "head.py").resolve()
                ),
                "wdir": "dir",
                "deps": ["bar", "foo", "foobar"],
                "outs": ["bar-1", {"foo-1": {"persist": True}}],
                "metrics": [{"foobar-1": {"cache": False}}],
            },
            **data["stages"],
        }
    }


@pytest.mark.parametrize("char", ["@:", "#", "$", ":", "/", "\\", ".", ";", ","])
def test_run_with_invalid_stage_name(run_copy, char):
    with pytest.raises(InvalidStageName):
        run_copy("foo", "bar", name=f"copy_name-{char}")


def test_run_with_name_having_hyphen_underscore(tmp_dir, dvc, run_copy):
    tmp_dir.dvc_gen("foo", "foo")
    run_copy("foo", "bar", name="copy-foo_bar")


def test_run_already_exists(tmp_dir, dvc, run_copy):
    tmp_dir.dvc_gen("foo", "foo")
    run_copy("foo", "bar", name="copy")
    with pytest.raises(DuplicateStageName):
        run_copy("bar", "foobar", name="copy", force=False)
    run_copy("bar", "foobar", name="copy", force=True)


supported_params = {
    "name": "Answer",
    "answer": 42,
    "floats": 42.0,
    "lists": [42, 42.0, "42"],
    "nested": {"nested1": {"nested2": "42", "nested2-2": 41.99999}},
}


def test_run_params_default(tmp_dir, dvc):
    from dvc.dependency import ParamsDependency

    (tmp_dir / "params.yaml").dump(supported_params)
    stage = dvc.run(
        name="read_params",
        params=["nested.nested1.nested2"],
        cmd="cat params.yaml",
    )
    assert isinstance(stage.deps[0], ParamsDependency)
    assert stage.deps[0].params == ["nested.nested1.nested2"]

    lockfile = stage.dvcfile._lockfile
    assert lockfile.load()["stages"]["read_params"]["params"] == {
        "params.yaml": {"nested.nested1.nested2": "42"}
    }

    data, _ = stage.dvcfile._load()
    assert data["stages"]["read_params"]["params"] == ["nested.nested1.nested2"]


def test_run_params_custom_file(tmp_dir, dvc):
    from dvc.dependency import ParamsDependency

    (tmp_dir / "params2.yaml").dump(supported_params)
    stage = dvc.run(
        name="read_params",
        params=["params2.yaml:lists"],
        cmd="cat params2.yaml",
    )

    isinstance(stage.deps[0], ParamsDependency)
    assert stage.deps[0].params == ["lists"]
    lockfile = stage.dvcfile._lockfile
    assert lockfile.load()["stages"]["read_params"]["params"] == {
        "params2.yaml": {"lists": [42, 42.0, "42"]}
    }

    data, _ = stage.dvcfile._load()
    assert data["stages"]["read_params"]["params"] == [{"params2.yaml": ["lists"]}]


def test_run_params_no_exec(tmp_dir, dvc):
    from dvc.dependency import ParamsDependency

    (tmp_dir / "params2.yaml").dump(supported_params)
    stage = dvc.run(
        name="read_params",
        params=["params2.yaml:lists"],
        cmd="cat params2.yaml",
        no_exec=True,
    )

    isinstance(stage.deps[0], ParamsDependency)
    assert stage.deps[0].params == ["lists"]
    assert not stage.dvcfile._lockfile.exists()

    data, _ = stage.dvcfile._load()
    assert data["stages"]["read_params"]["params"] == [{"params2.yaml": ["lists"]}]


@pytest.mark.parametrize(
    "kwargs",
    [
        {"outs": ["foo"], "deps": ["bar"]},
        {"outs": ["foo"], "deps": ["bar"], "name": "copy-foo-bar"},
    ],
)
def test_run_without_cmd(tmp_dir, dvc, kwargs):
    with pytest.raises(InvalidArgumentError) as exc:
        dvc.run(**kwargs)
    assert str(exc.value) == "command is not specified"


def test_run_overwrite_order(tmp_dir, dvc, run_copy):
    from dvc.dvcfile import PROJECT_FILE

    tmp_dir.gen({"foo": "foo", "foo1": "foo1"})
    run_copy("foo", "bar", name="copy-foo-bar")
    run_copy("bar", "foobar", name="copy-bar-foobar")

    run_copy("foo1", "bar1", name="copy-foo-bar", force=True)

    data = (tmp_dir / PROJECT_FILE).parse()
    assert list(data["stages"].keys()) == ["copy-foo-bar", "copy-bar-foobar"]


def test_run_overwrite_preserves_meta_and_comment(tmp_dir, dvc, run_copy):
    from dvc.dvcfile import PROJECT_FILE

    tmp_dir.gen({"foo": "foo", "foo1": "foo1"})
    text = textwrap.dedent(
        """\
        stages:
          copy-foo-bar:
            cmd: python copy.py {src} {dest}
            deps:
            - copy.py
            - {src}
            outs:
            # comments are preserved
            - {dest}
            meta:
              name: meta is preserved too
    """
    )
    (tmp_dir / PROJECT_FILE).write_text(text.format(src="foo", dest="bar"))
    assert dvc.reproduce(PROJECT_FILE)

    assert run_copy("foo1", "bar1", name="copy-foo-bar", force=True)

    assert (tmp_dir / PROJECT_FILE).read_text() == text.format(src="foo1", dest="bar1")


def test_run_external_outputs(
    tmp_dir,
    dvc,
    local_workspace,
):
    hash_name = "md5"
    foo_hash = "acbd18db4cc2f85cedef654fccc4a4d8"
    bar_hash = "37b51d194a7513e45b56f6524f2d51f2"

    local_workspace.gen("foo", "foo")
    dvc.run(
        name="mystage",
        cmd="mycmd",
        deps=["remote://workspace/foo"],
        outs=["remote://workspace/bar"],
        no_exec=True,
    )

    dvc_yaml = (
        "stages:\n"
        "  mystage:\n"
        "    cmd: mycmd\n"
        "    deps:\n"
        "    - remote://workspace/foo\n"
        "    outs:\n"
        "    - remote://workspace/bar\n"
    )

    assert (tmp_dir / "dvc.yaml").read_text() == dvc_yaml
    assert not (tmp_dir / "dvc.lock").exists()

    local_workspace.gen("bar", "bar")
    dvc.commit("dvc.yaml", force=True)

    assert (tmp_dir / "dvc.yaml").read_text() == dvc_yaml
    assert (tmp_dir / "dvc.lock").read_text() == (
        "schema: '2.0'\n"
        "stages:\n"
        "  mystage:\n"
        "    cmd: mycmd\n"
        "    deps:\n"
        "    - path: remote://workspace/foo\n"
        f"      {hash_name}: {foo_hash}\n"
        "      size: 3\n"
        "    outs:\n"
        "    - path: remote://workspace/bar\n"
        f"      {hash_name}: {bar_hash}\n"
        "      size: 3\n"
    )

    assert (local_workspace / "foo").read_text() == "foo"
    assert (local_workspace / "bar").read_text() == "bar"
    assert (
        local_workspace / "cache" / bar_hash[:2] / bar_hash[2:]
    ).read_text() == "bar"




tests/func/test_run_single_stage.py
import filecmp
import logging
import os
import textwrap
import uuid
from pathlib import Path

import pytest

from dvc.cli import main
from dvc.dependency.base import DependencyIsStageFileError
from dvc.dvcfile import DVC_FILE_SUFFIX
from dvc.exceptions import (
    ArgumentDuplicationError,
    CircularDependencyError,
    CyclicGraphError,
    OutputDuplicationError,
    OverlappingOutputPathsError,
    StagePathAsOutputError,
)
from dvc.output import OutputIsStageFileError
from dvc.stage import Stage
from dvc.stage.exceptions import (
    StageFileAlreadyExistsError,
    StageFileBadNameError,
    StagePathNotDirectoryError,
    StagePathNotFoundError,
    StagePathOutsideError,
)
from dvc.utils.serialize import load_yaml
from dvc_data.hashfile.hash import file_md5


def test_run(tmp_dir, copy_script, dvc):
    tmp_dir.gen("foo", "foo")
    cmd = "python copy.py foo out"
    deps = ["foo", "copy.py"]
    outs = [os.path.join(tmp_dir, "out")]
    outs_no_cache = []
    fname = "out.dvc"

    dvc.add("foo")
    stage = dvc.run(
        cmd=cmd,
        deps=deps,
        outs=outs,
        outs_no_cache=outs_no_cache,
        fname=fname,
        single_stage=True,
    )

    assert filecmp.cmp("foo", "out", shallow=False)
    assert os.path.isfile(stage.path)
    assert stage.cmd == cmd
    assert len(stage.deps) == len(deps)
    assert len(stage.outs) == len(outs + outs_no_cache)
    assert stage.outs[0].fspath == outs[0]
    assert stage.outs[0].hash_info.value == file_md5("foo")
    assert stage.path, fname

    with pytest.raises(OutputDuplicationError):
        dvc.run(
            cmd=cmd,
            deps=deps,
            outs=outs,
            outs_no_cache=outs_no_cache,
            fname="duplicate" + fname,
            single_stage=True,
        )


def test_run_empty(dvc):
    dvc.run(
        cmd="echo hello world",
        deps=[],
        outs=[],
        outs_no_cache=[],
        fname="empty.dvc",
        single_stage=True,
    )


def test_run_missing_dep(dvc):
    from dvc.dependency.base import DependencyDoesNotExistError

    with pytest.raises(DependencyDoesNotExistError):
        dvc.run(
            cmd="command",
            deps=["non-existing-dep"],
            outs=[],
            outs_no_cache=[],
            fname="empty.dvc",
            single_stage=True,
        )


def test_run_noexec(tmp_dir, dvc, scm):
    tmp_dir.gen("foo", "foo")
    dvc.run(
        cmd="cp foo bar",
        deps=["foo"],
        outs=["bar"],
        no_exec=True,
        single_stage=True,
    )
    assert not os.path.exists("bar")
    with open(".gitignore", encoding="utf-8") as fobj:
        assert fobj.read() == "/bar\n"


class TestRunCircularDependency:
    def test(self, dvc):
        with pytest.raises(CircularDependencyError):
            dvc.run(
                cmd="command",
                deps=["foo"],
                outs=["foo"],
                fname="circular-dependency.dvc",
                single_stage=True,
            )

    def test_outs_no_cache(self, dvc):
        with pytest.raises(CircularDependencyError):
            dvc.run(
                cmd="command",
                deps=["foo"],
                outs_no_cache=["foo"],
                fname="circular-dependency.dvc",
                single_stage=True,
            )

    def test_non_normalized_paths(self, dvc):
        with pytest.raises(CircularDependencyError):
            dvc.run(
                cmd="command",
                deps=["./foo"],
                outs=["foo"],
                fname="circular-dependency.dvc",
                single_stage=True,
            )

    def test_graph(self, tmp_dir, dvc):
        tmp_dir.gen("foo", "foo")
        dvc.run(
            deps=["foo"],
            outs=["bar.txt"],
            cmd="echo bar > bar.txt",
            single_stage=True,
        )

        dvc.run(
            deps=["bar.txt"],
            outs=["baz.txt"],
            cmd="echo baz > baz.txt",
            single_stage=True,
        )

        with pytest.raises(CyclicGraphError):
            dvc.run(
                deps=["baz.txt"],
                outs=["foo"],
                cmd="echo baz > foo",
                single_stage=True,
            )


class TestRunDuplicatedArguments:
    def test(self, dvc):
        with pytest.raises(ArgumentDuplicationError):
            dvc.run(
                cmd="command",
                deps=[],
                outs=["foo", "foo"],
                fname="circular-dependency.dvc",
                single_stage=True,
            )

    def test_outs_no_cache(self, dvc):
        with pytest.raises(ArgumentDuplicationError):
            dvc.run(
                cmd="command",
                outs=["foo"],
                outs_no_cache=["foo"],
                fname="circular-dependency.dvc",
                single_stage=True,
            )

    def test_non_normalized_paths(self, dvc):
        with pytest.raises(ArgumentDuplicationError):
            dvc.run(
                cmd="command",
                deps=[],
                outs=["foo", "./foo"],
                fname="circular-dependency.dvc",
                single_stage=True,
            )


class TestRunStageInsideOutput:
    def test_cwd(self, tmp_dir, dvc):
        tmp_dir.gen("data", {"foo": "foo", "bar": "bar"})
        dvc.run(
            cmd="mkdir data",
            deps=[],
            outs=["data"],
            single_stage=True,
        )

        with pytest.raises(StagePathAsOutputError):
            dvc.run(
                cmd="command",
                fname=os.path.join("data", "inside-cwd.dvc"),
                single_stage=True,
            )

    def test_file_name(self, tmp_dir, dvc):
        tmp_dir.gen("data", {"foo": "foo", "bar": "bar"})
        dvc.run(
            cmd="mkdir data",
            deps=[],
            outs=["data"],
            single_stage=True,
        )

        with pytest.raises(StagePathAsOutputError):
            dvc.run(
                cmd="command",
                outs=["foo"],
                fname=os.path.join("data", "inside-cwd.dvc"),
                single_stage=True,
            )


class TestRunBadCwd:
    def test(self, make_tmp_dir, dvc):
        with pytest.raises(StagePathOutsideError):
            dvc.run(cmd="command", wdir=make_tmp_dir("tmp"), single_stage=True)

    def test_same_prefix(self, tmp_dir, dvc):
        path = f"{tmp_dir}-{uuid.uuid4()}"
        os.mkdir(path)
        with pytest.raises(StagePathOutsideError):
            dvc.run(cmd="command", wdir=path, single_stage=True)


class TestRunBadWdir:
    def test(self, make_tmp_dir, dvc):
        with pytest.raises(StagePathOutsideError):
            dvc.run(cmd="command", wdir=make_tmp_dir("tmp"), single_stage=True)

    def test_same_prefix(self, tmp_dir, dvc):
        path = f"{tmp_dir}-{uuid.uuid4()}"
        os.mkdir(path)
        with pytest.raises(StagePathOutsideError):
            dvc.run(cmd="command", wdir=path, single_stage=True)

    def test_not_found(self, tmp_dir, dvc):
        path = os.path.join(tmp_dir, str(uuid.uuid4()))
        with pytest.raises(StagePathNotFoundError):
            dvc.run(cmd="command", wdir=path, single_stage=True)

    def test_not_dir(self, tmp_dir, dvc):
        path = tmp_dir / str(uuid.uuid4())
        path.mkdir()
        path = path / str(uuid.uuid4())
        path.touch()
        with pytest.raises(StagePathNotDirectoryError):
            dvc.run(cmd="command", wdir=os.fspath(path), single_stage=True)


class TestRunBadName:
    def test(self, make_tmp_dir, dvc):
        with pytest.raises(StagePathOutsideError):
            dvc.run(
                cmd="command",
                fname=os.path.join(make_tmp_dir("tmp"), "foo.dvc"),
                single_stage=True,
            )

    def test_same_prefix(self, tmp_dir, dvc):
        path = f"{tmp_dir}-{uuid.uuid4()}"
        os.mkdir(path)
        with pytest.raises(StagePathOutsideError):
            dvc.run(
                cmd="command",
                fname=os.path.join(path, "foo.dvc"),
                single_stage=True,
            )

    def test_not_found(self, tmp_dir, dvc):
        path = os.path.join(tmp_dir, str(uuid.uuid4()))
        with pytest.raises(StagePathNotFoundError):
            dvc.run(
                cmd="command",
                fname=os.path.join(path, "foo.dvc"),
                single_stage=True,
            )


def test_run_remove_outs(tmp_dir, dvc, append_foo_script):
    dvc.run(
        deps=["append_foo.py"],
        outs=["foo"],
        cmd="python append_foo.py foo",
        single_stage=True,
    )


class TestCmdRunWorkingDirectory:
    def test_default_wdir_is_not_written(self, tmp_dir, dvc):
        stage = dvc.run(
            cmd="echo test > foo",
            outs=["foo"],
            wdir=".",
            single_stage=True,
        )
        d = load_yaml(stage.relpath)
        assert Stage.PARAM_WDIR not in d.keys()

        stage = dvc.run(cmd="echo test > bar", outs=["bar"], single_stage=True)
        d = load_yaml(stage.relpath)
        assert Stage.PARAM_WDIR not in d.keys()

    def test_fname_changes_path_and_wdir(self, tmp_dir, dvc):
        dname = "dir"
        os.mkdir(os.path.join(tmp_dir, dname))
        foo = os.path.join(dname, "foo")
        fname = os.path.join(dname, "stage" + DVC_FILE_SUFFIX)
        stage = dvc.run(
            cmd=f"echo test > {foo}",
            outs=[foo],
            fname=fname,
            single_stage=True,
        )
        assert stage.wdir == os.path.realpath(tmp_dir)
        assert stage.path == os.path.join(os.path.realpath(tmp_dir), fname)

        # Check that it is dumped properly (relative to fname)
        d = load_yaml(stage.relpath)
        assert d[Stage.PARAM_WDIR] == ".."


def test_rerun_deterministic(tmp_dir, run_copy, mocker):
    from dvc.stage.run import subprocess

    tmp_dir.gen("foo", "foo content")

    spy = mocker.spy(subprocess, "Popen")

    run_copy("foo", "out", single_stage=True)
    assert spy.called

    spy.reset_mock()
    run_copy("foo", "out", single_stage=True)
    assert not spy.called


def test_rerun_deterministic_ignore_cache(tmp_dir, run_copy, mocker):
    from dvc.stage.run import subprocess

    tmp_dir.gen("foo", "foo content")

    spy = mocker.spy(subprocess, "Popen")

    run_copy("foo", "out", single_stage=True)
    assert spy.called

    spy.reset_mock()
    run_copy("foo", "out", run_cache=False, single_stage=True)
    assert spy.called


def test_rerun_callback(dvc):
    def run_callback(force=False):
        return dvc.run(cmd="echo content > out", force=force, single_stage=True)

    assert run_callback() is not None
    with pytest.raises(StageFileAlreadyExistsError):
        assert run_callback() is not None
    assert run_callback(force=True) is not None


def test_rerun_changed_dep(tmp_dir, run_copy):
    tmp_dir.gen("foo", "foo content")
    assert run_copy("foo", "out", single_stage=True) is not None

    tmp_dir.gen("foo", "changed content")
    with pytest.raises(StageFileAlreadyExistsError):
        run_copy("foo", "out", force=False, single_stage=True)
    assert run_copy("foo", "out", force=True, single_stage=True)


def test_rerun_changed_stage(tmp_dir, run_copy):
    tmp_dir.gen("foo", "foo content")
    assert run_copy("foo", "out", single_stage=True) is not None

    tmp_dir.gen("bar", "bar content")
    with pytest.raises(StageFileAlreadyExistsError):
        run_copy("bar", "out", force=False, single_stage=True)


def test_rerun_changed_out(tmp_dir, run_copy):
    tmp_dir.gen("foo", "foo content")
    assert run_copy("foo", "out", single_stage=True) is not None

    Path("out").write_text("modification", encoding="utf-8")
    with pytest.raises(StageFileAlreadyExistsError):
        run_copy("foo", "out", force=False, single_stage=True)


def test_should_raise_on_overlapping_output_paths(tmp_dir, dvc, append_foo_script):
    tmp_dir.gen("data", {"foo": "foo", "bar": "bar"})
    ret = main(["add", "data"])
    assert ret == 0

    foo_file = os.path.join("data", "foo")
    with pytest.raises(OverlappingOutputPathsError) as err:
        dvc.run(
            outs=["data/foo"],
            cmd=f"python append_foo.py {foo_file}",
            single_stage=True,
        )

    error_output = str(err.value)

    assert "The output paths:\n" in error_output
    assert "\n'data'('data.dvc')\n" in error_output
    assert f"\n'{foo_file}'('foo.dvc')\n" in error_output
    assert (
        "overlap and are thus in the same tracked directory.\n"
        "To keep reproducibility, outputs should be in separate "
        "tracked directories or tracked individually." in error_output
    )


def test_should_not_checkout_upon_corrupted_local_hardlink_cache(
    mocker, tmp_dir, dvc, copy_script
):
    tmp_dir.gen("foo", "foo")
    dvc.cache.local.cache_types = ["hardlink"]

    stage = dvc.run(
        deps=["foo"],
        outs=["bar"],
        cmd="python copy.py foo bar",
        single_stage=True,
    )

    os.chmod("bar", 0o644)
    with open("bar", "w", encoding="utf-8") as fd:
        fd.write("corrupting the output cache")

    spy_checkout = mocker.spy(stage.outs[0], "checkout")
    from dvc.stage import run as stage_run

    spy_run = mocker.spy(stage_run, "cmd_run")

    with dvc.lock:
        stage.run()

        spy_run.assert_called_once()
        spy_checkout.assert_not_called()


def test_bad_stage_fname(tmp_dir, dvc, run_copy):
    tmp_dir.dvc_gen("foo", "foo content")

    with pytest.raises(StageFileBadNameError):
        # fname should end with .dvc
        run_copy("foo", "foo_copy", fname="out_stage", single_stage=True)

    # Check that command hasn't been run
    assert not (tmp_dir / "foo_copy").exists()


def test_should_raise_on_stage_dependency(run_copy):
    with pytest.raises(DependencyIsStageFileError):
        run_copy("name.dvc", "stage_copy", single_stage=True)


def test_should_raise_on_stage_output(tmp_dir, dvc, run_copy):
    tmp_dir.dvc_gen("foo", "foo content")

    with pytest.raises(OutputIsStageFileError):
        run_copy("foo", "name.dvc", single_stage=True)


@pytest.mark.parametrize("metrics_type", ["metrics", "metrics_no_cache"])
def test_metrics_dir(tmp_dir, dvc, caplog, run_copy_metrics, metrics_type):
    copyargs = {metrics_type: ["dir_metric"]}
    tmp_dir.gen({"dir": {"file": "content"}})
    with caplog.at_level(logging.DEBUG, "dvc"):
        run_copy_metrics("dir", "dir_metric", **copyargs)
    assert "directory 'dir_metric' cannot be used as metrics." in caplog.messages


def test_run_force_preserves_comments_and_meta(tmp_dir, dvc, run_copy):
    tmp_dir.gen({"foo": "foo", "foo1": "foo1"})
    text = textwrap.dedent(
        """\
      desc: top desc
      cmd: python copy.py foo bar
      deps:
      - path: copy.py
      - path: foo
      outs:
      # comment preserved
      - path: bar
        desc: out desc
        type: mytype
        labels:
        - label1
        - label2
        meta:
          key: value
      meta:
        name: copy-foo-bar
    """
    )
    (tmp_dir / "bar.dvc").write_text(text)
    dvc.reproduce("bar.dvc")

    # CRLF on windows makes the generated file bigger in size
    code_size = 176 if os.name == "nt" else 167
    assert (tmp_dir / "bar.dvc").read_text() == textwrap.dedent(
        f"""\
        desc: top desc
        cmd: python copy.py foo bar
        deps:
        - path: copy.py
          md5: a618d3a2c3d5a35f0aa4707951d986f5
          size: {code_size}
        - path: foo
          md5: acbd18db4cc2f85cedef654fccc4a4d8
          size: 3
        outs:
        # comment preserved
        - path: bar
          desc: out desc
          type: mytype
          labels:
          - label1
          - label2
          meta:
            key: value
          md5: acbd18db4cc2f85cedef654fccc4a4d8
          size: 3
        meta:
          name: copy-foo-bar
        md5: 262f10a31f4b218b7b450b3511c2413f
    """
    )

    run_copy("foo1", "bar1", single_stage=True, force=True, fname="bar.dvc")
    assert (tmp_dir / "bar.dvc").read_text() == textwrap.dedent(
        f"""\
        desc: top desc
        cmd: python copy.py foo1 bar1
        deps:
        - path: foo1
          md5: 299a0be4a5a79e6a59fdd251b19d78bb
          size: 4
        - path: copy.py
          md5: a618d3a2c3d5a35f0aa4707951d986f5
          size: {code_size}
        outs:
        # comment preserved
        - path: bar1
          md5: 299a0be4a5a79e6a59fdd251b19d78bb
          size: 4
        meta:
          name: copy-foo-bar
        md5: 61a1506995d7550a366b80d2301530b7
    """
    )




tests/func/test_scm.py
import os

import pytest
from git import Repo

from dvc.scm import SCM, Git, NoSCM, SCMError


def test_init_none(tmp_dir):
    assert isinstance(SCM(os.fspath(tmp_dir), no_scm=True), NoSCM)


def test_init_git(tmp_dir):
    Repo.init(os.fspath(tmp_dir))
    assert isinstance(SCM(os.fspath(tmp_dir)), Git)


def test_init_no_git(tmp_dir):
    with pytest.raises(SCMError, match=r".* is not a git repository"):
        SCM(os.fspath(tmp_dir))


def test_init_sub_dir(tmp_dir):
    Repo.init(os.fspath(tmp_dir))
    subdir = tmp_dir / "dir"
    subdir.mkdir()

    scm = SCM(os.fspath(subdir))
    assert scm.root_dir == os.fspath(tmp_dir)




tests/func/test_scm_context.py
def test_scm_context_autostage(tmp_dir, scm, dvc):
    tmp_dir.gen("foo", "foo")
    with dvc.scm_context(autostage=True) as context:
        context.track_file("foo")

    scm._reset()
    assert scm.is_tracked("foo")


def test_scm_context_ignore(tmp_dir, scm, dvc):
    with dvc.scm_context as context:
        context.ignore(tmp_dir / "foo")
        assert context.files_to_track == {scm.GITIGNORE}

    scm._reset()
    assert scm.is_ignored("foo")


def test_scm_context_when_already_ignored(tmp_dir, scm, dvc):
    scm.ignore(tmp_dir / "foo")
    scm._reset()

    with dvc.scm_context() as context:
        context.ignore(tmp_dir / "foo")
        # If files are already ignored, dvc should not try to track a new
        # .gitignore file as it's a no-op.
        assert not context.files_to_track

    scm._reset()
    assert scm.is_ignored("foo")


def test_scm_context_ignore_remove(tmp_dir, scm, dvc):
    scm.ignore(tmp_dir / "foo")
    scm.ignore(tmp_dir / "bar")

    with dvc.scm_context:
        dvc.scm_context.ignore_remove(tmp_dir / "foo")
        assert dvc.scm_context.files_to_track == {scm.GITIGNORE}

    scm._reset()
    assert not scm.is_ignored("foo")


def test_scm_context_try_ignore_remove_non_existing_entry(tmp_dir, dvc, scm):
    with dvc.scm_context as context:
        context.ignore_remove(tmp_dir / "foo")
        assert not context.files_to_track
    scm._reset()
    assert not scm.is_ignored("foo")


def test_scm_context_no_track_on_ignore_remove(tmp_dir, dvc, scm):
    # DVC should not keep track of file when nothing actually changed
    # i.e. here ignore was reverted back.
    scm.ignore(tmp_dir / "foo")
    with dvc.scm_context:
        dvc.scm_context.ignore_remove(tmp_dir / "foo")
        assert not dvc.scm_context.files_to_track

    scm._reset()
    assert not scm.is_ignored("foo")




tests/func/test_stage.py
import os

import pytest

from dvc.annotations import Annotation
from dvc.dvcfile import SingleStageFile
from dvc.exceptions import OutputDuplicationError
from dvc.fs import LocalFileSystem
from dvc.output import Output
from dvc.repo import Repo
from dvc.stage import PipelineStage, Stage
from dvc.stage.utils import compute_md5
from dvc.utils import dict_md5
from dvc.utils.serialize import dump_yaml, load_yaml
from dvc.utils.strictyaml import YAMLValidationError


def test_cmd_obj():
    with pytest.raises(YAMLValidationError):
        SingleStageFile.validate({Stage.PARAM_CMD: {}})


def test_cmd_none():
    SingleStageFile.validate({Stage.PARAM_CMD: None})


def test_no_cmd():
    SingleStageFile.validate({})


def test_cmd_str():
    SingleStageFile.validate({Stage.PARAM_CMD: "cmd"})


def test_object():
    with pytest.raises(YAMLValidationError):
        SingleStageFile.validate({Stage.PARAM_DEPS: {}})

    with pytest.raises(YAMLValidationError):
        SingleStageFile.validate({Stage.PARAM_OUTS: {}})


def test_none():
    SingleStageFile.validate({Stage.PARAM_DEPS: None})
    SingleStageFile.validate({Stage.PARAM_OUTS: None})


def test_empty_list():
    d = {Stage.PARAM_DEPS: []}
    SingleStageFile.validate(d)

    d = {Stage.PARAM_OUTS: []}
    SingleStageFile.validate(d)


def test_list():
    lst = [
        {Output.PARAM_PATH: "foo", LocalFileSystem.PARAM_CHECKSUM: "123"},
        {Output.PARAM_PATH: "bar", LocalFileSystem.PARAM_CHECKSUM: None},
        {Output.PARAM_PATH: "baz"},
    ]
    d = {Stage.PARAM_DEPS: lst}
    SingleStageFile.validate(d)

    lst[0][Output.PARAM_CACHE] = True
    lst[1][Output.PARAM_CACHE] = False
    d = {Stage.PARAM_OUTS: lst}
    SingleStageFile.validate(d)


def test_reload(tmp_dir, dvc):
    (stage,) = tmp_dir.dvc_gen("foo", "foo")
    d = load_yaml(stage.relpath)

    # NOTE: checking that reloaded stage didn't change its checksum
    md5 = "11111111111111111111111111111111"
    d[stage.PARAM_MD5] = md5
    dump_yaml(stage.relpath, d)

    dvcfile = SingleStageFile(dvc, stage.relpath)
    stage = dvcfile.stage

    assert stage is not None
    dvcfile.dump(stage)

    d = load_yaml(stage.relpath)
    assert d[stage.PARAM_MD5] == md5


def test_default_wdir_ignored_in_checksum(tmp_dir, dvc):
    tmp_dir.gen("bar", "bar")
    stage = dvc.run(
        cmd="cp bar foo",
        deps=["bar"],
        outs=["foo"],
        single_stage=True,
    )

    d = stage.dumpd()
    assert Stage.PARAM_WDIR not in d.keys()

    d = load_yaml(stage.relpath)
    assert Stage.PARAM_WDIR not in d.keys()

    with dvc.lock:
        stage = SingleStageFile(dvc, stage.relpath).stage
        assert not stage.changed()


def test_external_remote_output_resolution(tmp_dir, dvc, make_remote):
    tmp_path = make_remote("tmp", default=False)
    tmp_dir.add_remote(url="remote://tmp/storage", name="storage", default=False)
    storage = tmp_path / "storage"
    storage.mkdir()
    file_path = storage / "file"

    dvc.run(
        cmd=f"echo file > {file_path}",
        outs_no_cache=["remote://storage/file"],
        single_stage=True,
    )
    assert os.path.exists(file_path)


def test_external_remote_dependency_resolution(tmp_dir, dvc, make_remote):
    tmp_path = make_remote("tmp", default=False)
    tmp_dir.add_remote(url="remote://tmp/storage", name="storage", default=False)
    storage = tmp_path / "storage"
    storage.mkdir()
    file_path = storage / "file"
    file_path.write_text("Isle of Dogs", encoding="utf-8")

    dvc.imp_url("remote://storage/file", "movie.txt")
    assert (tmp_dir / "movie.txt").read_text() == "Isle of Dogs"


def test_md5_ignores_comments(tmp_dir, dvc):
    (stage,) = tmp_dir.dvc_gen("foo", "foo content")

    with open(stage.path, "a", encoding="utf-8") as f:
        f.write("# End comment\n")

    new_stage = SingleStageFile(dvc, stage.path).stage
    assert not new_stage.changed_stage()


def test_md5_ignores_annotations(tmp_dir, dvc):
    data = {
        "desc": "stage desc",
        "meta": {"key1": "value1", "key2": "value2"},
        "outs": [
            {
                "md5": "d3b07384d113edec49eaa6238ad5ff00",
                "size": 4,
                "path": "foo",
                "desc": "foo desc",
                "type": "mytype",
                "labels": ["get-started", "dataset-registry"],
                "meta": {"key1": "value1"},
            }
        ],
    }
    (tmp_dir / "foo.dvc").dump(data)
    stage = dvc.stage.load_one("foo.dvc")
    assert compute_md5(stage) == "1822617147b53ae6f9eb4b3c87c0b6f3"
    assert (
        dict_md5({"outs": [{"md5": "d3b07384d113edec49eaa6238ad5ff00", "path": "foo"}]})
        == "1822617147b53ae6f9eb4b3c87c0b6f3"
    )


def test_meta_desc_is_preserved(tmp_dir, dvc):
    data = {
        "desc": "stage desc",
        "meta": {"key1": "value1", "key2": "value2"},
        "outs": [
            {
                "md5": "d3b07384d113edec49eaa6238ad5ff00",
                "size": 4,
                "path": "foo",
                "desc": "foo desc",
                "type": "mytype",
                "labels": ["get-started", "dataset-registry"],
                "meta": {"key": "value"},
            }
        ],
    }
    (tmp_dir / "foo.dvc").dump(data)
    stage = dvc.stage.load_one("foo.dvc")

    assert stage.meta == {"key1": "value1", "key2": "value2"}
    assert stage.desc == "stage desc"
    assert stage.outs[0].annot == Annotation(
        desc="foo desc",
        type="mytype",
        labels=["get-started", "dataset-registry"],
        meta={"key": "value"},
    )

    # sanity check
    stage.dump()
    assert (tmp_dir / "foo.dvc").parse() == data


def test_parent_repo_collect_stages(tmp_dir, scm, dvc):
    tmp_dir.gen({"subdir": {}})
    tmp_dir.gen({"deep": {"dir": {}}})
    subrepo_dir = tmp_dir / "subdir"
    deep_subrepo_dir = tmp_dir / "deep" / "dir"

    with subrepo_dir.chdir():
        subrepo = Repo.init(subdir=True)
        subrepo_dir.gen("subrepo_file", "subrepo file content")
        subrepo.add("subrepo_file")

    with deep_subrepo_dir.chdir():
        deep_subrepo = Repo.init(subdir=True)
        deep_subrepo_dir.gen("subrepo_file", "subrepo file content")
        deep_subrepo.add("subrepo_file")

    dvc._reset()

    stages = dvc.stage.collect(None)
    subrepo_stages = subrepo.stage.collect(None)
    deep_subrepo_stages = deep_subrepo.stage.collect(None)

    assert stages == []
    assert subrepo_stages != []
    assert deep_subrepo_stages != []


@pytest.mark.parametrize("with_deps", (False, True))
def test_collect_symlink(tmp_dir, dvc, with_deps):
    tmp_dir.gen({"data": {"foo": "foo contents"}})
    foo_path = os.path.join("data", "foo")
    dvc.add(foo_path)

    data_link = tmp_dir / "data_link"
    data_link.symlink_to("data")
    stage = list(
        dvc.stage.collect(target=str(data_link / "foo.dvc"), with_deps=with_deps)
    )[0]

    assert stage.addressing == f"{foo_path}.dvc"


def test_stage_strings_representation(tmp_dir, dvc, run_copy):
    tmp_dir.dvc_gen("foo", "foo")
    stage1 = run_copy("foo", "bar", single_stage=True)
    assert stage1.addressing == "bar.dvc"
    assert repr(stage1) == "Stage: 'bar.dvc'"
    assert str(stage1) == "stage: 'bar.dvc'"

    stage2 = run_copy("bar", "baz", name="copy-bar-baz")
    assert stage2.addressing == "copy-bar-baz"
    assert repr(stage2) == "Stage: 'copy-bar-baz'"
    assert str(stage2) == "stage: 'copy-bar-baz'"

    folder = tmp_dir / "dir"
    folder.mkdir()
    with folder.chdir():
        # `Stage` caches `relpath` results, forcing it to reset
        stage1.path = stage1.path
        stage2.path = stage2.path

        rel_path = os.path.relpath(stage1.path)
        assert stage1.addressing == rel_path
        assert repr(stage1) == f"Stage: '{rel_path}'"
        assert str(stage1) == f"stage: '{rel_path}'"

        rel_path = os.path.relpath(stage2.path)
        assert stage2.addressing == f"{rel_path}:{stage2.name}"
        assert repr(stage2) == f"Stage: '{rel_path}:{stage2.name}'"
        assert str(stage2) == f"stage: '{rel_path}:{stage2.name}'"


def test_stage_on_no_path_string_repr(tmp_dir, dvc):
    s = Stage(dvc)
    assert s.addressing == "No path"
    assert repr(s) == "Stage: 'No path'"
    assert str(s) == "stage: 'No path'"

    p = PipelineStage(dvc, name="stage_name")
    assert p.addressing == "No path:stage_name"
    assert repr(p) == "Stage: 'No path:stage_name'"
    assert str(p) == "stage: 'No path:stage_name'"


def test_stage_remove_pipeline_stage(tmp_dir, dvc, run_copy):
    tmp_dir.gen("foo", "foo")
    stage = run_copy("foo", "bar", name="copy-foo-bar")
    run_copy("bar", "foobar", name="copy-bar-foobar")

    dvc_file = stage.dvcfile
    with dvc.lock:
        stage.remove(purge=False)
    assert stage.name in dvc_file.stages

    with dvc.lock:
        stage.remove()

    dvc_file._reset()
    assert stage.name not in dvc_file.stages
    assert "copy-bar-foobar" in dvc_file.stages


def test_stage_remove_pointer_stage(tmp_dir, dvc, run_copy):
    (stage,) = tmp_dir.dvc_gen("foo", "foo")

    with dvc.lock:
        stage.remove(purge=False)
    assert not (tmp_dir / "foo").exists()
    assert (tmp_dir / stage.relpath).exists()

    with dvc.lock:
        stage.remove()
    assert not (tmp_dir / stage.relpath).exists()


def test_stage_add_duplicated_output(tmp_dir, dvc):
    tmp_dir.dvc_gen("foo", "foo")
    dvc.add("foo")

    with pytest.raises(
        OutputDuplicationError,
        match="Use `dvc remove foo.dvc` to stop tracking the overlapping output.",
    ):
        dvc.stage.add(name="duplicated", cmd="echo bar > foo", outs=["foo"])




tests/func/test_stage_load.py
import os
from operator import itemgetter

import pytest
from funcy import raiser

from dvc.dvcfile import PROJECT_FILE, FileIsGitIgnored
from dvc.exceptions import NoOutputOrStageError
from dvc.repo import Repo
from dvc.stage.exceptions import (
    StageFileDoesNotExistError,
    StageNameUnspecified,
    StageNotFound,
)
from dvc.utils import relpath
from dvc.utils.fs import remove
from dvc.utils.strictyaml import YAMLValidationError


def test_collect(tmp_dir, scm, dvc, run_copy):
    def collect_outs(*args, **kwargs):
        return {
            str(out)
            for stage in dvc.stage.collect(*args, **kwargs)
            for out in stage.outs
        }

    tmp_dir.dvc_gen("foo", "foo")
    run_copy("foo", "bar", single_stage=True)
    scm.add([".gitignore", "foo.dvc", "bar.dvc"])
    scm.commit("Add foo and bar")

    scm.checkout("new-branch", create_new=True)

    run_copy("bar", "buzz", single_stage=True)
    scm.add([".gitignore", "buzz.dvc"])
    scm.commit("Add buzz")

    assert collect_outs("bar.dvc", with_deps=True) == {"foo", "bar"}
    assert collect_outs("buzz.dvc", with_deps=True) == {"foo", "bar", "buzz"}
    assert collect_outs("buzz.dvc", with_deps=False) == {"buzz"}

    run_copy("foo", "foobar", name="copy-foo-foobar")
    assert collect_outs(":copy-foo-foobar") == {"foobar"}
    assert collect_outs(":copy-foo-foobar", with_deps=True) == {
        "foobar",
        "foo",
    }
    assert collect_outs("dvc.yaml:copy-foo-foobar", recursive=True) == {"foobar"}
    assert collect_outs("copy-foo-foobar") == {"foobar"}
    assert collect_outs("copy-foo-foobar", with_deps=True) == {"foobar", "foo"}
    assert collect_outs("copy-foo-foobar", recursive=True) == {"foobar"}

    run_copy("foobar", "baz", name="copy-foobar-baz")
    assert collect_outs("dvc.yaml") == {"foobar", "baz"}
    assert collect_outs("dvc.yaml", with_deps=True) == {"foobar", "baz", "foo"}


def test_collect_dir_recursive(tmp_dir, dvc, run_head):
    tmp_dir.gen({"dir": {"foo": "foo"}})
    (stage1,) = dvc.add("dir/*", glob=True)
    with (tmp_dir / "dir").chdir():
        stage2 = run_head("foo", name="copy-foo-bar")
        stage3 = run_head("foo-1", single_stage=True)
    assert set(dvc.stage.collect("dir", recursive=True)) == {
        stage1,
        stage2,
        stage3,
    }


def test_collect_with_not_existing_output_or_stage_name(tmp_dir, dvc, run_copy):
    with pytest.raises(StageFileDoesNotExistError):
        dvc.stage.collect("some_file")
    tmp_dir.dvc_gen("foo", "foo")
    run_copy("foo", "bar", name="copy-foo-bar")
    with pytest.raises(StageNotFound):
        dvc.stage.collect("some_file")


def test_stages(tmp_dir, dvc):
    def collect_stages():
        return {stage.relpath for stage in Repo(os.fspath(tmp_dir)).index.stages}

    tmp_dir.dvc_gen({"file": "a", "dir/file": "b", "dir/subdir/file": "c"})

    assert collect_stages() == {
        "file.dvc",
        os.path.join("dir", "file.dvc"),
        os.path.join("dir", "subdir", "file.dvc"),
    }

    tmp_dir.gen(".dvcignore", "dir")

    assert collect_stages() == {"file.dvc"}


@pytest.fixture
def stages(tmp_dir, run_copy):
    stage1, stage2 = tmp_dir.dvc_gen({"foo": "foo", "lorem": "lorem"})
    return {
        "foo-generate": stage1,
        "lorem-generate": stage2,
        "copy-foo-bar": run_copy("foo", "bar", single_stage=True),
        "copy-bar-foobar": run_copy("bar", "foobar", name="copy-bar-foobar"),
        "copy-lorem-ipsum": run_copy("lorem", "ipsum", name="copy-lorem-ipsum"),
    }


def test_collect_not_a_group_stage_with_group_flag(tmp_dir, dvc, stages):
    assert set(dvc.stage.collect("copy-bar-foobar")) == {stages["copy-bar-foobar"]}
    assert set(dvc.stage.collect("copy-bar-foobar", with_deps=True)) == {
        stages["copy-bar-foobar"],
        stages["copy-foo-bar"],
        stages["foo-generate"],
    }
    assert set(dvc.stage.collect_granular("copy-bar-foobar")) == {
        (stages["copy-bar-foobar"], None)
    }
    assert set(dvc.stage.collect_granular("copy-bar-foobar", with_deps=True)) == {
        (stages["copy-bar-foobar"], None),
        (stages["copy-foo-bar"], None),
        (stages["foo-generate"], None),
    }


def test_collect_generated(tmp_dir, dvc):
    d = {
        "vars": [{"vars": [1, 2, 3, 4, 5]}],
        "stages": {"build": {"foreach": "${vars}", "do": {"cmd": "echo ${item}"}}},
    }
    (tmp_dir / "dvc.yaml").dump(d)

    all_stages = set(dvc.index.stages)
    assert len(all_stages) == 5

    assert set(dvc.stage.collect()) == all_stages
    assert set(dvc.stage.collect("build")) == all_stages
    assert set(dvc.stage.collect("build", with_deps=True)) == all_stages
    assert set(dvc.stage.collect("build*", glob=True)) == all_stages
    assert set(dvc.stage.collect("build*", glob=True, with_deps=True)) == all_stages

    stages_info = {(stage, None) for stage in all_stages}
    assert set(dvc.stage.collect_granular("build")) == stages_info
    assert set(dvc.stage.collect_granular("build", with_deps=True)) == stages_info


def test_collect_glob(tmp_dir, dvc, stages):
    assert set(dvc.stage.collect("copy*", glob=True)) == {
        stages[key] for key in ["copy-bar-foobar", "copy-lorem-ipsum"]
    }
    assert set(dvc.stage.collect("copy-lorem*", glob=True, with_deps=True)) == {
        stages[key] for key in ["copy-lorem-ipsum", "lorem-generate"]
    }


def test_collect_granular_with_no_target(tmp_dir, dvc, stages):
    assert set(map(itemgetter(0), dvc.stage.collect_granular())) == set(stages.values())
    assert list(map(itemgetter(1), dvc.stage.collect_granular())) == [None] * len(
        stages
    )


def test_collect_granular_with_target(tmp_dir, dvc, stages):
    assert dvc.stage.collect_granular("bar.dvc") == [(stages["copy-foo-bar"], None)]
    assert dvc.stage.collect_granular(PROJECT_FILE) == [
        (stages["copy-bar-foobar"], None),
        (stages["copy-lorem-ipsum"], None),
    ]
    assert dvc.stage.collect_granular(":") == [
        (stages["copy-bar-foobar"], None),
        (stages["copy-lorem-ipsum"], None),
    ]
    assert dvc.stage.collect_granular("copy-bar-foobar") == [
        (stages["copy-bar-foobar"], None)
    ]
    assert dvc.stage.collect_granular(":copy-bar-foobar") == [
        (stages["copy-bar-foobar"], None)
    ]
    assert dvc.stage.collect_granular("dvc.yaml:copy-bar-foobar") == [
        (stages["copy-bar-foobar"], None)
    ]

    with (tmp_dir / dvc.DVC_DIR).chdir():
        assert dvc.stage.collect_granular(
            relpath(tmp_dir / PROJECT_FILE) + ":copy-bar-foobar"
        ) == [(stages["copy-bar-foobar"], None)]

    assert dvc.stage.collect_granular("foobar") == [
        (stages["copy-bar-foobar"], os.path.join(tmp_dir, "foobar"))
    ]


@pytest.mark.parametrize(
    "target",
    [
        "not_existing.dvc",
        "not_existing.dvc:stage_name",
        "not_existing/dvc.yaml",
        "not_existing/dvc.yaml:stage_name",
    ],
)
def test_collect_with_not_existing_dvcfile(tmp_dir, dvc, target):
    with pytest.raises(StageFileDoesNotExistError):
        dvc.stage.collect_granular(target)
    with pytest.raises(StageFileDoesNotExistError):
        dvc.stage.collect(target)


def test_collect_granular_with_not_existing_output_or_stage_name(tmp_dir, dvc):
    with pytest.raises(NoOutputOrStageError):
        dvc.stage.collect_granular("some_file")
    with pytest.raises(NoOutputOrStageError):
        dvc.stage.collect_granular("some_file", recursive=True)


def test_collect_granular_with_deps(tmp_dir, dvc, stages):
    assert set(
        map(
            itemgetter(0),
            dvc.stage.collect_granular("bar.dvc", with_deps=True),
        )
    ) == {stages["copy-foo-bar"], stages["foo-generate"]}
    assert set(
        map(
            itemgetter(0),
            dvc.stage.collect_granular("copy-bar-foobar", with_deps=True),
        )
    ) == {
        stages["copy-bar-foobar"],
        stages["copy-foo-bar"],
        stages["foo-generate"],
    }
    assert set(
        map(
            itemgetter(0),
            dvc.stage.collect_granular(PROJECT_FILE, with_deps=True),
        )
    ) == set(stages.values())


def test_collect_granular_same_output_name_stage_name(tmp_dir, dvc, run_copy):
    (stage1,) = tmp_dir.dvc_gen("foo", "foo")
    (stage2,) = tmp_dir.dvc_gen("copy-foo-bar", "copy-foo-bar")
    stage3 = run_copy("foo", "bar", name="copy-foo-bar")

    assert dvc.stage.collect_granular("copy-foo-bar") == [(stage3, None)]

    coll = dvc.stage.collect_granular("copy-foo-bar", with_deps=True)
    assert set(map(itemgetter(0), coll)) == {stage3, stage1}
    assert list(map(itemgetter(1), coll)) == [None] * 2

    assert dvc.stage.collect_granular("./copy-foo-bar") == [
        (stage2, os.path.join(tmp_dir / "copy-foo-bar"))
    ]
    assert dvc.stage.collect_granular("./copy-foo-bar", with_deps=True) == [
        (stage2, os.path.join(tmp_dir / "copy-foo-bar"))
    ]


def test_collect_granular_priority_on_collision(tmp_dir, dvc, run_copy):
    tmp_dir.gen({"dir": {"foo": "foo"}, "foo": "foo"})
    (stage1,) = dvc.add("dir/*", glob=True)
    stage2 = run_copy("foo", "bar", name="dir")

    assert dvc.stage.collect_granular("dir") == [(stage2, None)]
    assert dvc.stage.collect_granular("dir", recursive=True) == [(stage1, None)]

    remove(tmp_dir / "dir")

    assert dvc.stage.collect_granular("dir") == [(stage2, None)]
    assert dvc.stage.collect_granular("dir", recursive=True) == [(stage2, None)]


def test_collect_granular_collision_output_dir_stage_name(tmp_dir, dvc, run_copy):
    stage1, *_ = tmp_dir.dvc_gen({"dir": {"foo": "foo"}, "foo": "foo"})
    stage3 = run_copy("foo", "bar", name="dir")

    assert dvc.stage.collect_granular("dir") == [(stage3, None)]
    assert not dvc.stage.collect_granular("dir", recursive=True)
    assert dvc.stage.collect_granular("./dir") == [
        (stage1, os.path.join(tmp_dir / "dir"))
    ]


def test_collect_granular_not_existing_stage_name(tmp_dir, dvc, run_copy):
    tmp_dir.dvc_gen("foo", "foo")
    (stage,) = tmp_dir.dvc_gen("copy-foo-bar", "copy-foo-bar")
    run_copy("foo", "bar", name="copy-foo-bar")

    assert dvc.stage.collect_granular("copy-foo-bar.dvc:stage_name_not_needed") == [
        (stage, None)
    ]
    with pytest.raises(StageNotFound):
        dvc.stage.collect_granular("dvc.yaml:does-not-exist")


def test_get_stages(tmp_dir, dvc, run_copy):
    with pytest.raises(StageFileDoesNotExistError):
        dvc.stage.load_all()

    tmp_dir.gen("foo", "foo")
    stage1 = run_copy("foo", "bar", name="copy-foo-bar")
    stage2 = run_copy("bar", "foobar", name="copy-bar-foobar")

    assert set(dvc.stage.load_all()) == {stage1, stage2}
    assert set(dvc.stage.load_all(path=PROJECT_FILE)) == {stage1, stage2}
    assert set(dvc.stage.load_all(name="copy-bar-foobar")) == {stage2}
    assert set(dvc.stage.load_all(path=PROJECT_FILE, name="copy-bar-foobar")) == {
        stage2
    }

    with pytest.raises(StageFileDoesNotExistError):
        dvc.stage.load_all(path=relpath(tmp_dir / ".." / PROJECT_FILE))

    with pytest.raises(StageNotFound):
        dvc.stage.load_all(path=PROJECT_FILE, name="copy")


def test_get_stages_old_dvcfile(tmp_dir, dvc):
    (stage1,) = tmp_dir.dvc_gen("foo", "foo")
    assert set(dvc.stage.load_all("foo.dvc")) == {stage1}
    assert set(dvc.stage.load_all("foo.dvc", name="foo-generate")) == {stage1}

    with pytest.raises(StageFileDoesNotExistError):
        dvc.stage.load_all(path=relpath(tmp_dir / ".." / "foo.dvc"))


def test_get_stage(tmp_dir, dvc, run_copy):
    tmp_dir.gen("foo", "foo")
    stage1 = run_copy("foo", "bar", name="copy-foo-bar")

    with pytest.raises(StageNameUnspecified):
        dvc.stage.load_one()

    with pytest.raises(StageNameUnspecified):
        dvc.stage.load_one(path=PROJECT_FILE)

    assert dvc.stage.load_one(path=PROJECT_FILE, name="copy-foo-bar") == stage1
    assert dvc.stage.load_one(name="copy-foo-bar") == stage1

    with pytest.raises(StageFileDoesNotExistError):
        dvc.stage.load_one(path="something.yaml", name="name")

    with pytest.raises(StageNotFound):
        dvc.stage.load_one(name="random_name")


def test_get_stage_single_stage_dvcfile(tmp_dir, dvc):
    (stage1,) = tmp_dir.dvc_gen("foo", "foo")
    assert dvc.stage.load_one("foo.dvc") == stage1
    assert dvc.stage.load_one("foo.dvc", name="jpt") == stage1
    with pytest.raises(StageFileDoesNotExistError):
        dvc.stage.load_one(path="bar.dvc", name="name")


def test_collect_optimization(tmp_dir, dvc, mocker):
    (stage,) = tmp_dir.dvc_gen("foo", "foo text")

    # Forget cached stages and graph and error out on collection
    dvc._reset()
    mocker.patch(
        "dvc.repo.Repo.index",
        property(raiser(Exception("Should not collect"))),
    )

    # Should read stage directly instead of collecting the whole graph
    dvc.stage.collect(stage.path)
    dvc.stage.collect_granular(stage.path)


def test_collect_optimization_on_stage_name(tmp_dir, dvc, mocker, run_copy):
    tmp_dir.dvc_gen("foo", "foo")
    stage = run_copy("foo", "bar", name="copy-foo-bar")
    # Forget cached stages and graph and error out on collection
    dvc._reset()
    mocker.patch(
        "dvc.repo.Repo.index",
        property(raiser(Exception("Should not collect"))),
    )

    # Should read stage directly instead of collecting the whole graph
    assert dvc.stage.collect("copy-foo-bar") == [stage]
    assert dvc.stage.collect_granular("copy-foo-bar") == [(stage, None)]


def test_collect_repo_callback(tmp_dir, dvc, mocker):
    mock = mocker.Mock()
    dvc.stage_collection_error_handler = mock

    (stage,) = tmp_dir.dvc_gen("foo", "foo")
    (tmp_dir / PROJECT_FILE).dump({"stages": {"cmd": "echo hello world"}})

    dvc._reset()
    assert dvc.index.stages == [stage]
    mock.assert_called_once()

    file_path, exc = mock.call_args[0]
    assert file_path == PROJECT_FILE
    assert isinstance(exc, YAMLValidationError)


def test_gitignored_file_try_collect_granular_for_data_files(tmp_dir, dvc, scm):
    (stage,) = tmp_dir.dvc_gen({"data": {"foo": "foo", "bar": "bar"}})
    path = os.path.join("data", "foo")

    assert dvc.stage.collect_granular(path) == [(stage, os.path.join(tmp_dir, path))]

    scm.ignore(stage.path)
    dvc._reset()

    with pytest.raises(NoOutputOrStageError):
        dvc.stage.collect_granular(path)


def test_gitignored_file_try_collect_granular_for_dvc_yaml_files(
    tmp_dir, dvc, scm, stages
):
    assert dvc.stage.collect_granular("bar") == [
        (stages["copy-foo-bar"], os.path.join(tmp_dir, "bar"))
    ]

    scm.ignore(tmp_dir / "dvc.yaml")
    scm._reset()

    with pytest.raises(FileIsGitIgnored):
        dvc.stage.collect_granular("bar")




tests/func/test_state.py
import os

from dvc_data.hashfile.hash import file_md5
from dvc_data.hashfile.hash_info import HashInfo
from dvc_data.hashfile.state import State


def test_state(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo content")
    path = tmp_dir / "foo"
    hash_info = HashInfo("md5", file_md5(path, dvc.fs))

    state = State(dvc.root_dir, dvc.tmp_dir, dvc.dvcignore)

    state.save(str(path), dvc.fs, hash_info)
    assert state.get(str(path), dvc.fs)[1] == hash_info

    path.unlink()
    path.write_text("1")

    assert state.get(str(path), dvc.fs) == (None, None)

    hash_info = HashInfo("md5", file_md5(path, dvc.fs))
    state.save(str(path), dvc.fs, hash_info)

    assert state.get(str(path), dvc.fs)[1] == hash_info


def test_state_overflow(tmp_dir, dvc):
    # NOTE: trying to add more entries than state can handle,
    # to see if it will clean up and vacuum successfully
    dvc.config["state"]["row_limit"] = 10

    path = tmp_dir / "dir"
    path.mkdir()
    for i in range(20):
        (path / str(i)).write_text(str(i))

    dvc.add("dir")


def mock_get_inode(inode):
    def get_inode_mocked(_):
        return inode

    return get_inode_mocked


def test_remove_links(tmp_dir, dvc):
    tmp_dir.dvc_gen({"foo": "foo_content", "bar": "bar_content"})

    assert len(dvc.state.links) == 2

    dvc.state.remove_links(["foo", "bar"], dvc.fs)

    assert len(dvc.state.links) == 0


def test_get_unused_links(tmp_dir, dvc):
    tmp_dir.dvc_gen({"foo": "foo_content", "bar": "bar_content"})

    links = [os.path.join(dvc.root_dir, link) for link in ["foo", "bar"]]
    assert set(dvc.state.get_unused_links([], dvc.fs)) == {"foo", "bar"}
    assert set(dvc.state.get_unused_links(links[:1], dvc.fs)) == {"bar"}
    assert set(dvc.state.get_unused_links(links, dvc.fs)) == set()
    assert set(
        dvc.state.get_unused_links(
            (links[:1] + [os.path.join(dvc.root_dir, "not-existing-file")]),
            dvc.fs,
        )
    ) == {"bar"}




tests/func/test_status.py
import os

from dvc.cli import main
from dvc.fs import localfs


def test_quiet(tmp_dir, dvc, capsys):
    tmp_dir.dvc_gen("foo", "foo")

    # clear
    capsys.readouterr()

    assert main(["status", "--quiet"]) == 0
    out_err = capsys.readouterr()
    assert not out_err.out
    assert not out_err.err

    tmp_dir.gen("foo", "barr")

    assert main(["status", "--quiet"]) == 1
    out_err = capsys.readouterr()
    assert not out_err.out
    assert not out_err.err


def test_implied_cloud(dvc, mocker):
    mock_status = mocker.patch("dvc.repo.status._cloud_status", return_value=True)

    main(["status", "--remote", "something"])
    assert mock_status.called


def test_status_non_dvc_repo_import(tmp_dir, dvc, git_dir):
    with git_dir.branch("branch", new=True):
        git_dir.scm_gen("file", "first version", commit="first version")

    dvc.imp(os.fspath(git_dir), "file", "file", rev="branch")

    assert dvc.status(["file.dvc"]) == {}

    with git_dir.branch("branch", new=False):
        git_dir.scm_gen("file", "second version", commit="update file")

    (status,) = dvc.status(["file.dvc"])["file.dvc"]
    assert status == {"changed deps": {f"file ({git_dir})": "update available"}}


def test_status_before_and_after_dvc_init(tmp_dir, dvc, git_dir):
    git_dir.scm_gen("file", "first version", commit="first version")
    old_rev = git_dir.scm.get_rev()

    dvc.imp(os.fspath(git_dir), "file", "file")

    assert dvc.status(["file.dvc"]) == {}

    with git_dir.chdir():
        git_dir.init(dvc=True)
        git_dir.scm.gitpython.repo.index.remove(["file"])
        os.remove("file")
        git_dir.dvc_gen("file", "second version", commit="with dvc")
        new_rev = git_dir.scm.get_rev()

    assert old_rev != new_rev

    (status,) = dvc.status(["file.dvc"])["file.dvc"]
    assert status == {
        "changed deps": {f"file ({os.fspath(git_dir)})": "update available"}
    }


def test_status_on_pipeline_stages(tmp_dir, dvc, run_copy):
    tmp_dir.dvc_gen("foo", "foo")
    stage = run_copy("foo", "bar", name="copy-foo-bar")

    stage.cmd = "  ".join(stage.cmd.split())
    stage.dvcfile._dump_pipeline_file(stage)
    assert dvc.status("copy-foo-bar") == {"copy-foo-bar": ["changed command"]}

    # delete outputs
    (tmp_dir / "bar").unlink()
    assert dvc.status() == {
        "copy-foo-bar": [
            {"changed outs": {"bar": "deleted"}},
            "changed command",
        ]
    }
    (tmp_dir / "foo").unlink()
    assert dvc.status() == {
        "foo.dvc": [{"changed outs": {"foo": "deleted"}}],
        "copy-foo-bar": [
            {"changed deps": {"foo": "deleted"}},
            {"changed outs": {"bar": "deleted"}},
            "changed command",
        ],
    }


def test_status_recursive(tmp_dir, dvc):
    tmp_dir.gen({"dir": {"file": "text1", "subdir": {"file2": "text2"}}})
    stages = dvc.add(localfs.find("dir"), no_commit=True)

    assert len(stages) == 2

    assert dvc.status(targets=["dir"], recursive=True) == {
        os.path.join("dir", "file.dvc"): [
            {"changed outs": {os.path.join("dir", "file"): "not in cache"}}
        ],
        os.path.join("dir", "subdir", "file2.dvc"): [
            {"changed outs": {os.path.join("dir", "subdir", "file2"): "not in cache"}}
        ],
    }


def test_status_outputs(tmp_dir, dvc):
    tmp_dir.dvc_gen({"foo": "foo", "bar": "bar"})
    dvc.run(
        outs=["alice", "bob"],
        deps=["foo", "bar"],
        cmd="echo alice>alice && echo bob>bob",
        name="alice_bob",
    )
    tmp_dir.gen({"alice": "new alice", "bob": "new bob"})

    assert dvc.status(targets=["alice_bob"]) == {
        "alice_bob": [{"changed outs": {"alice": "modified", "bob": "modified"}}]
    }

    assert dvc.status(targets=["alice"]) == {
        "alice_bob": [{"changed outs": {"alice": "modified"}}]
    }


def test_params_without_targets(tmp_dir, dvc):
    dvc.stage.add(name="test", cmd="echo params.yaml", params=[{"params.yaml": None}])
    assert dvc.status() == {"test": [{"changed deps": {"params.yaml": "deleted"}}]}

    (tmp_dir / "params.yaml").touch()
    assert dvc.status() == {"test": [{"changed deps": {"params.yaml": "new"}}]}

    dvc.commit("test", force=True)
    # make sure that we are able to keep track of "empty" contents
    # and be able to distinguish between no-lock-entry and empty-lock-entry.
    assert (tmp_dir / "dvc.lock").parse() == {
        "schema": "2.0",
        "stages": {"test": {"cmd": "echo params.yaml", "params": {"params.yaml": {}}}},
    }
    assert dvc.status() == {}

    (tmp_dir / "params.yaml").dump({"foo": "foo", "bar": "bar"})
    assert dvc.status() == {
        "test": [{"changed deps": {"params.yaml": {"bar": "new", "foo": "new"}}}]
    }
    dvc.commit("test", force=True)

    (tmp_dir / "params.yaml").dump({"foo": "foobar", "lorem": "ipsum"})
    assert dvc.status() == {
        "test": [
            {
                "changed deps": {
                    "params.yaml": {
                        "bar": "deleted",
                        "foo": "modified",
                        "lorem": "new",
                    }
                }
            }
        ]
    }




tests/func/test_unprotect.py
import os


def test_unprotect(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")

    dvc.cache.local.cache_types = ["hardlink"]
    dvc.add("foo")
    cache = os.path.join(".dvc", "cache", "ac", "bd18db4cc2f85cedef654fccc4a4d8")
    assert not os.access("foo", os.W_OK)
    assert not os.access(cache, os.W_OK)

    dvc.unprotect("foo")
    assert os.access("foo", os.W_OK)

    if os.name == "nt":
        # NOTE: cache is now unprotected, because NTFS doesn't allow
        # deleting read-only files, so we have to try to set write perms
        # on files that we try to delete, which propagates to the cache
        # file. But it should be restored after the next cache check, hence
        # why we call `dvc status` here.
        assert os.access(cache, os.W_OK)
        dvc.status()

    assert not os.access(cache, os.W_OK)




tests/func/test_update.py
import os

import pytest

from dvc.dependency import base
from dvc.dvcfile import load_file
from dvc.exceptions import InvalidArgumentError
from dvc.testing.tmp_dir import make_subrepo


@pytest.mark.parametrize("cached", [True, False])
def test_update_import(tmp_dir, dvc, erepo_dir, cached):
    gen = erepo_dir.dvc_gen if cached else erepo_dir.scm_gen

    with erepo_dir.branch("branch", new=True), erepo_dir.chdir():
        gen(
            {
                "version": "branch",
                "dir": {"version": "branch", "subdir": {"file": "file"}},
            },
            commit="add version file",
        )
        old_rev = erepo_dir.scm.get_rev()

    stage = dvc.imp(os.fspath(erepo_dir), "version", "version", rev="branch")
    dir_stage = dvc.imp(os.fspath(erepo_dir), "dir", "dir", rev="branch")
    assert dvc.status() == {}

    assert (tmp_dir / "version").read_text() == "branch"
    assert (tmp_dir / "dir").read_text() == {
        "version": "branch",
        "subdir": {"file": "file"},
    }
    assert stage.deps[0].def_repo["rev_lock"] == old_rev
    assert dir_stage.deps[0].def_repo["rev_lock"] == old_rev

    # Update version file
    with erepo_dir.branch("branch", new=False), erepo_dir.chdir():
        gen(
            {
                "version": "updated",
                "dir": {"version": "updated", "subdir": {"file": "file"}},
            },
            commit="update version content",
        )
        new_rev = erepo_dir.scm.get_rev()

    assert old_rev != new_rev

    assert dvc.status() == {
        "dir.dvc": [
            {"changed deps": {f"dir ({os.fspath(erepo_dir)})": "update available"}}
        ],
        "version.dvc": [
            {"changed deps": {f"version ({os.fspath(erepo_dir)})": "update available"}}
        ],
    }

    (stage,) = dvc.update(stage.path)
    (dir_stage,) = dvc.update(dir_stage.path)
    assert dvc.status() == {}

    assert (tmp_dir / "version").read_text() == "updated"
    assert (tmp_dir / "dir").read_text() == {
        "version": "updated",
        "subdir": {"file": "file"},
    }

    assert stage.deps[0].def_repo["rev_lock"] == new_rev
    assert dir_stage.deps[0].def_repo["rev_lock"] == new_rev


def test_update_import_after_remote_updates_to_dvc(tmp_dir, dvc, erepo_dir):
    old_rev = None
    with erepo_dir.branch("branch", new=True), erepo_dir.chdir():
        erepo_dir.scm_gen("version", "branch", commit="add version file")
        old_rev = erepo_dir.scm.get_rev()

    stage = dvc.imp(os.fspath(erepo_dir), "version", "version", rev="branch")

    imported = tmp_dir / "version"
    assert imported.is_file()
    assert imported.read_text() == "branch"
    assert stage.deps[0].def_repo == {
        "url": os.fspath(erepo_dir),
        "rev": "branch",
        "rev_lock": old_rev,
    }

    new_rev = None
    with erepo_dir.branch("branch", new=False), erepo_dir.chdir():
        erepo_dir.scm.gitpython.repo.index.remove(["version"])
        erepo_dir.dvc_gen("version", "updated", commit="upgrade to DVC tracking")
        new_rev = erepo_dir.scm.get_rev()

    assert old_rev != new_rev

    (status,) = dvc.status([stage.path])["version.dvc"]
    (changed_dep,) = list(status["changed deps"].items())
    assert changed_dep[0].startswith("version ")
    assert changed_dep[1] == "update available"

    dvc.update([stage.path])

    assert dvc.status([stage.path]) == {}

    assert imported.is_file()
    assert imported.read_text() == "updated"

    stage = load_file(dvc, stage.path).stage
    assert stage.deps[0].def_repo == {
        "url": os.fspath(erepo_dir),
        "rev": "branch",
        "rev_lock": new_rev,
    }


def test_update_before_and_after_dvc_init(tmp_dir, dvc, git_dir):
    with git_dir.chdir():
        git_dir.scm_gen("file", "first version", commit="first version")
        old_rev = git_dir.scm.get_rev()

    stage = dvc.imp(os.fspath(git_dir), "file", "file")

    with git_dir.chdir():
        git_dir.init(dvc=True)
        git_dir.scm.gitpython.repo.index.remove(["file"])
        os.remove("file")
        git_dir.dvc_gen("file", "second version", commit="with dvc")
        new_rev = git_dir.scm.get_rev()

    assert old_rev != new_rev

    assert dvc.status([stage.path]) == {
        "file.dvc": [
            {"changed deps": {f"file ({os.fspath(git_dir)})": "update available"}}
        ]
    }

    dvc.update([stage.path])

    assert (tmp_dir / "file").read_text() == "second version"
    assert dvc.status([stage.path]) == {}


def test_update_unchanged(tmp_dir, dvc, erepo_dir, mocker):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("file", "file content", commit="add file")

    assert (erepo_dir / "file").exists()
    stage = dvc.imp(os.fspath(erepo_dir), "file")

    spy = mocker.spy(base, "fs_download")
    dvc.update([stage.path])

    assert not spy.called


@pytest.mark.parametrize("outs_exist", (False, True))
def test_update_no_download(tmp_dir, dvc, erepo_dir, outs_exist, mocker):
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("file", "file content", commit="add file")
        initial_rev = erepo_dir.scm.get_rev()

    stage = dvc.imp(os.fspath(erepo_dir), "file", no_download=not outs_exist)

    assert stage.deps[0].def_repo["rev_lock"] == initial_rev

    dst = tmp_dir / "file"
    assert dst.exists() is outs_exist

    with erepo_dir.chdir():
        erepo_dir.dvc_gen("file", "updated file content", commit="update file")
        new_rev = erepo_dir.scm.get_rev()

    updated_stage = dvc.update([stage.path], rev=new_rev, no_download=True)[0]
    assert not dst.exists()

    assert updated_stage.deps[0].def_repo["rev_lock"] == new_rev

    # output must have no information since no_download=True
    out = updated_stage.outs[0]
    assert out.hash_info.value is None
    assert out.meta.size is None


def test_update_import_url(tmp_dir, dvc, workspace):
    workspace.gen("file", "file content")

    dst = tmp_dir / "imported_file"
    stage = dvc.imp_url("remote://workspace/file", os.fspath(dst))

    assert dst.is_file()
    assert dst.read_text() == "file content"

    # update data
    workspace.gen("file", "updated file content")

    assert dvc.status([stage.path]) == {}
    dvc.update([stage.path])
    assert dvc.status([stage.path]) == {}

    assert dst.is_file()
    assert dst.read_text() == "updated file content"


@pytest.mark.parametrize("outs_exist", (False, True))
def test_update_import_url_no_download(tmp_dir, dvc, workspace, outs_exist, mocker):
    workspace.gen("file", "file content")

    dst = tmp_dir / "imported_file"
    stage = dvc.imp_url(
        "remote://workspace/file", os.fspath(dst), no_download=not outs_exist
    )

    assert dst.exists() is outs_exist
    hash_info = stage.deps[0].hash_info
    assert hash_info.value == "d10b4c3ff123b26dc068d43a8bef2d23"

    workspace.gen("file", "updated file content")

    updated_stage = dvc.update([stage.path], no_download=True)[0]
    assert not dst.exists()

    updated_hash_info = updated_stage.deps[0].hash_info
    assert updated_hash_info != hash_info
    assert updated_hash_info.value == "6ffba511ce3aa40b8231d1b1f8c5fba5"

    # output must have no information since no_download=True
    out = updated_stage.outs[0]
    assert out.hash_info.value is None
    assert out.hash_info.name is None
    assert out.meta.size is None


def test_update_import_url_unchanged(tmp_dir, dvc, workspace, mocker):
    workspace.gen("file", "file content")

    dst = tmp_dir / "imported_file"
    stage = dvc.imp_url("remote://workspace/file", os.fspath(dst))

    spy = mocker.spy(base, "fs_download")

    dvc.update([stage.path])
    assert not spy.called


def test_update_rev(tmp_dir, dvc, scm, git_dir):
    with git_dir.chdir():
        git_dir.scm_gen({"foo": "foo"}, commit="first")

    dvc.imp(os.fspath(git_dir), "foo")
    assert (tmp_dir / "foo.dvc").exists()

    with git_dir.chdir(), git_dir.branch("branch1", new=True):
        git_dir.scm_gen({"foo": "foobar"}, commit="branch1 commit")
        branch1_head = git_dir.scm.get_rev()

    with git_dir.chdir(), git_dir.branch("branch2", new=True):
        git_dir.scm_gen({"foo": "foobar foo"}, commit="branch2 commit")
        branch2_head = git_dir.scm.get_rev()

    stage = dvc.update(["foo.dvc"], rev="branch1")[0]
    assert stage.deps[0].def_repo == {
        "url": os.fspath(git_dir),
        "rev": "branch1",
        "rev_lock": branch1_head,
    }
    with open(tmp_dir / "foo", encoding="utf-8") as f:
        assert f.read() == "foobar"

    stage = dvc.update(["foo.dvc"], rev="branch2")[0]
    assert stage.deps[0].def_repo == {
        "url": os.fspath(git_dir),
        "rev": "branch2",
        "rev_lock": branch2_head,
    }
    with open(tmp_dir / "foo", encoding="utf-8") as f:
        assert f.read() == "foobar foo"


def test_update_recursive(tmp_dir, dvc, erepo_dir):
    with erepo_dir.branch("branch", new=True), erepo_dir.chdir():
        erepo_dir.scm_gen(
            {"foo1": "text1", "foo2": "text2", "foo3": "text3"},
            commit="add foo files",
        )
        old_rev = erepo_dir.scm.get_rev()

    tmp_dir.gen({"dir": {"subdir": {}}})
    stage1 = dvc.imp(
        os.fspath(erepo_dir), "foo1", os.path.join("dir", "foo1"), rev="branch"
    )
    stage2 = dvc.imp(
        os.fspath(erepo_dir),
        "foo2",
        os.path.join("dir", "subdir", "foo2"),
        rev="branch",
    )
    stage3 = dvc.imp(
        os.fspath(erepo_dir),
        "foo3",
        os.path.join("dir", "subdir", "foo3"),
        rev="branch",
    )

    assert (tmp_dir / os.path.join("dir", "foo1")).read_text() == "text1"
    assert (tmp_dir / os.path.join("dir", "subdir", "foo2")).read_text() == "text2"
    assert (tmp_dir / os.path.join("dir", "subdir", "foo3")).read_text() == "text3"

    assert stage1.deps[0].def_repo["rev_lock"] == old_rev
    assert stage2.deps[0].def_repo["rev_lock"] == old_rev
    assert stage3.deps[0].def_repo["rev_lock"] == old_rev

    with erepo_dir.branch("branch", new=False), erepo_dir.chdir():
        erepo_dir.scm_gen(
            {"foo1": "updated1", "foo2": "updated2", "foo3": "updated3"},
            "",
            "update foo content",
        )
        new_rev = erepo_dir.scm.get_rev()

    assert old_rev != new_rev

    dvc.update(["dir"], recursive=True)

    stage1 = load_file(dvc, stage1.path).stage
    stage2 = load_file(dvc, stage2.path).stage
    stage3 = load_file(dvc, stage3.path).stage
    assert stage1.deps[0].def_repo["rev_lock"] == new_rev
    assert stage2.deps[0].def_repo["rev_lock"] == new_rev
    assert stage3.deps[0].def_repo["rev_lock"] == new_rev


@pytest.mark.parametrize("is_dvc", [True, False])
def test_update_from_subrepos(tmp_dir, dvc, erepo_dir, is_dvc):
    subrepo = erepo_dir / "subrepo"
    make_subrepo(subrepo, erepo_dir.scm)
    gen = subrepo.dvc_gen if is_dvc else subrepo.scm_gen
    with subrepo.chdir():
        gen("foo", "foo", commit="subrepo initial")

    path = os.path.join("subrepo", "foo")
    repo_path = os.fspath(erepo_dir)
    dvc.imp(repo_path, path, out="out")
    assert dvc.status() == {}

    with subrepo.chdir():
        gen("foo", "foobar", commit="subrepo second commit")

    assert dvc.status()["out.dvc"][0]["changed deps"] == {
        f"{path} ({repo_path})": "update available"
    }
    (stage,) = dvc.update(["out.dvc"])

    assert (tmp_dir / "out").read_text() == "foobar"
    assert stage.deps[0].def_path == os.path.join("subrepo", "foo")
    assert stage.deps[0].def_repo == {
        "url": repo_path,
        "rev_lock": erepo_dir.scm.get_rev(),
    }


def test_update_import_to_remote(tmp_dir, dvc, erepo_dir, local_remote):
    erepo_dir.scm_gen({"foo": "foo"}, commit="add foo")
    stage = dvc.imp(os.fspath(erepo_dir), "foo")
    erepo_dir.scm_gen({"foo": "bar"}, commit="update foo")
    with pytest.raises(InvalidArgumentError):
        dvc.update(stage.path, to_remote=True)


def test_update_import_url_to_remote(tmp_dir, dvc, workspace, local_remote):
    workspace.gen("foo", "foo")
    stage = dvc.imp_url("remote://workspace/foo", to_remote=True)

    workspace.gen("foo", "bar")
    stage = dvc.update(stage.path, to_remote=True)

    dvc.pull("foo")
    assert (tmp_dir / "foo").read_text() == "bar"


def test_update_import_url_to_remote_directory(
    mocker, tmp_dir, dvc, workspace, local_remote
):
    workspace.gen({"data": {"foo": "foo", "bar": {"baz": "baz"}}})
    stage = dvc.imp_url("remote://workspace/data", to_remote=True)

    workspace.gen(
        {
            "data": {
                "foo2": "foo2",
                "bar": {"baz2": "baz2"},
                "repeated_hashes": {
                    "foo": "foo",
                    "baz": "baz",
                    "foo_with_different_name": "foo",
                },
            }
        }
    )

    stage = dvc.update(stage.path, to_remote=True)

    dvc.pull("data")
    assert (tmp_dir / "data").read_text() == {
        "foo": "foo",
        "foo2": "foo2",
        "bar": {"baz": "baz", "baz2": "baz2"},
        "repeated_hashes": {
            "foo": "foo",
            "baz": "baz",
            "foo_with_different_name": "foo",
        },
    }


def test_update_import_url_to_remote_directory_changed_contents(
    tmp_dir, dvc, local_workspace, local_remote
):
    local_workspace.gen({"data": {"foo": "foo", "bar": {"baz": "baz"}}})
    stage = dvc.imp_url("remote://workspace/data", to_remote=True)

    local_workspace.gen(
        {"data": {"foo": "not_foo", "foo2": "foo", "bar": {"baz2": "baz2"}}}
    )
    stage = dvc.update(stage.path, to_remote=True)

    dvc.pull("data")
    assert (tmp_dir / "data").read_text() == {
        "foo": "not_foo",
        "foo2": "foo",
        "bar": {"baz": "baz", "baz2": "baz2"},
    }


def test_update_import_url_to_remote_directory_same_hash(
    tmp_dir, dvc, local_workspace, local_remote
):
    local_workspace.gen({"data": {"foo": "foo", "bar": {"baz": "baz"}, "same": "same"}})
    stage = dvc.imp_url("remote://workspace/data", to_remote=True)

    local_workspace.gen({"data": {"foo": "baz", "bar": {"baz": "foo"}, "same": "same"}})
    stage = dvc.update(stage.path, to_remote=True)

    dvc.pull("data")
    assert (tmp_dir / "data").read_text() == {
        "foo": "baz",
        "bar": {"baz": "foo"},
        "same": "same",
    }




tests/func/test_used_objs.py
import json
import os

import pytest


@pytest.mark.parametrize(
    "stage_wdir, cwd, target",
    [
        (os.curdir, os.curdir, "foo"),
        (os.curdir, os.curdir, "train"),
        (os.curdir, os.curdir, "dvc.yaml:train"),
        (os.curdir, "sub", os.path.join(os.pardir, "foo")),
        (
            os.curdir,
            "sub",
            os.path.join(os.pardir, "dvc.yaml:train"),
        ),
        ("sub", os.curdir, os.path.join("sub", "foo")),
        ("sub", os.curdir, os.path.join("sub", "dvc.yaml:train")),
        ("sub", "sub", "foo"),
        ("sub", "sub", "train"),
        ("sub", "sub", "dvc.yaml:train"),
        ("sub", "dir", os.path.join(os.pardir, "sub", "foo")),
        (
            "sub",
            "dir",
            os.path.join(os.pardir, "sub", "dvc.yaml:train"),
        ),
    ],
)
def test_from_gitfs_when_pwd_not_in_root(tmp_dir, scm, dvc, stage_wdir, cwd, target):
    path = tmp_dir.joinpath(stage_wdir).resolve()
    path.mkdir(parents=True, exist_ok=True)
    wdir = tmp_dir.joinpath(cwd).resolve()
    wdir.mkdir(parents=True, exist_ok=True)

    (path / "dvc.yaml").write_text(
        json.dumps({"stages": {"train": {"cmd": "echo foo > foo", "outs": ["foo"]}}})
    )
    path.gen({"foo": "foo"})
    dvc.commit(None, force=True)
    tmp_dir.scm_add(
        [path / file for file in ("dvc.yaml", "dvc.lock", ".gitignore")],
        commit="add files",
    )

    with wdir.chdir():
        assert dvc.used_objs([target], revs=[scm.get_rev()])


def test_used_objs_push(tmp_dir, scm, dvc):
    stage = tmp_dir.dvc_gen("foo", "foo")[0]
    hash_info = stage.outs[0].hash_info

    stage.outs[0].can_push = True
    assert stage.get_used_objs(push=False) == {None: {hash_info}}
    assert stage.get_used_objs(push=True) == {None: {hash_info}}

    stage.outs[0].can_push = False
    assert stage.get_used_objs(push=False) == {None: {hash_info}}
    assert stage.get_used_objs(push=True) == {}




tests/func/test_utils.py
import re

import pytest

from dvc import utils
from dvc.exceptions import DvcException


def test_dict_md5():
    d = {
        "cmd": "python code.py foo file1",
        "locked": "true",
        "outs": [
            {
                "path": "file1",
                "metric": {"type": "raw"},
                "cache": False,
                "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
            }
        ],
        "deps": [
            {"path": "foo", "md5": "acbd18db4cc2f85cedef654fccc4a4d8"},
            {"path": "code.py", "md5": "d05447644b89960913c7eee5fd776adb"},
        ],
    }

    md5 = "8b263fa05ede6c3145c164829be694b4"

    assert md5 == utils.dict_md5(d, exclude=["metric", "locked"])


def test_boxify():
    expected = (
        "+-----------------+\n"
        "|                 |\n"
        "|     message     |\n"
        "|                 |\n"
        "+-----------------+\n"
    )

    assert expected == utils.boxify("message")


def test_glob_no_match():
    with pytest.raises(
        DvcException, match=re.escape("Glob ['invalid*'] has no matches.")
    ):
        utils.glob_targets(["invalid*"], glob=True)




tests/func/test_version.py
import re

from dvc.cli import main
from tests.unit.test_info import (
    DVC_VERSION_REGEX,
    PYTHON_VERSION_REGEX,
    SUBPROJECTS,
    find_supported_remotes,
)


def test_(tmp_dir, dvc, scm, capsys):
    assert main(["version"]) == 0

    out, _ = capsys.readouterr()
    assert re.search(rf"DVC version: {DVC_VERSION_REGEX}", out)
    assert re.search(f"Platform: {PYTHON_VERSION_REGEX} on .*", out)
    for subproject in SUBPROJECTS:
        assert re.search(rf"{subproject} = .*", out)

    assert find_supported_remotes(out)
    assert re.search(r"Cache types: .*", out)
    assert re.search(r"Caches: local", out)
    assert re.search(r"Remotes: None", out)
    assert "Repo: dvc, git" in out


def test_import_error(tmp_dir, dvc, scm, capsys, monkeypatch):
    import importlib.metadata as importlib_metadata

    original = importlib_metadata.version

    def _import_error(name):
        if name == "dvclive":
            raise ImportError
        return original(name)

    monkeypatch.setattr(importlib_metadata, "version", _import_error)
    assert main(["version"]) == 0

    out, _ = capsys.readouterr()

    for subproject in SUBPROJECTS:
        match = re.search(rf"{subproject} = {DVC_VERSION_REGEX}", out)
        if subproject != "dvclive":
            assert match
        else:
            assert match is None




tests/func/test_virtual_directory.py
import shutil
from os.path import join

from dvc_data.hashfile.hash_info import HashInfo
from dvc_data.hashfile.meta import Meta


def test_virtual_add(tmp_dir, dvc, remote):
    tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})

    (stage,) = dvc.add("dir")
    out = stage.outs[0]

    assert out.hash_info == HashInfo(
        name="md5", value="5ea40360f5b4ec688df672a4db9c17d1.dir"
    )
    assert out.meta == Meta(isdir=True, size=6, nfiles=2)

    assert dvc.push() == 3
    dvc.cache.local.clear()

    tmp_dir.gen(
        {"dir": {"foobar": "foobar", "lorem": "ipsum", "subdir": {"file": "file"}}}
    )
    (stage,) = dvc.add("dir/foobar")

    out = stage.outs[0]
    assert out.hash_info == HashInfo(
        name="md5", value="a5beca056acbef9e0013347efdc2b751.dir"
    )
    assert out.meta == Meta(isdir=True, size=12, nfiles=3)
    assert dvc.push() == 2

    (stage,) = dvc.add("dir/subdir")
    out = stage.outs[0]
    assert out.hash_info == HashInfo(
        name="md5", value="de78e9fff7c3478c6b316bf08437d0f6.dir"
    )
    assert out.meta == Meta(isdir=True, size=16, nfiles=4)
    assert dvc.push() == 2


def test_virtual_remove(tmp_dir, dvc, remote):
    tmp_dir.gen(
        {
            "dir": {
                "foo": "foo",
                "bar": "bar",
                "subdir": {"lorem": "lorem", "ipsum": "ipsum"},
            }
        }
    )

    (stage,) = dvc.add("dir")
    out = stage.outs[0]

    assert out.hash_info == HashInfo(
        name="md5", value="15b0e3c73ad2c748ce206988cb6b7319.dir"
    )
    assert out.meta == Meta(isdir=True, size=16, nfiles=4)

    assert dvc.push() == 5
    dvc.cache.local.clear()

    (tmp_dir / "dir" / "foo").unlink()
    (stage,) = dvc.add("dir/foo")

    out = stage.outs[0]
    assert out.hash_info == HashInfo(
        name="md5", value="991ea7d558d320d8817a0798e9c676f1.dir"
    )
    assert out.meta == Meta(isdir=True, size=None, nfiles=3)

    assert dvc.push() == 1

    shutil.rmtree(tmp_dir / "dir" / "subdir")
    (stage,) = dvc.add("dir/subdir")

    out = stage.outs[0]
    assert out.hash_info == HashInfo(
        name="md5", value="91aaa9bb58b657d623ef143b195a67e4.dir"
    )
    assert out.meta == Meta(isdir=True, size=None, nfiles=1)
    assert dvc.push() == 1


def test_virtual_update_dir(tmp_dir, dvc, remote):
    tmp_dir.gen({"dir": {"foo": "foo", "subdir": {"lorem": "lorem"}}})
    (stage,) = dvc.add("dir")
    out = stage.outs[0]

    assert out.hash_info == HashInfo(
        name="md5", value="22a16c9bf84b3068bc2206d88a6b5776.dir"
    )
    assert out.meta == Meta(isdir=True, size=8, nfiles=2)

    assert dvc.push() == 3
    dvc.cache.local.clear()
    shutil.rmtree("dir")

    tmp_dir.gen({"dir": {"subdir": {"ipsum": "lorem ipsum", "file": "file"}}})
    (stage,) = dvc.add("dir/subdir")

    out = stage.outs[0]
    assert out.hash_info == HashInfo(
        name="md5", value="32f5734ea1a2aa1a067c0c15f0ae5781.dir"
    )
    assert out.meta == Meta(isdir=True, size=None, nfiles=3)
    assert dvc.push() == 3


def test_virtual_update_file(tmp_dir, dvc, remote):
    tmp_dir.gen({"dir": {"foo": "foo", "subdir": {"lorem": "lorem"}}})
    (stage,) = dvc.add("dir")
    out = stage.outs[0]

    assert out.hash_info == HashInfo(
        name="md5", value="22a16c9bf84b3068bc2206d88a6b5776.dir"
    )
    assert out.meta == Meta(isdir=True, size=8, nfiles=2)

    assert dvc.push() == 3
    dvc.cache.local.clear()
    shutil.rmtree("dir")

    tmp_dir.gen({"dir": {"foo": "foobar"}})
    (stage,) = dvc.add("dir/foo")
    out = stage.outs[0]
    assert out.hash_info == HashInfo(
        name="md5", value="49408ac059c76086a3a892129a324b60.dir"
    )
    assert out.meta == Meta(isdir=True, size=None, nfiles=2)
    assert dvc.push() == 2


def test_virtual_update_noop(tmp_dir, dvc, remote):
    tmp_dir.gen({"dir": {"foo": "foo", "subdir": {"lorem": "lorem"}}})

    (stage,) = dvc.add("dir")
    out = stage.outs[0]
    hash_info = HashInfo(name="md5", value="22a16c9bf84b3068bc2206d88a6b5776.dir")
    meta = Meta(isdir=True, size=8, nfiles=2)

    assert out.hash_info == hash_info
    assert out.meta == meta
    assert dvc.push() == 3

    dvc.cache.local.clear()
    shutil.rmtree("dir")

    tmp_dir.gen({"dir": {"foo": "foo", "subdir": {"lorem": "lorem"}}})

    (stage,) = dvc.add("dir/foo")
    out = stage.outs[0]
    assert out.hash_info == hash_info
    assert out.meta == meta
    assert not dvc.push()

    dvc.cache.local.clear()

    (stage,) = dvc.add("dir/subdir")
    out = stage.outs[0]
    assert out.hash_info == hash_info
    assert out.meta == meta
    assert not dvc.push()


def test_partial_checkout_and_update(M, tmp_dir, dvc, remote):
    tmp_dir.gen({"dir": {"foo": "foo", "subdir": {"lorem": "lorem"}}})

    (stage,) = dvc.add("dir")
    out = stage.outs[0]

    assert out.hash_info == HashInfo(
        name="md5", value="22a16c9bf84b3068bc2206d88a6b5776.dir"
    )
    assert out.meta == Meta(isdir=True, size=8, nfiles=2)

    assert dvc.push() == 3
    dvc.cache.local.clear()
    shutil.rmtree("dir")

    assert dvc.pull("dir/subdir") == M.dict(
        added=[join("dir", "")],
        fetched=1,
    )
    assert (tmp_dir / "dir").read_text() == {"subdir": {"lorem": "lorem"}}

    tmp_dir.gen({"dir": {"subdir": {"ipsum": "ipsum"}}})
    (stage,) = dvc.add("dir/subdir/ipsum")

    out = stage.outs[0]
    assert out.hash_info == HashInfo(
        name="md5", value="06d953a10e0b0ffacba04876a9351e39.dir"
    )
    assert out.meta == Meta(isdir=True, size=13, nfiles=3)
    assert dvc.push() == 2




tests/func/api/__init__.py




tests/func/api/test_data.py
import os

import pytest
from funcy import first, get_in

from dvc import api
from dvc.exceptions import OutputNotFoundError, PathMissingError
from dvc.testing.api_tests import TestAPI  # noqa, pylint: disable=unused-import
from dvc.testing.tmp_dir import make_subrepo
from dvc.utils.fs import remove


def test_get_url_external(tmp_dir, erepo_dir, cloud):
    erepo_dir.add_remote(config=cloud.config)
    with erepo_dir.chdir():
        erepo_dir.dvc_gen("foo", "foo", commit="add foo")

    # Using file url to force clone to tmp repo
    repo_url = f"file://{erepo_dir.as_posix()}"
    expected_url = (cloud / "ac/bd18db4cc2f85cedef654fccc4a4d8").url
    assert api.get_url("foo", repo=repo_url) == expected_url


def test_get_url_requires_dvc(tmp_dir, scm):
    tmp_dir.scm_gen({"foo": "foo"}, commit="initial")

    with pytest.raises(OutputNotFoundError, match="output 'foo'"):
        api.get_url("foo", repo=os.fspath(tmp_dir))

    with pytest.raises(OutputNotFoundError, match="output 'foo'"):
        api.get_url("foo", repo=f"file://{tmp_dir.as_posix()}")


def test_open_external(tmp_dir, erepo_dir, cloud):
    erepo_dir.add_remote(config=cloud.config)

    with erepo_dir.chdir():
        erepo_dir.dvc_gen("version", "master", commit="add version")

        with erepo_dir.branch("branch", new="True"):
            # NOTE: need file to be other size for Mac
            erepo_dir.dvc_gen("version", "branchver", commit="add version")

    erepo_dir.dvc.push(all_branches=True)

    # Remove cache to force download
    remove(erepo_dir.dvc.cache.local.path)

    # Using file url to force clone to tmp repo
    repo_url = f"file://{erepo_dir.as_posix()}"
    with api.open("version", repo=repo_url) as fd:
        assert fd.read() == "master"

    assert api.read("version", repo=repo_url, rev="branch") == "branchver"


def test_open_granular(tmp_dir, dvc, remote):
    tmp_dir.dvc_gen({"dir": {"foo": "foo-text"}})
    dvc.push()

    # Remove cache to force download
    remove(dvc.cache.local.path)

    with api.open("dir/foo") as fd:
        assert fd.read() == "foo-text"


def test_missing(tmp_dir, dvc, remote):
    tmp_dir.dvc_gen("foo", "foo")

    # Remove cache to make foo missing
    remove(dvc.cache.local.path)

    api.read("foo")

    remove("foo")

    with pytest.raises(PathMissingError):
        api.read("foo")


def test_open_scm_controlled(tmp_dir, erepo_dir):
    erepo_dir.scm_gen({"scm_controlled": "file content"}, commit="create file")

    with api.open("scm_controlled", repo=os.fspath(erepo_dir)) as fd:
        assert fd.read() == "file content"


def test_open_not_cached(dvc):
    metric_file = "metric.txt"
    metric_content = "0.6"
    metric_code = f"open('{metric_file}', 'w').write('{metric_content}')"
    dvc.run(
        single_stage=True,
        metrics_no_cache=[metric_file],
        cmd=f'python -c "{metric_code}"',
    )

    with api.open(metric_file) as fd:
        assert fd.read() == metric_content

    os.remove(metric_file)
    with pytest.raises(PathMissingError):
        api.read(metric_file)


def test_open_rev(tmp_dir, scm, dvc):
    tmp_dir.scm_gen("foo", "foo", commit="foo")

    (tmp_dir / "foo").write_text("bar")

    with api.open("foo", rev="master") as fobj:
        assert fobj.read() == "foo"


@pytest.mark.parametrize("as_external", [True, False])
@pytest.mark.parametrize(
    "files, to_read",
    [
        ({"foo": "foo"}, "foo"),
        ({"dir": {"foo": "foo", "bar": "bar"}}, os.path.join("dir", "foo")),
    ],
    ids=["file", "inside-dir"],
)
def test_api_missing_local_cache_exists_on_remote(
    tmp_dir, scm, dvc, as_external, remote, files, to_read
):
    tmp_dir.dvc_gen(files, commit="DVC track files")
    dvc.push()

    # Remove cache to make foo missing
    remove(dvc.cache.local.path)
    remove(first(files))

    repo_url = f"file://{tmp_dir.as_posix()}" if as_external else None
    file_content = get_in(files, to_read.split(os.sep))
    assert api.read(to_read, repo=repo_url) == file_content


@pytest.mark.parametrize("local_repo", [False, True])
def test_read_with_subrepos(tmp_dir, scm, local_cloud, local_repo):
    tmp_dir.scm_gen("foo.txt", "foo.txt", commit="add foo.txt")
    subrepo = tmp_dir / "dir" / "subrepo"
    make_subrepo(subrepo, scm, config=local_cloud.config)
    with subrepo.chdir():
        subrepo.scm_gen({"lorem": "lorem"}, commit="add lorem")
        subrepo.dvc_gen({"dir": {"file.txt": "file.txt"}}, commit="add dir")
        subrepo.dvc_gen("dvc-file", "dvc-file", commit="add dir")
        subrepo.dvc.push()

    repo_path = None if local_repo else f"file://{tmp_dir.as_posix()}"
    subrepo_path = os.path.join("dir", "subrepo")

    assert api.read("foo.txt", repo=repo_path) == "foo.txt"
    assert api.read(os.path.join(subrepo_path, "lorem"), repo=repo_path) == "lorem"
    assert (
        api.read(os.path.join(subrepo_path, "dvc-file"), repo=repo_path) == "dvc-file"
    )
    assert (
        api.read(os.path.join(subrepo_path, "dir", "file.txt"), repo=repo_path)
        == "file.txt"
    )


def test_get_url_granular(tmp_dir, dvc, cloud):
    tmp_dir.add_remote(config=cloud.config)
    tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar", "nested": {"file": "file"}}})

    expected_url = (cloud / "5f" / "c28ea78987408341668eba6525ebd1.dir").url
    assert api.get_url("dir") == expected_url

    expected_url = (cloud / "ac" / "bd18db4cc2f85cedef654fccc4a4d8").url
    assert api.get_url("dir/foo") == expected_url

    expected_url = (cloud / "37" / "b51d194a7513e45b56f6524f2d51f2").url
    assert api.get_url("dir/bar") == expected_url

    expected_url = (cloud / "8c" / "7dd922ad47494fc02c388e12c00eac").url
    assert api.get_url(os.path.join("dir", "nested", "file")) == expected_url


def test_get_url_subrepos(tmp_dir, scm, local_cloud):
    subrepo = tmp_dir / "subrepo"
    make_subrepo(subrepo, scm, config=local_cloud.config)
    with subrepo.chdir():
        subrepo.dvc_gen({"dir": {"foo": "foo"}, "bar": "bar"}, commit="add files")
        subrepo.dvc.push()

    expected_url = os.fspath(local_cloud / "ac" / "bd18db4cc2f85cedef654fccc4a4d8")
    assert api.get_url(os.path.join("subrepo", "dir", "foo")) == expected_url

    expected_url = os.fspath(local_cloud / "37" / "b51d194a7513e45b56f6524f2d51f2")
    assert api.get_url("subrepo/bar") == expected_url


def test_open_from_remote(tmp_dir, erepo_dir, cloud, local_cloud):
    erepo_dir.add_remote(config=cloud.config, name="other")
    erepo_dir.add_remote(config=local_cloud.config, default=True)
    erepo_dir.dvc_gen({"dir": {"foo": "foo content"}}, commit="create file")
    erepo_dir.dvc.push(remote="other")
    remove(erepo_dir.dvc.cache.local.path)

    with api.open(
        os.path.join("dir", "foo"),
        repo=f"file://{erepo_dir.as_posix()}",
        remote="other",
    ) as fd:
        assert fd.read() == "foo content"




tests/func/api/test_experiments.py
import pytest

from dvc import api
from dvc.repo.experiments.exceptions import ExperimentExistsError
from tests.unit.repo.experiments.conftest import exp_stage  # noqa: F401


def test_exp_save(tmp_dir, dvc, scm):
    tmp_dir.scm_gen({"foo": "foo"}, commit="initial")

    api.exp_save()

    api.exp_save("foo")
    with pytest.raises(
        ExperimentExistsError,
        match="Experiment conflicts with existing experiment 'foo'.",
    ):
        api.exp_save("foo")
    api.exp_save("foo", force=True)


def test_exp_show(tmp_dir, dvc, scm, exp_stage):  # noqa: F811
    exps = api.exp_show()

    assert len(exps) == 2
    assert isinstance(exps, list)
    assert isinstance(exps[0], dict)
    assert isinstance(exps[1], dict)
    # Postprocessing casting to float
    assert exps[0]["metrics.yaml:foo"] == 1.0
    # Postprocessing using `None` as fill value
    assert exps[0]["State"] is None
    # Postprocessing empty string as `None`
    assert exps[0]["Experiment"] is None



tests/func/api/test_scm.py
from dvc.api.scm import all_branches, all_commits, all_tags


def test_all_branches(tmp_dir, scm, dvc):
    assert all_branches() == ["master"]

    with tmp_dir.branch("branch", new=True):
        tmp_dir.scm_gen("branch", "branch", "commit")

    assert all_branches() == ["branch", "master"]


def test_all_commits(tmp_dir, scm, dvc):
    first = scm.get_rev()
    assert all_commits() == [first]

    tmp_dir.scm_gen("foo", "foo", "commit")
    second = scm.get_rev()

    assert set(all_commits()) == {first, second}


def test_all_tags(tmp_dir, scm, dvc):
    scm.tag("v1")
    assert all_tags() == ["v1"]

    tmp_dir.scm_gen("foo", "foo", "commit")
    scm.tag("v2")

    assert set(all_tags()) == {"v1", "v2"}




tests/func/api/test_show.py
import json
import os
from textwrap import dedent
from typing import Dict, List

import pytest

from dvc import api

TRAIN_METRICS: List[Dict[str, Dict[str, float]]] = [
    {
        "avg_prec": {"train": 0.85, "val": 0.75},
        "roc_auc": {"train": 0.80, "val": 0.70},
    },
    {
        "avg_prec": {"train": 0.97, "val": 0.92},
        "roc_auc": {"train": 0.98, "val": 0.94},
    },
]
TEST_METRICS: List[Dict[str, Dict[str, float]]] = [
    {"avg_prec": {"test": 0.72}, "roc_auc": {"test": 0.77}},
    {
        "avg_prec": {"test": 0.91},
        "roc_auc": {"test": 0.92},
    },
]


@pytest.fixture
def params_repo(tmp_dir, scm, dvc):
    tmp_dir.gen("params.yaml", "foo: 1")
    tmp_dir.gen("params.json", '{"bar": 2, "foobar": 3}')
    tmp_dir.gen("other_params.json", '{"foo": {"bar": 4}}')

    dvc.run(
        name="stage-0",
        cmd="echo stage-0",
    )

    dvc.run(
        name="stage-1",
        cmd="echo stage-1",
        params=["foo", "params.json:bar"],
    )

    dvc.run(
        name="stage-2",
        cmd="echo stage-2",
        params=["other_params.json:foo"],
    )

    dvc.run(
        name="stage-3",
        cmd="echo stage-2",
        params=["params.json:foobar"],
    )

    scm.add(
        [
            "params.yaml",
            "params.json",
            "other_params.json",
            "dvc.yaml",
            "dvc.lock",
        ]
    )
    scm.commit("commit dvc files")

    tmp_dir.gen("params.yaml", "foo: 5")
    scm.add(["params.yaml"])
    scm.commit("update params.yaml")


@pytest.fixture
def metrics_repo(tmp_dir, scm, dvc, run_copy_metrics):
    dvc.run(name="prepare", cmd="echo preparing data")
    scm.add(["dvc.yaml", "dvc.lock"])
    scm.commit("prepare data")
    sub_dir = tmp_dir / "eval"
    sub_dir.mkdir()
    tmp_dir.gen(
        "tmp_train_val_metrics.json",
        json.dumps(TRAIN_METRICS[0]),
    )
    train_metrics_file = os.path.join(sub_dir, "train_val_metrics.json")
    run_copy_metrics(
        "tmp_train_val_metrics.json",
        train_metrics_file,
        name="train",
        metrics_no_cache=[train_metrics_file],
    )
    (tmp_dir / "tmp_train_val_metrics.json").unlink()

    scm.add(["dvc.yaml", "dvc.lock", train_metrics_file])
    scm.commit("train model")

    test_metrics_file = os.path.join(sub_dir, "test_metrics.json")
    tmp_dir.gen("tmp_test_metrics.json", json.dumps(TEST_METRICS[0]))
    run_copy_metrics(
        "tmp_test_metrics.json",
        test_metrics_file,
        name="test",
        metrics_no_cache=[test_metrics_file],
    )
    (tmp_dir / "tmp_test_metrics.json").unlink()

    scm.add(["dvc.yaml", "dvc.lock", test_metrics_file])
    scm.commit("test model")

    with tmp_dir.branch("better-model", new=True):
        tmp_dir.gen(
            "tmp_train_val_metrics.json",
            json.dumps(TRAIN_METRICS[1]),
        )
        run_copy_metrics(
            "tmp_train_val_metrics.json",
            train_metrics_file,
            name="train",
            metrics_no_cache=[train_metrics_file],
        )
        (tmp_dir / "tmp_train_val_metrics.json").unlink()

        scm.add(["dvc.yaml", "dvc.lock", train_metrics_file])
        scm.commit("train better model")

        tmp_dir.gen("tmp_test_metrics.json", json.dumps(TEST_METRICS[1]))
        run_copy_metrics(
            "tmp_test_metrics.json",
            test_metrics_file,
            name="test",
            metrics_no_cache=[test_metrics_file],
        )
        (tmp_dir / "tmp_test_metrics.json").unlink()

        scm.add(["dvc.yaml", "dvc.lock", test_metrics_file])
        scm.commit("test better model")

    scm.checkout("master")

    return (
        os.path.relpath(train_metrics_file, tmp_dir),
        os.path.relpath(test_metrics_file, tmp_dir),
    )


def test_params_show_no_args(params_repo):
    assert api.params_show() == {
        "params.yaml:foo": 5,
        "bar": 2,
        "foobar": 3,
        "other_params.json:foo": {"bar": 4},
    }


def test_params_show_targets(params_repo):
    assert api.params_show("params.yaml") == {"foo": 5}
    assert api.params_show("params.yaml", "params.json") == {
        "foo": 5,
        "bar": 2,
        "foobar": 3,
    }
    assert api.params_show("params.yaml", stages="stage-1") == {
        "foo": 5,
    }


def test_params_show_deps(params_repo):
    params = api.params_show(deps=True)
    assert params == {
        "params.yaml:foo": 5,
        "bar": 2,
        "foobar": 3,
        "other_params.json:foo": {"bar": 4},
    }


def test_params_show_stages(params_repo):
    assert api.params_show(stages="stage-2") == {"foo": {"bar": 4}}

    assert api.params_show() == api.params_show(
        stages=["stage-1", "stage-2", "stage-3"]
    )

    assert api.params_show("params.json", stages="stage-3") == {"foobar": 3}

    assert api.params_show(stages="stage-0") == {}


def test_params_show_stage_addressing(tmp_dir, dvc):
    for subdir in {"subdir1", "subdir2"}:
        subdir = tmp_dir / subdir
        subdir.mkdir()
        with subdir.chdir():
            subdir.gen("params.yaml", "foo: 1")

            dvc.run(name="stage-0", cmd="echo stage-0", params=["foo"])

    for s in {"subdir1", "subdir2"}:
        dvcyaml = os.path.join(s, "dvc.yaml")
        assert api.params_show(stages=f"{dvcyaml}:stage-0") == {"foo": 1}

    with subdir.chdir():
        nested = subdir / "nested"
        nested.mkdir()
        with nested.chdir():
            dvcyaml = os.path.join("..", "dvc.yaml")
            assert api.params_show(stages=f"{dvcyaml}:stage-0") == {"foo": 1}


def test_params_show_revs(params_repo):
    assert api.params_show(rev="HEAD~1") == {
        "params.yaml:foo": 1,
        "bar": 2,
        "foobar": 3,
        "other_params.json:foo": {"bar": 4},
    }


def test_params_show_while_running_stage(tmp_dir, dvc):
    (tmp_dir / "params.yaml").dump({"foo": {"bar": 1}})
    (tmp_dir / "params.json").dump({"bar": 2})

    tmp_dir.gen(
        "merge.py",
        dedent(
            """
            import json
            from dvc import api
            with open("merged.json", "w") as f:
                json.dump(api.params_show(stages="merge"), f)
        """
        ),
    )
    dvc.stage.add(
        name="merge",
        cmd="python merge.py",
        params=["foo.bar", {"params.json": ["bar"]}],
        outs=["merged.json"],
    )

    dvc.reproduce()

    assert (tmp_dir / "merged.json").parse() == {"foo": {"bar": 1}, "bar": 2}


def test_params_show_repo(tmp_dir, erepo_dir):
    with erepo_dir.chdir():
        erepo_dir.scm_gen("params.yaml", "foo: 1", commit="Create params.yaml")
        erepo_dir.dvc.run(
            name="stage-1",
            cmd="echo stage-1",
            params=["foo"],
        )
    assert api.params_show(repo=erepo_dir) == {"foo": 1}


def test_params_show_no_params_found(tmp_dir, dvc):
    # Empty repo
    assert api.params_show() == {}

    # params.yaml but no dvc.yaml
    (tmp_dir / "params.yaml").dump({"foo": 1})
    assert api.params_show() == {"foo": 1}

    # dvc.yaml but no params.yaml
    (tmp_dir / "params.yaml").unlink()
    dvc.stage.add(name="echo", cmd="echo foo")
    assert api.params_show() == {}


def test_params_show_stage_without_params(tmp_dir, dvc):
    tmp_dir.gen("params.yaml", "foo: 1")

    dvc.run(
        name="stage-0",
        cmd="echo stage-0",
    )

    assert api.params_show(stages="stage-0") == {}

    assert api.params_show(deps=True) == {}


def test_params_show_untracked_target(params_repo, tmp_dir):
    tmp_dir.gen("params_foo.yaml", "foo: 1")

    assert api.params_show("params_foo.yaml") == {"foo": 1}

    assert api.params_show("params_foo.yaml", stages="stage-0") == {}

    assert api.params_show("params_foo.yaml", deps=True) == {}


def test_metrics_show_no_args(metrics_repo):
    train_metrics_file, test_metrics_file = metrics_repo
    assert api.metrics_show() == {
        f"{train_metrics_file}:avg_prec": TRAIN_METRICS[0]["avg_prec"],
        f"{train_metrics_file}:roc_auc": TRAIN_METRICS[0]["roc_auc"],
        f"{test_metrics_file}:avg_prec": TEST_METRICS[0]["avg_prec"],
        f"{test_metrics_file}:roc_auc": TEST_METRICS[0]["roc_auc"],
    }


def test_metrics_show_targets(metrics_repo):
    train_metrics_file, test_metrics_file = metrics_repo
    assert api.metrics_show(train_metrics_file) == TRAIN_METRICS[0]
    assert api.metrics_show(test_metrics_file) == TEST_METRICS[0]
    assert api.metrics_show(train_metrics_file, test_metrics_file) == {
        f"{train_metrics_file}:avg_prec": TRAIN_METRICS[0]["avg_prec"],
        f"{train_metrics_file}:roc_auc": TRAIN_METRICS[0]["roc_auc"],
        f"{test_metrics_file}:avg_prec": TEST_METRICS[0]["avg_prec"],
        f"{test_metrics_file}:roc_auc": TEST_METRICS[0]["roc_auc"],
    }


def test_metrics_show_no_metrics_found(tmp_dir, dvc):
    # Empty repo
    assert api.metrics_show() == {}

    # dvc.yaml but no metrics
    dvc.stage.add(name="echo", cmd="echo foo")
    assert api.metrics_show() == {}


def test_metrics_show_rev_without_metrics(metrics_repo):
    assert api.metrics_show(rev="HEAD~2") == {}


def test_metrics_show_rev_with_metrics(metrics_repo):
    train_metrics_file, test_metrics_file = metrics_repo
    assert api.metrics_show(rev="HEAD~1") == TRAIN_METRICS[0]
    assert api.metrics_show(rev="HEAD") == {
        f"{train_metrics_file}:avg_prec": TRAIN_METRICS[0]["avg_prec"],
        f"{train_metrics_file}:roc_auc": TRAIN_METRICS[0]["roc_auc"],
        f"{test_metrics_file}:avg_prec": TEST_METRICS[0]["avg_prec"],
        f"{test_metrics_file}:roc_auc": TEST_METRICS[0]["roc_auc"],
    }
    assert api.metrics_show(rev="better-model~1") == {
        f"{train_metrics_file}:avg_prec": TRAIN_METRICS[1]["avg_prec"],
        f"{train_metrics_file}:roc_auc": TRAIN_METRICS[1]["roc_auc"],
        f"{test_metrics_file}:avg_prec": TEST_METRICS[0]["avg_prec"],
        f"{test_metrics_file}:roc_auc": TEST_METRICS[0]["roc_auc"],
    }
    assert api.metrics_show(rev="better-model") == {
        f"{train_metrics_file}:avg_prec": TRAIN_METRICS[1]["avg_prec"],
        f"{train_metrics_file}:roc_auc": TRAIN_METRICS[1]["roc_auc"],
        f"{test_metrics_file}:avg_prec": TEST_METRICS[1]["avg_prec"],
        f"{test_metrics_file}:roc_auc": TEST_METRICS[1]["roc_auc"],
    }


def test_metrics_show_dirty_working_dir(metrics_repo, tmp_dir):
    train_metrics_file, test_metrics_file = metrics_repo
    new_metrics = {"acc": 1}
    (tmp_dir / train_metrics_file).unlink()
    (tmp_dir / train_metrics_file).dump(new_metrics)
    (tmp_dir / test_metrics_file).unlink()
    (tmp_dir / test_metrics_file).dump(new_metrics)

    assert api.metrics_show() == {
        f"{train_metrics_file}:acc": new_metrics["acc"],
        f"{test_metrics_file}:acc": new_metrics["acc"],
    }




tests/func/artifacts/__init__.py




tests/func/artifacts/test_artifacts.py
import os

import pytest

from dvc.annotations import Artifact
from dvc.exceptions import InvalidArgumentError
from dvc.repo.artifacts import name_is_compatible
from dvc.utils.strictyaml import YAMLSyntaxError, YAMLValidationError

dvcyaml = {
    "artifacts": {
        "myart": {"type": "model", "path": "myart.pkl"},
        "hello": {"type": "file", "path": "hello.txt"},
        "world": {
            "type": "object",
            "path": "world.txt",
            "desc": "The world is not enough",
            "labels": ["but", "this", "is"],
            "meta": {"such": "a", "perfect": "place to start"},
        },
    }
}


def test_artifacts_read_subdir(tmp_dir, dvc):
    (tmp_dir / "dvc.yaml").dump(dvcyaml)

    subdir = tmp_dir / "subdir"
    subdir.mkdir()

    (subdir / "dvc.yaml").dump(dvcyaml)

    artifacts = {
        name: Artifact(**values) for name, values in dvcyaml["artifacts"].items()
    }
    assert tmp_dir.dvc.artifacts.read() == {
        "dvc.yaml": artifacts,
        f"subdir{os.path.sep}dvc.yaml": artifacts,
    }


def test_artifacts_add_subdir(tmp_dir, dvc):
    subdir = tmp_dir / "subdir"
    subdir.mkdir()

    (subdir / "dvc.yaml").dump(dvcyaml)

    new_art = Artifact(path="path")
    tmp_dir.dvc.artifacts.add("new", new_art, dvcfile="subdir/dvc.yaml")

    artifacts = {
        name: Artifact(**values) for name, values in dvcyaml["artifacts"].items()
    }
    artifacts["new"] = new_art
    assert tmp_dir.dvc.artifacts.read() == {
        f"subdir{os.path.sep}dvc.yaml": artifacts,
    }


def test_artifacts_add_abspath(tmp_dir, dvc):
    subdir = tmp_dir / "subdir"
    subdir.mkdir()

    new_art = Artifact(path="path")
    tmp_dir.dvc.artifacts.add(
        "new", new_art, dvcfile=os.path.abspath("subdir/dvc.yaml")
    )

    assert tmp_dir.dvc.artifacts.read() == {
        f"subdir{os.path.sep}dvc.yaml": {"new": new_art},
    }


def test_artifacts_add_fails_on_dvc_subrepo(tmp_dir, dvc):
    # adding artifact to the DVC subrepo from the parent DVC repo
    # shouldn't work
    subdir = tmp_dir / "subdir"
    (subdir / ".dvc").mkdir(parents=True)

    with pytest.raises(InvalidArgumentError):
        tmp_dir.dvc.artifacts.add(
            "failing", Artifact(path="path"), dvcfile="subdir/dvc.yaml"
        )

    with pytest.raises(InvalidArgumentError):
        tmp_dir.dvc.artifacts.add(
            "failing", Artifact(path="path"), dvcfile="subdir/dvclive/dvc.yaml"
        )


bad_dvcyaml_extra_field = {
    "artifacts": {
        "lol": {"kek": "cheburek", "path": "lol"},
        "hello": {"type": "file", "path": "hello.txt"},
    }
}


bad_dvcyaml_missing_path = {
    "artifacts": {
        "lol": {},
    }
}


@pytest.mark.parametrize(
    "bad_dvcyaml", [bad_dvcyaml_extra_field, bad_dvcyaml_missing_path]
)
def test_broken_dvcyaml_extra_field(tmp_dir, dvc, bad_dvcyaml):
    (tmp_dir / "dvc.yaml").dump(bad_dvcyaml)

    with pytest.raises(YAMLValidationError):
        tmp_dir.dvc.artifacts.read()


bad_dvcyaml_id_duplication = """
artifacts:
  lol:
    type: kek
  lol: {}
"""


def test_artifacts_read_fails_on_id_duplication(tmp_dir, dvc):
    with open(tmp_dir / "dvc.yaml", "w") as f:
        f.write(bad_dvcyaml_id_duplication)

    with pytest.raises(YAMLSyntaxError):
        tmp_dir.dvc.artifacts.read()


@pytest.mark.parametrize(
    "name",
    [
        "1",
        "m",
        "nn",
        "m1",
        "1nn",
        "model-prod",
        "model-prod-v1",
    ],
)
def test_name_is_compatible(name):
    assert name_is_compatible(name)


@pytest.mark.parametrize(
    "name",
    [
        "",
        "m/",
        "/m",
        "###",
        "@@@",
        "a model",
        "a_model",
        "-model",
        "model-",
        "model@1",
        "model#1",
        "@namespace/model",
    ],
)
def test_name_is_compatible_fails(name):
    assert not name_is_compatible(name)




tests/func/data/__init__.py




tests/func/data/db/__init__.py




tests/func/data/db/test_index.py
import os

import pytest

from dvc.exceptions import DownloadError, UploadError
from dvc.utils.fs import remove
from dvc_data.hashfile.db import get_index


@pytest.fixture
def index(dvc, local_remote, mocker):
    odb = dvc.cloud.get_remote_odb("upstream")
    return get_index(odb)


def test_indexed_on_status(tmp_dir, dvc, index):
    foo = tmp_dir.dvc_gen({"foo": "foo content"})[0].outs[0]
    bar = tmp_dir.dvc_gen({"bar": {"baz": "baz content"}})[0].outs[0]
    baz_hash = bar.obj._trie.get(("baz",))[1]
    dvc.push()
    index.clear()

    dvc.status(cloud=True)
    assert {bar.hash_info.value, baz_hash.value} == set(index.hashes())
    assert [bar.hash_info.value] == list(index.dir_hashes())
    assert foo.hash_info.value not in index.hashes()


def test_indexed_on_push(tmp_dir, dvc, index):
    foo = tmp_dir.dvc_gen({"foo": "foo content"})[0].outs[0]
    bar = tmp_dir.dvc_gen({"bar": {"baz": "baz content"}})[0].outs[0]
    baz_hash = bar.obj._trie.get(("baz",))[1]

    dvc.push()
    assert {bar.hash_info.value, baz_hash.value} == set(index.hashes())
    assert [bar.hash_info.value] == list(index.dir_hashes())
    assert foo.hash_info.value not in index.hashes()


def test_indexed_dir_missing(tmp_dir, dvc, index):
    bar = tmp_dir.dvc_gen({"bar": {"baz": "baz content"}})[0].outs[0]
    index.update([bar.hash_info.value], [])
    dvc.status(cloud=True)
    assert not list(index.hashes())


def test_clear_on_gc(tmp_dir, dvc, index):
    (foo,) = tmp_dir.dvc_gen({"dir": {"foo": "foo content"}})
    dvc.push()
    dvc.remove(foo.relpath)

    assert list(index.hashes())
    dvc.gc(workspace=True, cloud=True)
    assert not list(index.hashes())


def test_clear_on_download_err(tmp_dir, dvc, index, mocker):
    out = tmp_dir.dvc_gen({"dir": {"foo": "foo content"}})[0].outs[0]
    dvc.push()

    for _, _, hi in out.obj:
        remove(dvc.cache.local.get(hi.value).path)
    remove(out.fs_path)

    assert list(index.hashes())

    def unreliable_download(_from_fs, from_info, _to_fs, to_info, **kwargs):
        on_error = kwargs["on_error"]
        assert on_error
        if isinstance(from_info, str):
            from_info = [from_info]
        if isinstance(to_info, str):
            to_info = [to_info]
        for from_i, to_i in zip(from_info, to_info):
            on_error(from_i, to_i, Exception())

    mocker.patch("dvc_objects.fs.generic.transfer", unreliable_download)
    with pytest.raises(DownloadError):
        dvc.pull()
    assert not list(index.hashes())


def test_partial_upload(tmp_dir, dvc, index, mocker):
    from dvc_objects.fs import generic

    tmp_dir.dvc_gen({"foo": "foo content"})
    baz = tmp_dir.dvc_gen({"bar": {"baz": "baz content"}})[0].outs[0]

    original = generic.transfer
    odb = dvc.cloud.get_remote_odb("upstream")

    def unreliable_upload(from_fs, from_info, to_fs, to_info, **kwargs):
        on_error = kwargs["on_error"]
        assert on_error
        if isinstance(from_info, str):
            from_info = [from_info]
        else:
            from_info = list(from_info)
        if isinstance(to_info, str):
            to_info = [to_info]
        else:
            to_info = list(to_info)
        for i in range(len(from_info) - 1, -1, -1):
            from_i = from_info[i]
            to_i = to_info[i]
            if os.path.abspath(to_i) == os.path.abspath(
                odb.get(baz.hash_info.value).path
            ):
                if on_error:
                    on_error(from_i, to_i, Exception("stop baz"))
                del from_info[i]
                del to_info[i]

        return original(from_fs, from_info, to_fs, to_info, **kwargs)

    mocker.patch("dvc_objects.fs.generic.transfer", unreliable_upload)
    with pytest.raises(UploadError):
        dvc.push()
    assert not list(index.hashes())




tests/func/experiments/__init__.py




tests/func/experiments/conftest.py
import pytest

from tests.unit.repo.experiments.conftest import (  # noqa, pylint disable=unused-argument
    exp_stage,
    failed_exp_stage,
    session_app,
    session_queue,
    session_worker,
    test_queue,
)


@pytest.fixture
def http_auth_patch(mocker):
    from dulwich.client import HTTPUnauthorized

    url = "https://0.0.0.0"
    client = mocker.MagicMock()
    client.get_refs.side_effect = HTTPUnauthorized("", url)
    client.send_pack.side_effect = HTTPUnauthorized("", url)

    patch = mocker.patch("dulwich.client.get_transport_and_path")
    patch.return_value = (client, url)
    return url


@pytest.fixture(params=[True, False])
def workspace(request, session_queue) -> bool:  # noqa: F811
    return request.param


@pytest.fixture
def params_repo(tmp_dir, scm, dvc):
    (tmp_dir / "params.yaml").dump(
        {"foo": [{"bar": 1}, {"baz": 2}], "goo": {"bag": 3.0}, "lorem": False}
    )
    dvc.run(
        cmd="echo foo",
        params=["params.yaml:"],
        name="foo",
    )
    scm.add(["dvc.yaml", "dvc.lock", "copy.py", "params.yaml"])
    scm.commit("init")




tests/func/experiments/test_apply.py
import pytest
from funcy import first

from dvc.repo.experiments.refs import CELERY_STASH


def test_apply(tmp_dir, scm, dvc, exp_stage):
    from dvc.exceptions import InvalidArgumentError

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"], tmp_dir=True)
    exp_a = first(results)

    dvc.experiments.run(
        exp_stage.addressing, params=["foo=3"], tmp_dir=True, name="foo"
    )

    with pytest.raises(InvalidArgumentError):
        dvc.experiments.apply("bar")

    dvc.experiments.apply(exp_a)
    assert (tmp_dir / "params.yaml").read_text().strip() == "foo: 2"
    assert (tmp_dir / "metrics.yaml").read_text().strip() == "foo: 2"

    dvc.experiments.apply("foo")
    assert (tmp_dir / "params.yaml").read_text().strip() == "foo: 3"
    assert (tmp_dir / "metrics.yaml").read_text().strip() == "foo: 3"


def test_apply_failed(tmp_dir, scm, dvc, failed_exp_stage, mocker):
    from dvc.repo.experiments.queue.base import QueueDoneResult, QueueEntry

    dvc.experiments.run(
        failed_exp_stage.addressing, params=["foo=3"], queue=True, name="foo"
    )
    exp_rev = dvc.experiments.scm.resolve_rev(f"{CELERY_STASH}@{{0}}")

    # patch iter_done to return exp_rev as a failed exp (None-type result)
    queue = dvc.experiments.celery_queue
    mocker.patch.object(
        queue,
        "iter_done",
        return_value=[
            QueueDoneResult(
                QueueEntry("", "", queue.ref, exp_rev, "", None, "foo", None),
                None,
            ),
        ],
    )
    mocker.patch.object(
        queue,
        "iter_queued",
        return_value=[],
    )

    dvc.experiments.apply(exp_rev)
    assert (tmp_dir / "params.yaml").read_text().strip() == "foo: 3"

    scm.reset(hard=True)
    assert (tmp_dir / "params.yaml").read_text().strip() == "foo: 1"
    dvc.experiments.apply("foo")
    assert (tmp_dir / "params.yaml").read_text().strip() == "foo: 3"


def test_apply_queued(tmp_dir, scm, dvc, exp_stage):
    metrics_original = (tmp_dir / "metrics.yaml").read_text().strip()
    dvc.experiments.run(
        exp_stage.addressing, params=["foo=2"], name="exp-a", queue=True
    )
    dvc.experiments.run(
        exp_stage.addressing, params=["foo=3"], name="exp-b", queue=True
    )
    queue_revs = {
        entry.name: entry.stash_rev
        for entry in dvc.experiments.celery_queue.iter_queued()
    }

    dvc.experiments.apply(queue_revs["exp-a"])
    assert (tmp_dir / "params.yaml").read_text().strip() == "foo: 2"
    assert (tmp_dir / "metrics.yaml").read_text().strip() == metrics_original

    dvc.experiments.apply(queue_revs["exp-b"])
    assert (tmp_dir / "params.yaml").read_text().strip() == "foo: 3"
    assert (tmp_dir / "metrics.yaml").read_text().strip() == metrics_original


def test_apply_untracked(tmp_dir, scm, dvc, exp_stage):
    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
    exp = first(results)
    tmp_dir.gen("untracked", "untracked")
    tmp_dir.gen("params.yaml", "conflict")

    dvc.experiments.apply(exp)
    assert (tmp_dir / "untracked").read_text() == "untracked"
    assert (tmp_dir / "params.yaml").read_text().strip() == "foo: 2"


def test_apply_unchanged_head(tmp_dir, scm, dvc, exp_stage):
    # see https://github.com/iterative/dvc/issues/8764
    tmp_dir.gen("params.yaml", "foo: 2")
    scm.add(["dvc.yaml", "dvc.lock", "params.yaml", "metrics.yaml"])
    scm.commit("commit foo=2")
    results = dvc.experiments.run(exp_stage.addressing)
    # workspace now contains unchanged (git-committed) params.yaml w/foo: 2
    exp = first(results)
    dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
    # workspace now contains changed params.yaml w/foo: 3

    dvc.experiments.apply(exp)
    assert (tmp_dir / "params.yaml").read_text().strip() == "foo: 2"




tests/func/experiments/test_diff.py
from funcy import first


def test_diff_empty(tmp_dir, scm, dvc, exp_stage):
    assert dvc.experiments.diff() == {"params": {}, "metrics": {}}


def test_diff_head(tmp_dir, scm, dvc, exp_stage):
    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
    exp = first(results)

    assert dvc.experiments.diff(a_rev="HEAD", b_rev=exp) == {
        "params": {"params.yaml": {"foo": {"diff": 1, "old": 1, "new": 2}}},
        "metrics": {"metrics.yaml": {"foo": {"diff": 1, "old": 1, "new": 2}}},
    }


def test_diff_exp(tmp_dir, scm, dvc, exp_stage):
    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
    exp_a = first(results)
    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
    exp_b = first(results)

    assert dvc.experiments.diff(a_rev=exp_a, b_rev=exp_b) == {
        "params": {"params.yaml": {"foo": {"diff": 1, "old": 2, "new": 3}}},
        "metrics": {"metrics.yaml": {"foo": {"diff": 1, "old": 2, "new": 3}}},
    }




tests/func/experiments/test_experiments.py
import itertools
import logging
import os
import stat
from textwrap import dedent

import pytest
from configobj import ConfigObj
from funcy import first

from dvc.dvcfile import PROJECT_FILE
from dvc.env import (
    DVC_EXP_BASELINE_REV,
    DVC_EXP_NAME,
    DVC_STUDIO_OFFLINE,
    DVC_STUDIO_REPO_URL,
    DVC_STUDIO_TOKEN,
    DVC_STUDIO_URL,
)
from dvc.exceptions import DvcException, ReproductionError
from dvc.repo.experiments.exceptions import ExperimentExistsError
from dvc.repo.experiments.queue.base import BaseStashQueue
from dvc.repo.experiments.refs import CELERY_STASH
from dvc.repo.experiments.utils import exp_refs_by_rev
from dvc.scm import SCMError, resolve_rev
from dvc.stage.exceptions import StageFileDoesNotExistError
from dvc.utils.serialize import PythonFileCorruptedError
from tests.scripts import COPY_SCRIPT


@pytest.mark.parametrize("name", [None, "foo"])
def test_new_simple(tmp_dir, scm, dvc, exp_stage, mocker, name, workspace):
    baseline = scm.get_rev()
    tmp_dir.gen("params.yaml", "foo: 2")

    new_mock = mocker.spy(dvc.experiments, "new")
    results = dvc.experiments.run(
        exp_stage.addressing, name=name, tmp_dir=not workspace
    )
    exp = first(results)
    ref_info = first(exp_refs_by_rev(scm, exp))
    assert ref_info
    assert ref_info.baseline_sha == baseline

    new_mock.assert_called_once()
    fs = scm.get_fs(exp)
    with fs.open("metrics.yaml", mode="r", encoding="utf-8") as fobj:
        assert fobj.read().strip() == "foo: 2"

    if workspace:
        assert (tmp_dir / "metrics.yaml").read_text().strip() == "foo: 2"

    exp_name = name if name else ref_info.name
    assert dvc.experiments.get_exact_name([exp])[exp] == exp_name
    assert resolve_rev(scm, exp_name) == exp


def test_experiment_exists(tmp_dir, scm, dvc, exp_stage, mocker, workspace):
    dvc.experiments.run(
        exp_stage.addressing,
        name="foo",
        params=["foo=2"],
        tmp_dir=not workspace,
    )

    new_mock = mocker.spy(BaseStashQueue, "_stash_exp")
    with pytest.raises(ExperimentExistsError):
        dvc.experiments.run(
            exp_stage.addressing,
            name="foo",
            params=["foo=3"],
            tmp_dir=not workspace,
        )
    new_mock.assert_not_called()

    results = dvc.experiments.run(
        exp_stage.addressing,
        name="foo",
        params=["foo=3"],
        force=True,
        tmp_dir=not workspace,
    )
    exp = first(results)

    fs = scm.get_fs(exp)
    with fs.open("metrics.yaml", mode="r", encoding="utf-8") as fobj:
        assert fobj.read().strip() == "foo: 3"


@pytest.mark.skipif(os.name == "nt", reason="Not supported for Windows.")
def test_file_permissions(tmp_dir, scm, dvc, exp_stage, mocker):
    mode = 0o755
    os.chmod(tmp_dir / "copy.py", mode)
    scm.add(["copy.py"])
    scm.commit("set exec")

    tmp_dir.gen("params.yaml", "foo: 2")
    dvc.experiments.run(exp_stage.addressing)
    assert stat.S_IMODE(os.stat(tmp_dir / "copy.py").st_mode) == mode


def test_failed_exp_workspace(
    tmp_dir,
    scm,
    dvc,
    failed_exp_stage,
    mocker,
    capsys,
):
    tmp_dir.gen("params.yaml", "foo: 2")
    with pytest.raises(ReproductionError):
        dvc.experiments.run(failed_exp_stage.addressing)
    assert not dvc.fs.exists(
        os.path.join(dvc.experiments.workspace_queue.pid_dir, "workspace")
    )


def test_get_baseline(tmp_dir, scm, dvc, exp_stage):
    init_rev = scm.get_rev()
    assert dvc.experiments.get_baseline(init_rev) is None

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
    exp_rev = first(results)
    assert dvc.experiments.get_baseline(exp_rev) == init_rev

    dvc.experiments.run(exp_stage.addressing, params=["foo=3"], queue=True)
    assert dvc.experiments.get_baseline(f"{CELERY_STASH}@{{0}}") == init_rev

    scm.add(["dvc.yaml", "dvc.lock", "copy.py", "params.yaml", "metrics.yaml"])
    scm.commit("promote exp")
    promote_rev = scm.get_rev()
    assert dvc.experiments.get_baseline(promote_rev) is None

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=4"])
    exp_rev = first(results)
    assert dvc.experiments.get_baseline(exp_rev) == promote_rev

    dvc.experiments.run(exp_stage.addressing, params=["foo=5"], queue=True)
    assert dvc.experiments.get_baseline(f"{CELERY_STASH}@{{0}}") == promote_rev
    assert dvc.experiments.get_baseline(f"{CELERY_STASH}@{{1}}") == init_rev


def test_update_py_params(tmp_dir, scm, dvc, session_queue, copy_script):
    tmp_dir.gen("params.py", "INT = 1\n")
    stage = dvc.run(
        cmd="python copy.py params.py metrics.py",
        metrics_no_cache=["metrics.py"],
        params=["params.py:INT"],
        name="copy-file",
    )
    scm.add(["dvc.yaml", "dvc.lock", "copy.py", "params.py", "metrics.py"])
    scm.commit("init")

    results = dvc.experiments.run(
        stage.addressing, params=["params.py:INT=2"], tmp_dir=True
    )
    exp_a = first(results)

    fs = scm.get_fs(exp_a)
    with fs.open("params.py", mode="r", encoding="utf-8") as fobj:
        assert fobj.read().strip() == "INT = 2"
    with fs.open("metrics.py", mode="r", encoding="utf-8") as fobj:
        assert fobj.read().strip() == "INT = 2"

    tmp_dir.gen(
        "params.py",
        (
            "INT = 1\nFLOAT = 0.001\nDICT = {'a': 1}\n\n"
            "class Train:\n    seed = 2020\n\n"
            "class Klass:\n    def __init__(self):\n        self.a = 111\n"
        ),
    )
    stage = dvc.run(
        cmd="python copy.py params.py metrics.py",
        metrics_no_cache=["metrics.py"],
        params=["params.py:INT,FLOAT,DICT,Train,Klass"],
        name="copy-file",
    )
    scm.add(["dvc.yaml", "dvc.lock", "copy.py", "params.py", "metrics.py"])
    scm.commit("init")

    results = dvc.experiments.run(
        stage.addressing,
        params=[
            "params.py:FLOAT=0.1",
            "params.py:Train.seed=2121",
            "params.py:Klass.a=222",
        ],
        tmp_dir=True,
    )
    exp_a = first(results)

    result = (
        "INT = 1\nFLOAT = 0.1\nDICT = {'a': 1}\n\n"
        "class Train:\n    seed = 2121\n\n"
        "class Klass:\n    def __init__(self):\n        self.a = 222"
    )

    def _dos2unix(text):
        if os.name != "nt":
            return text

        # NOTE: git on windows will use CRLF, so we have to convert it to LF
        # in order to compare with the original
        return text.replace("\r\n", "\n")

    fs = scm.get_fs(exp_a)
    with fs.open("params.py", mode="r", encoding="utf-8") as fobj:
        assert _dos2unix(fobj.read().strip()) == result
    with fs.open("metrics.py", mode="r", encoding="utf-8") as fobj:
        assert _dos2unix(fobj.read().strip()) == result

    tmp_dir.gen("params.py", "INT = 1\n")
    stage = dvc.run(
        cmd="python copy.py params.py metrics.py",
        metrics_no_cache=["metrics.py"],
        params=["params.py:INT"],
        name="copy-file",
    )
    scm.add(["dvc.yaml", "dvc.lock", "copy.py", "params.py", "metrics.py"])
    scm.commit("init")

    with pytest.raises(PythonFileCorruptedError):
        dvc.experiments.run(stage.addressing, params=["params.py:INT=2a"], tmp_dir=True)


def test_detached_parent(tmp_dir, scm, dvc, exp_stage, mocker):
    detached_rev = scm.get_rev()

    tmp_dir.gen("params.yaml", "foo: 2")
    dvc.reproduce(exp_stage.addressing)
    scm.add(["dvc.yaml", "dvc.lock", "copy.py", "params.yaml", "metrics.yaml"])
    scm.commit("v2")

    scm.checkout(detached_rev)
    assert scm.gitpython.repo.head.is_detached
    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])

    exp_rev = first(results)
    assert dvc.experiments.get_baseline(exp_rev) == detached_rev
    assert (tmp_dir / "params.yaml").read_text().strip() == "foo: 3"


def test_branch(tmp_dir, scm, dvc, exp_stage):
    from dvc.exceptions import InvalidArgumentError

    with pytest.raises(InvalidArgumentError):
        dvc.experiments.branch("foo", "branch")

    scm.branch("branch-exists")

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"], name="foo")
    exp_a = first(results)
    ref_a = dvc.experiments.get_branch_by_rev(exp_a)

    with pytest.raises(InvalidArgumentError):
        dvc.experiments.branch("foo", "branch-exists")
    dvc.experiments.branch("foo")
    dvc.experiments.branch("foo", "branch-name")
    dvc.experiments.branch(exp_a, "branch-rev")
    dvc.experiments.branch(ref_a, "branch-ref")

    for name in ["foo-branch", "branch-name", "branch-rev", "branch-ref"]:
        assert name in scm.list_branches()
        assert scm.resolve_rev(name) == exp_a

    tmp_dir.scm_gen({"new_file": "new_file"}, commit="new baseline")
    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"], name="foo")
    exp_b = first(results)
    ref_b = dvc.experiments.get_branch_by_rev(exp_b)

    with pytest.raises(InvalidArgumentError):
        dvc.experiments.branch("foo", "branch-name")
    dvc.experiments.branch(ref_b, "branch-ref-b")

    assert "branch-ref-b" in scm.list_branches()
    assert scm.resolve_rev("branch-ref-b") == exp_b


def test_no_scm(tmp_dir):
    from dvc.repo import Repo as DvcRepo
    from dvc.scm import NoSCMError

    dvc = DvcRepo.init(no_scm=True)

    for cmd in [
        "apply",
        "branch",
        "diff",
        "show",
        "run",
        "gc",
        "push",
        "pull",
        "ls",
    ]:
        with pytest.raises(NoSCMError):
            getattr(dvc.experiments, cmd)()


def test_untracked(tmp_dir, scm, dvc, caplog, workspace, copy_script):
    tmp_dir.scm_gen("params.yaml", "foo: 1", commit="track params")
    stage = dvc.run(
        cmd="python copy.py params.yaml metrics.yaml",
        metrics_no_cache=["metrics.yaml"],
        params=["foo"],
        deps=["copy.py"],
        name="copy-file",
        no_exec=True,
    )

    # copy.py is untracked
    # with caplog.at_level(logging.ERROR):
    #     results = dvc.experiments.run(
    #         stage.addressing, params=["foo=2"], tmp_dir=True
    #     )
    #     assert "Failed to reproduce experiment" in caplog.text
    #     assert not results

    # dvc.yaml, copy.py are staged as new file but not committed
    scm.add(["dvc.yaml", "copy.py"])
    results = dvc.experiments.run(
        stage.addressing, params=["foo=2"], tmp_dir=not workspace
    )
    exp = first(results)
    fs = scm.get_fs(exp)
    assert fs.exists("dvc.yaml")
    assert fs.exists("dvc.lock")
    assert fs.exists("copy.py")
    with fs.open("metrics.yaml", mode="r", encoding="utf-8") as fobj:
        assert fobj.read().strip() == "foo: 2"


def test_packed_args_exists(tmp_dir, scm, dvc, exp_stage, caplog):
    from dvc.repo.experiments.executor.base import BaseExecutor

    tmp_dir.scm_gen(
        tmp_dir / ".dvc" / "tmp" / BaseExecutor.PACKED_ARGS_FILE,
        "",
        commit="commit args file",
        force=True,
    )

    with caplog.at_level(logging.WARNING):
        dvc.experiments.run(exp_stage.addressing)
        assert "Temporary DVC file" in caplog.text
    assert not (tmp_dir / ".dvc" / "tmp" / BaseExecutor.PACKED_ARGS_FILE).exists()


def test_list(tmp_dir, scm, dvc, exp_stage):
    baseline_a = scm.get_rev()
    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
    exp_a = first(results)
    ref_info_a = first(exp_refs_by_rev(scm, exp_a))

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
    exp_b = first(results)
    ref_info_b = first(exp_refs_by_rev(scm, exp_b))

    tmp_dir.scm_gen("new", "new", commit="new")
    results = dvc.experiments.run(exp_stage.addressing, params=["foo=4"])
    exp_c = first(results)
    ref_info_c = first(exp_refs_by_rev(scm, exp_c))

    assert dvc.experiments.ls() == {"master": [(ref_info_c.name, exp_c)]}

    exp_list = dvc.experiments.ls(rev=ref_info_a.baseline_sha)
    assert {key: set(val) for key, val in exp_list.items()} == {
        baseline_a: {(ref_info_a.name, exp_a), (ref_info_b.name, exp_b)}
    }

    exp_list = dvc.experiments.ls(rev=[baseline_a, scm.get_rev()])
    assert {key: set(val) for key, val in exp_list.items()} == {
        baseline_a: {(ref_info_a.name, exp_a), (ref_info_b.name, exp_b)},
        "master": {(ref_info_c.name, exp_c)},
    }

    exp_list = dvc.experiments.ls(all_commits=True)
    assert {key: set(val) for key, val in exp_list.items()} == {
        baseline_a: {(ref_info_a.name, exp_a), (ref_info_b.name, exp_b)},
        "master": {(ref_info_c.name, exp_c)},
    }

    scm.checkout("branch", True)
    exp_list = dvc.experiments.ls(all_commits=True)
    assert {key: set(val) for key, val in exp_list.items()} == {
        baseline_a: {(ref_info_a.name, exp_a), (ref_info_b.name, exp_b)},
        "branch": {(ref_info_c.name, exp_c)},
    }


def test_subdir(tmp_dir, scm, dvc, workspace):
    subdir = tmp_dir / "dir"
    subdir.gen("copy.py", COPY_SCRIPT)
    subdir.gen("params.yaml", "foo: 1")

    with subdir.chdir():
        dvc.run(
            cmd="python copy.py params.yaml metrics.yaml",
            metrics_no_cache=["metrics.yaml"],
            params=["foo"],
            name="copy-file",
            no_exec=True,
        )
        scm.add([subdir / "dvc.yaml", subdir / "copy.py", subdir / "params.yaml"])
        scm.commit("init")

        results = dvc.experiments.run(
            PROJECT_FILE, params=["foo=2"], tmp_dir=not workspace
        )
        assert results

    exp = first(results)
    ref_info = first(exp_refs_by_rev(scm, exp))

    fs = scm.get_fs(exp)
    for fname in ["metrics.yaml", "dvc.lock"]:
        assert fs.exists(f"dir/{fname}")
    with fs.open("dir/metrics.yaml", mode="r", encoding="utf-8") as fobj:
        assert fobj.read().strip() == "foo: 2"

    assert dvc.experiments.get_exact_name([exp])[exp] == ref_info.name
    assert resolve_rev(scm, ref_info.name) == exp


def test_subrepo(tmp_dir, request, scm, workspace):
    from dvc.testing.tmp_dir import make_subrepo

    subrepo = tmp_dir / "dir" / "repo"
    make_subrepo(subrepo, scm)
    request.addfinalizer(subrepo.dvc.close)

    subrepo.gen("copy.py", COPY_SCRIPT)
    subrepo.gen("params.yaml", "foo: 1")

    with subrepo.chdir():
        subrepo.dvc.run(
            cmd="python copy.py params.yaml metrics.yaml",
            metrics_no_cache=["metrics.yaml"],
            params=["foo"],
            name="copy-file",
            no_exec=True,
        )
        scm.add(
            [
                subrepo / "dvc.yaml",
                subrepo / "copy.py",
                subrepo / "params.yaml",
            ]
        )
        scm.commit("init")

        results = subrepo.dvc.experiments.run(
            PROJECT_FILE, params=["foo=2"], tmp_dir=not workspace
        )
        assert results

    exp = first(results)
    ref_info = first(exp_refs_by_rev(scm, exp))

    fs = scm.get_fs(exp)
    for fname in ["metrics.yaml", "dvc.lock"]:
        assert fs.exists(f"dir/repo/{fname}")
    with fs.open("dir/repo/metrics.yaml", mode="r", encoding="utf-8") as fobj:
        assert fobj.read().strip() == "foo: 2"

    assert subrepo.dvc.experiments.get_exact_name([exp])[exp] == ref_info.name
    assert resolve_rev(scm, ref_info.name) == exp


def test_run_celery(tmp_dir, scm, dvc, exp_stage, mocker):
    """Test running with full (non-pytest-celery) dvc-task queue."""
    dvc.experiments.run(exp_stage.addressing, params=["foo=2"], queue=True)
    dvc.experiments.run(exp_stage.addressing, params=["foo=3"], queue=True)
    assert len(dvc.experiments.stash_revs) == 2

    repro_spy = mocker.spy(dvc.experiments, "reproduce_celery")
    results = dvc.experiments.run(run_all=True)
    assert len(results) == 2
    repro_spy.assert_called_once_with(jobs=1)

    expected = {"foo: 2", "foo: 3"}
    metrics = set()
    for exp in results:
        fs = scm.get_fs(exp)
        with fs.open("metrics.yaml", mode="r", encoding="utf-8") as fobj:
            metrics.add(fobj.read().strip())
    assert expected == metrics


def test_run_metrics(tmp_dir, scm, dvc, exp_stage, mocker):
    from dvc.cli import main

    mocker.patch.object(dvc.experiments, "run", return_value={"abc123": "abc123"})
    show_mock = mocker.patch.object(dvc.metrics, "show", return_value={})

    main(["exp", "run", "-m"])
    assert show_mock.called_once()


def test_checkout_targets_deps(tmp_dir, scm, dvc, exp_stage):
    from dvc.utils.fs import remove

    tmp_dir.dvc_gen({"foo": "foo", "bar": "bar"}, commit="add files")
    stage = dvc.stage.add(
        cmd="python copy.py params.yaml metrics.yaml",
        metrics_no_cache=["metrics.yaml"],
        params=["foo"],
        name="copy-file",
        deps=["copy.py", "foo"],
        force=True,
    )
    remove("foo")
    remove("bar")

    dvc.experiments.run(stage.addressing, params=["foo=2"])
    assert (tmp_dir / "foo").exists()
    assert (tmp_dir / "foo").read_text() == "foo"
    assert not (tmp_dir / "bar").exists()


@pytest.mark.parametrize("tail", ["", "~1", "^"])
def test_fix_exp_head(tmp_dir, scm, tail):
    from dvc.repo.experiments.refs import EXEC_BASELINE
    from dvc.repo.experiments.utils import fix_exp_head

    head = "HEAD" + tail
    assert head == fix_exp_head(scm, head)

    rev = "1" * 40
    scm.set_ref(EXEC_BASELINE, rev)
    assert EXEC_BASELINE + tail == fix_exp_head(scm, head)
    assert "foo" + tail == fix_exp_head(scm, "foo" + tail)


@pytest.mark.parametrize(
    "params, target",
    itertools.product(("foo: 1", "foo: 2"), (True, False)),
)
def test_modified_data_dep(tmp_dir, scm, dvc, workspace, params, target, copy_script):
    tmp_dir.dvc_gen("data", "data")
    tmp_dir.gen("params.yaml", "foo: 1")
    exp_stage = dvc.run(
        cmd="python copy.py params.yaml metrics.yaml",
        metrics_no_cache=["metrics.yaml"],
        params=["foo"],
        name="copy-file",
        deps=["copy.py", "data"],
    )
    scm.add(
        [
            "dvc.yaml",
            "dvc.lock",
            "copy.py",
            "params.yaml",
            "metrics.yaml",
            "data.dvc",
            ".gitignore",
        ]
    )
    scm.commit("init")

    tmp_dir.gen("params.yaml", params)
    tmp_dir.gen("data", "modified")

    results = dvc.experiments.run(
        exp_stage.addressing if target else None, tmp_dir=not workspace
    )
    exp = first(results)

    for rev in dvc.brancher(revs=[exp]):
        if rev != exp:
            continue
        with dvc.dvcfs.open("metrics.yaml") as fobj:
            assert fobj.read().strip() == params
        with dvc.dvcfs.open("data") as fobj:
            assert fobj.read().strip() == "modified"

    if workspace:
        assert (tmp_dir / "metrics.yaml").read_text().strip() == params
        assert (tmp_dir / "data").read_text().strip() == "modified"


def test_exp_run_recursive(tmp_dir, scm, dvc, run_copy_metrics):
    tmp_dir.dvc_gen("metric_t.json", '{"foo": 1}')
    run_copy_metrics(
        "metric_t.json", "metric.json", metrics=["metric.json"], no_exec=True
    )
    assert dvc.experiments.run(".", recursive=True)
    assert (tmp_dir / "metric.json").parse() == {"foo": 1}


def test_experiment_name_invalid(tmp_dir, scm, dvc, exp_stage, mocker):
    from dvc.exceptions import InvalidArgumentError

    new_mock = mocker.spy(BaseStashQueue, "_stash_exp")
    with pytest.raises(InvalidArgumentError):
        dvc.experiments.run(
            exp_stage.addressing,
            name="fo^o",
            params=["foo=3"],
        )
    new_mock.assert_not_called()


def test_experiments_workspace_not_log_exception(caplog, dvc, scm):
    """Experiments run in workspace should not log exception.

    Instead it should just leave it to be handled in the main entrypoints.
    """
    with caplog.at_level(logging.ERROR):
        with pytest.raises(StageFileDoesNotExistError):
            dvc.experiments.run()

    assert not caplog.text


@pytest.mark.vscode
def test_run_env(tmp_dir, dvc, scm, mocker):
    dump_run_env = dedent(
        """\
        import os
        from dvc.env import (
            DVC_EXP_BASELINE_REV,
            DVC_EXP_NAME,
            DVC_STUDIO_OFFLINE,
            DVC_STUDIO_REPO_URL,
            DVC_STUDIO_TOKEN,
            DVC_STUDIO_URL
        )
        for v in (
            DVC_EXP_BASELINE_REV,
            DVC_EXP_NAME,
            DVC_STUDIO_OFFLINE,
            DVC_STUDIO_REPO_URL,
            DVC_STUDIO_TOKEN,
            DVC_STUDIO_URL
        ):
            with open(v, "w") as f:
                f.write(os.environ.get(v, ""))
        """
    )
    mocker.patch(
        "dvc.repo.experiments.queue.base.get_studio_config",
        return_value={
            "token": "TOKEN",
            "repo_url": "REPO_URL",
            "url": "BASE_URL",
            "offline": "false",
        },
    )
    (tmp_dir / "dump_run_env.py").write_text(dump_run_env)
    baseline = scm.get_rev()
    dvc.stage.add(
        cmd="python dump_run_env.py",
        name="run_env",
    )
    dvc.experiments.run()
    assert (tmp_dir / DVC_EXP_BASELINE_REV).read_text().strip() == baseline
    assert (tmp_dir / DVC_EXP_NAME).read_text().strip()
    assert (tmp_dir / DVC_STUDIO_TOKEN).read_text().strip() == "TOKEN"
    assert (tmp_dir / DVC_STUDIO_REPO_URL).read_text().strip() == "REPO_URL"
    assert (tmp_dir / DVC_STUDIO_URL).read_text().strip() == "BASE_URL"
    assert (tmp_dir / DVC_STUDIO_OFFLINE).read_text().strip() == "false"

    dvc.experiments.run(name="foo")
    assert (tmp_dir / DVC_EXP_BASELINE_REV).read_text().strip() == baseline
    assert (tmp_dir / DVC_EXP_NAME).read_text().strip() == "foo"


def test_experiment_unchanged(tmp_dir, scm, dvc, exp_stage):
    dvc.experiments.run(exp_stage.addressing)
    dvc.experiments.run(exp_stage.addressing)

    assert len(dvc.experiments.ls()["master"]) == 2


def test_experiment_run_dry(tmp_dir, scm, dvc, exp_stage):
    dvc.experiments.run(exp_stage.addressing, dry=True)

    assert len(dvc.experiments.ls()["master"]) == 0


def test_clean(tmp_dir, scm, dvc, mocker):
    clean = mocker.spy(dvc.experiments.celery_queue.celery, "clean")
    dvc.experiments.clean()
    clean.assert_called_once_with()


def test_experiment_no_commit(tmp_dir):
    from scmrepo.git import Git

    from dvc.repo import Repo

    Git.init(tmp_dir.fs_path).close()

    repo = Repo.init()
    assert repo.scm.no_commits

    try:
        with pytest.raises(SCMError):  # noqa: PT011
            repo.experiments.ls()
    finally:
        repo.close()


def test_local_config_is_propagated_to_tmp(tmp_dir, scm, dvc):
    with dvc.config.edit("local") as conf:
        conf["cache"]["type"] = "hardlink"

    stage = dvc.stage.add(
        cmd="cat .dvc/config.local > file", name="foo", outs_no_cache=["file"]
    )
    scm.add_commit(["dvc.yaml"], message="add dvc.yaml")

    results = dvc.experiments.run(stage.addressing, tmp_dir=True)
    exp = first(results)
    fs = scm.get_fs(exp)

    with fs.open("file") as fobj:
        conf_obj = ConfigObj(fobj)
        assert conf_obj["cache"]["type"] == "hardlink"


@pytest.mark.parametrize("tmp", [True, False])
def test_untracked_top_level_files_are_included_in_exp(tmp_dir, scm, dvc, tmp):
    (tmp_dir / "dvc.yaml").dump(
        {
            "metrics": ["metrics.json"],
            "params": ["params.yaml"],
            "plots": ["plots.csv"],
        }
    )
    stage = dvc.stage.add(
        cmd="touch metrics.json && touch params.yaml && touch plots.csv",
        name="top-level",
    )
    scm.add_commit(["dvc.yaml"], message="add dvc.yaml")
    results = dvc.experiments.run(stage.addressing, tmp_dir=tmp)
    exp = first(results)
    fs = scm.get_fs(exp)
    for file in ["metrics.json", "params.yaml", "plots.csv"]:
        assert fs.exists(file)


@pytest.mark.parametrize("tmp", [True, False])
def test_copy_paths(tmp_dir, scm, dvc, tmp):
    stage = dvc.stage.add(
        cmd="cat file && ls dir",
        name="foo",
    )
    scm.add_commit(["dvc.yaml"], message="add dvc.yaml")

    (tmp_dir / "dir").mkdir()
    (tmp_dir / "dir" / "file").write_text("dir/file")
    scm.ignore(tmp_dir / "dir")
    (tmp_dir / "file").write_text("file")
    scm.ignore(tmp_dir / "file")

    results = dvc.experiments.run(
        stage.addressing, tmp_dir=tmp, copy_paths=["dir", "file"]
    )
    exp = first(results)
    fs = scm.get_fs(exp)
    assert not fs.exists("dir")
    assert not fs.exists("file")


def test_copy_paths_errors(tmp_dir, scm, dvc, mocker):
    stage = dvc.stage.add(
        cmd="echo foo",
        name="foo",
    )
    scm.add_commit(["dvc.yaml"], message="add dvc.yaml")

    with pytest.raises(DvcException, match="Unable to copy"):
        dvc.experiments.run(stage.addressing, tmp_dir=True, copy_paths=["foo"])

    (tmp_dir / "foo").write_text("foo")
    mocker.patch("shutil.copy", side_effect=OSError)

    with pytest.raises(DvcException, match="Unable to copy"):
        dvc.experiments.run(stage.addressing, tmp_dir=True, copy_paths=["foo"])


def test_mixed_git_dvc_out(tmp_dir, scm, dvc, exp_stage):
    (tmp_dir / "dir").mkdir()
    dir_metrics = os.path.join("dir", "metrics.yaml")
    dvc.stage.add(
        cmd=f"python copy.py params.yaml {dir_metrics}",
        metrics=[dir_metrics],
        params=["foo"],
        name="copy-file",
        deps=["copy.py"],
        force=True,
    )
    dvc.stage.add(
        cmd=f"python copy.py {dir_metrics} metrics.yaml",
        metrics_no_cache=["metrics.yaml"],
        name="copy-dir-file",
        deps=["dir"],
    )
    scm.add(["dvc.yaml", "dvc.lock"])
    scm.commit("add dir stage")

    exp = first(dvc.experiments.run())
    assert (tmp_dir / "dir" / "metrics.yaml").exists()
    git_fs = scm.get_fs(exp)
    assert not git_fs.exists("dir/metrics.yaml")


@pytest.mark.parametrize("tmp", [True, False])
def test_custom_commit_message(tmp_dir, scm, dvc, tmp):
    stage = dvc.stage.add(
        cmd="echo foo",
        name="foo",
    )
    scm.add_commit(["dvc.yaml"], message="add dvc.yaml")

    exp = first(
        dvc.experiments.run(
            stage.addressing, tmp_dir=tmp, message="custom commit message"
        )
    )
    assert scm.gitpython.repo.commit(exp).message == "custom commit message"




tests/func/experiments/test_queue.py
import pytest
from funcy import first


def to_dict(tasks):
    status_dict = {}
    for task in tasks:
        status_dict[task["name"]] = task["status"]
    return status_dict


@pytest.mark.parametrize("follow", [True, False])
def test_celery_logs(
    tmp_dir,
    scm,
    dvc,
    failed_exp_stage,
    follow,
    capsys,
    test_queue,
):
    celery_queue = dvc.experiments.celery_queue
    dvc.experiments.run(failed_exp_stage.addressing, queue=True, name="foo")
    dvc.experiments.run(run_all=True)
    test_queue.wait(["foo"])

    done_result = first(celery_queue.iter_done())

    name = done_result.entry.stash_rev
    captured = capsys.readouterr()
    celery_queue.logs(name, follow=follow)
    captured = capsys.readouterr()
    assert "failed to reproduce 'failed-copy-file'" in captured.out


def test_queue_doesnt_remove_untracked_params_file(tmp_dir, dvc, scm):
    """Regression test for https://github.com/iterative/dvc/issues/7842"""
    tmp_dir.gen("params.yaml", "foo: 1")
    stage = dvc.run(cmd="echo ${foo}", params=["foo"], name="echo-foo")
    scm.add(
        [
            "dvc.yaml",
            "dvc.lock",
            ".gitignore",
        ]
    )
    scm.commit("init")
    dvc.experiments.run(stage.addressing, params=["foo=2"], queue=True)
    assert (tmp_dir / "params.yaml").exists()


def test_copy_paths_queue(tmp_dir, scm, dvc):
    stage = dvc.stage.add(
        cmd="cat file && ls dir",
        name="foo",
    )
    scm.add_commit(["dvc.yaml"], message="add dvc.yaml")

    (tmp_dir / "dir").mkdir()
    (tmp_dir / "dir" / "file").write_text("dir/file")
    scm.ignore(tmp_dir / "dir")
    (tmp_dir / "file").write_text("file")
    scm.ignore(tmp_dir / "file")

    dvc.experiments.run(stage.addressing, queue=True)
    results = dvc.experiments.run(run_all=True)

    exp = first(results)
    fs = scm.get_fs(exp)
    assert not fs.exists("dir")
    assert not fs.exists("file")


def test_custom_commit_message_queue(tmp_dir, scm, dvc):
    stage = dvc.stage.add(
        cmd="echo foo",
        name="foo",
    )
    scm.add_commit(["dvc.yaml"], message="add dvc.yaml")

    dvc.experiments.run(stage.addressing, queue=True, message="custom commit message")

    exp = first(dvc.experiments.run(run_all=True))
    assert scm.gitpython.repo.commit(exp).message == "custom commit message"




tests/func/experiments/test_remote.py
import pytest
from funcy import first

from dvc.repo.experiments.utils import exp_refs_by_rev


@pytest.mark.parametrize("use_url", [True, False])
def test_push(tmp_dir, scm, dvc, git_upstream, exp_stage, use_url):
    from dvc.exceptions import InvalidArgumentError

    remote = git_upstream.url if use_url else git_upstream.remote
    with pytest.raises(InvalidArgumentError):
        dvc.experiments.push(remote, ["foo"])

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=1"])
    exp1 = first(results)
    ref_info1 = first(exp_refs_by_rev(scm, exp1))

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
    exp2 = first(results)
    ref_info2 = first(exp_refs_by_rev(scm, exp2))

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
    exp3 = first(results)
    ref_info3 = first(exp_refs_by_rev(scm, exp3))

    dvc.experiments.push(remote, [ref_info1.name, ref_info2.name])
    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info1)) == exp1
    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info2)) == exp2
    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info3)) is None

    git_upstream.tmp_dir.scm.remove_ref(str(ref_info1))
    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info1)) is None

    dvc.experiments.push(remote, [ref_info1.name])
    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info1)) == exp1


@pytest.mark.parametrize("all_,rev,result3", [(True, False, True), (False, True, None)])
def test_push_args(tmp_dir, scm, dvc, git_upstream, exp_stage, all_, rev, result3):
    remote = git_upstream.url
    baseline = scm.get_rev()

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=1"])
    exp1 = first(results)
    ref_info1 = first(exp_refs_by_rev(scm, exp1))
    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
    exp2 = first(results)
    ref_info2 = first(exp_refs_by_rev(scm, exp2))

    scm.commit("new_baseline")

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
    exp3 = first(results)
    ref_info3 = first(exp_refs_by_rev(scm, exp3))

    if rev:
        rev = baseline
    dvc.experiments.push(remote, [], all_commits=all_, rev=rev)
    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info1)) == exp1
    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info2)) == exp2
    if result3:
        result3 = exp3
    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info3)) == result3


def test_push_multi_rev(tmp_dir, scm, dvc, git_upstream, exp_stage):
    remote = git_upstream.url
    baseline = scm.get_rev()

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=1"])
    exp1 = first(results)
    ref_info1 = first(exp_refs_by_rev(scm, exp1))
    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
    exp2 = first(results)
    ref_info2 = first(exp_refs_by_rev(scm, exp2))

    scm.commit("new_baseline")

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
    exp3 = first(results)
    ref_info3 = first(exp_refs_by_rev(scm, exp3))

    dvc.experiments.push(remote, [], rev=[baseline, scm.get_rev()])
    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info1)) == exp1
    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info2)) == exp2
    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info3)) == exp3


def test_push_diverged(tmp_dir, scm, dvc, git_upstream, exp_stage):
    git_upstream.tmp_dir.scm_gen("foo", "foo", commit="init")
    remote_rev = git_upstream.tmp_dir.scm.get_rev()

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
    exp = first(results)
    ref_info = first(exp_refs_by_rev(scm, exp))

    git_upstream.tmp_dir.scm.set_ref(str(ref_info), remote_rev)

    assert dvc.experiments.push(git_upstream.remote, [ref_info.name]) == {
        "diverged": [ref_info.name],
        "url": None,
        "uploaded": 0,
    }
    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info)) == remote_rev

    dvc.experiments.push(git_upstream.remote, [ref_info.name], force=True)
    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info)) == exp


def test_push_ambiguous_name(tmp_dir, scm, dvc, git_upstream, exp_stage):
    from dvc.exceptions import InvalidArgumentError

    remote = git_upstream.remote

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"], name="foo")
    exp_a = first(results)
    ref_info_a = first(exp_refs_by_rev(scm, exp_a))

    tmp_dir.scm_gen("new", "new", commit="new")
    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"], name="foo")
    exp_b = first(results)
    ref_info_b = first(exp_refs_by_rev(scm, exp_b))

    dvc.experiments.push(remote, ["foo"])
    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info_b)) == exp_b

    tmp_dir.scm_gen("new", "new 2", commit="new 2")

    with pytest.raises(InvalidArgumentError):
        dvc.experiments.push(remote, ["foo"])

    dvc.experiments.push(remote, [str(ref_info_a)])
    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info_a)) == exp_a


@pytest.mark.parametrize("use_url", [True, False])
def test_list_remote(tmp_dir, scm, dvc, git_downstream, exp_stage, use_url):
    baseline_a = scm.get_rev()
    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
    exp_a = first(results)
    ref_info_a = first(exp_refs_by_rev(scm, exp_a))

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
    exp_b = first(results)
    ref_info_b = first(exp_refs_by_rev(scm, exp_b))

    tmp_dir.scm_gen("new", "new", commit="new")
    results = dvc.experiments.run(exp_stage.addressing, params=["foo=4"])
    exp_c = first(results)
    ref_info_c = first(exp_refs_by_rev(scm, exp_c))

    remote = git_downstream.url if use_url else git_downstream.remote

    assert git_downstream.tmp_dir.scm.get_ref("HEAD") != scm.get_ref("HEAD")
    downstream_exp = git_downstream.tmp_dir.dvc.experiments
    assert downstream_exp.ls(git_remote=remote) == {}

    git_downstream.tmp_dir.scm.fetch_refspecs(remote, ["master:master"])
    exp_list = downstream_exp.ls(rev=baseline_a, git_remote=remote)
    assert {key: set(val) for key, val in exp_list.items()} == {
        baseline_a: {(ref_info_a.name, None), (ref_info_b.name, None)}
    }

    exp_list = downstream_exp.ls(all_commits=True, git_remote=remote)
    assert {key: set(val) for key, val in exp_list.items()} == {
        baseline_a: {(ref_info_a.name, None), (ref_info_b.name, None)},
        "master": {(ref_info_c.name, None)},
    }


@pytest.mark.parametrize("use_url", [True, False])
def test_pull(tmp_dir, scm, dvc, git_downstream, exp_stage, use_url):
    from dvc.exceptions import InvalidArgumentError

    remote = git_downstream.url if use_url else git_downstream.remote
    downstream_exp = git_downstream.tmp_dir.dvc.experiments
    with pytest.raises(InvalidArgumentError):
        downstream_exp.pull(remote, ["foo"])

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=1"])
    exp1 = first(results)
    ref_info1 = first(exp_refs_by_rev(scm, exp1))
    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
    exp2 = first(results)
    ref_info2 = first(exp_refs_by_rev(scm, exp2))
    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
    exp3 = first(results)
    ref_info3 = first(exp_refs_by_rev(scm, exp3))

    downstream_exp.pull(
        git_downstream.remote, [ref_info1.name, ref_info2.name], force=True
    )
    assert git_downstream.tmp_dir.scm.get_ref(str(ref_info1)) == exp1
    assert git_downstream.tmp_dir.scm.get_ref(str(ref_info2)) == exp2
    assert git_downstream.tmp_dir.scm.get_ref(str(ref_info3)) is None

    git_downstream.tmp_dir.scm.remove_ref(str(ref_info1))

    downstream_exp.pull(remote, [str(ref_info1)])
    assert git_downstream.tmp_dir.scm.get_ref(str(ref_info1)) == exp1


@pytest.mark.parametrize("all_,rev,result3", [(True, False, True), (False, True, None)])
def test_pull_args(tmp_dir, scm, dvc, git_downstream, exp_stage, all_, rev, result3):
    baseline = scm.get_rev()

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=1"])
    exp1 = first(results)
    ref_info1 = first(exp_refs_by_rev(scm, exp1))
    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
    exp2 = first(results)
    ref_info2 = first(exp_refs_by_rev(scm, exp2))

    scm.commit("new_baseline")

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
    exp3 = first(results)
    ref_info3 = first(exp_refs_by_rev(scm, exp3))

    if rev:
        rev = baseline

    downstream_exp = git_downstream.tmp_dir.dvc.experiments
    git_downstream.tmp_dir.scm.fetch_refspecs(str(tmp_dir), ["master:master"])
    downstream_exp.pull(git_downstream.remote, [], all_commits=all_, rev=rev)
    assert git_downstream.tmp_dir.scm.get_ref(str(ref_info1)) == exp1
    assert git_downstream.tmp_dir.scm.get_ref(str(ref_info2)) == exp2
    if result3:
        result3 = exp3
    assert git_downstream.tmp_dir.scm.get_ref(str(ref_info3)) == result3


def test_pull_multi_rev(tmp_dir, scm, dvc, git_downstream, exp_stage):
    baseline = scm.get_rev()

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=1"])
    exp1 = first(results)
    ref_info1 = first(exp_refs_by_rev(scm, exp1))
    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
    exp2 = first(results)
    ref_info2 = first(exp_refs_by_rev(scm, exp2))

    scm.commit("new_baseline")

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
    exp3 = first(results)
    ref_info3 = first(exp_refs_by_rev(scm, exp3))

    downstream_exp = git_downstream.tmp_dir.dvc.experiments
    git_downstream.tmp_dir.scm.fetch_refspecs(str(tmp_dir), ["master:master"])
    downstream_exp.pull(git_downstream.remote, [], rev=[baseline, scm.get_rev()])
    assert git_downstream.tmp_dir.scm.get_ref(str(ref_info1)) == exp1
    assert git_downstream.tmp_dir.scm.get_ref(str(ref_info2)) == exp2
    assert git_downstream.tmp_dir.scm.get_ref(str(ref_info3)) == exp3


def test_pull_diverged(tmp_dir, scm, dvc, git_downstream, exp_stage):
    git_downstream.tmp_dir.scm_gen("foo", "foo", commit="init")
    remote_rev = git_downstream.tmp_dir.scm.get_rev()

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
    exp = first(results)
    ref_info = first(exp_refs_by_rev(scm, exp))

    git_downstream.tmp_dir.scm.set_ref(str(ref_info), remote_rev)

    downstream_exp = git_downstream.tmp_dir.dvc.experiments
    assert downstream_exp.pull(git_downstream.remote, ref_info.name) == []
    assert git_downstream.tmp_dir.scm.get_ref(str(ref_info)) == remote_rev

    downstream_exp.pull(git_downstream.remote, ref_info.name, force=True)
    assert git_downstream.tmp_dir.scm.get_ref(str(ref_info)) == exp


def test_pull_ambiguous_name(tmp_dir, scm, dvc, git_downstream, exp_stage):
    from dvc.exceptions import InvalidArgumentError

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"], name="foo")
    exp_a = first(results)
    ref_info_a = first(exp_refs_by_rev(scm, exp_a))

    tmp_dir.scm_gen("new", "new", commit="new")
    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"], name="foo")
    exp_b = first(results)
    ref_info_b = first(exp_refs_by_rev(scm, exp_b))

    remote = git_downstream.remote
    downstream_exp = git_downstream.tmp_dir.dvc.experiments
    with pytest.raises(InvalidArgumentError):
        downstream_exp.pull(remote, ["foo"])

    downstream_exp.pull(remote, [str(ref_info_b)])
    assert git_downstream.tmp_dir.scm.get_ref(str(ref_info_b)) == exp_b

    with git_downstream.tmp_dir.scm.detach_head(ref_info_a.baseline_sha):
        downstream_exp.pull(remote, ["foo"])
    assert git_downstream.tmp_dir.scm.get_ref(str(ref_info_a)) == exp_a


def test_auth_error_list(tmp_dir, scm, dvc, http_auth_patch):
    from dvc.scm import GitAuthError

    with pytest.raises(
        GitAuthError,
        match=f"Authentication failed for: '{http_auth_patch}'",
    ):
        dvc.experiments.ls(git_remote=http_auth_patch)


def test_auth_error_pull(tmp_dir, scm, dvc, http_auth_patch):
    from dvc.scm import GitAuthError

    with pytest.raises(
        GitAuthError,
        match=f"Authentication failed for: '{http_auth_patch}'",
    ):
        dvc.experiments.pull(http_auth_patch, ["foo"])


def test_auth_error_push(tmp_dir, scm, dvc, exp_stage, http_auth_patch):
    from dvc.scm import GitAuthError

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
    exp = first(results)
    ref_info = first(exp_refs_by_rev(scm, exp))

    with pytest.raises(
        GitAuthError,
        match=f"Authentication failed for: '{http_auth_patch}'",
    ):
        dvc.experiments.push(http_auth_patch, [ref_info.name])


@pytest.mark.parametrize("use_ref", [True, False])
def test_get(tmp_dir, scm, dvc, exp_stage, erepo_dir, use_ref):
    from dvc.repo import Repo

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
    exp_rev = first(results)
    exp_ref = first(exp_refs_by_rev(scm, exp_rev))

    with erepo_dir.chdir():
        Repo.get(
            str(tmp_dir),
            "params.yaml",
            rev=exp_ref.name if use_ref else exp_rev,
        )
        assert (erepo_dir / "params.yaml").read_text().strip() == "foo: 2"




tests/func/experiments/test_remove.py
import pytest
from funcy import first

from dvc.exceptions import InvalidArgumentError
from dvc.repo.experiments.exceptions import UnresolvedExpNamesError
from dvc.repo.experiments.utils import exp_refs_by_rev


def test_remove_experiments_by_ref(tmp_dir, scm, dvc, exp_stage, caplog):
    queue_length = 3
    ref_info_list = []
    ref_name_list = []

    for i in range(queue_length):
        results = dvc.experiments.run(exp_stage.addressing, params=[f"foo={i}"])
        ref_info = first(exp_refs_by_rev(scm, first(results)))
        ref_info_list.append(ref_info)
        ref_name_list.append(str(ref_info))

    with pytest.raises(InvalidArgumentError):
        dvc.experiments.remove(ref_name_list[:2] + ["non-exist"])
    assert scm.get_ref(ref_name_list[0]) is not None
    assert scm.get_ref(ref_name_list[1]) is not None
    assert scm.get_ref(ref_name_list[2]) is not None

    assert set(dvc.experiments.remove(ref_name_list[:2])) == set(ref_name_list[:2])
    assert scm.get_ref(ref_name_list[0]) is None
    assert scm.get_ref(ref_name_list[1]) is None
    assert scm.get_ref(ref_name_list[2]) is not None


def test_remove_all_queued_experiments(tmp_dir, scm, dvc, exp_stage):
    queue_length = 3
    for i in range(queue_length):
        dvc.experiments.run(exp_stage.addressing, params=[f"foo={i}"], queue=True)

    results = dvc.experiments.run(exp_stage.addressing, params=[f"foo={queue_length}"])
    ref_info = first(exp_refs_by_rev(scm, first(results)))

    assert len(dvc.experiments.stash_revs) == queue_length
    assert len(dvc.experiments.remove(queue=True)) == queue_length
    assert len(dvc.experiments.stash_revs) == 0
    assert scm.get_ref(str(ref_info)) is not None


def test_remove_special_queued_experiments(tmp_dir, scm, dvc, exp_stage):
    dvc.experiments.run(
        exp_stage.addressing, params=["foo=1"], queue=True, name="queue1"
    )
    dvc.experiments.run(
        exp_stage.addressing, params=["foo=2"], queue=True, name="queue2"
    )
    dvc.experiments.run(
        exp_stage.addressing, params=["foo=3"], queue=True, name="queue3"
    )
    queue_revs = {
        entry.name: entry.stash_rev
        for entry in dvc.experiments.celery_queue.iter_queued()
    }
    assert len(queue_revs) == 3

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=4"])
    ref_info1 = first(exp_refs_by_rev(scm, first(results)))
    results = dvc.experiments.run(exp_stage.addressing, params=["foo=5"])
    ref_info2 = first(exp_refs_by_rev(scm, first(results)))

    assert scm.get_ref(str(ref_info1)) is not None
    assert scm.get_ref(str(ref_info2)) is not None

    rev2 = queue_revs["queue2"]
    assert set(dvc.experiments.remove(["queue1", rev2[:5], str(ref_info1)])) == {
        "queue1",
        rev2[:5],
        str(ref_info1),
    }
    assert len(list(dvc.experiments.celery_queue.iter_queued())) == 1
    assert scm.get_ref(str(ref_info1)) is None
    assert scm.get_ref(str(ref_info2)) is not None


def test_remove_all(tmp_dir, scm, dvc, exp_stage):
    results = dvc.experiments.run(exp_stage.addressing, params=["foo=1"])
    ref_info1 = first(exp_refs_by_rev(scm, first(results)))
    dvc.experiments.run(exp_stage.addressing, params=["foo=2"], queue=True)
    scm.add(["dvc.yaml", "dvc.lock", "copy.py", "params.yaml", "metrics.yaml"])
    scm.commit("update baseline")

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
    ref_info2 = first(exp_refs_by_rev(scm, first(results)))
    dvc.experiments.run(exp_stage.addressing, params=["foo=4"], queue=True)

    assert set(dvc.experiments.remove(all_commits=True)) == {
        ref_info1.name,
        ref_info2.name,
    }
    assert len(dvc.experiments.stash_revs) == 2
    assert scm.get_ref(str(ref_info2)) is None
    assert scm.get_ref(str(ref_info1)) is None


@pytest.mark.parametrize("use_url", [True, False])
def test_remove_remote(tmp_dir, scm, dvc, exp_stage, git_upstream, use_url):
    remote = git_upstream.url if use_url else git_upstream.remote

    ref_info_list = []
    exp_list = []
    for i in range(3):
        results = dvc.experiments.run(exp_stage.addressing, params=[f"foo={i}"])
        exp = first(results)
        exp_list.append(exp)
        ref_info = first(exp_refs_by_rev(scm, exp))
        ref_info_list.append(ref_info)
        dvc.experiments.push(remote, [ref_info.name])
        assert git_upstream.tmp_dir.scm.get_ref(str(ref_info)) == exp

    dvc.experiments.remove(
        git_remote=remote,
        exp_names=[str(ref_info_list[0]), ref_info_list[1].name],
    )

    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info_list[0])) is None
    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info_list[1])) is None
    assert git_upstream.tmp_dir.scm.get_ref(str(ref_info_list[2])) == exp_list[2]

    with pytest.raises(
        UnresolvedExpNamesError, match=f"Experiment 'foo' does not exist in '{remote}'"
    ):
        dvc.experiments.remove(git_remote=remote, exp_names=["foo"])


def test_remove_experiments_by_rev(tmp_dir, scm, dvc, exp_stage):
    baseline = scm.get_rev()

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=1"])
    baseline_exp_ref = first(exp_refs_by_rev(scm, first(results)))

    dvc.experiments.run(
        exp_stage.addressing, params=["foo=2"], queue=True, name="queue2"
    )
    scm.commit("new_baseline")

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
    ref_info = first(exp_refs_by_rev(scm, first(results)))
    new_exp_ref = str(ref_info)

    dvc.experiments.run(
        exp_stage.addressing, params=["foo=4"], queue=True, name="queue4"
    )

    assert dvc.experiments.remove(rev=baseline) == [baseline_exp_ref.name]
    queue_revs = {
        entry.name: entry.stash_rev
        for entry in dvc.experiments.celery_queue.iter_queued()
    }
    assert scm.get_ref(str(baseline_exp_ref)) is None
    assert "queue2" in queue_revs
    assert scm.get_ref(new_exp_ref) is not None
    assert "queue4" in queue_revs


def test_remove_multi_rev(tmp_dir, scm, dvc, exp_stage):
    baseline = scm.get_rev()

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=1"])
    baseline_exp_ref = first(exp_refs_by_rev(scm, first(results)))

    dvc.experiments.run(
        exp_stage.addressing, params=["foo=2"], queue=True, name="queue2"
    )
    scm.commit("new_baseline")

    results = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
    new_exp_ref = first(exp_refs_by_rev(scm, first(results)))

    assert set(dvc.experiments.remove(rev=[baseline, scm.get_rev()])) == {
        baseline_exp_ref.name,
        new_exp_ref.name,
    }

    assert scm.get_ref(str(baseline_exp_ref)) is None
    assert scm.get_ref(str(new_exp_ref)) is None




tests/func/experiments/test_save.py
import pytest
from funcy import first

from dvc.repo.experiments.exceptions import ExperimentExistsError, InvalidArgumentError
from dvc.repo.experiments.utils import exp_refs_by_rev
from dvc.scm import resolve_rev


def setup_stage(tmp_dir, dvc, scm):
    tmp_dir.gen("params.yaml", "foo: 1")
    dvc.run(name="echo-foo", outs=["bar"], cmd="echo foo > bar")
    scm.add(["dvc.yaml", "dvc.lock", ".gitignore", "params.yaml"])
    scm.commit("init")


def test_exp_save_unchanged(tmp_dir, dvc, scm):
    setup_stage(tmp_dir, dvc, scm)
    dvc.experiments.save()


@pytest.mark.parametrize("name", (None, "test"))
def test_exp_save(tmp_dir, dvc, scm, name):
    setup_stage(tmp_dir, dvc, scm)
    baseline = scm.get_rev()

    exp = dvc.experiments.save(name=name)
    ref_info = first(exp_refs_by_rev(scm, exp))
    assert ref_info
    assert ref_info.baseline_sha == baseline

    exp_name = name if name else ref_info.name
    assert dvc.experiments.get_exact_name([exp])[exp] == exp_name
    assert resolve_rev(scm, exp_name) == exp


def test_exp_save_overwrite_experiment(tmp_dir, dvc, scm):
    setup_stage(tmp_dir, dvc, scm)
    name = "dummy"
    dvc.experiments.save(name=name)

    tmp_dir.gen("params.yaml", "foo: 2")
    with pytest.raises(ExperimentExistsError):
        dvc.experiments.save(name=name)

    dvc.experiments.save(name=name, force=True)


@pytest.mark.parametrize(
    "name",
    (
        "invalid/name",
        "invalid..name",
        "invalid~name",
        "invalid?name",
        "invalidname.",
    ),
)
def test_exp_save_invalid_name(tmp_dir, dvc, scm, name):
    setup_stage(tmp_dir, dvc, scm)
    with pytest.raises(InvalidArgumentError):
        dvc.experiments.save(name=name, force=True)


def test_exp_save_after_commit(tmp_dir, dvc, scm):
    setup_stage(tmp_dir, dvc, scm)
    baseline = scm.get_rev()
    dvc.experiments.save(name="exp-1", force=True)

    tmp_dir.scm_gen({"new_file": "new_file"}, commit="new baseline")
    dvc.experiments.save(name="exp-2", force=True)

    all_exps = dvc.experiments.ls(all_commits=True)
    assert all_exps[baseline][0][0] == "exp-1"
    assert all_exps["master"][0][0] == "exp-2"


def test_exp_save_with_staged_changes(tmp_dir, dvc, scm):
    setup_stage(tmp_dir, dvc, scm)
    tmp_dir.gen({"deleted": "deleted", "modified": "modified"})
    scm.add_commit(["deleted", "modified"], "init")

    (tmp_dir / "deleted").unlink()
    tmp_dir.gen({"new_file": "new_file"})
    (tmp_dir / "modified").write_text("foo")
    scm.add(["deleted", "new_file", "modified"])

    exp_rev = dvc.experiments.save(name="exp")
    scm.checkout(exp_rev, force=True)
    assert not (tmp_dir / "deleted").exists()
    assert (tmp_dir / "new_file").exists()
    assert (tmp_dir / "modified").read_text() == "foo"


def test_exp_save_include_untracked(tmp_dir, dvc, scm):
    setup_stage(tmp_dir, dvc, scm)

    new_file = tmp_dir / "new_file"
    new_file.write_text("new_file")
    dvc.experiments.save(name="exp", include_untracked=["new_file"])

    _, _, unstaged = scm.status()
    assert "new_file" in unstaged
    assert new_file.read_text() == "new_file"


def test_exp_save_include_untracked_warning(tmp_dir, dvc, scm, mocker):
    """Regression test for https://github.com/iterative/dvc/issues/9061"""
    setup_stage(tmp_dir, dvc, scm)

    new_dir = tmp_dir / "new_dir"
    new_dir.mkdir()
    (new_dir / "foo").write_text("foo")
    (new_dir / "bar").write_text("bar")

    logger = mocker.patch("dvc.repo.experiments.executor.base.logger")

    dvc.experiments.save(name="exp", include_untracked=["new_dir"])
    assert not logger.warning.called


def test_untracked_top_level_files_are_included_in_exp(tmp_dir, scm, dvc):
    (tmp_dir / "dvc.yaml").dump(
        {
            "metrics": ["metrics.json"],
            "params": ["params.yaml"],
            "plots": ["plots.csv"],
        }
    )
    stage = dvc.stage.add(
        cmd="touch metrics.json && touch params.yaml && touch plots.csv",
        name="top-level",
    )
    scm.add_commit(["dvc.yaml"], message="add dvc.yaml")
    dvc.reproduce(stage.addressing)
    exp = dvc.experiments.save()
    fs = scm.get_fs(exp)
    for file in ["metrics.json", "params.yaml", "plots.csv", "dvc.lock"]:
        assert fs.exists(file)


def test_untracked_dvclock_is_included_in_exp(tmp_dir, scm, dvc):
    stage = dvc.stage.add(
        cmd="echo foo",
        name="foo",
    )
    scm.add_commit(["dvc.yaml"], message="add dvc.yaml")
    dvc.reproduce(stage.addressing)

    # dvc.reproduce automatically stages `dvc.lock`
    # force it to be untracked
    scm.reset()

    exp = dvc.experiments.save()
    fs = scm.get_fs(exp)
    assert fs.exists("dvc.lock")


def test_exp_save_custom_message(tmp_dir, dvc, scm):
    setup_stage(tmp_dir, dvc, scm)

    exp = dvc.experiments.save(message="custom commit message")
    assert scm.gitpython.repo.commit(exp).message == "custom commit message"




tests/func/experiments/test_set_params.py
import pytest

from dvc.exceptions import InvalidArgumentError
from tests.func.utils.test_hydra import hydra_setup


@pytest.mark.parametrize(
    "changes, expected",
    [
        [["foo=baz"], "foo: baz\ngoo:\n  bag: 3.0\nlorem: false"],
        [["params.yaml:foo=baz"], "foo: baz\ngoo:\n  bag: 3.0\nlorem: false"],
    ],
)
def test_modify_params(params_repo, dvc, changes, expected):
    dvc.experiments.run(params=changes)
    with open("params.yaml") as fobj:
        assert fobj.read().strip() == expected


@pytest.mark.parametrize("hydra_enabled", [True, False])
@pytest.mark.parametrize(
    "config_dir,config_name",
    [
        (None, None),
        (None, "bar"),
        ("conf", "bar"),
    ],
)
def test_hydra_compose_and_dump(
    tmp_dir, params_repo, dvc, hydra_enabled, config_dir, config_name
):
    hydra_setup(
        tmp_dir,
        config_dir=config_dir or "conf",
        config_name=config_name or "config",
    )

    dvc.experiments.run()
    assert (tmp_dir / "params.yaml").parse() == {
        "foo": [{"bar": 1}, {"baz": 2}],
        "goo": {"bag": 3.0},
        "lorem": False,
    }

    with dvc.config.edit() as conf:
        if hydra_enabled:
            conf["hydra"]["enabled"] = True
        if config_dir is not None:
            conf["hydra"]["config_dir"] = config_dir
        if config_name is not None:
            conf["hydra"]["config_name"] = config_name

    dvc.experiments.run()

    if hydra_enabled:
        assert (tmp_dir / "params.yaml").parse() == {
            "db": {"driver": "mysql", "user": "omry", "pass": "secret"},
        }

        dvc.experiments.run(params=["db=postgresql"])
        assert (tmp_dir / "params.yaml").parse() == {
            "db": {
                "driver": "postgresql",
                "user": "foo",
                "pass": "bar",
                "timeout": 10,
            }
        }
    else:
        assert (tmp_dir / "params.yaml").parse() == {
            "foo": [{"bar": 1}, {"baz": 2}],
            "goo": {"bag": 3.0},
            "lorem": False,
        }


@pytest.mark.parametrize(
    "hydra_enabled,overrides,expected",
    [
        (
            True,
            ["db=mysql,postgresql"],
            [
                {"params.yaml": ["db=mysql"]},
                {"params.yaml": ["db=postgresql"]},
            ],
        ),
        (
            False,
            ["foo=bar,baz"],
            [{"params.yaml": ["foo=bar"]}, {"params.yaml": ["foo=baz"]}],
        ),
        (
            False,
            [],
            [{}],
        ),
    ],
)
def test_hydra_sweep(
    tmp_dir, params_repo, dvc, mocker, hydra_enabled, overrides, expected
):
    patched = mocker.patch.object(dvc.experiments, "queue_one")

    if hydra_enabled:
        hydra_setup(
            tmp_dir,
            config_dir="conf",
            config_name="config",
        )
        with dvc.config.edit() as conf:
            conf["hydra"]["enabled"] = True

    dvc.experiments.run(params=overrides, queue=True)

    assert patched.call_count == len(expected)
    for e in expected:
        patched.assert_any_call(
            mocker.ANY,
            params=e,
            targets=None,
            copy_paths=None,
            message=None,
        )


def test_hydra_sweep_requires_queue(params_repo, dvc):
    with pytest.raises(
        InvalidArgumentError,
        match="Sweep overrides can't be used without `--queue`",
    ):
        dvc.experiments.run(params=["db=mysql,postgresql"])


def test_hydra_sweep_prefix_name(tmp_dir, params_repo, dvc):
    prefix = "foo"
    db_values = ["mysql", "postgresql"]
    param = "+db=" + ",".join(db_values)
    dvc.experiments.run(params=[param], queue=True, name=prefix)
    expected_names = [f"{prefix}-{i+1}" for i, _ in enumerate(db_values)]
    exp_names = [entry.name for entry in dvc.experiments.celery_queue.iter_queued()]
    for name, expected in zip(exp_names, expected_names):
        assert name == expected




tests/func/experiments/test_show.py
import logging
import os
from datetime import datetime
from unittest.mock import ANY

import pytest
from funcy import first
from scmrepo.exceptions import SCMError

from dvc.cli import main
from dvc.repo.experiments.executor.base import BaseExecutor, ExecutorInfo, TaskStatus
from dvc.repo.experiments.refs import CELERY_STASH
from dvc.repo.experiments.utils import EXEC_PID_DIR, EXEC_TMP_DIR, exp_refs_by_rev
from dvc.utils import relpath

LOCK_CONTENTS = {
    "read": {
        "data/MNIST": [{"pid": 54062, "cmd": "dvc exp run"}],
    },
    "write": {
        "data/MNIST": {"pid": 54062, "cmd": "dvc exp run"},
    },
}


def make_executor_info(**kwargs):
    # set default values for required info fields
    for key in (
        "git_url",
        "baseline_rev",
        "location",
        "root_dir",
        "dvc_dir",
    ):
        if key not in kwargs:
            kwargs[key] = ""
    return ExecutorInfo(**kwargs)


def make_executor(local=None, **kwargs):
    if local:
        local_executor = {
            "root": ANY,
            "log": ANY,
            "pid": ANY,
            "returncode": ANY,
            "task_id": ANY,
        }
        local_executor.update(local)
    else:
        local_executor = ANY
    data = {
        "state": ANY,
        "local": local_executor,
        "name": ANY,
    }
    data.update(kwargs)
    return data


def make_data(params=None, **kwargs):
    params = {"data": params or {"foo": 1}}
    data = {
        "rev": ANY,
        "deps": {
            "copy.py": {
                "hash": ANY,
                "size": ANY,
                "nfiles": None,
            }
        },
        "metrics": {"metrics.yaml": params},
        "outs": {},
        "params": {"params.yaml": params},
        "timestamp": ANY,
        "meta": ANY,
    }
    data.update(kwargs)
    return data


@pytest.mark.vscode
def test_show_branch_and_tag_name(tmp_dir, scm, dvc, exp_stage):
    with tmp_dir.branch("new/branch", new=True):
        tmp_dir.scm_gen("branch", "branch", "commit")

    result = dvc.experiments.show(all_branches=True)
    expected = [None, "master", "new/branch"]
    assert [exp.name for exp in result] == expected

    scm.tag("new/tag")
    tag_rev = scm.get_rev()
    with scm.detach_head(tag_rev):
        result = dvc.experiments.show(all_tags=True)
    expected = [None, "new/tag"]
    assert [exp.name for exp in result] == expected


@pytest.mark.vscode
def test_show_simple(tmp_dir, scm, dvc, exp_stage):
    assert dvc.experiments.show()[0].dumpd() == {
        "rev": "workspace",
        "name": None,
        "data": make_data(rev="workspace"),
        "error": None,
        "experiments": None,
    }


@pytest.mark.vscode
@pytest.mark.parametrize("workspace", [True, False])
def test_show_experiment(tmp_dir, scm, dvc, exp_stage, workspace):
    baseline_rev = scm.get_rev()
    timestamp = datetime.fromtimestamp(
        scm.gitpython.repo.rev_parse(baseline_rev).committed_date
    )

    exp_rev = first(
        dvc.experiments.run(
            exp_stage.addressing, params=["foo=2"], tmp_dir=not workspace
        )
    )
    results = dvc.experiments.show()
    assert results[1].dumpd() == {
        "rev": baseline_rev,
        "name": "master",
        "data": make_data(rev=baseline_rev, timestamp=timestamp),
        "error": None,
        "experiments": [
            {
                "revs": [
                    {
                        "rev": exp_rev,
                        "name": ANY,
                        "data": make_data(rev=exp_rev, params={"foo": 2}),
                        "error": None,
                        "experiments": None,
                    }
                ],
                "executor": None,
                "name": ANY,
            }
        ],
    }


@pytest.mark.vscode
def test_show_queued(tmp_dir, scm, dvc, exp_stage):
    baseline_rev = scm.get_rev()

    dvc.experiments.run(
        exp_stage.addressing, params=["foo=2"], queue=True, name="test_name"
    )
    exp_rev = dvc.experiments.scm.resolve_rev(f"{CELERY_STASH}@{{0}}")

    results = dvc.experiments.show()
    assert results[1].dumpd() == {
        "rev": baseline_rev,
        "name": "master",
        "data": make_data(rev=baseline_rev),
        "error": None,
        "experiments": [
            {
                "revs": [
                    {
                        "rev": exp_rev,
                        "name": "test_name",
                        "data": make_data(rev=exp_rev, params={"foo": 2}, metrics=ANY),
                        "error": None,
                        "experiments": None,
                    }
                ],
                "executor": make_executor(state="queued"),
                "name": "test_name",
            }
        ],
    }

    # test that only queued experiments for the current baseline are returned
    tmp_dir.gen("foo", "foo")
    scm.add(["foo"])
    scm.commit("new commit")
    new_rev = scm.get_rev()

    dvc.experiments.run(exp_stage.addressing, params=["foo=3"], queue=True)
    exp_rev = dvc.experiments.scm.resolve_rev(f"{CELERY_STASH}@{{0}}")

    results = dvc.experiments.show()
    assert results[1].dumpd() == {
        "rev": new_rev,
        "name": "master",
        "data": make_data(rev=new_rev),
        "error": None,
        "experiments": [
            {
                "revs": [
                    {
                        "rev": exp_rev,
                        "name": ANY,
                        "data": make_data(rev=exp_rev, params={"foo": 3}, metrics=ANY),
                        "error": None,
                        "experiments": None,
                    }
                ],
                "executor": make_executor(state="queued"),
                "name": ANY,
            }
        ],
    }


@pytest.mark.vscode
def test_show_failed_experiment(tmp_dir, scm, dvc, failed_exp_stage, test_queue):
    baseline_rev = scm.get_rev()
    dvc.experiments.run(failed_exp_stage.addressing, params=["foo=2"], queue=True)
    exp_rev = dvc.experiments.scm.resolve_rev(f"{CELERY_STASH}@{{0}}")
    dvc.experiments.run(run_all=True)

    results = dvc.experiments.show()
    assert results[1].dumpd() == {
        "rev": baseline_rev,
        "name": "master",
        "data": make_data(rev=baseline_rev, metrics=ANY),
        "error": None,
        "experiments": [
            {
                "revs": [
                    {
                        "rev": exp_rev,
                        "name": ANY,
                        "data": make_data(rev=exp_rev, params={"foo": 2}, metrics=ANY),
                        "error": {"msg": "Experiment run failed", "type": ANY},
                        "experiments": None,
                    }
                ],
                "executor": make_executor(
                    state="failed",
                    local={"returncode": 255},
                ),
                "name": ANY,
            }
        ],
    }


def test_show_filter(
    tmp_dir,
    scm,
    dvc,
    capsys,
    copy_script,
):
    capsys.readouterr()

    params_file = tmp_dir / "params.yaml"
    params_data = {
        "foo": 1,
        "bar": 1,
        "train/foo": 1,
        "train/bar": 1,
        "nested": {"foo": 1, "bar": 1},
    }
    (tmp_dir / params_file).dump(params_data)

    dvc.run(
        cmd="python copy.py params.yaml metrics.yaml",
        metrics_no_cache=["metrics.yaml"],
        params=["foo"],
        name="copy-file",
        deps=["copy.py"],
    )
    scm.add(
        [
            "dvc.yaml",
            "dvc.lock",
            "copy.py",
            "params.yaml",
            "metrics.yaml",
            ".gitignore",
        ]
    )
    scm.commit("init")

    capsys.readouterr()
    assert main(["exp", "show", "--drop=.*foo"]) == 0
    cap = capsys.readouterr()
    for filtered in ["foo", "train/foo", "nested.foo"]:
        assert f"params.yaml:{filtered}" not in cap.out
        assert f"metrics.yaml:{filtered}" not in cap.out

    capsys.readouterr()
    assert main(["exp", "show", "--drop=.*foo", "--keep=.*train"]) == 0
    cap = capsys.readouterr()
    for filtered in ["foo", "nested.foo"]:
        assert f"params.yaml:{filtered}" not in cap.out
        assert f"metrics.yaml:{filtered}" not in cap.out
    assert "params.yaml:train/foo" in cap.out
    assert "metrics.yaml:train/foo" in cap.out

    capsys.readouterr()
    assert main(["exp", "show", "--drop=params.yaml:.*foo"]) == 0
    cap = capsys.readouterr()
    for filtered in ["foo", "train/foo", "nested.foo"]:
        assert f"params.yaml:{filtered}" not in cap.out
        assert f"metrics.yaml:{filtered}" in cap.out

    capsys.readouterr()
    assert main(["exp", "show", "--drop=Created"]) == 0
    cap = capsys.readouterr()
    assert "Created" not in cap.out

    capsys.readouterr()
    assert main(["exp", "show", "--drop=Created|Experiment"]) == 0
    cap = capsys.readouterr()
    assert "Created" not in cap.out
    assert "Experiment" not in cap.out


@pytest.mark.vscode
def test_show_multiple_commits(tmp_dir, scm, dvc, exp_stage):
    init_rev = scm.get_rev()
    tmp_dir.scm_gen("file", "file", "commit")
    next_rev = scm.get_rev()

    dvc.experiments.show(num=-2)

    expected = ["workspace", next_rev, init_rev]
    results = dvc.experiments.show(num=2)
    assert [exp.rev for exp in results] == expected

    expected = ["workspace", *scm.branch_revs("master")]
    results = dvc.experiments.show(all_commits=True)
    assert [exp.rev for exp in results] == expected

    results = dvc.experiments.show(num=100)
    assert [exp.rev for exp in results] == expected


def test_show_sort(tmp_dir, scm, dvc, exp_stage, caplog):
    dvc.experiments.run(exp_stage.addressing, params=["foo=2"])

    with caplog.at_level(logging.ERROR):
        assert main(["exp", "show", "--no-pager", "--sort-by=bar"]) != 0
        assert "Unknown sort column" in caplog.text

    with caplog.at_level(logging.ERROR):
        assert main(["exp", "show", "--no-pager", "--sort-by=foo"]) != 0
        assert "Ambiguous sort column" in caplog.text

    assert main(["exp", "show", "--no-pager", "--sort-by=params.yaml:foo"]) == 0

    assert main(["exp", "show", "--no-pager", "--sort-by=metrics.yaml:foo"]) == 0


@pytest.mark.vscode
@pytest.mark.parametrize(
    "status, pid_exists",
    [
        (TaskStatus.RUNNING, True),
        (TaskStatus.RUNNING, False),
        (TaskStatus.FAILED, False),
    ],
)
def test_show_running(
    tmp_dir, scm, dvc, exp_stage, capsys, caplog, status, pid_exists, mocker
):
    from dvc.rwlock import RWLOCK_FILE
    from dvc_task.proc.process import ProcessInfo

    baseline_rev = scm.get_rev()
    pid_dir = os.path.join(dvc.tmp_dir, EXEC_TMP_DIR, EXEC_PID_DIR)
    lock_file = relpath(os.path.join(dvc.tmp_dir, RWLOCK_FILE), str(tmp_dir))
    info = make_executor_info(
        location=BaseExecutor.DEFAULT_LOCATION,
        status=status,
        baseline_rev=baseline_rev,
    )
    pidfile = os.path.join(
        pid_dir,
        "workspace",
        f"workspace{BaseExecutor.INFOFILE_EXT}",
    )
    os.makedirs(os.path.dirname(pidfile), exist_ok=True)
    (tmp_dir / pidfile).dump_json(info.asdict())
    (tmp_dir / lock_file).dump_json(LOCK_CONTENTS)

    mocker.patch.object(ProcessInfo, "load", return_value=mocker.Mock(pid=123))
    mocker.patch("psutil.pid_exists", return_value=pid_exists)

    tempdir_active = mocker.spy(dvc.experiments.tempdir_queue, "collect_active_data")
    celery_active = mocker.spy(dvc.experiments.celery_queue, "collect_active_data")
    results = dvc.experiments.show()
    assert results[1].dumpd() == {
        "rev": ANY,
        "name": "master",
        "data": make_data(),
        "error": None,
        "experiments": [
            {
                "revs": ANY,
                "executor": make_executor(state="running"),
                "name": ANY,
            }
        ]
        if pid_exists
        else None,
    }
    tempdir_active.assert_called_once()
    celery_active.assert_called_once()


def test_show_with_broken_repo(tmp_dir, scm, dvc, exp_stage, caplog):
    dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
    with open("dvc.yaml", "a", encoding="utf-8") as fd:
        fd.write("breaking the yaml!")

    results = dvc.experiments.show()
    assert results[0].error
    assert results[0].error.type == "YAMLSyntaxError"

    for exp_range in results[1].experiments:
        assert not any(exp.error for exp in exp_range)


def test_show_csv(tmp_dir, scm, dvc, exp_stage, capsys):
    import time

    baseline_rev = scm.get_rev()

    def _get_rev_isotimestamp(rev):
        return datetime.fromtimestamp(
            scm.gitpython.repo.rev_parse(rev).committed_date
        ).isoformat()

    result1 = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
    rev1 = first(result1)
    ref_info1 = first(exp_refs_by_rev(scm, rev1))

    # at least 1 second gap between these experiments to make sure
    # the previous experiment to be regarded as branch_base
    time.sleep(1)
    result2 = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
    rev2 = first(result2)
    ref_info2 = first(exp_refs_by_rev(scm, rev2))

    capsys.readouterr()
    assert main(["exp", "show", "--csv"]) == 0
    cap = capsys.readouterr()
    data_dep = first(x for x in dvc.index.deps if "copy.py" in x.fspath)
    data_hash = data_dep.hash_info.value[:7]
    assert "Experiment,rev,typ,Created,parent" in cap.out
    assert "metrics.yaml:foo,params.yaml:foo,copy.py" in cap.out
    assert f",workspace,baseline,,,3,3,{data_hash}" in cap.out
    assert (
        ",master,baseline,{},,1,1,{}".format(
            _get_rev_isotimestamp(baseline_rev), data_hash
        )
        in cap.out
    )
    assert (
        "{},{},branch_base,{},,2,2,{}".format(
            ref_info1.name, rev1[:7], _get_rev_isotimestamp(rev1), data_hash
        )
        in cap.out
    )
    assert (
        "{},{},branch_commit,{},,3,3,{}".format(
            ref_info2.name, rev2[:7], _get_rev_isotimestamp(rev2), data_hash
        )
        in cap.out
    )


def test_show_only_changed(tmp_dir, dvc, scm, capsys, copy_script):
    params_file = tmp_dir / "params.yaml"
    params_data = {
        "foo": 1,
        "goobar": 1,
    }
    (tmp_dir / params_file).dump(params_data)

    dvc.run(
        cmd="python copy.py params.yaml metrics.yaml",
        metrics_no_cache=["metrics.yaml"],
        params=["foo", "goobar"],
        name="copy-file",
        deps=["copy.py"],
    )
    scm.add(
        [
            "dvc.yaml",
            "dvc.lock",
            "copy.py",
            "params.yaml",
            "metrics.yaml",
            ".gitignore",
        ]
    )
    scm.commit("init")

    dvc.experiments.run(params=["foo=2"])

    capsys.readouterr()
    assert main(["exp", "show"]) == 0
    cap = capsys.readouterr()
    assert "goobar" in cap.out

    capsys.readouterr()
    assert main(["exp", "show", "--only-changed"]) == 0
    cap = capsys.readouterr()
    assert "goobar" not in cap.out

    capsys.readouterr()
    assert main(["exp", "show", "--only-changed", "--keep=.*bar"]) == 0
    cap = capsys.readouterr()
    assert "params.yaml:goobar" in cap.out
    assert "metrics.yaml:goobar" in cap.out


@pytest.mark.vscode
def test_show_outs(tmp_dir, dvc, scm, erepo_dir, copy_script):
    params_file = tmp_dir / "params.yaml"
    params_data = {
        "foo": 1,
        "bar": 1,
    }
    (tmp_dir / params_file).dump(params_data)

    dvc.run(
        cmd="python copy.py params.yaml metrics.yaml && echo out > out",
        metrics_no_cache=["metrics.yaml"],
        params=["foo", "bar"],
        name="copy-file",
        deps=["copy.py"],
        outs=["out"],
    )

    scm.commit("init")

    results = dvc.experiments.show()
    assert results[0].dumpd() == {
        "rev": "workspace",
        "name": None,
        "data": make_data(
            params=ANY,
            outs={
                "out": {
                    "hash": ANY,
                    "size": ANY,
                    "nfiles": None,
                    "use_cache": True,
                    "is_data_source": False,
                }
            },
        ),
        "error": None,
        "experiments": None,
    }

    tmp_dir.dvc_gen("out_add", "foo", commit="dvc add output")
    results = dvc.experiments.show()
    assert results[0].dumpd() == {
        "rev": "workspace",
        "name": None,
        "data": make_data(
            params=ANY,
            outs={
                "out": {
                    "hash": ANY,
                    "size": ANY,
                    "nfiles": None,
                    "use_cache": True,
                    "is_data_source": False,
                },
                "out_add": {
                    "hash": ANY,
                    "size": ANY,
                    "nfiles": None,
                    "use_cache": True,
                    "is_data_source": True,
                },
            },
        ),
        "error": None,
        "experiments": None,
    }

    with erepo_dir.chdir():
        erepo_dir.dvc_gen("out", "out content", commit="create out")

    dvc.imp(os.fspath(erepo_dir), "out", "out_imported")

    results = dvc.experiments.show()
    assert results[0].dumpd() == {
        "rev": "workspace",
        "name": None,
        "data": make_data(
            params=ANY,
            outs={
                "out": {
                    "hash": ANY,
                    "size": ANY,
                    "nfiles": None,
                    "use_cache": True,
                    "is_data_source": False,
                },
                "out_add": {
                    "hash": ANY,
                    "size": ANY,
                    "nfiles": None,
                    "use_cache": True,
                    "is_data_source": True,
                },
                "out_imported": {
                    "hash": ANY,
                    "size": ANY,
                    "nfiles": None,
                    "use_cache": True,
                    "is_data_source": True,
                },
            },
        ),
        "error": None,
        "experiments": None,
    }


def test_metrics_renaming(tmp_dir, dvc, scm, capsys, copy_script):
    params_file = tmp_dir / "params.yaml"
    params_data = {
        "foo": 1,
    }
    (tmp_dir / params_file).dump(params_data)

    dvc.run(
        cmd="python copy.py params.yaml metrics.yaml",
        metrics_no_cache=["metrics.yaml"],
        params=["foo"],
        name="copy-file",
        deps=["copy.py"],
    )
    scm.add(
        [
            "dvc.yaml",
            "dvc.lock",
            "copy.py",
            "params.yaml",
            "metrics.yaml",
            ".gitignore",
        ]
    )

    scm.commit("metrics.yaml")
    metrics_rev = scm.get_rev()

    dvc.run(
        cmd="python copy.py params.yaml scores.yaml",
        metrics_no_cache=["scores.yaml"],
        params=["foo"],
        name="copy-file",
        deps=["copy.py"],
    )
    scm.add(
        [
            "dvc.yaml",
            "dvc.lock",
            "params.yaml",
            "scores.yaml",
        ]
    )
    scm.commit("scores.yaml")
    scores_rev = scm.get_rev()

    capsys.readouterr()
    assert main(["exp", "show", "--csv", "-A"]) == 0
    cap = capsys.readouterr()

    def _get_rev_isotimestamp(rev):
        return datetime.fromtimestamp(
            scm.gitpython.repo.rev_parse(rev).committed_date
        ).isoformat()

    assert f",master,baseline,{_get_rev_isotimestamp(scores_rev)},,1,,1" in cap.out
    assert (
        ",{},baseline,{},,,1,1".format(
            metrics_rev[:7], _get_rev_isotimestamp(metrics_rev)
        )
        in cap.out
    )


def test_show_sorted_deps(tmp_dir, dvc, scm, capsys):
    tmp_dir.gen("a", "a")
    tmp_dir.gen("b", "b")
    tmp_dir.gen("c", "c")
    tmp_dir.gen("z", "z")

    dvc.run(
        cmd="echo foo",
        name="deps",
        deps=["a", "b", "z", "c"],
    )

    capsys.readouterr()
    assert main(["exp", "show", "--csv"]) == 0
    cap = capsys.readouterr()
    assert "a,b,c,z" in cap.out


@pytest.mark.vscode
def test_show_queued_error(tmp_dir, scm, dvc, exp_stage, mocker):
    dvc.experiments.run(
        exp_stage.addressing, params=["foo=2"], queue=True, name="test_name"
    )
    exp_rev_2 = dvc.experiments.scm.resolve_rev(f"{CELERY_STASH}@{{0}}")
    commit_2 = scm.resolve_commit(exp_rev_2)

    dvc.experiments.run(exp_stage.addressing, params=["foo=3"], queue=True)
    exp_rev_3 = dvc.experiments.scm.resolve_rev(f"{CELERY_STASH}@{{0}}")

    def resolve_commit(rev):
        if rev == exp_rev_3:
            raise SCMError
        return commit_2

    mocker.patch.object(
        scm,
        "resolve_commit",
        side_effect=mocker.MagicMock(side_effect=resolve_commit),
    )

    results = dvc.experiments.show()[1].experiments
    assert len(results) == 2
    queued = results[0]
    assert queued.executor.state == "queued"
    errored = results[1]
    assert errored.revs[0].error


@pytest.mark.vscode
def test_show_completed_error(tmp_dir, scm, dvc, exp_stage, mocker):
    result_2 = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
    exp_rev_2 = first(result_2)
    commit_2 = scm.resolve_commit(exp_rev_2)
    result_3 = dvc.experiments.run(exp_stage.addressing, params=["foo=3"])
    exp_rev_3 = first(result_3)

    def resolve_commit(rev):
        if rev == exp_rev_3:
            raise SCMError
        return commit_2

    mocker.patch.object(
        scm,
        "resolve_commit",
        side_effect=mocker.MagicMock(side_effect=resolve_commit),
    )
    results = dvc.experiments.show()[1].experiments
    assert len(results) == 1
    assert not results[0].revs[0].error


@pytest.mark.vscode
def test_show_baseline_error(tmp_dir, scm, dvc, exp_stage, mocker):
    baseline_rev = scm.get_rev()

    result_2 = dvc.experiments.run(exp_stage.addressing, params=["foo=2"])
    exp_rev_2 = first(result_2)
    commit_2 = scm.resolve_commit(exp_rev_2)

    def resolve_commit(rev):
        if rev == baseline_rev:
            raise SCMError
        return commit_2

    mocker.patch.object(
        scm,
        "resolve_commit",
        side_effect=mocker.MagicMock(side_effect=resolve_commit),
    )

    results = dvc.experiments.show()
    assert results[1].error
    assert len(results[1].experiments) == 1




tests/func/experiments/test_stash_exp.py
import pytest
from funcy import first

from dvc.dependency.base import DependencyDoesNotExistError
from dvc.exceptions import ReproductionError


@pytest.mark.parametrize("tmp", [True, False])
@pytest.mark.parametrize("staged", [True, False])
def test_deleted(tmp_dir, scm, dvc, tmp, staged):
    tmp_dir.scm_gen("file", "file", commit="commit file")
    stage = dvc.stage.add(
        cmd="cat file",
        deps=["file"],
        name="foo",
    )
    scm.add_commit(["dvc.yaml"], message="add dvc.yaml")

    file = tmp_dir / "file"
    file.unlink()
    if staged:
        scm.add(["file"])

    with pytest.raises(ReproductionError) as exc_info:
        dvc.experiments.run(stage.addressing, tmp_dir=tmp)

    cause = exc_info._excinfo[1].__cause__
    assert isinstance(cause, DependencyDoesNotExistError)
    assert not file.exists()


@pytest.mark.parametrize("tmp", [True, False])
@pytest.mark.parametrize("staged", [True, False])
def test_modified(tmp_dir, scm, dvc, caplog, tmp, staged):
    tmp_dir.scm_gen("file", "file", commit="commit file")
    stage = dvc.stage.add(
        cmd="cat file",
        name="foo",
    )
    scm.add_commit(["dvc.yaml"], message="add dvc.yaml")

    (tmp_dir / "file").write_text("modified_file")
    if staged:
        scm.add(["file"])

    results = dvc.experiments.run(stage.addressing, tmp_dir=tmp)

    exp = first(results)
    scm.checkout(exp, force=True)
    assert (tmp_dir / "file").read_text() == "modified_file"


@pytest.mark.parametrize("tmp", [True, False])
def test_staged_new_file(tmp_dir, scm, dvc, tmp):
    stage = dvc.stage.add(
        cmd="cat file",
        name="foo",
    )
    scm.add_commit(["dvc.yaml"], message="add dvc.yaml")

    (tmp_dir / "file").write_text("file")
    scm.add(["file"])

    results = dvc.experiments.run(stage.addressing, tmp_dir=tmp)
    exp = first(results)
    fs = scm.get_fs(exp)
    assert fs.exists("file")




tests/func/experiments/test_utils.py
from funcy import first


def test_generate_random_exp_name(tmp_dir, dvc, scm, exp_stage, mocker):
    mocked_generator = mocker.MagicMock()
    mocked_generator.choice.side_effect = [
        0,
        0,
        0,
        0,
        1,
        1,
        0,
        0,
    ]
    mocker.patch(
        "dvc.repo.experiments.utils.random.Random", return_value=mocked_generator
    )

    ref = first(dvc.experiments.run(exp_stage.addressing, params=["foo=1"]))
    assert dvc.experiments.get_exact_name([ref])[ref] == "0-0"

    # Causes 1 retry
    ref = first(dvc.experiments.run(exp_stage.addressing, params=["foo=2"]))
    assert dvc.experiments.get_exact_name([ref])[ref] == "1-1"

    tmp_dir.scm_gen({"foo": "bar"}, commit="foo")
    # Can use same name because of different baseline_rev
    ref = first(dvc.experiments.run(exp_stage.addressing, params=["foo=1"]))
    assert dvc.experiments.get_exact_name([ref])[ref] == "0-0"




tests/func/experiments/executor/__init__.py




tests/func/experiments/executor/test_ssh.py
import posixpath
from contextlib import contextmanager
from functools import partial
from urllib.parse import urlparse

import pytest
from dvc_ssh import SSHFileSystem
from dvc_ssh.tests.cloud import TEST_SSH_KEY_PATH, TEST_SSH_USER

from dvc.repo.experiments.executor.base import ExecutorInfo, ExecutorResult
from dvc.repo.experiments.executor.ssh import SSHExecutor
from dvc.repo.experiments.refs import EXEC_HEAD, EXEC_MERGE
from tests.func.machine.conftest import *  # noqa, pylint: disable=wildcard-import


@contextmanager
def _ssh_factory(cloud):
    yield SSHFileSystem(
        host=cloud.host,
        port=cloud.port,
        user=TEST_SSH_USER,
        keyfile=TEST_SSH_KEY_PATH,
    )


def test_init_from_stash(tmp_dir, scm, dvc, machine_instance, mocker):
    mock = mocker.patch.object(SSHExecutor, "_from_stash_entry")
    mock_entry = mocker.Mock()
    mock_entry.name = ""
    SSHExecutor.from_stash_entry(
        dvc,
        mock_entry,
        machine_name="foo",
    )
    _args, kwargs = mock.call_args
    assert kwargs["host"] == machine_instance["instance_ip"]


@pytest.mark.needs_internet
@pytest.mark.parametrize("cloud", [pytest.lazy_fixture("git_ssh")])
def test_init_git(tmp_dir, dvc, scm, cloud, mocker):
    tmp_dir.scm_gen({"foo": "foo", "dir": {"bar": "bar"}}, commit="init")
    baseline_rev = scm.get_rev()
    tmp_dir.gen("foo", "stashed")
    scm.gitpython.git.stash()
    rev = scm.resolve_rev("stash@{0}")

    mock = mocker.Mock(baseline_rev=baseline_rev, head_rev=baseline_rev)

    root_url = cloud / SSHExecutor.gen_dirname()

    executor = SSHExecutor(
        root_dir=root_url.path,
        dvc_dir=".dvc",
        baseline_rev=baseline_rev,
        host=root_url.host,
        port=root_url.port,
        username=TEST_SSH_USER,
        fs_factory=partial(_ssh_factory, cloud),
    )
    infofile = str((root_url / "foo.run").path)
    executor.init_git(dvc, scm, rev, mock, infofile=infofile)
    assert root_url.path == executor._repo_abspath

    fs = cloud._ssh
    assert fs.exists(posixpath.join(executor._repo_abspath, "foo"))
    assert fs.exists(posixpath.join(executor._repo_abspath, "dir"))
    assert fs.exists(posixpath.join(executor._repo_abspath, "dir", "bar"))


@pytest.mark.needs_internet
@pytest.mark.parametrize("cloud", [pytest.lazy_fixture("git_ssh")])
def test_init_cache(tmp_dir, dvc, scm, cloud):
    foo = tmp_dir.dvc_gen("foo", "foo", commit="init")[0].outs[0]
    rev = scm.get_rev()
    scm.set_ref(EXEC_HEAD, rev)
    scm.set_ref(EXEC_MERGE, rev)
    root_url = cloud / SSHExecutor.gen_dirname()

    executor = SSHExecutor(
        root_dir=root_url.path,
        dvc_dir=".dvc",
        baseline_rev=rev,
        host=root_url.host,
        port=root_url.port,
        username=TEST_SSH_USER,
        fs_factory=partial(_ssh_factory, cloud),
    )
    executor.init_cache(dvc, rev)

    fs = cloud._ssh
    foo_hash = foo.hash_info.value
    assert fs.exists(
        posixpath.join(
            executor._repo_abspath, ".dvc", "cache", foo_hash[:2], foo_hash[2:]
        )
    )


@pytest.mark.needs_internet
@pytest.mark.parametrize("cloud", [pytest.lazy_fixture("git_ssh")])
def test_reproduce(tmp_dir, scm, dvc, cloud, exp_stage, mocker):
    from sshfs import SSHFileSystem as _sshfs  # noqa: N813

    rev = scm.get_rev()
    root_url = cloud / SSHExecutor.gen_dirname()
    mocker.patch.object(SSHFileSystem, "exists", return_value=True)
    mock_execute = mocker.patch.object(_sshfs, "execute")
    info = ExecutorInfo(
        str(root_url),
        rev,
        "machine-foo",
        str(root_url.path),
        ".dvc",
    )
    infofile = str((root_url / "foo.run").path)
    SSHExecutor.reproduce(
        info,
        rev,
        fs_factory=partial(_ssh_factory, cloud),
    )
    assert mock_execute.called_once()
    _name, args, _kwargs = mock_execute.mock_calls[0]
    assert f"dvc exp exec-run --infofile {infofile}" in args[0]


@pytest.mark.needs_internet
@pytest.mark.parametrize("cloud", [pytest.lazy_fixture("git_ssh")])
def test_run_machine(tmp_dir, scm, dvc, cloud, exp_stage, mocker):
    baseline = scm.get_rev()
    factory = partial(_ssh_factory, cloud)
    mocker.patch.object(
        dvc.machine,
        "get_executor_kwargs",
        return_value={
            "host": cloud.host,
            "port": cloud.port,
            "username": TEST_SSH_USER,
            "fs_factory": factory,
        },
    )
    mocker.patch.object(dvc.machine, "get_setup_script", return_value=None)
    mock_repro = mocker.patch.object(
        SSHExecutor,
        "reproduce",
        return_value=ExecutorResult("abc123", None, False),
    )

    tmp_dir.gen("params.yaml", "foo: 2")
    dvc.experiments.run(exp_stage.addressing, machine="foo")
    assert mock_repro.called_once()
    _name, _args, kwargs = mock_repro.mock_calls[0]
    info = kwargs["info"]
    url = urlparse(info.git_url)
    assert url.scheme == "ssh"
    assert url.hostname == cloud.host
    assert url.port == cloud.port
    assert info.baseline_rev == baseline
    assert kwargs["infofile"] is not None
    assert kwargs["fs_factory"] is not None




tests/func/machine/__init__.py




tests/func/machine/conftest.py
import textwrap

import pytest

BASIC_CONFIG = textwrap.dedent(
    """\
        [feature]
            machine = true
        ['machine "foo"']
            cloud = aws
    """
)

TEST_INSTANCE = {
    "aws_security_group": None,
    "cloud": "aws",
    "id": "iterative-2jyhw8j9ieov6",
    "image": "ubuntu-bionic-18.04-amd64-server-20210818",
    "instance_gpu": None,
    "instance_hdd_size": 35,
    "instance_ip": "123.123.123.123",
    "instance_launch_time": "2021-08-25T07:13:03Z",
    "instance_type": "m",
    "name": "test-resource",
    "region": "us-west",
    "spot": False,
    "spot_price": -1,
    "ssh_name": None,
    "ssh_private": "-----BEGIN RSA PRIVATE KEY-----\\n",
    "startup_script": "IyEvYmluL2Jhc2g=",
    "timeouts": None,
}


@pytest.fixture
def machine_config(tmp_dir):
    (tmp_dir / ".dvc" / "config").write_text(BASIC_CONFIG)
    return BASIC_CONFIG


@pytest.fixture
def machine_instance(tmp_dir, dvc, mocker):
    with dvc.config.edit() as conf:
        conf["machine"]["foo"] = {"cloud": "aws"}

    def mock_instances(name=None, **kwargs):
        if name == "foo":
            return iter([TEST_INSTANCE])
        return iter([])

    mocker.patch(
        "tpi.terraform.TerraformBackend.instances",
        mocker.MagicMock(side_effect=mock_instances),
    )
    return TEST_INSTANCE




tests/func/machine/test_machine_config.py
import os
import textwrap

import pytest

from dvc.cli import main
from dvc.ui import ui
from tests.utils import console_width

from .conftest import BASIC_CONFIG


@pytest.mark.parametrize(
    "slot,value",
    [
        ("region", "us-west"),
        ("image", "iterative-cml"),
        ("spot", "True"),
        ("spot_price", "1.2345"),
        ("spot_price", "12345"),
        ("instance_hdd_size", "10"),
        ("instance_type", "l"),
        ("instance_gpu", "tesla"),
        ("ssh_private", "secret"),
    ],
)
def test_machine_modify_susccess(tmp_dir, dvc, machine_config, slot, value):
    assert main(["machine", "modify", "foo", slot, value]) == 0
    assert (
        tmp_dir / ".dvc" / "config"
    ).read_text() == machine_config + f"    {slot} = {value}\n"
    assert main(["machine", "modify", "--unset", "foo", slot]) == 0
    assert (tmp_dir / ".dvc" / "config").read_text() == machine_config


def test_machine_modify_startup_script(tmp_dir, dvc, machine_config):
    slot, value = "startup_script", "start.sh"
    assert main(["machine", "modify", "foo", slot, value]) == 0
    assert (
        tmp_dir / ".dvc" / "config"
    ).read_text() == machine_config + f"    {slot} = ../{value}\n"
    assert main(["machine", "modify", "--unset", "foo", slot]) == 0
    assert (tmp_dir / ".dvc" / "config").read_text() == machine_config


@pytest.mark.parametrize(
    "slot,value,msg",
    [
        (
            "region",
            "other-west",
            "expected one of us-west, us-east, eu-west, eu-north",
        ),
        ("spot_price", "NUM", "expected float"),
        ("instance_hdd_size", "BIG", "expected int"),
    ],
)
def test_machine_modify_fail(tmp_dir, dvc, machine_config, caplog, slot, value, msg):
    assert main(["machine", "modify", "foo", slot, value]) == 251
    assert (tmp_dir / ".dvc" / "config").read_text() == machine_config
    assert msg in caplog.text


FULL_CONFIG_TEXT = textwrap.dedent(
    """\
        [feature]
            machine = true
        ['machine \"bar\"']
            cloud = azure
        ['machine \"foo\"']
            cloud = aws
            region = us-west
            image = iterative-cml
            spot = True
            spot_price = 1.2345
            instance_hdd_size = 10
            instance_type = l
            instance_gpu = tesla
            ssh_private = secret
            startup_script = {}
    """.format(
        os.path.join("..", "start.sh")
    )
)


def test_machine_list(tmp_dir, dvc, capsys):
    from dvc.commands.machine import CmdMachineList

    (tmp_dir / ".dvc" / "config").write_text(FULL_CONFIG_TEXT)

    with console_width(ui.rich_console, 255):
        assert main(["machine", "list"]) == 0
    out, _ = capsys.readouterr()
    for key in CmdMachineList.TABLE_COLUMNS:
        assert f"{key}" in out
    assert "bar     azure    -         -" in out
    assert "foo     aws      us-west   iterative-cml  True    1.2345" in out
    assert "10                   l                ***            ***" in out
    assert "tesla" in out

    with console_width(ui.rich_console, 255):
        assert main(["machine", "list", "bar"]) == 0
    out, _ = capsys.readouterr()
    assert "foo" not in out
    assert "name    cloud" in out
    assert "bar     azure" in out


def test_machine_rename_success(tmp_dir, scm, dvc, machine_config, capsys, mocker):
    import tpi

    config_file = tmp_dir / ".dvc" / "config"

    mocker.patch.object(
        tpi.terraform.TerraformBackend,
        "state_mv",
        autospec=True,
        return_value=True,
    )

    os.makedirs(tmp_dir / ".dvc" / "tmp" / "machine" / "terraform" / "foo")

    assert main(["machine", "rename", "foo", "bar"]) == 0
    cap = capsys.readouterr()
    assert "Rename machine 'foo' to 'bar'." in cap.out
    assert config_file.read_text() == machine_config.replace("foo", "bar")
    assert not (tmp_dir / ".dvc" / "tmp" / "machine" / "terraform" / "foo").exists()
    assert (tmp_dir / ".dvc" / "tmp" / "machine" / "terraform" / "bar").exists()


def test_machine_rename_none_exist(tmp_dir, scm, dvc, caplog):
    config_alice = BASIC_CONFIG.replace("foo", "alice")
    config_file = tmp_dir / ".dvc" / "config"
    config_file.write_text(config_alice)
    assert main(["machine", "rename", "foo", "bar"]) == 251
    assert config_file.read_text() == config_alice
    assert "machine 'foo' doesn't exist." in caplog.text


def test_machine_rename_exist(tmp_dir, scm, dvc, caplog):
    config_bar = BASIC_CONFIG + "['machine \"bar\"']\n    cloud = aws"
    config_file = tmp_dir / ".dvc" / "config"
    config_file.write_text(config_bar)
    assert main(["machine", "rename", "foo", "bar"]) == 251
    assert config_file.read_text() == config_bar
    assert "Machine 'bar' already exists." in caplog.text


def test_machine_rename_error(tmp_dir, scm, dvc, machine_config, caplog, mocker):
    import tpi

    config_file = tmp_dir / ".dvc" / "config"
    os.makedirs(tmp_dir / ".dvc" / "tmp" / "machine" / "terraform" / "foo")

    def cmd_error(self, source, destination, **kwargs):
        raise tpi.TPIError("test error")

    mocker.patch.object(tpi.terraform.TerraformBackend, "state_mv", cmd_error)

    assert main(["machine", "rename", "foo", "bar"]) == 251
    assert config_file.read_text() == machine_config
    assert "rename failed" in caplog.text




tests/func/machine/test_machine_status.py
from dvc.cli import main
from dvc.ui import ui
from tests.utils import console_width


def test_status(tmp_dir, scm, dvc, machine_config, machine_instance, capsys):
    assert main(["machine", "add", "bar", "aws"]) == 0
    with console_width(ui.rich_console, 255):
        assert main(["machine", "status"]) == 0
    cap = capsys.readouterr()
    assert (
        "name    instance    status    cloud    instance_ip      "
        "instance_type    instance_hdd_size    instance_gpu" in cap.out
    )
    assert (
        "bar     -           offline   -        -                "
        "-                -                    -" in cap.out
    )
    assert (
        "foo     num_1       running   aws      123.123.123.123  "
        "m                35                   None" in cap.out
    )




tests/func/metrics/__init__.py




tests/func/metrics/test_diff.py
import json
from os.path import join

import pytest

from dvc.cli import main
from dvc.utils import relpath


def test_metrics_diff_simple(tmp_dir, scm, dvc, run_copy_metrics):
    def _gen(val):
        tmp_dir.gen({"m_temp.yaml": str(val)})
        run_copy_metrics("m_temp.yaml", "m.yaml", metrics=["m.yaml"])
        dvc.scm.commit(str(val))

    _gen(1)
    _gen(2)
    _gen(3)

    expected = {"m.yaml": {"": {"old": 1, "new": 3, "diff": 2}}}

    assert dvc.metrics.diff(a_rev="HEAD~2") == expected


def test_metrics_diff_yaml(tmp_dir, scm, dvc, run_copy_metrics):
    def _gen(val):
        metrics = {"a": {"b": {"c": val, "d": 1, "e": str(val)}}}
        (tmp_dir / "m_temp.yaml").dump(metrics)
        run_copy_metrics("m_temp.yaml", "m.yaml", metrics=["m.yaml"], commit=str(val))

    _gen(1)
    _gen(2)
    _gen(3)

    expected = {
        "m.yaml": {
            "a.b.e": {"old": "1", "new": "3"},
            "a.b.c": {"old": 1, "new": 3, "diff": 2},
        }
    }

    assert dvc.metrics.diff(a_rev="HEAD~2") == expected


def test_metrics_diff_json(tmp_dir, scm, dvc, run_copy_metrics):
    def _gen(val):
        metrics = {"a": {"b": {"c": val, "d": 1, "e": str(val)}}}
        (tmp_dir / "m_temp.json").dump(metrics)
        run_copy_metrics("m_temp.json", "m.json", metrics=["m.json"], commit=str(val))

    _gen(1)
    _gen(2)
    _gen(3)

    expected = {
        "m.json": {
            "a.b.e": {"old": "1", "new": "3"},
            "a.b.c": {"old": 1, "new": 3, "diff": 2},
        }
    }
    assert dvc.metrics.diff(a_rev="HEAD~2") == expected


def test_metrics_diff_json_unchanged(tmp_dir, scm, dvc, run_copy_metrics):
    def _gen(val):
        metrics = {"a": {"b": {"c": val, "d": 1, "e": str(val)}}}
        (tmp_dir / "m_temp.json").dump(metrics)
        run_copy_metrics("m_temp.json", "m.json", metrics=["m.json"], commit=str(val))

    _gen(1)
    _gen(2)
    _gen(1)

    assert dvc.metrics.diff(a_rev="HEAD~2") == {}


def test_metrics_diff_broken_json(tmp_dir, scm, dvc, run_copy_metrics):
    metrics = {"a": {"b": {"c": 1, "d": 1, "e": "3"}}}
    (tmp_dir / "m_temp.json").dump(metrics)
    run_copy_metrics(
        "m_temp.json",
        "m.json",
        metrics_no_cache=["m.json"],
        commit="add metrics",
    )

    (tmp_dir / "m.json").write_text(json.dumps(metrics) + "ma\nlformed\n")

    assert dvc.metrics.diff() == {
        "m.json": {
            "a.b.e": {"old": "3", "new": None},
            "a.b.c": {"old": 1, "new": None},
            "a.b.d": {"old": 1, "new": None},
        }
    }


def test_metrics_diff_no_metrics(tmp_dir, scm, dvc):
    tmp_dir.scm_gen({"foo": "foo"}, commit="add foo")
    assert dvc.metrics.diff(a_rev="HEAD~1") == {}


def test_metrics_diff_new_metric(tmp_dir, scm, dvc, run_copy_metrics):
    metrics = {"a": {"b": {"c": 1, "d": 1, "e": "3"}}}
    (tmp_dir / "m_temp.json").dump(metrics)
    run_copy_metrics("m_temp.json", "m.json", metrics_no_cache=["m.json"])

    assert dvc.metrics.diff() == {
        "m.json": {
            "a.b.e": {"old": None, "new": "3"},
            "a.b.c": {"old": None, "new": 1},
            "a.b.d": {"old": None, "new": 1},
        }
    }


def test_metrics_diff_deleted_metric(tmp_dir, scm, dvc, run_copy_metrics):
    metrics = {"a": {"b": {"c": 1, "d": 1, "e": "3"}}}
    (tmp_dir / "m_temp.json").dump(metrics)
    run_copy_metrics(
        "m_temp.json",
        "m.json",
        metrics_no_cache=["m.json"],
        commit="add metrics",
    )

    (tmp_dir / "m.json").unlink()

    assert dvc.metrics.diff() == {
        "m.json": {
            "a.b.e": {"old": "3", "new": None},
            "a.b.c": {"old": 1, "new": None},
            "a.b.d": {"old": 1, "new": None},
        }
    }


def test_metrics_diff_with_unchanged(tmp_dir, scm, dvc, run_copy_metrics):
    tmp_dir.gen("metrics_temp.yaml", "foo: 1\nxyz: 10")
    run_copy_metrics(
        "metrics_temp.yaml",
        "metrics.yaml",
        metrics_no_cache=["metrics.yaml"],
        commit="1",
    )

    tmp_dir.scm_gen("metrics.yaml", "foo: 2\nxyz: 10", commit="2")
    tmp_dir.scm_gen("metrics.yaml", "foo: 3\nxyz: 10", commit="3")

    assert dvc.metrics.diff(a_rev="HEAD~2", all=True) == {
        "metrics.yaml": {
            "foo": {"old": 1, "new": 3, "diff": 2},
            "xyz": {"old": 10, "new": 10, "diff": 0},
        }
    }


def test_no_commits(tmp_dir):
    from dvc.repo import Repo
    from dvc.scm import Git

    git = Git.init(tmp_dir.fs_path)
    assert git.no_commits

    assert Repo.init().metrics.diff() == {}


def test_metrics_diff_dirty(tmp_dir, scm, dvc, run_copy_metrics):
    def _gen(val):
        tmp_dir.gen({"m_temp.yaml": str(val)})
        run_copy_metrics("m_temp.yaml", "m.yaml", metrics=["m.yaml"])
        dvc.scm.commit(str(val))

    _gen(1)
    _gen(2)
    _gen(3)

    tmp_dir.gen({"m.yaml": "4"})

    expected = {"m.yaml": {"": {"old": 3, "new": 4, "diff": 1}}}

    assert dvc.metrics.diff() == expected


def test_metrics_diff_cli(tmp_dir, scm, dvc, run_copy_metrics, caplog, capsys):
    def _gen(val):
        tmp_dir.gen({"m_temp.yaml": f"foo: {val}"})
        run_copy_metrics("m_temp.yaml", "m.yaml", metrics=["m.yaml"])
        dvc.scm.commit(str(val))

    _gen(1.23456789)
    _gen(2.34567891011)
    _gen(3.45678910111213)

    caplog.clear()
    capsys.readouterr()  # clearing the buffer
    assert main(["metrics", "diff", "HEAD~2"]) == 0

    captured = capsys.readouterr()

    assert (
        captured.out == "Path    Metric    HEAD~2    workspace    Change\n"
        "m.yaml  foo       1.23457   3.45679      2.22222\n"
    )


def test_metrics_diff_non_metrics(tmp_dir, scm, dvc):
    def _gen(val):
        tmp_dir.scm_gen({"some_file.yaml": f"foo: {val}"}, commit=str(val))

    _gen(1)
    _gen(2)
    _gen(3)

    result = dvc.metrics.diff(targets=["some_file.yaml"], a_rev="HEAD~2")
    assert result == {"some_file.yaml": {"foo": {"old": 1, "new": 3, "diff": 2}}}


@pytest.mark.parametrize(
    "dvcfile, metrics_file",
    [
        ("dvc.yaml", "my_metrics.yaml"),
        ("dir/dvc.yaml", "my_metrics.yaml"),
        ("dir/dvc.yaml", join("..", "my_metrics.yaml")),
    ],
)
def test_diff_top_level_metrics(tmp_dir, dvc, scm, dvcfile, metrics_file):
    directory = (tmp_dir / dvcfile).parent
    directory.mkdir(exist_ok=True)
    (tmp_dir / dvcfile).dump({"metrics": [metrics_file]})

    metrics_file = directory / metrics_file
    metrics_file.dump({"foo": 3})
    scm.add_commit([metrics_file, tmp_dir / dvcfile], message="add metrics")

    metrics_file.dump({"foo": 5})
    assert dvc.metrics.diff() == {
        relpath(directory / metrics_file): {"foo": {"diff": 2, "new": 5, "old": 3}}
    }


def test_metrics_diff_active_branch_unchanged(tmp_dir, scm, dvc, run_copy_metrics):
    def _gen(val):
        metrics = {"a": {"b": {"c": val, "d": 1, "e": str(val)}}}
        (tmp_dir / "m_temp.yaml").dump(metrics)
        run_copy_metrics("m_temp.yaml", "m.yaml", metrics=["m.yaml"], commit=str(val))

    _gen(1)
    _gen(2)
    _gen(1)

    assert dvc.metrics.diff(a_rev=tmp_dir.scm.active_branch()) == {}




tests/func/metrics/test_show.py
import os

import pytest
from funcy import get_in

from dvc.dvcfile import PROJECT_FILE
from dvc.exceptions import OverlappingOutputPathsError
from dvc.repo import Repo
from dvc.utils.fs import remove
from dvc.utils.serialize import JSONFileCorruptedError, YAMLFileCorruptedError


def test_show_simple(tmp_dir, dvc, run_copy_metrics):
    tmp_dir.gen("metrics_t.yaml", "1.1")
    run_copy_metrics("metrics_t.yaml", "metrics.yaml", metrics=["metrics.yaml"])
    assert dvc.metrics.show() == {"": {"data": {"metrics.yaml": {"data": 1.1}}}}


def test_show_simple_from_subdir(tmp_dir, dvc, run_copy_metrics):
    subdir = tmp_dir / "subdir"
    subdir.mkdir()
    tmp_dir.gen("metrics_t.yaml", "1.1")
    run_copy_metrics(
        "metrics_t.yaml",
        "subdir/metrics.yaml",
        metrics=["subdir/metrics.yaml"],
    )

    expected_path = os.path.join("subdir", "metrics.yaml")
    assert dvc.metrics.show() == {"": {"data": {expected_path: {"data": 1.1}}}}

    expected_path = os.path.join("..", "subdir", "metrics.yaml")
    with subdir.chdir():
        assert dvc.metrics.show() == {"": {"data": {expected_path: {"data": 1.1}}}}
    subdir2 = tmp_dir / "subdir2"
    subdir2.mkdir()
    with subdir2.chdir():
        assert dvc.metrics.show() == {"": {"data": {expected_path: {"data": 1.1}}}}


def test_show(tmp_dir, dvc, run_copy_metrics):
    tmp_dir.gen("metrics_t.yaml", "foo: 1.1")
    run_copy_metrics("metrics_t.yaml", "metrics.yaml", metrics=["metrics.yaml"])
    assert dvc.metrics.show() == {
        "": {"data": {"metrics.yaml": {"data": {"foo": 1.1}}}}
    }


def test_show_toml(tmp_dir, dvc, run_copy_metrics):
    tmp_dir.gen("metrics_t.toml", "[foo]\nbar = 1.2")
    run_copy_metrics("metrics_t.toml", "metrics.toml", metrics=["metrics.toml"])
    assert dvc.metrics.show() == {
        "": {"data": {"metrics.toml": {"data": {"foo": {"bar": 1.2}}}}}
    }


def test_show_targets(tmp_dir, dvc, run_copy_metrics):
    tmp_dir.gen("metrics_t.yaml", "foo: 1.1")
    run_copy_metrics("metrics_t.yaml", "metrics.yaml", metrics=["metrics.yaml"])
    expected = {"": {"data": {"metrics.yaml": {"data": {"foo": 1.1}}}}}
    assert dvc.metrics.show(targets=["metrics.yaml"]) == expected
    assert dvc.metrics.show(targets=(tmp_dir / "metrics.yaml").fs_path) == expected


def test_show_multiple(tmp_dir, dvc, run_copy_metrics):
    tmp_dir.gen("foo_temp", "foo: 1\n")
    tmp_dir.gen("baz_temp", "baz: 2\n")
    run_copy_metrics("foo_temp", "foo", fname="foo.dvc", metrics=["foo"])
    run_copy_metrics("baz_temp", "baz", fname="baz.dvc", metrics=["baz"])
    assert dvc.metrics.show() == {
        "": {"data": {"foo": {"data": {"foo": 1}}, "baz": {"data": {"baz": 2}}}}
    }


def test_show_branch(tmp_dir, scm, dvc, run_copy_metrics):
    tmp_dir.gen("metrics_temp.yaml", "foo: 1")
    run_copy_metrics(
        "metrics_temp.yaml", "metrics.yaml", metrics_no_cache=["metrics.yaml"]
    )
    scm.add(["metrics.yaml", "metrics.yaml.dvc"])
    scm.commit("init")

    with tmp_dir.branch("branch", new=True):
        tmp_dir.scm_gen("metrics.yaml", "foo: 2", commit="branch")

    assert dvc.metrics.show(revs=["branch"]) == {
        "workspace": {"data": {"metrics.yaml": {"data": {"foo": 1}}}},
        "branch": {"data": {"metrics.yaml": {"data": {"foo": 2}}}},
    }


def test_show_subrepo_with_preexisting_tags(tmp_dir, scm):
    tmp_dir.gen("foo", "foo")
    scm.add("foo")
    scm.commit("init")
    scm.tag("no-metrics")

    tmp_dir.gen({"subdir": {}})
    subrepo_dir = tmp_dir / "subdir"
    with subrepo_dir.chdir():
        dvc = Repo.init(subdir=True)
        scm.commit("init dvc")

        dvc.run(
            cmd="echo foo: 1 > metrics.yaml",
            metrics=["metrics.yaml"],
            single_stage=True,
        )

    scm.add(
        [
            str(subrepo_dir / "metrics.yaml"),
            str(subrepo_dir / "metrics.yaml.dvc"),
        ]
    )
    scm.commit("init metrics")
    scm.tag("v1")

    expected_path = os.path.join("subdir", "metrics.yaml")
    assert dvc.metrics.show(all_tags=True) == {
        "workspace": {"data": {expected_path: {"data": {"foo": 1}}}},
        "v1": {"data": {expected_path: {"data": {"foo": 1}}}},
    }


def test_missing_cache(tmp_dir, dvc, run_copy_metrics):
    tmp_dir.gen("metrics_t.yaml", "1.1")
    run_copy_metrics("metrics_t.yaml", "metrics.yaml", metrics=["metrics.yaml"])

    # This one should be skipped
    stage = run_copy_metrics(
        "metrics_t.yaml", "metrics2.yaml", metrics=["metrics2.yaml"]
    )
    remove(stage.outs[0].fspath)
    remove(stage.outs[0].cache_path)

    result = dvc.metrics.show()
    metrics2 = result[""]["data"].pop("metrics2.yaml")
    assert isinstance(metrics2["error"], FileNotFoundError)
    assert result == {
        "": {
            "data": {
                "metrics.yaml": {"data": 1.1},
            }
        }
    }


@pytest.mark.parametrize("use_dvc", [True, False])
def test_show_non_metric(tmp_dir, scm, use_dvc):
    tmp_dir.gen("metrics.yaml", "foo: 1.1")

    if use_dvc:
        dvc = Repo.init()
    else:
        dvc = Repo(uninitialized=True)

    assert dvc.metrics.show(targets=["metrics.yaml"]) == {
        "": {"data": {"metrics.yaml": {"data": {"foo": 1.1}}}}
    }

    if not use_dvc:
        assert not (tmp_dir / ".dvc").exists()


@pytest.mark.parametrize("use_dvc", [True, False])
def test_show_non_metric_branch(tmp_dir, scm, use_dvc):
    tmp_dir.scm_gen("metrics.yaml", "foo: 1.1", commit="init")
    with tmp_dir.branch("branch", new=True):
        tmp_dir.scm_gen("metrics.yaml", "foo: 2.2", commit="other")

    if use_dvc:
        dvc = Repo.init()
    else:
        dvc = Repo(uninitialized=True)

    assert dvc.metrics.show(targets=["metrics.yaml"], revs=["branch"]) == {
        "workspace": {"data": {"metrics.yaml": {"data": {"foo": 1.1}}}},
        "branch": {"data": {"metrics.yaml": {"data": {"foo": 2.2}}}},
    }

    if not use_dvc:
        assert not (tmp_dir / ".dvc").exists()


def test_non_metric_and_recurisve_show(tmp_dir, dvc, run_copy_metrics):
    tmp_dir.gen({"metrics_t.yaml": "foo: 1.1", "metrics": {"metric1.yaml": "bar: 1.2"}})

    metric2 = os.fspath(tmp_dir / "metrics" / "metric2.yaml")
    run_copy_metrics("metrics_t.yaml", metric2, metrics=[metric2])

    assert dvc.metrics.show(targets=["metrics_t.yaml", "metrics"], recursive=True) == {
        "": {
            "data": {
                os.path.join("metrics", "metric1.yaml"): {"data": {"bar": 1.2}},
                os.path.join("metrics", "metric2.yaml"): {"data": {"foo": 1.1}},
                "metrics_t.yaml": {"data": {"foo": 1.1}},
            }
        }
    }


def test_show_falsey(tmp_dir, dvc):
    tmp_dir.gen("metrics.json", '{"foo": 0, "bar": 0.0, "baz": {}}')
    assert dvc.metrics.show(targets=["metrics.json"]) == {
        "": {"data": {"metrics.json": {"data": {"foo": 0, "bar": 0.0}}}}
    }


def test_show_no_repo(tmp_dir):
    tmp_dir.gen("metrics.json", '{"foo": 0, "bar": 0.0, "baz": {}}')

    dvc = Repo(uninitialized=True)

    assert dvc.metrics.show(targets=["metrics.json"]) == {
        "": {"data": {"metrics.json": {"data": {"foo": 0, "bar": 0.0}}}}
    }


def test_show_malformed_metric(tmp_dir, scm, dvc, caplog):
    tmp_dir.gen("metric.json", '{"m":1')

    assert isinstance(
        dvc.metrics.show(targets=["metric.json"])[""]["data"]["metric.json"]["error"],
        JSONFileCorruptedError,
    )


def test_metrics_show_no_target(tmp_dir, dvc, capsys):
    assert dvc.metrics.show(targets=["metrics.json"]) == {"": {}}


def test_show_no_metrics_files(tmp_dir, dvc, caplog):
    assert dvc.metrics.show() == {"": {}}


@pytest.mark.parametrize("clear_before_run", [True, False])
def test_metrics_show_overlap(tmp_dir, dvc, run_copy_metrics, clear_before_run):
    data_dir = tmp_dir / "data"
    data_dir.mkdir()

    (data_dir / "m1_temp.yaml").dump({"a": {"b": {"c": 2, "d": 1}}})
    run_copy_metrics(
        str(data_dir / "m1_temp.yaml"),
        str(data_dir / "m1.yaml"),
        single_stage=False,
        commit="add m1",
        name="cp-m1",
        metrics=[str(data_dir / "m1.yaml")],
    )
    with (tmp_dir / "dvc.yaml").modify() as d:
        # trying to make an output overlaps error
        d["stages"]["corrupted-stage"] = {
            "cmd": "mkdir data",
            "outs": ["data"],
        }

    # running by clearing and not clearing stuffs
    # so as it works even for optimized cases
    if clear_before_run:
        remove(data_dir)
        remove(dvc.cache.local.path)

    dvc._reset()

    res = dvc.metrics.show()
    assert isinstance(res[""]["error"], OverlappingOutputPathsError)


@pytest.mark.parametrize(
    "file,error_path",
    (
        (PROJECT_FILE, ["workspace", "error"]),
        ("metrics.yaml", ["workspace", "data", "metrics.yaml", "error"]),
    ),
)
def test_log_errors(tmp_dir, scm, dvc, capsys, run_copy_metrics, file, error_path):
    tmp_dir.gen("metrics_t.yaml", "m: 1.1")
    run_copy_metrics(
        "metrics_t.yaml",
        "metrics.yaml",
        metrics=["metrics.yaml"],
        single_stage=False,
        name="train",
    )
    scm.tag("v1")

    with open(file, "a", encoding="utf-8") as fd:
        fd.write("\nMALFORMED!")

    result = dvc.metrics.show(revs=["v1"])

    _, error = capsys.readouterr()

    assert isinstance(get_in(result, error_path), YAMLFileCorruptedError)
    assert (
        "DVC failed to load some metrics for following revisions: 'workspace'." in error
    )




tests/func/params/__init__.py




tests/func/params/test_diff.py
from os.path import join

import pytest

from dvc.utils import relpath


def test_diff_no_params(tmp_dir, scm, dvc):
    assert dvc.params.diff() == {}


def test_diff_no_changes(tmp_dir, scm, dvc):
    tmp_dir.gen("params.yaml", "foo: bar")
    dvc.run(cmd="echo params.yaml", params=["foo"], single_stage=True)
    scm.add(["params.yaml", "Dvcfile"])
    scm.commit("bar")
    assert dvc.params.diff() == {}


def test_diff(tmp_dir, scm, dvc):
    tmp_dir.gen("params.yaml", "foo: bar")
    dvc.run(cmd="echo params.yaml", params=["foo"], single_stage=True)
    scm.add(["params.yaml", "Dvcfile"])
    scm.commit("bar")

    tmp_dir.scm_gen("params.yaml", "foo: baz", commit="baz")
    tmp_dir.scm_gen("params.yaml", "foo: qux", commit="qux")

    assert dvc.params.diff(a_rev="HEAD~2") == {
        "params.yaml": {"foo": {"old": "bar", "new": "qux"}}
    }


def test_diff_dirty(tmp_dir, scm, dvc):
    tmp_dir.gen("params.yaml", "foo: bar")
    dvc.run(cmd="echo params.yaml", params=["foo"], single_stage=True)
    scm.add(["params.yaml", "Dvcfile"])
    scm.commit("bar")

    tmp_dir.scm_gen("params.yaml", "foo: baz", commit="baz")
    tmp_dir.gen("params.yaml", "foo: qux")

    assert dvc.params.diff() == {"params.yaml": {"foo": {"old": "baz", "new": "qux"}}}


def test_diff_new(tmp_dir, scm, dvc):
    tmp_dir.gen("params.yaml", "foo: bar")
    dvc.run(cmd="echo params.yaml", params=["foo"], single_stage=True)

    assert dvc.params.diff() == {"params.yaml": {"foo": {"old": None, "new": "bar"}}}


def test_diff_deleted(tmp_dir, scm, dvc):
    tmp_dir.gen("params.yaml", "foo: bar")
    dvc.run(cmd="echo params.yaml", params=["foo"], single_stage=True)
    scm.add(["params.yaml", "Dvcfile"])
    scm.commit("bar")

    (tmp_dir / "params.yaml").unlink()

    assert dvc.params.diff() == {"params.yaml": {"foo": {"old": "bar", "new": None}}}


def test_diff_list(tmp_dir, scm, dvc):
    tmp_dir.gen("params.yaml", "foo:\n- bar\n- baz")
    dvc.run(cmd="echo params.yaml", params=["foo"], single_stage=True)
    scm.add(["params.yaml", "Dvcfile"])
    scm.commit("foo")

    tmp_dir.gen("params.yaml", "foo:\n- bar\n- baz\n- qux")

    assert dvc.params.diff() == {
        "params.yaml": {
            "foo": {"old": "['bar', 'baz']", "new": "['bar', 'baz', 'qux']"}
        }
    }


def test_diff_dict(tmp_dir, scm, dvc):
    tmp_dir.gen("params.yaml", "foo:\n  bar: baz")
    dvc.run(cmd="echo params.yaml", params=["foo"], single_stage=True)
    scm.add(["params.yaml", "Dvcfile"])
    scm.commit("foo")

    tmp_dir.gen("params.yaml", "foo:\n  bar: qux")

    assert dvc.params.diff() == {
        "params.yaml": {"foo.bar": {"old": "baz", "new": "qux"}}
    }


def test_diff_with_unchanged(tmp_dir, scm, dvc):
    tmp_dir.gen("params.yaml", "foo: bar\nxyz: val")
    dvc.run(cmd="echo params.yaml", params=["foo,xyz"], single_stage=True)
    scm.add(["params.yaml", "Dvcfile"])
    scm.commit("bar")

    tmp_dir.scm_gen("params.yaml", "foo: baz\nxyz: val", commit="baz")
    tmp_dir.scm_gen("params.yaml", "foo: qux\nxyz: val", commit="qux")

    assert dvc.params.diff(a_rev="HEAD~2", all=True) == {
        "params.yaml": {
            "foo": {"old": "bar", "new": "qux"},
            "xyz": {"old": "val", "new": "val"},
        }
    }


def test_pipeline_tracked_params(tmp_dir, scm, dvc, run_copy):
    from dvc.dvcfile import PROJECT_FILE

    tmp_dir.gen({"foo": "foo", "params.yaml": "foo: bar\nxyz: val"})
    run_copy("foo", "bar", name="copy-foo-bar", params=["foo,xyz"])

    scm.add(["params.yaml", PROJECT_FILE])
    scm.commit("add stage")

    tmp_dir.scm_gen("params.yaml", "foo: baz\nxyz: val", commit="baz")
    tmp_dir.scm_gen("params.yaml", "foo: qux\nxyz: val", commit="qux")

    assert dvc.params.diff(a_rev="HEAD~2") == {
        "params.yaml": {"foo": {"old": "bar", "new": "qux"}}
    }


def test_no_commits(tmp_dir):
    from dvc.repo import Repo
    from dvc.scm import Git

    git = Git.init(tmp_dir.fs_path)
    assert git.no_commits

    assert Repo.init().params.diff() == {}


def test_vars_shows_on_params_diff(tmp_dir, scm, dvc):
    params_file = tmp_dir / "test_params.yaml"
    param_data = {"vars": {"model1": {"epoch": 15}, "model2": {"epoch": 35}}}
    (tmp_dir / params_file).dump(param_data)
    d = {
        "vars": ["test_params.yaml"],
        "stages": {
            "build": {
                "foreach": "${vars}",
                "do": {"cmd": "script --epoch ${item.epoch}"},
            }
        },
    }
    (tmp_dir / "dvc.yaml").dump(d)
    assert dvc.params.diff() == {
        "test_params.yaml": {
            "vars.model1.epoch": {"new": 15, "old": None},
            "vars.model2.epoch": {"new": 35, "old": None},
        }
    }
    scm.add(["dvc.yaml", "test_params.yaml"])
    scm.commit("added stages")

    param_data["vars"]["model1"]["epoch"] = 20
    (tmp_dir / params_file).dump(param_data)
    assert dvc.params.diff() == {
        "test_params.yaml": {"vars.model1.epoch": {"new": 20, "old": 15, "diff": 5}}
    }

    data_dir = tmp_dir / "data"
    data_dir.mkdir()
    with data_dir.chdir():
        assert dvc.params.diff() == {
            relpath(params_file): {
                "vars.model1.epoch": {"new": 20, "old": 15, "diff": 5}
            }
        }


def test_diff_targeted(tmp_dir, scm, dvc, run_copy):
    from dvc.dvcfile import PROJECT_FILE

    tmp_dir.gen(
        {
            "foo": "foo",
            "params.yaml": "foo: bar",
            "other_params.yaml": "xyz: val",
        }
    )
    run_copy(
        "foo",
        "bar",
        name="copy-foo-bar",
        params=["foo", "other_params.yaml:xyz"],
    )

    scm.add(["params.yaml", "other_params.yaml", PROJECT_FILE])
    scm.commit("add stage")

    tmp_dir.scm_gen(
        {"params.yaml": "foo: baz", "other_params.yaml": "xyz: val2"},
        commit="baz",
    )
    tmp_dir.scm_gen(
        {"params.yaml": "foo: qux", "other_params.yaml": "xyz: val3"},
        commit="qux",
    )

    assert dvc.params.diff(a_rev="HEAD~2") == {
        "params.yaml": {"foo": {"old": "bar", "new": "qux"}},
        "other_params.yaml": {"xyz": {"old": "val", "new": "val3"}},
    }

    assert dvc.params.diff(a_rev="HEAD~2", targets=["params.yaml"]) == {
        "params.yaml": {"foo": {"old": "bar", "new": "qux"}}
    }

    assert dvc.params.diff(a_rev="HEAD~2", targets=["other_params.yaml"]) == {
        "other_params.yaml": {"xyz": {"old": "val", "new": "val3"}}
    }


@pytest.mark.parametrize("file", ["params.yaml", "other_params.yaml"])
def test_diff_without_targets_specified(tmp_dir, dvc, scm, file):
    params_file = tmp_dir / file
    params_file.dump({"foo": {"bar": "bar"}, "x": "0"})
    dvc.stage.add(
        name="test",
        cmd=f"echo {file}",
        params=[{file: None}],
    )
    scm.add_commit([params_file, "dvc.yaml"], message="foo")

    params_file.dump({"foo": {"bar": "baz"}, "y": "100"})
    assert dvc.params.diff() == {
        file: {
            "foo.bar": {"new": "baz", "old": "bar"},
            "x": {"new": None, "old": "0"},
            "y": {"new": "100", "old": None},
        }
    }


@pytest.mark.parametrize(
    "dvcfile, params_file",
    [
        ("dvc.yaml", "my_params.yaml"),
        ("dir/dvc.yaml", "my_params.yaml"),
        ("dir/dvc.yaml", join("..", "my_params.yaml")),
    ],
)
def test_diff_top_level_params(tmp_dir, dvc, scm, dvcfile, params_file):
    directory = (tmp_dir / dvcfile).parent
    directory.mkdir(exist_ok=True)
    (tmp_dir / dvcfile).dump({"params": [params_file]})

    params_file = directory / params_file
    params_file.dump({"foo": 3})
    scm.add_commit([params_file, tmp_dir / dvcfile], message="add params")

    params_file.dump({"foo": 5})
    assert dvc.params.diff() == {
        relpath(directory / params_file): {"foo": {"diff": 2, "new": 5, "old": 3}}
    }


def test_diff_active_branch_no_changes(tmp_dir, scm, dvc):
    tmp_dir.gen("params.yaml", "foo: bar")
    dvc.run(cmd="echo params.yaml", params=["foo"], single_stage=True)
    scm.add(["params.yaml", "Dvcfile"])
    scm.commit("bar")
    assert dvc.params.diff(a_rev=tmp_dir.scm.active_branch()) == {}




tests/func/params/test_show.py
import operator
from functools import reduce

import pytest

from dvc.repo import Repo
from dvc.repo.stage import PROJECT_FILE
from dvc.utils.serialize import YAMLFileCorruptedError


def test_show_empty(dvc):
    assert dvc.params.show() == {}


def test_show(tmp_dir, dvc):
    tmp_dir.gen("params.yaml", "foo: bar")
    dvc.run(cmd="echo params.yaml", params=["foo"], single_stage=True)
    assert dvc.params.show() == {
        "": {"data": {"params.yaml": {"data": {"foo": "bar"}}}}
    }


def test_show_targets(tmp_dir, dvc):
    tmp_dir.gen("params.yaml", "foo: bar")
    dvc.run(cmd="echo params.yaml", params=["foo"], single_stage=True)
    expected = {"": {"data": {"params.yaml": {"data": {"foo": "bar"}}}}}
    assert dvc.params.show(targets=["params.yaml"]) == expected
    assert dvc.params.show(targets=(tmp_dir / "params.yaml").fs_path) == expected


def test_show_toml(tmp_dir, dvc):
    tmp_dir.gen("params.toml", "[foo]\nbar = 42\nbaz = [1, 2]\n")
    dvc.run(cmd="echo params.toml", params=["params.toml:foo"], single_stage=True)
    assert dvc.params.show() == {
        "": {"data": {"params.toml": {"data": {"foo": {"bar": 42, "baz": [1, 2]}}}}}
    }


def test_show_py(tmp_dir, dvc):
    tmp_dir.gen(
        "params.py",
        "CONST = 1\nIS_DIR: bool = True\n\n\nclass Config:\n    foo = 42\n",
    )
    dvc.run(
        cmd="echo params.py",
        params=["params.py:CONST,IS_DIR,Config.foo"],
        single_stage=True,
    )
    assert dvc.params.show() == {
        "": {
            "data": {
                "params.py": {
                    "data": {"CONST": 1, "Config": {"foo": 42}, "IS_DIR": True}
                }
            }
        }
    }


def test_show_multiple(tmp_dir, dvc):
    tmp_dir.gen("params.yaml", "foo: bar\nbaz: qux\n")
    dvc.run(
        cmd="echo params.yaml",
        fname="foo.dvc",
        params=["foo"],
        single_stage=True,
    )
    dvc.run(
        cmd="echo params.yaml",
        fname="baz.dvc",
        params=["baz"],
        single_stage=True,
    )
    assert dvc.params.show() == {
        "": {"data": {"params.yaml": {"data": {"baz": "qux", "foo": "bar"}}}}
    }


def test_show_list(tmp_dir, dvc):
    tmp_dir.gen("params.yaml", "foo:\n- bar\n- baz\n")
    dvc.run(cmd="echo params.yaml", params=["foo"], single_stage=True)
    assert dvc.params.show() == {
        "": {"data": {"params.yaml": {"data": {"foo": ["bar", "baz"]}}}}
    }


def test_show_branch(tmp_dir, scm, dvc):
    tmp_dir.gen("params.yaml", "foo: bar")
    dvc.run(cmd="echo params.yaml", params=["foo"], single_stage=True)
    scm.add(["params.yaml", "Dvcfile"])
    scm.commit("init")

    with tmp_dir.branch("branch", new=True):
        tmp_dir.scm_gen("params.yaml", "foo: baz", commit="branch")

    assert dvc.params.show(revs=["branch"]) == {
        "branch": {"data": {"params.yaml": {"data": {"foo": "baz"}}}},
        "workspace": {"data": {"params.yaml": {"data": {"foo": "bar"}}}},
    }


def test_pipeline_params(tmp_dir, scm, dvc, run_copy):
    tmp_dir.gen({"foo": "foo", "params.yaml": "foo: bar\nxyz: val\nabc: ignore"})
    run_copy("foo", "bar", name="copy-foo-bar", params=["foo,xyz"])
    scm.add(["params.yaml", PROJECT_FILE])
    scm.commit("add stage")

    tmp_dir.scm_gen("params.yaml", "foo: baz\nxyz: val\nabc: ignore", commit="baz")
    tmp_dir.scm_gen("params.yaml", "foo: qux\nxyz: val\nabc: ignore", commit="qux")

    assert dvc.params.show(revs=["master"], deps=True) == {
        "master": {"data": {"params.yaml": {"data": {"foo": "qux", "xyz": "val"}}}}
    }
    assert dvc.params.show(revs=["master"]) == {
        "master": {
            "data": {
                "params.yaml": {"data": {"abc": "ignore", "foo": "qux", "xyz": "val"}}
            }
        }
    }


def test_show_no_repo(tmp_dir):
    tmp_dir.gen({"foo": "foo", "params_file.yaml": "foo: bar\nxyz: val"})

    dvc = Repo(uninitialized=True)

    assert dvc.params.show(targets=["params_file.yaml"]) == {
        "": {"data": {"params_file.yaml": {"data": {"foo": "bar", "xyz": "val"}}}}
    }


@pytest.mark.parametrize(
    "file,error_path",
    (
        (PROJECT_FILE, ["v1", "error"]),
        ("params_other.yaml", ["v1", "data", "params_other.yaml", "error"]),
    ),
)
def test_log_errors(tmp_dir, scm, dvc, capsys, file, error_path):
    tmp_dir.gen("params_other.yaml", "foo: bar")
    dvc.run(
        cmd="echo params_other.yaml",
        params=["params_other.yaml:foo"],
        name="train",
    )

    rename = (tmp_dir / file).read_text()
    with open(tmp_dir / file, "a", encoding="utf-8") as fd:
        fd.write("\nmalformed!")

    scm.add([PROJECT_FILE, "params_other.yaml"])
    scm.commit("init")
    scm.tag("v1")

    (tmp_dir / file).write_text(rename)

    result = dvc.params.show(revs=["v1"])

    _, error = capsys.readouterr()

    assert isinstance(
        reduce(operator.getitem, error_path, result), YAMLFileCorruptedError
    )
    assert "DVC failed to load some parameters for following revisions: 'v1'." in error


@pytest.mark.parametrize("file", ["params.yaml", "other_params.yaml"])
def test_show_without_targets_specified(tmp_dir, dvc, scm, file):
    params_file = tmp_dir / file
    data = {"foo": {"bar": "bar"}, "x": "0"}
    params_file.dump(data)
    dvc.stage.add(
        name="test",
        cmd=f"echo {file}",
        params=[{file: None}],
    )

    assert dvc.params.show() == {"": {"data": {file: {"data": data}}}}


def test_deps_multi_stage(tmp_dir, scm, dvc, run_copy):
    tmp_dir.gen({"foo": "foo", "params.yaml": "foo: bar\nxyz: val\nabc: ignore"})
    run_copy("foo", "bar", name="copy-foo-bar", params=["foo"])
    run_copy("foo", "bar1", name="copy-foo-bar-1", params=["xyz"])

    scm.add(["params.yaml", PROJECT_FILE])
    scm.commit("add stage")

    assert dvc.params.show(revs=["master"], deps=True) == {
        "master": {"data": {"params.yaml": {"data": {"foo": "bar", "xyz": "val"}}}}
    }


def test_deps_with_targets(tmp_dir, scm, dvc, run_copy):
    tmp_dir.gen({"foo": "foo", "params.yaml": "foo: bar\nxyz: val\nabc: ignore"})
    run_copy("foo", "bar", name="copy-foo-bar", params=["foo"])
    run_copy("foo", "bar1", name="copy-foo-bar-1", params=["xyz"])

    scm.add(["params.yaml", PROJECT_FILE])
    scm.commit("add stage")

    assert dvc.params.show(targets=["params.yaml"], deps=True) == {
        "": {"data": {"params.yaml": {"data": {"foo": "bar", "xyz": "val"}}}}
    }


def test_deps_with_bad_target(tmp_dir, scm, dvc, run_copy):
    tmp_dir.gen(
        {
            "foo": "foo",
            "foobar": "",
            "params.yaml": "foo: bar\nxyz: val\nabc: ignore",
        }
    )
    run_copy("foo", "bar", name="copy-foo-bar", params=["foo"])
    run_copy("foo", "bar1", name="copy-foo-bar-1", params=["xyz"])
    scm.add(["params.yaml", PROJECT_FILE])
    scm.commit("add stage")
    assert dvc.params.show(targets=["foobar"], deps=True) == {}




tests/func/parsing/__init__.py
from dvc.parsing import DataResolver, EntryDefinition, ForeachDefinition
from dvc.parsing.context import Context

TEMPLATED_DVC_YAML_DATA = {
    "stages": {
        "stage1": {
            "cmd": "python script.py ${dict.foo} --out ${dict.bar}",
            "outs": ["${dict.bar}"],
            "deps": ["${dict.foo}"],
            "frozen": "${freeze}",
        },
        "stage2": {"cmd": "echo ${dict.foo} ${dict.bar}"},
    }
}

CONTEXT_DATA = {
    "dict": {"foo": "foo", "bar": "bar"},
    "list": ["param1", "param2"],
    "freeze": True,
}

RESOLVED_DVC_YAML_DATA = {
    "stages": {
        "stage1": {
            "cmd": "python script.py foo --out bar",
            "outs": ["bar"],
            "deps": ["foo"],
            "frozen": True,
        },
        "stage2": {"cmd": "echo foo bar"},
    }
}

USED_VARS = {
    "stage1": {"dict.foo": "foo", "dict.bar": "bar", "freeze": True},
    "stage2": {"dict.foo": "foo", "dict.bar": "bar"},
}


def make_entry_definition(wdir, name, data, context=None) -> EntryDefinition:
    return EntryDefinition(
        DataResolver(wdir.dvc, wdir.fs_path, {}),
        context or Context(),
        name,
        data,
    )


def make_foreach_def(
    wdir, name, foreach_data, do_data=None, context=None
) -> ForeachDefinition:
    return ForeachDefinition(
        DataResolver(wdir.dvc, wdir.fs_path, {}),
        context or Context(),
        name,
        {"foreach": foreach_data, "do": do_data or {}},
    )




tests/func/parsing/test_errors.py
"""Negative tests for the parametrization."""


import logging
import re

import pytest

from dvc.parsing import ResolveError
from dvc.parsing.context import Context
from dvc.parsing.interpolate import embrace
from dvc.utils.humanize import join

from . import make_entry_definition, make_foreach_def


def escape_ansi(line):
    ansi_escape = re.compile(r"(\x9B|\x1B\[)[0-?]*[ -\/]*[@-~]")
    return ansi_escape.sub("", line)


# Tests for the interpolated entries


@pytest.mark.parametrize("vars_", ["${file}_params.yaml", {"foo": "${foo}"}])
def test_vars_interpolation_errors(tmp_dir, dvc, vars_):
    definition = make_entry_definition(tmp_dir, "build", {"vars": [vars_]})
    with pytest.raises(ResolveError) as exc_info:
        definition.resolve()

    assert (
        str(exc_info.value) == "failed to parse 'stages.build.vars' in 'dvc.yaml': "
        "interpolating is not allowed"
    )


def test_failed_to_interpolate(tmp_dir, dvc):
    context = Context(models={"foo": "bar"})
    definition = make_entry_definition(
        tmp_dir, "build", {"cmd": "echo ${models.foo.}"}, context
    )

    with pytest.raises(ResolveError) as exc_info:
        definition.resolve()

    assert (
        escape_ansi(str(exc_info.value))
        == "failed to parse 'stages.build.cmd' in 'dvc.yaml':\n"
        "${models.foo.}\n"
        "            ^\n"
        "ParseException: Expected end of text, found '.'"
        "  (at char 12), (line:1, col:13)"
    )
    assert definition.context == {"models": {"foo": "bar"}}


def test_local_vars_params_file_not_exist(tmp_dir, dvc):
    definition = make_entry_definition(
        tmp_dir,
        "build",
        {"vars": ["not_existing_params.yaml"], "cmd": "echo ${models.foo}"},
    )

    with pytest.raises(ResolveError) as exc_info:
        definition.resolve()

    assert (
        str(exc_info.value) == "failed to parse stage 'build' in 'dvc.yaml': "
        "'not_existing_params.yaml' does not exist"
    )
    assert not definition.context


def test_specified_key_does_not_exist(tmp_dir, dvc):
    definition = make_entry_definition(
        tmp_dir,
        "build",
        {"cmd": "echo ${models.foobar}"},
        Context(models={"foo": "foo"}),
    )
    with pytest.raises(ResolveError) as exc_info:
        definition.resolve()

    assert (
        str(exc_info.value) == "failed to parse 'stages.build.cmd' in 'dvc.yaml': "
        "Could not find 'models.foobar'"
    )
    assert definition.context == {"models": {"foo": "foo"}}


@pytest.mark.parametrize(
    "wdir, expected_msg",
    [
        ("${models[foobar]}", " Could not find 'models.foobar'"),
        (
            "${models.foo]}",
            (
                "\n${models.foo]}\n"
                "            ^\n"
                "ParseException: Expected end of text, found ']'"
                "  (at char 12), (line:1, col:13)"
            ),
        ),
    ],
)
def test_wdir_failed_to_interpolate(tmp_dir, dvc, wdir, expected_msg):
    definition = make_entry_definition(
        tmp_dir,
        "build",
        {"wdir": wdir, "cmd": "echo ${models.bar}"},
        Context(models={"bar": "bar"}),
    )
    with pytest.raises(ResolveError) as exc_info:
        definition.resolve()

    assert escape_ansi(str(exc_info.value)) == (
        "failed to parse 'stages.build.wdir' in 'dvc.yaml':" + expected_msg
    )
    assert definition.context == {"models": {"bar": "bar"}}


def test_interpolate_non_string(tmp_dir, dvc):
    definition = make_entry_definition(
        tmp_dir, "build", {"outs": "${models}"}, Context(models={})
    )
    with pytest.raises(ResolveError) as exc_info:
        definition.resolve()

    assert (
        str(exc_info.value) == "failed to parse 'stages.build.outs' in 'dvc.yaml':\n"
        "Cannot interpolate data of type 'dict'"
    )
    assert definition.context == {"models": {}}


def test_interpolate_nested_iterable(tmp_dir, dvc):
    definition = make_entry_definition(
        tmp_dir,
        "build",
        {"cmd": "echo ${models}"},
        Context(models={"list": [1, [2, 3]]}),
    )
    with pytest.raises(ResolveError) as exc_info:
        definition.resolve()

    assert (
        str(exc_info.value) == "failed to parse 'stages.build.cmd' in 'dvc.yaml':\n"
        "Cannot interpolate nested iterable in 'list'"
    )


def test_partial_vars_doesnot_exist(tmp_dir, dvc):
    (tmp_dir / "test_params.yaml").dump({"sub1": "sub1", "sub2": "sub2"})

    definition = make_entry_definition(
        tmp_dir,
        "build",
        {"vars": ["test_params.yaml:sub3"], "cmd": "echo ${sub1} ${sub2}"},
    )

    with pytest.raises(ResolveError) as exc_info:
        definition.resolve()

    assert (
        str(exc_info.value) == "failed to parse stage 'build' in 'dvc.yaml': "
        "could not find 'sub3' in 'test_params.yaml'"
    )
    assert not definition.context


# Tests foreach generated stages and their error messages


def test_foreach_data_syntax_error(tmp_dir, dvc):
    definition = make_foreach_def(tmp_dir, "build", "${syntax.[error}", {})
    with pytest.raises(ResolveError) as exc_info:
        definition.resolve_all()

    assert (
        escape_ansi(str(exc_info.value))
        == "failed to parse 'stages.build.foreach' in 'dvc.yaml':\n"
        "${syntax.[error}\n"
        "        ^\n"
        "ParseException: Expected end of text, found '.'"
        "  (at char 8), (line:1, col:9)"
    )


@pytest.mark.parametrize("key", ["modelss", "modelss.123"])
def test_foreach_data_key_does_not_exists(tmp_dir, dvc, key):
    definition = make_foreach_def(tmp_dir, "build", embrace(key), {})
    with pytest.raises(ResolveError) as exc_info:
        definition.resolve_all()
    assert (
        str(exc_info.value) == "failed to parse 'stages.build.foreach' in 'dvc.yaml': "
        f"Could not find '{key}'"
    )


@pytest.mark.parametrize(
    "foreach_data", ["${foo}", "${dct.model1}", "${lst.0}", "foobar"]
)
def test_foreach_data_expects_list_or_dict(tmp_dir, dvc, foreach_data):
    context = Context({"foo": "bar", "dct": {"model1": "a-out"}, "lst": ["foo", "bar"]})
    definition = make_foreach_def(tmp_dir, "build", foreach_data, {}, context)
    with pytest.raises(ResolveError) as exc_info:
        definition.resolve_all()
    assert (
        str(exc_info.value)
        == "failed to resolve 'stages.build.foreach' in 'dvc.yaml': "
        "expected list/dictionary, got str"
    )


@pytest.mark.parametrize(
    "global_data, where",
    [
        ({"item": 10, "key": 10}, "item and key are"),
        ({"item": 10}, "item is"),
        ({"key": 5}, "key is"),
    ],
)
def test_foreach_overwriting_item_in_list(tmp_dir, dvc, caplog, global_data, where):
    context = Context(global_data)
    definition = make_foreach_def(
        tmp_dir, "build", {"model1": 10, "model2": 5}, {}, context
    )
    with caplog.at_level(logging.WARNING, logger="dvc.parsing"):
        definition.resolve_all()

    assert caplog.messages == [
        f"{where} already specified, "
        "will be overwritten for stages generated from 'build'"
    ]


def test_foreach_do_syntax_errors(tmp_dir, dvc):
    definition = make_foreach_def(
        tmp_dir, "build", ["foo", "bar"], {"cmd": "echo ${syntax.[error}"}
    )

    with pytest.raises(ResolveError) as exc_info:
        definition.resolve_all()

    assert (
        escape_ansi(str(exc_info.value))
        == "failed to parse 'stages.build.cmd' in 'dvc.yaml':\n"
        "${syntax.[error}\n"
        "        ^\n"
        "ParseException: Expected end of text, found '.'"
        "  (at char 8), (line:1, col:9)"
    )


@pytest.mark.parametrize(
    "key, loc",
    [
        (
            "item.thresh",  # the `thresh` in not available on model2`
            "stages.build@1.cmd",
        ),
        ("foo.bar", "stages.build@0.cmd"),  # not available on any stages
    ],
)
def test_foreach_do_definition_item_does_not_exist(tmp_dir, dvc, key, loc):
    context = Context(foo="bar")
    definition = make_foreach_def(
        tmp_dir,
        "build",
        [{"thresh": "10"}, {}],
        {"cmd": embrace(key)},
        context,
    )

    with pytest.raises(ResolveError) as exc_info:
        definition.resolve_all()

    assert (
        str(exc_info.value)
        == f"failed to parse '{loc}' in 'dvc.yaml': Could not find '{key}'"
    )

    # should have no `item` and `key` even though it failed to resolve.
    assert context == {"foo": "bar"}


@pytest.mark.parametrize(
    "redefine",
    [
        {"item": 5},
        {"key": 5},
        {"item": 5, "key": 10},
        {"item": {"epochs": 10}},
    ],
)
@pytest.mark.parametrize("from_file", [True, False])
def test_item_key_in_generated_stage_vars(tmp_dir, dvc, redefine, from_file):
    context = Context(foo="bar")
    vars_ = [redefine]
    if from_file:
        (tmp_dir / "test_params.yaml").dump(redefine)
        vars_ = ["test_params.yaml"]

    definition = make_foreach_def(
        tmp_dir,
        "build",
        {"model1": {"thresh": "10"}, "model2": {"thresh": 5}},
        {"vars": vars_, "cmd": "${item}"},
        context,
    )

    with pytest.raises(ResolveError) as exc_info:
        definition.resolve_all()

    message = str(exc_info.value)
    assert (
        "failed to parse stage 'build@model1' in 'dvc.yaml': "
        "attempted to modify reserved" in message
    )

    key_or_keys = "keys" if len(redefine) > 1 else "key"
    assert f"{key_or_keys} {join(redefine)}" in message
    if from_file:
        assert "in 'test_params.yaml'" in message
    assert context == {"foo": "bar"}


def test_foreach_wdir_key_does_not_exist(tmp_dir, dvc):
    definition = make_foreach_def(
        tmp_dir,
        "build",
        "${models}",
        {"wdir": "${ite}", "cmd": "echo ${item}"},
        Context(models=["foo", "bar"]),
    )
    with pytest.raises(ResolveError) as exc_info:
        definition.resolve_all()
    assert (
        str(exc_info.value)
        == "failed to parse 'stages.build@foo.wdir' in 'dvc.yaml': Could not find 'ite'"
    )
    assert definition.context == {"models": ["foo", "bar"]}




tests/func/parsing/test_foreach.py
"""Testing happy paths for the foreach."""
import os

import pytest

from dvc.parsing import DEFAULT_PARAMS_FILE, DataResolver, ForeachDefinition
from dvc.parsing.context import Context


def test_with_simple_list_data(tmp_dir, dvc):
    """Testing a simple non-nested list as a foreach data"""
    resolver = DataResolver(dvc, tmp_dir.fs_path, {})

    context = Context()
    data = {"foreach": ["foo", "bar", "baz"], "do": {"cmd": "echo ${item}"}}
    definition = ForeachDefinition(resolver, context, "build", data)

    assert definition.resolve_one("foo") == {"build@foo": {"cmd": "echo foo"}}
    assert definition.resolve_one("bar") == {"build@bar": {"cmd": "echo bar"}}
    # check that `foreach` item-key replacement didnot leave any leftovers.
    assert not context
    assert not resolver.tracked_vars["build@foo"]
    assert not resolver.tracked_vars["build@bar"]


def test_with_dict_data(tmp_dir, dvc):
    resolver = DataResolver(dvc, tmp_dir.fs_path, {})
    context = Context()

    foreach_data = {"model1": "foo", "model2": "bar"}
    data = {"foreach": foreach_data, "do": {"cmd": "echo ${key} ${item}"}}
    definition = ForeachDefinition(resolver, context, "build", data)

    assert definition.resolve_one("model1") == {
        "build@model1": {"cmd": "echo model1 foo"}
    }
    assert definition.resolve_one("model2") == {
        "build@model2": {"cmd": "echo model2 bar"}
    }

    # check that `foreach` item-key replacement didnot leave any leftovers.
    assert not context
    assert not resolver.tracked_vars["build@model1"]
    assert not resolver.tracked_vars["build@model2"]


def test_with_dict_with_non_str_keys(tmp_dir, dvc):
    resolver = DataResolver(dvc, tmp_dir.fs_path, {})
    context = Context()

    foreach_data = {2021: {"thresh": "foo"}, 2022: {"thresh": "bar"}}
    data = {"foreach": foreach_data, "do": {"cmd": "echo ${key} ${item.thresh}"}}
    definition = ForeachDefinition(resolver, context, "build", data)

    assert definition.resolve_one("2021") == {"build@2021": {"cmd": "echo 2021 foo"}}
    assert definition.resolve_one("2022") == {"build@2022": {"cmd": "echo 2022 bar"}}

    # check that `foreach` item-key replacement didnot leave any leftovers.
    assert not context
    assert not resolver.tracked_vars["build@2021"]
    assert not resolver.tracked_vars["build@2022"]


def test_with_composite_list(tmp_dir, dvc):
    resolver = DataResolver(dvc, tmp_dir.fs_path, {})

    context = Context()
    foreach_data = [{"thresh": "foo"}, {"thresh": "bar"}]
    data = {"foreach": foreach_data, "do": {"cmd": "echo ${item.thresh}"}}
    definition = ForeachDefinition(resolver, context, "build", data)

    assert definition.resolve_one("0") == {"build@0": {"cmd": "echo foo"}}
    # check that `foreach` item-key replacement didnot leave any leftovers.
    assert not context

    assert definition.resolve_one("1") == {"build@1": {"cmd": "echo bar"}}
    assert not context
    assert not resolver.tracked_vars["build@0"]


def test_foreach_interpolated_simple_list(tmp_dir, dvc):
    foreach_data = ["foo", "bar", "baz"]
    vars_ = {"models": foreach_data}
    resolver = DataResolver(dvc, tmp_dir.fs_path, {"vars": [vars_]})
    data = {"foreach": "${models}", "do": {"cmd": "echo ${item}"}}
    definition = ForeachDefinition(resolver, resolver.context, "build", data)

    assert definition.resolve_all() == {
        "build@foo": {"cmd": "echo foo"},
        "build@bar": {"cmd": "echo bar"},
        "build@baz": {"cmd": "echo baz"},
    }
    assert resolver.context == {"models": foreach_data}
    assert not any(item for item in resolver.tracked_vars.values())


@pytest.mark.parametrize("foreach_def", ["${item.thresh}", "${item[thresh]}"])
@pytest.mark.parametrize(
    "foreach_data, result",
    [
        (
            {"model1": {"thresh": "foo"}, "model2": {"thresh": "bar"}},
            {
                "build@model1": {"cmd": "echo foo"},
                "build@model2": {"cmd": "echo bar"},
            },
        ),
        (
            [{"thresh": "foo"}, {"thresh": "bar"}],
            {"build@0": {"cmd": "echo foo"}, "build@1": {"cmd": "echo bar"}},
        ),
    ],
)
def test_foreach_interpolate_with_composite_data(
    tmp_dir, dvc, foreach_def, foreach_data, result
):
    vars_ = [{"models": foreach_data}]
    resolver = DataResolver(dvc, tmp_dir.fs_path, {"vars": vars_})
    data = {"foreach": "${models}", "do": {"cmd": f"echo {foreach_def}"}}
    definition = ForeachDefinition(resolver, resolver.context, "build", data)

    assert definition.resolve_all() == result
    assert resolver.context == {"models": foreach_data}
    assert not any(item for item in resolver.tracked_vars.values())


def test_params_file_with_dict_tracked(tmp_dir, dvc):
    foreach_data = {"model1": {"thresh": "foo"}, "model2": {"thresh": "bar"}}
    params = {"models": foreach_data}
    (tmp_dir / "params.yaml").dump(params)

    resolver = DataResolver(dvc, tmp_dir.fs_path, {})
    data = {"foreach": "${models}", "do": {"cmd": "echo ${item.thresh}"}}
    definition = ForeachDefinition(resolver, resolver.context, "build", data)

    assert definition.resolve_all() == {
        "build@model1": {"cmd": "echo foo"},
        "build@model2": {"cmd": "echo bar"},
    }
    # check that `foreach` item-key replacement didnot leave any leftovers.
    assert resolver.context == {"models": foreach_data}
    assert resolver.tracked_vars == {
        "build@model1": {"params.yaml": {"models.model1.thresh": "foo"}},
        "build@model2": {"params.yaml": {"models.model2.thresh": "bar"}},
    }


def test_params_file_tracked_for_composite_list(tmp_dir, dvc):
    foreach_data = [{"thresh": "foo"}, {"thresh": "bar"}]
    params = {"models": foreach_data}
    (tmp_dir / "params.yaml").dump(params)

    resolver = DataResolver(dvc, tmp_dir.fs_path, {})
    data = {"foreach": "${models}", "do": {"cmd": "echo ${item.thresh}"}}
    definition = ForeachDefinition(resolver, resolver.context, "build", data)

    assert definition.resolve_all() == {
        "build@0": {"cmd": "echo foo"},
        "build@1": {"cmd": "echo bar"},
    }
    assert resolver.context == {"models": foreach_data}
    assert resolver.tracked_vars == {
        "build@0": {"params.yaml": {"models.0.thresh": "foo"}},
        "build@1": {"params.yaml": {"models.1.thresh": "bar"}},
    }


def test_foreach_data_from_nested_vars(tmp_dir, dvc):
    vars_ = {"models": {"lst": [{"thresh": 10}, {"thresh": 15}]}}
    resolver = DataResolver(dvc, tmp_dir.fs_path, {"vars": [vars_]})
    data = {"foreach": "${models.lst}", "do": {"cmd": "echo ${item.thresh}"}}
    definition = ForeachDefinition(resolver, resolver.context, "build", data)

    assert definition.resolve_all() == {
        "build@0": {"cmd": "echo 10"},
        "build@1": {"cmd": "echo 15"},
    }
    assert resolver.context == vars_
    assert not any(item for item in resolver.tracked_vars.values())


def test_foreach_partial_interpolations(tmp_dir, dvc):
    resolver = DataResolver(dvc, tmp_dir.fs_path, {"vars": [{"bar": "bar"}]})
    foreach_data = {"model1": "foo", "model2": "${bar}"}
    data = {"foreach": foreach_data, "do": {"cmd": "echo ${item}"}}
    definition = ForeachDefinition(resolver, resolver.context, "build", data)

    assert definition.resolve_all() == {
        "build@model1": {"cmd": "echo foo"},
        "build@model2": {"cmd": "echo bar"},
    }
    assert resolver.context == {"bar": "bar"}
    assert not any(item for item in resolver.tracked_vars.values())


def test_mixed_vars_for_foreach_data(tmp_dir, dvc):
    (tmp_dir / "params.yaml").dump({"models": {"model1": "foo"}})
    (tmp_dir / "test_params.yaml").dump({"models": {"model2": "bar"}})

    resolver = DataResolver(dvc, tmp_dir.fs_path, {"vars": ["test_params.yaml"]})
    data = {"foreach": "${models}", "do": {"cmd": "echo ${item}"}}
    definition = ForeachDefinition(resolver, resolver.context, "build", data)

    assert definition.resolve_all() == {
        "build@model1": {"cmd": "echo foo"},
        "build@model2": {"cmd": "echo bar"},
    }
    assert resolver.context == {"models": {"model1": "foo", "model2": "bar"}}
    assert resolver.tracked_vars == {
        "build@model1": {"params.yaml": {"models.model1": "foo"}},
        "build@model2": {"test_params.yaml": {"models.model2": "bar"}},
    }


def test_mixed_vars_for_foreach_data_2(tmp_dir, dvc):
    (tmp_dir / "params.yaml").dump(
        {"models": {"model1": {"thresh": 10}, "model2": {"thresh": 15}}},
    )
    (tmp_dir / "test_params.yaml").dump(
        {"models": {"model1": {"epochs": 5}, "model2": {"epochs": 10}}},
    )

    resolver = DataResolver(dvc, tmp_dir.fs_path, {"vars": ["test_params.yaml"]})
    data = {
        "foreach": "${models}",
        "do": {"cmd": "echo ${item.thresh} ${item.epochs}"},
    }
    definition = ForeachDefinition(resolver, resolver.context, "build", data)

    assert definition.resolve_all() == {
        "build@model1": {"cmd": "echo 10 5"},
        "build@model2": {"cmd": "echo 15 10"},
    }
    assert resolver.context == {
        "models": {
            "model1": {"thresh": 10, "epochs": 5},
            "model2": {"thresh": 15, "epochs": 10},
        }
    }
    assert resolver.tracked_vars == {
        "build@model1": {
            "params.yaml": {"models.model1.thresh": 10},
            "test_params.yaml": {"models.model1.epochs": 5},
        },
        "build@model2": {
            "params.yaml": {"models.model2.thresh": 15},
            "test_params.yaml": {"models.model2.epochs": 10},
        },
    }


def test_foreach_with_interpolated_wdir(tmp_dir, dvc):
    resolver = DataResolver(dvc, (tmp_dir / "data").fs_path, {})
    foreach_data = ["foo", "bar"]
    data = {
        "foreach": foreach_data,
        "do": {"wdir": "${item}", "cmd": "echo hello"},
    }
    definition = ForeachDefinition(resolver, resolver.context, "build", data)

    assert definition.resolve_all() == {
        # note that the resolver generates `wdir` relative to file's wdir
        # so, this is just `foo`, not `data/foo`.
        # figuring out `wdir` is the responsibility of the `load_stage`/`Stage`
        "build@foo": {"wdir": "foo", "cmd": "echo hello"},
        "build@bar": {"wdir": "bar", "cmd": "echo hello"},
    }

    assert not resolver.context
    assert not any(item for item in resolver.tracked_vars.values())


def test_foreach_with_local_vars(tmp_dir, dvc):
    resolver = DataResolver(dvc, (tmp_dir / "data").fs_path, {})
    foreach_data = ["foo", "bar"]
    data = {
        "foreach": foreach_data,
        "do": {
            "vars": [{"foobar": "foobar"}],
            "cmd": "echo ${item} ${foobar}",
        },
    }
    definition = ForeachDefinition(resolver, resolver.context, "build", data)

    assert definition.resolve_all() == {
        # note that the resolver generates `wdir` relative to file's wdir
        # so, this is just `foo`, not `data/foo`.
        # figuring out `wdir` is the responsibility of the `load_stage`/`Stage`
        "build@foo": {"cmd": "echo foo foobar"},
        "build@bar": {"cmd": "echo bar foobar"},
    }
    assert not resolver.context
    assert not any(item for item in resolver.tracked_vars.values())


@pytest.mark.parametrize(
    "local_import",
    [
        "test_params.yaml",
        "test_params.yaml:train",
        "test_params.yaml:train,prepare",
    ],
)
def test_foreach_with_imported_vars(tmp_dir, dvc, local_import):
    (tmp_dir / "params.yaml").dump({"models": {"model1": {"thresh": "foo"}}})
    (tmp_dir / "test_params.yaml").dump(
        {"train": {"epochs": 10}, "prepare": {"nums": 25}}
    )
    resolver = DataResolver(dvc, tmp_dir.fs_path, {})
    foreach_data = ["foo", "bar"]
    data = {
        "foreach": foreach_data,
        "do": {"vars": [local_import], "cmd": "echo ${item} ${train.epochs}"},
    }
    definition = ForeachDefinition(resolver, resolver.context, "build", data)

    assert definition.resolve_all() == {
        # note that the resolver generates `wdir` relative to file's wdir
        # so, this is just `foo`, not `data/foo`.
        # figuring out `wdir` is the responsibility of the `load_stage`/`Stage`
        "build@foo": {"cmd": "echo foo 10"},
        "build@bar": {"cmd": "echo bar 10"},
    }

    assert resolver.context == {"models": {"model1": {"thresh": "foo"}}}
    assert resolver.tracked_vars == {
        "build@foo": {"test_params.yaml": {"train.epochs": 10}},
        "build@bar": {"test_params.yaml": {"train.epochs": 10}},
    }


@pytest.mark.parametrize("local_import", ["params.yaml", "params.yaml:train,prepare"])
def test_foreach_with_interpolated_wdir_and_local_vars(tmp_dir, dvc, local_import):
    (tmp_dir / "params.yaml").dump({"models": {"model1": {"thresh": "foo"}}})

    for i in range(5):
        build_dir = tmp_dir / ("model-" + str(i))
        build_dir.mkdir()
        (build_dir / "params.yaml").dump(
            {"train": {"epochs": 1 + i}, "prepare": {"nums": 10 * i}},
        )

    resolver = DataResolver(dvc, tmp_dir.fs_path, {})
    data = {
        "foreach": [0, 1, 2, 3, 4],
        "do": {
            "wdir": "model-${item}",
            "vars": [local_import],
            "cmd": "echo ${item} ${train.epochs} ${prepare.nums}",
        },
    }
    definition = ForeachDefinition(resolver, resolver.context, "build", data)

    assert definition.resolve_all() == {
        # note that the resolver generates `wdir` relative to file's wdir
        # so, this is just `foo`, not `data/foo`.
        # figuring out `wdir` is the responsibility of the `load_stage`/`Stage`
        "build@0": {"wdir": "model-0", "cmd": "echo 0 1 0"},
        "build@1": {"wdir": "model-1", "cmd": "echo 1 2 10"},
        "build@2": {"wdir": "model-2", "cmd": "echo 2 3 20"},
        "build@3": {"wdir": "model-3", "cmd": "echo 3 4 30"},
        "build@4": {"wdir": "model-4", "cmd": "echo 4 5 40"},
    }

    assert resolver.context == {"models": {"model1": {"thresh": "foo"}}}
    assert resolver.tracked_vars == {
        "build@0": {
            os.path.join("model-0", "params.yaml"): {
                "train.epochs": 1,
                "prepare.nums": 0,
            }
        },
        "build@1": {
            os.path.join("model-1", "params.yaml"): {
                "train.epochs": 2,
                "prepare.nums": 10,
            }
        },
        "build@2": {
            os.path.join("model-2", "params.yaml"): {
                "train.epochs": 3,
                "prepare.nums": 20,
            }
        },
        "build@3": {
            os.path.join("model-3", "params.yaml"): {
                "train.epochs": 4,
                "prepare.nums": 30,
            }
        },
        "build@4": {
            os.path.join("model-4", "params.yaml"): {
                "train.epochs": 5,
                "prepare.nums": 40,
            }
        },
    }
    assert resolver.context.imports == {DEFAULT_PARAMS_FILE: None}


def test_foreach_do_syntax_is_checked_once(tmp_dir, dvc, mocker):
    do_def = {"cmd": "python script.py --epochs ${item}"}
    data = {"foreach": [0, 1, 2, 3, 4], "do": do_def}
    definition = ForeachDefinition(
        DataResolver(dvc, tmp_dir.fs_path, {}), Context(), "build", data
    )
    mock = mocker.patch("dvc.parsing.check_syntax_errors", return_value=True)
    definition.resolve_all()

    mock.assert_called_once_with(do_def, "build", "dvc.yaml")


def test_foreach_data_is_only_resolved_once(tmp_dir, dvc, mocker):
    context = Context(models=["foo", "bar", "baz"])
    data = {"foreach": "${models}", "do": {}}
    definition = ForeachDefinition(
        DataResolver(dvc, tmp_dir.fs_path, {}), context, "build", data
    )
    mock = mocker.spy(definition, "_resolve_foreach_data")

    definition.resolve_all()

    mock.assert_called_once_with()




tests/func/parsing/test_interpolated_entry.py
import os
from copy import deepcopy

import pytest

from dvc.dependency import _merge_params
from dvc.parsing import DEFAULT_PARAMS_FILE, DataResolver
from dvc.parsing.context import recurse_not_a_node
from dvc.parsing.interpolate import escape_str

from . import CONTEXT_DATA, RESOLVED_DVC_YAML_DATA, TEMPLATED_DVC_YAML_DATA, USED_VARS


def assert_stage_equal(d1, d2):
    """Keeps the params section in order, and then checks for equality."""
    for d in [d1, d2]:
        assert recurse_not_a_node(d)
        for _, stage_d in d.get("stages", {}).items():
            params = _merge_params(stage_d.get("params", []))
            for k in params:
                params[k] = sorted(params[k])
            if params:
                stage_d["params"] = params
    assert d1 == d2


def test_simple(tmp_dir, dvc):
    (tmp_dir / DEFAULT_PARAMS_FILE).dump(CONTEXT_DATA)
    resolver = DataResolver(dvc, tmp_dir.fs_path, deepcopy(TEMPLATED_DVC_YAML_DATA))
    assert_stage_equal(resolver.resolve(), deepcopy(RESOLVED_DVC_YAML_DATA))
    assert resolver.tracked_vars == {
        "stage1": {DEFAULT_PARAMS_FILE: USED_VARS["stage1"]},
        "stage2": {DEFAULT_PARAMS_FILE: USED_VARS["stage2"]},
    }


def test_vars_import(tmp_dir, dvc):
    """
    Test that different file can be loaded using `vars`
    instead of default params.yaml.
    """
    (tmp_dir / "params2.yaml").dump(CONTEXT_DATA)
    d = deepcopy(TEMPLATED_DVC_YAML_DATA)
    d["vars"] = ["params2.yaml"]
    resolver = DataResolver(dvc, tmp_dir.fs_path, d)

    resolved_data = deepcopy(RESOLVED_DVC_YAML_DATA)
    assert_stage_equal(resolver.resolve(), resolved_data)
    assert resolver.tracked_vars == {
        "stage1": {"params2.yaml": USED_VARS["stage1"]},
        "stage2": {"params2.yaml": USED_VARS["stage2"]},
    }


def test_vars_and_params_import(tmp_dir, dvc):
    """
    Test that vars and params are both merged together for interpolation,
    whilst tracking the "used" variables from params.
    """
    d = {
        "vars": [DEFAULT_PARAMS_FILE, {"dict": {"foo": "foobar"}}],
        "stages": {"stage1": {"cmd": "echo ${dict.foo} ${dict.bar}"}},
    }
    (tmp_dir / DEFAULT_PARAMS_FILE).dump({"dict": {"bar": "bar"}})
    resolver = DataResolver(dvc, tmp_dir.fs_path, d)

    assert_stage_equal(
        resolver.resolve(), {"stages": {"stage1": {"cmd": "echo foobar bar"}}}
    )
    assert resolver.tracked_vars == {
        "stage1": {DEFAULT_PARAMS_FILE: {"dict.bar": "bar"}}
    }


def test_stage_with_wdir(tmp_dir, dvc):
    """
    Test that params file from wdir are also loaded
    """
    d = {
        "stages": {
            "stage1": {
                "cmd": "echo ${dict.foo} ${dict.bar}",
                "params": ["value1"],
                "wdir": "data",
                "vars": [DEFAULT_PARAMS_FILE],
            }
        }
    }

    data_dir = tmp_dir / "data"
    data_dir.mkdir()
    (tmp_dir / DEFAULT_PARAMS_FILE).dump({"dict": {"bar": "bar"}})
    (data_dir / DEFAULT_PARAMS_FILE).dump({"dict": {"foo": "foo"}})
    resolver = DataResolver(dvc, tmp_dir.fs_path, d)

    assert_stage_equal(
        resolver.resolve(),
        {
            "stages": {
                "stage1": {
                    "cmd": "echo foo bar",
                    "wdir": "data",
                    "params": ["value1"],
                }
            }
        },
    )
    assert resolver.tracked_vars == {
        "stage1": {
            os.path.join("data", DEFAULT_PARAMS_FILE): {"dict.foo": "foo"},
            DEFAULT_PARAMS_FILE: {"dict.bar": "bar"},
        }
    }


def test_with_templated_wdir(tmp_dir, dvc):
    """
    Test that params from the resolved wdir are still loaded
    and is used in the interpolation.
    """
    d = {
        "stages": {
            "stage1": {
                "cmd": "echo ${dict.foo} ${dict.bar}",
                "params": ["value1"],
                "wdir": "${dict.ws}",
                "vars": [DEFAULT_PARAMS_FILE],
            }
        }
    }
    (tmp_dir / DEFAULT_PARAMS_FILE).dump({"dict": {"bar": "bar", "ws": "data"}})
    data_dir = tmp_dir / "data"
    data_dir.mkdir()
    (data_dir / DEFAULT_PARAMS_FILE).dump({"dict": {"foo": "foo"}})
    resolver = DataResolver(dvc, tmp_dir.fs_path, d)

    assert_stage_equal(
        resolver.resolve(),
        {
            "stages": {
                "stage1": {
                    "cmd": "echo foo bar",
                    "wdir": "data",
                    "params": ["value1"],
                }
            }
        },
    )
    assert resolver.tracked_vars == {
        "stage1": {
            os.path.join("data", DEFAULT_PARAMS_FILE): {"dict.foo": "foo"},
            DEFAULT_PARAMS_FILE: {"dict.bar": "bar", "dict.ws": "data"},
        }
    }
    assert resolver.context.imports == {"params.yaml": None}
    assert resolver.context == {"dict": {"bar": "bar", "ws": "data"}}


def test_resolve_local_tries_to_load_globally_used_files(tmp_dir, dvc):
    iterable = {"bar": "bar", "foo": "foo"}
    (tmp_dir / "params.json").dump(iterable)

    d = {
        "vars": ["params.json"],
        "stages": {
            "build": {
                "cmd": "command --value ${bar}",
                "params": [{"params.json": ["foo"]}],
                "vars": ["params.json"],
            }
        },
    }
    resolver = DataResolver(dvc, tmp_dir.fs_path, d)
    assert_stage_equal(
        resolver.resolve(),
        {
            "stages": {
                "build": {
                    "cmd": "command --value bar",
                    "params": [{"params.json": ["foo"]}],
                }
            }
        },
    )
    assert resolver.tracked_vars == {"build": {"params.json": {"bar": "bar"}}}


def test_resolve_local_tries_to_load_globally_used_params_yaml(tmp_dir, dvc):
    iterable = {"bar": "bar", "foo": "foo"}
    (tmp_dir / "params.yaml").dump(iterable)

    d = {
        "stages": {
            "build": {
                "cmd": "command --value ${bar}",
                "params": [{"params.yaml": ["foo"]}],
                "vars": ["params.yaml"],
            }
        }
    }
    resolver = DataResolver(dvc, tmp_dir.fs_path, d)
    assert_stage_equal(
        resolver.resolve(),
        {
            "stages": {
                "build": {
                    "cmd": "command --value bar",
                    "params": [{"params.yaml": ["foo"]}],
                }
            }
        },
    )
    assert resolver.tracked_vars == {"build": {"params.yaml": {"bar": "bar"}}}


def test_vars_relpath_overwrite(tmp_dir, dvc):
    iterable = {"bar": "bar", "foo": "foo"}
    (tmp_dir / "params.yaml").dump(iterable)
    d = {
        "vars": ["params.yaml"],
        "stages": {
            "build": {
                "wdir": "data",
                "cmd": "echo ${bar}",
                "vars": ["../params.yaml"],
            }
        },
    }
    resolver = DataResolver(dvc, tmp_dir.fs_path, d)
    resolver.resolve()
    assert resolver.context.imports == {"params.yaml": None}


@pytest.mark.parametrize("local", [True, False])
@pytest.mark.parametrize(
    "vars_",
    [
        ["test_params.yaml:bar", "test_params.yaml:foo"],
        ["test_params.yaml:foo,bar"],
        ["test_params.yaml"],
        ["test_params.yaml", "test_params.yaml"],
    ],
)
def test_vars_load_partial(tmp_dir, dvc, local, vars_):
    iterable = {"bar": "bar", "foo": "foo"}
    (tmp_dir / "test_params.yaml").dump(iterable)
    d = {"stages": {"build": {"cmd": "echo ${bar}"}}}
    if local:
        d["stages"]["build"]["vars"] = vars_
    else:
        d["vars"] = vars_
    resolver = DataResolver(dvc, tmp_dir.fs_path, d)
    resolver.resolve()


@pytest.mark.parametrize(
    "bool_config, list_config",
    [(None, None), ("store_true", "nargs"), ("boolean_optional", "append")],
)
def test_cmd_dict(tmp_dir, dvc, bool_config, list_config):
    with dvc.config.edit() as conf:
        if bool_config:
            conf["parsing"]["bool"] = bool_config
        if list_config:
            conf["parsing"]["list"] = list_config

    string = "spaced string"
    mixed_quote_string = "quote\"'d"
    data = {
        "dict": {
            "foo": "foo",
            "bar": 2,
            "string": string,
            "mixed_quote_string": mixed_quote_string,
            "bool": True,
            "bool-false": False,
            "list": [1, 2, "foo", mixed_quote_string],
            "nested": {"foo": "foo"},
        }
    }
    (tmp_dir / DEFAULT_PARAMS_FILE).dump(data)
    resolver = DataResolver(
        dvc,
        tmp_dir.fs_path,
        {"stages": {"stage1": {"cmd": "python script.py ${dict}"}}},
    )

    if bool_config is None or bool_config == "store_true":
        bool_resolved = " --bool"
    else:
        bool_resolved = " --bool --no-bool-false"

    if list_config is None or list_config == "nargs":
        list_resolved = f" --list 1 2 foo {escape_str(mixed_quote_string)}"
    else:
        list_resolved = " --list 1 --list 2 --list foo"
        list_resolved += f" --list {escape_str(mixed_quote_string)}"

    assert_stage_equal(
        resolver.resolve(),
        {
            "stages": {
                "stage1": {
                    "cmd": (
                        "python script.py"
                        " --foo foo --bar 2"
                        f" --string {escape_str(string)}"
                        " --mixed_quote_string"
                        f" {escape_str(mixed_quote_string)}"
                        f"{bool_resolved}"
                        f"{list_resolved}"
                        " --nested.foo foo"
                    )
                }
            }
        },
    )




tests/func/parsing/test_resolver.py
from copy import deepcopy

import pytest

from dvc.parsing import DEFAULT_PARAMS_FILE, DataResolver, ResolveError
from dvc.parsing.context import Context
from dvc.utils.serialize import dumps_yaml

from . import CONTEXT_DATA, RESOLVED_DVC_YAML_DATA, TEMPLATED_DVC_YAML_DATA

DATA = {"models": {"bar": "bar", "foo": "foo"}}


def test_resolver(tmp_dir, dvc):
    resolver = DataResolver(dvc, tmp_dir.fs_path, TEMPLATED_DVC_YAML_DATA)
    resolver.context.merge_update(Context(CONTEXT_DATA))
    assert resolver.resolve() == RESOLVED_DVC_YAML_DATA


def test_default_params_file_not_exist(tmp_dir, dvc):
    d = {"vars": [DATA["models"]]}
    resolver = DataResolver(dvc, tmp_dir.fs_path, d)
    assert resolver.context == d["vars"][0]


def test_no_params_yaml_and_vars(tmp_dir, dvc):
    resolver = DataResolver(dvc, tmp_dir.fs_path, {})
    assert not resolver.context


def test_local_vars(tmp_dir, dvc):
    resolver = DataResolver(
        dvc, tmp_dir.fs_path, {"vars": [{"foo": "bar", "bar": "foo"}]}
    )
    assert resolver.context == {"foo": "bar", "bar": "foo"}


@pytest.mark.parametrize("vars_", ["${file}_params.yaml", {"foo": "${foo}"}])
def test_vars_interpolation_errors(tmp_dir, dvc, vars_):
    with pytest.raises(ResolveError) as exc_info:
        DataResolver(dvc, tmp_dir.fs_path, {"vars": [vars_, {"bar": "foo"}]})
    assert (
        str(exc_info.value)
        == "failed to parse 'vars' in 'dvc.yaml': interpolating is not allowed"
    )


@pytest.mark.parametrize("vars_", [{}, {"vars": []}, {"vars": [DEFAULT_PARAMS_FILE]}])
def test_default_params_file(tmp_dir, dvc, vars_):
    (tmp_dir / DEFAULT_PARAMS_FILE).dump(DATA)
    resolver = DataResolver(dvc, tmp_dir.fs_path, vars_)
    assert resolver.context == DATA


def test_load_vars_from_file(tmp_dir, dvc):
    (tmp_dir / DEFAULT_PARAMS_FILE).dump(DATA)

    datasets = {"datasets": ["foo", "bar"]}
    (tmp_dir / "params.json").dump(datasets)
    d = {"vars": [DEFAULT_PARAMS_FILE, "params.json"]}
    resolver = DataResolver(dvc, tmp_dir.fs_path, d)

    expected = deepcopy(DATA)
    expected.update(datasets)
    assert resolver.context == expected


def test_load_vars_with_relpath(tmp_dir, scm, dvc):
    tmp_dir.scm_gen(DEFAULT_PARAMS_FILE, dumps_yaml(DATA), commit="add params")

    revisions = ["HEAD", "workspace"]
    for rev in dvc.brancher(revs=["HEAD"]):
        assert rev == revisions.pop()
        d = {
            "vars": [f"../{DEFAULT_PARAMS_FILE}"],
        }
        resolver = DataResolver(dvc, "subdir", d)
        assert resolver.context == deepcopy(DATA)


def test_partial_vars_doesnot_exist(tmp_dir, dvc):
    (tmp_dir / "test_params.yaml").dump({"sub1": "sub1"})

    with pytest.raises(ResolveError) as exc_info:
        DataResolver(dvc, tmp_dir.fs_path, {"vars": ["test_params.yaml:sub2"]})

    assert (
        str(exc_info.value) == "failed to parse 'vars' in 'dvc.yaml': "
        "could not find 'sub2' in 'test_params.yaml'"
    )


def test_global_overwrite_error_on_imports(tmp_dir, dvc):
    (tmp_dir / DEFAULT_PARAMS_FILE).dump(DATA)
    (tmp_dir / "params.json").dump(DATA)

    d = {"vars": [DEFAULT_PARAMS_FILE, "params.json"]}
    with pytest.raises(ResolveError) as exc_info:
        DataResolver(dvc, tmp_dir.fs_path, d)

    assert (
        str(exc_info.value) == "failed to parse 'vars' in 'dvc.yaml':\n"
        "cannot redefine 'models.bar' from 'params.json' "
        "as it already exists in 'params.yaml'"
    )


def test_global_overwrite_vars(tmp_dir, dvc):
    (tmp_dir / DEFAULT_PARAMS_FILE).dump(DATA)
    d = {"vars": [DATA]}

    with pytest.raises(ResolveError) as exc_info:
        DataResolver(dvc, tmp_dir.fs_path, d)

    assert (
        str(exc_info.value) == "failed to parse 'vars' in 'dvc.yaml':\n"
        "cannot redefine 'models.bar' from 'vars[0]' "
        "as it already exists in 'params.yaml'"
    )


def test_local_declared_vars_overwrite(tmp_dir, dvc):
    (tmp_dir / DEFAULT_PARAMS_FILE).dump(DATA)

    d = {"vars": [DATA["models"], DATA["models"]]}
    with pytest.raises(ResolveError) as exc_info:
        DataResolver(dvc, tmp_dir.fs_path, d)

    assert (
        str(exc_info.value) == "failed to parse 'vars' in 'dvc.yaml':\n"
        "cannot redefine 'bar' from 'vars[1]' "
        "as it already exists in 'vars[0]'"
    )


def test_specified_params_file_not_exist(tmp_dir, dvc):
    d = {"vars": ["not_existing_params.yaml"]}
    with pytest.raises(ResolveError) as exc_info:
        DataResolver(dvc, tmp_dir.fs_path, d)

    assert (
        str(exc_info.value) == "failed to parse 'vars' in 'dvc.yaml': "
        "'not_existing_params.yaml' does not exist"
    )


@pytest.mark.parametrize("local", [True, False])
@pytest.mark.parametrize(
    "vars_",
    [
        ["test_params.yaml", "test_params.yaml:sub1"],
        ["test_params.yaml:sub1", "test_params.yaml"],
        ["test_params.yaml:sub1", "test_params.yaml:sub1,sub2"],
    ],
)
def test_vars_already_loaded_message(tmp_dir, dvc, local, vars_):
    d = {"stages": {"build": {"cmd": "echo ${sub1} ${sub2}"}}}
    (tmp_dir / "test_params.yaml").dump({"sub1": "sub1", "sub2": "sub2"})
    if not local:
        d["vars"] = vars_
    else:
        d["stages"]["build"]["vars"] = vars_

    with pytest.raises(ResolveError) as exc_info:  # noqa: PT012
        resolver = DataResolver(dvc, tmp_dir.fs_path, d)
        resolver.resolve()
    assert "partially" in str(exc_info.value)


@pytest.mark.parametrize(
    "vars_, loc", [(DATA, "build.vars[0]"), ("params.json", "params.json")]
)
def test_local_overwrite_error(tmp_dir, dvc, vars_, loc):
    (tmp_dir / DEFAULT_PARAMS_FILE).dump(DATA)
    (tmp_dir / "params.json").dump(DATA)

    d = {"stages": {"build": {"cmd": "echo ${models.foo}", "vars": [vars_]}}}

    resolver = DataResolver(dvc, tmp_dir.fs_path, d)
    with pytest.raises(ResolveError) as exc_info:
        resolver.resolve()

    assert (
        str(exc_info.value) == "failed to parse stage 'build' in 'dvc.yaml':\n"
        f"cannot redefine 'models.bar' from '{loc}' "
        "as it already exists in 'params.yaml'"
    )




tests/func/plots/__init__.py




tests/func/plots/test_diff.py
import pytest

from tests.utils.plots import get_plot


def test_diff_dirty(tmp_dir, scm, dvc, run_copy_metrics):
    (tmp_dir / "metric_t.json").dump([{"y": 2}, {"y": 3}], sort_keys=True)
    run_copy_metrics(
        "metric_t.json",
        "metric.json",
        plots=["metric.json"],
        name="train",
        commit="init",
    )

    metric_head = [{"y": 3}, {"y": 5}]
    (tmp_dir / "metric_t.json").dump_json(metric_head, sort_keys=True)
    dvc.reproduce()
    scm.add(["dvc.lock"])
    scm.commit("second")

    metric_1 = [{"y": 5}, {"y": 6}]
    (tmp_dir / "metric_t.json").dump_json(metric_1, sort_keys=True)
    dvc.reproduce()

    props = {"fields": ["y"]}
    diff_result = dvc.plots.diff(props=props)

    assert get_plot(diff_result, "workspace", file="metric.json") == metric_1
    assert get_plot(
        diff_result, "workspace", "definitions", file="", endkey="data"
    ) == {"metric.json": props}
    assert get_plot(diff_result, "HEAD", file="metric.json") == metric_head
    assert get_plot(diff_result, "HEAD", "definitions", file="", endkey="data") == {
        "metric.json": props
    }

    metric_2 = [{"y": 7}, {"y": 8}]
    (tmp_dir / "metric.json").dump_json(metric_2, sort_keys=True)

    diff_result = dvc.plots.diff(props=props)
    assert get_plot(diff_result, "workspace", file="metric.json") == metric_2
    assert get_plot(
        diff_result, "workspace", "definitions", file="", endkey="data"
    ) == {"metric.json": props}

    assert get_plot(diff_result, "HEAD", file="metric.json") == metric_head
    assert get_plot(
        diff_result, "workspace", "definitions", file="", endkey="data"
    ) == {"metric.json": props}


@pytest.mark.vscode
def test_no_commits(tmp_dir):
    from dvc.repo import Repo
    from dvc.scm import Git

    git = Git.init(tmp_dir.fs_path)
    assert git.no_commits

    assert Repo.init().plots.diff() == {}




tests/func/plots/test_modify.py
import pytest

from dvc.dvcfile import LOCK_FILE
from dvc.repo.plots import PropsNotFoundError
from dvc.utils import relpath
from tests.utils.plots import get_plot


def test_plots_modify_existing_template(
    tmp_dir, dvc, run_copy_metrics, custom_template
):
    metric = [{"a": 1, "b": 2}, {"a": 2, "b": 3}]
    (tmp_dir / "metric_t.json").dump_json(metric, sort_keys=True)
    stage = run_copy_metrics(
        "metric_t.json",
        "metric.json",
        plots_no_cache=["metric.json"],
        name="copy-metrics",
        single_stage=False,
    )
    dvc.plots.modify("metric.json", props={"template": relpath(custom_template)})
    stage = stage.reload()
    assert stage.outs[0].plot == {"template": relpath(custom_template)}


def test_plots_modify_should_not_change_lockfile(
    tmp_dir, dvc, run_copy_metrics, custom_template
):
    (tmp_dir / "metric_t.json").dump_json([{"a": 1, "b": 2}], sort_keys=True)
    run_copy_metrics(
        "metric_t.json",
        "metric.json",
        plots_no_cache=["metric.json"],
        name="copy-metrics",
        single_stage=False,
    )

    (tmp_dir / LOCK_FILE).unlink()
    dvc.plots.modify("metric.json", props={"template": relpath(custom_template)})
    assert not (tmp_dir / LOCK_FILE).exists()


def test_plots_modify_not_existing_template(dvc):
    from dvc_render.vega_templates import TemplateNotFoundError

    with pytest.raises(TemplateNotFoundError):
        dvc.plots.modify(
            "metric.json", props={"template": "not-existing-template.json"}
        )


def test_unset_nonexistent(tmp_dir, dvc, run_copy_metrics, custom_template):
    metric = [{"a": 1, "b": 2}, {"a": 2, "b": 3}]
    (tmp_dir / "metric_t.json").dump_json(metric, sort_keys=True)
    run_copy_metrics(
        "metric_t.json",
        "metric.json",
        plots_no_cache=["metric.json"],
        name="copy-metrics",
        single_stage=False,
    )

    with pytest.raises(PropsNotFoundError):
        dvc.plots.modify("metric.json", unset=["nonexistent"])


def test_dir_plots(tmp_dir, dvc, run_copy_metrics):
    subdir = tmp_dir / "subdir"
    subdir.mkdir()

    metric = [{"first_val": 100, "val": 2}, {"first_val": 200, "val": 3}]

    fname = "file.json"
    (tmp_dir / fname).dump_json(metric, sort_keys=True)

    p1 = "subdir/p1.json"
    p2 = "subdir/p2.json"
    tmp_dir.dvc.run(
        cmd=(
            f"mkdir subdir && python copy.py {fname} {p1} && "
            f"python copy.py {fname} {p2}"
        ),
        deps=[fname],
        single_stage=False,
        plots=["subdir"],
        name="copy_double",
    )
    dvc.plots.modify("subdir", {"title": "TITLE"})

    result = dvc.plots.show()
    assert get_plot(result, "workspace", typ="definitions", file="") == {
        p1: {"title": "TITLE"},
        p2: {"title": "TITLE"},
    }




tests/func/plots/test_show.py
import json
import os

import pytest

from dvc.cli import main
from dvc.dvcfile import PROJECT_FILE
from dvc.exceptions import OverlappingOutputPathsError
from dvc.repo import Repo
from dvc.repo.plots import PlotMetricTypeError
from dvc.utils import onerror_collect
from dvc.utils.fs import remove
from dvc.utils.serialize import EncodingError, YAMLFileCorruptedError, modify_yaml
from tests.utils.plots import get_plot


def test_show_targets(tmp_dir, dvc):
    metric = [{"first_val": 100, "val": 2}, {"first_val": 200, "val": 3}]
    (tmp_dir / "metric.json").dump_json(metric, sort_keys=True)

    plots = dvc.plots.show(targets=["metric.json"])
    assert get_plot(plots, "workspace", file="metric.json") == metric

    plots = dvc.plots.show(targets=(tmp_dir / "metric.json").fs_path)
    assert get_plot(plots, "workspace", file="metric.json") == metric


def test_plot_cache_missing(tmp_dir, scm, dvc, caplog, run_copy_metrics):
    metric1 = [{"y": 2}, {"y": 3}]
    (tmp_dir / "metric_t.json").dump_json(metric1, sort_keys=True)
    run_copy_metrics(
        "metric_t.json",
        "metric.json",
        plots=["metric.json"],
        commit="there is metric",
    )
    scm.tag("v1")

    # Make a different plot and then remove its datafile
    metric2 = [{"y": 3}, {"y": 4}]
    (tmp_dir / "metric_t.json").dump_json(metric2, sort_keys=True)
    stage = run_copy_metrics(
        "metric_t.json",
        "metric.json",
        plots=["metric.json"],
        commit="there is an another metric",
    )
    scm.tag("v2")
    remove(stage.outs[0].fspath)
    remove(stage.outs[0].cache_path)

    plots_data = dvc.plots.show(revs=["v1", "v2"], targets=["metric.json"])

    assert get_plot(plots_data, "v1", file="metric.json") == metric1
    assert isinstance(
        get_plot(plots_data, "v2", file="metric.json", endkey="error"),
        FileNotFoundError,
    )


def test_plot_wrong_metric_type(tmp_dir, scm, dvc, run_copy_metrics):
    tmp_dir.gen("metric_t.txt", "some text")
    run_copy_metrics(
        "metric_t.txt",
        "metric.txt",
        plots_no_cache=["metric.txt"],
        commit="add text metric",
    )

    result = dvc.plots.show(targets=["metric.txt"], onerror=onerror_collect)
    assert isinstance(
        get_plot(result, "workspace", file="metric.txt", endkey="error"),
        PlotMetricTypeError,
    )


@pytest.mark.parametrize("use_dvc", [True, False])
def test_show_non_plot(tmp_dir, scm, use_dvc):
    metric = [{"first_val": 100, "val": 2}, {"first_val": 200, "val": 3}]
    (tmp_dir / "metric.json").dump_json(metric, sort_keys=True)

    if use_dvc:
        dvc = Repo.init()
    else:
        dvc = Repo(uninitialized=True)

    plots = dvc.plots.show(targets=["metric.json"])

    assert get_plot(plots, "workspace", file="metric.json") == metric


def test_show_non_plot_and_plot_with_params(tmp_dir, scm, dvc, run_copy_metrics):
    metric = [{"first_val": 100, "val": 2}, {"first_val": 200, "val": 3}]
    (tmp_dir / "metric.json").dump_json(metric, sort_keys=True)
    run_copy_metrics(
        "metric.json",
        "metric2.json",
        plots_no_cache=["metric2.json"],
        name="train",
    )
    props = {"title": "TITLE"}
    dvc.plots.modify("metric2.json", props=props)

    result = dvc.plots.show(targets=["metric.json", "metric2.json"])

    assert get_plot(result, "workspace", file="metric.json") == metric
    assert get_plot(result, "workspace", file="metric2.json") == metric
    assert get_plot(result, "workspace", file="metric2.json", endkey="props") == props


def test_show_from_subdir(tmp_dir, dvc, capsys):
    subdir = tmp_dir / "subdir"

    subdir.mkdir()
    metric = [{"first_val": 100, "val": 2}, {"first_val": 200, "val": 3}]
    (subdir / "metric.json").dump_json(metric, sort_keys=True)

    with subdir.chdir():
        assert main(["plots", "show", "metric.json"]) == 0

    out, _ = capsys.readouterr()
    assert subdir.as_uri() in out
    assert (subdir / "dvc_plots").is_dir()
    assert (subdir / "dvc_plots" / "index.html").is_file()


def test_plots_show_non_existing(tmp_dir, dvc, capsys):
    result = dvc.plots.show(targets=["plot.json"])
    assert isinstance(
        get_plot(result, "workspace", file="plot.json", endkey="error"),
        FileNotFoundError,
    )

    cap = capsys.readouterr()
    assert (
        "DVC failed to load some plots for following revisions: 'workspace'" in cap.err
    )


@pytest.mark.parametrize("clear_before_run", [True, False])
def test_plots_show_overlap(tmp_dir, dvc, run_copy_metrics, clear_before_run):
    data_dir = tmp_dir / "data"
    data_dir.mkdir()

    (data_dir / "m1_temp.yaml").dump({"a": {"b": {"c": 2, "d": 1}}})
    run_copy_metrics(
        str(data_dir / "m1_temp.yaml"),
        str(data_dir / "m1.yaml"),
        single_stage=False,
        commit="add m1",
        name="cp-m1",
        plots=[str(data_dir / "m1.yaml")],
    )
    with (tmp_dir / "dvc.yaml").modify() as d:
        # trying to make an output overlaps error
        d["stages"]["corrupted-stage"] = {
            "cmd": "mkdir data",
            "outs": ["data"],
        }

    # running by clearing and not clearing stuffs
    # so as it works even for optimized cases
    if clear_before_run:
        remove(data_dir)
        remove(dvc.cache.local.path)

    dvc._reset()

    result = dvc.plots.show(onerror=onerror_collect)
    assert isinstance(
        get_plot(result, "workspace", endkey="error"),
        OverlappingOutputPathsError,
    )


def test_dir_plots(tmp_dir, dvc, run_copy_metrics):
    subdir = tmp_dir / "subdir"
    subdir.mkdir()

    metric = [{"first_val": 100, "val": 2}, {"first_val": 200, "val": 3}]

    fname = "file.json"
    (tmp_dir / fname).dump_json(metric, sort_keys=True)

    p1 = "subdir/p1.json"
    p2 = "subdir/p2.json"
    tmp_dir.dvc.run(
        cmd=(
            f"mkdir subdir && python copy.py {fname} {p1} && "
            f"python copy.py {fname} {p2}"
        ),
        deps=[fname],
        single_stage=False,
        plots=["subdir"],
        name="copy_double",
    )
    props = {"title": "TITLE"}
    dvc.plots.modify("subdir", props)

    result = dvc.plots.show()

    assert set(get_plot(result, "workspace")) == {p1, p2}
    assert get_plot(result, "workspace", typ="definitions", file="") == {
        p1: props,
        p2: props,
    }


def test_ignore_parsing_error(tmp_dir, dvc, run_copy_metrics):
    with open("file", "wb", encoding=None) as fobj:
        fobj.write(b"\xc1")

    run_copy_metrics("file", "plot_file.json", plots=["plot_file.json"])
    result = dvc.plots.show(onerror=onerror_collect)

    assert isinstance(
        get_plot(result, "workspace", file="plot_file.json", endkey="error"),
        EncodingError,
    )


@pytest.mark.parametrize(
    "file,path_kwargs",
    (
        (PROJECT_FILE, {"revision": "workspace", "endkey": "error"}),
        (
            "plot.yaml",
            {"revision": "workspace", "file": "plot.yaml", "endkey": "error"},
        ),
    ),
)
def test_log_errors(tmp_dir, scm, dvc, run_copy_metrics, file, path_kwargs, capsys):
    metric = [{"val": 2}, {"val": 3}]
    (tmp_dir / "metric_t.yaml").dump(metric)
    run_copy_metrics(
        "metric_t.yaml",
        "plot.yaml",
        plots=["plot.yaml"],
        single_stage=False,
        name="train",
    )
    scm.tag("v1")

    with open(file, "a", encoding="utf-8") as fd:
        fd.write("\nMALFORMED!")

    result = dvc.plots.show(onerror=onerror_collect)
    _, error = capsys.readouterr()

    assert isinstance(get_plot(result, **path_kwargs), YAMLFileCorruptedError)
    assert (
        "DVC failed to load some plots for following revisions: 'workspace'." in error
    )


@pytest.mark.parametrize("ext", ["jpg", "svg"])
def test_plots_binary(tmp_dir, scm, dvc, run_copy_metrics, custom_template, ext):
    file1 = f"image.{ext}"
    file2 = f"plot.{ext}"
    with open(file1, "wb") as fd:
        fd.write(b"content")

    dvc.add([file1])
    run_copy_metrics(
        file1,
        file2,
        commit="run training",
        plots=[file2],
        name="s2",
        single_stage=False,
    )

    scm.add(["dvc.yaml", "dvc.lock"])
    scm.commit("initial")

    scm.tag("v1")

    with open(file2, "wb") as fd:
        fd.write(b"content2")

    result = dvc.plots.show(revs=["v1", "workspace"])
    assert get_plot(result, "v1", file=file2) == b"content"
    assert get_plot(result, "workspace", file=file2) == b"content2"


def test_collect_non_existing_dir(tmp_dir, dvc, run_copy_metrics):
    subdir = tmp_dir / "subdir"
    subdir.mkdir()

    metric = [{"first_val": 100, "val": 2}, {"first_val": 200, "val": 3}]
    subdir_metric = [{"y": 101, "x": 3}, {"y": 202, "x": 4}]

    pname = "source.json"
    (tmp_dir / pname).dump_json(metric, sort_keys=True)

    sname = "subdir_source.json"
    (tmp_dir / sname).dump_json(subdir_metric, sort_keys=True)

    p1 = os.path.join("subdir", "p1.json")
    p2 = os.path.join("subdir", "p2.json")
    subdir_stage = tmp_dir.dvc.run(
        cmd=(
            f"mkdir subdir && python copy.py {sname} {p1} && "
            f"python copy.py {sname} {p2}"
        ),
        deps=[sname],
        single_stage=False,
        plots=["subdir"],
        name="copy_double",
    )

    run_copy_metrics(
        pname,
        "plot.json",
        plots=["plot.json"],
        commit="there is metric",
    )

    remove(subdir_stage.outs[0].cache_path)
    remove(subdir_stage.outs[0].fs_path)

    result = dvc.plots.show()
    assert get_plot(result, "workspace", typ="definitions", file="", endkey="error")
    # make sure others gets loaded
    assert get_plot(result, "workspace", file="plot.json") == metric


@pytest.mark.parametrize(
    "plot_config,expected_datafiles",
    [
        (
            {
                "comparison": {
                    "x": {"data1.json": "a"},
                    "y": {"sub/dir/data2.json": "b"},
                }
            },
            ["data1.json", os.path.join("sub", "dir", "data2.json")],
        ),
        (
            {"data1.json": {"x": "c", "y": "a", "title": "File as key test"}},
            ["data1.json"],
        ),
        (
            {
                "infer_data_from_y": {
                    "x": "a",
                    "y": {"data1.json": "b", "sub/dir/data2.json": "c"},
                }
            },
            ["data1.json", os.path.join("sub", "dir", "data2.json")],
        ),
    ],
)
def test_top_level_plots(
    tmp_dir,
    dvc,
    plot_config,
    expected_datafiles,
):
    data = {
        "data1.json": [
            {"a": 1, "b": 0.1, "c": 0.01},
            {"a": 2, "b": 0.2, "c": 0.02},
        ],
        os.path.join("sub", "dir", "data.json"): [
            {"a": 6, "b": 0.6, "c": 0.06},
            {"a": 7, "b": 0.7, "c": 0.07},
        ],
    }

    for filename, content in data.items():
        dirname = os.path.dirname(filename)
        if dirname:
            os.makedirs(dirname)
        (tmp_dir / filename).dump_json(content, sort_keys=True)

    config_file = "dvc.yaml"
    with modify_yaml(config_file) as dvcfile_content:
        dvcfile_content["plots"] = [plot_config]

    result = dvc.plots.show()

    assert plot_config == get_plot(
        result, "workspace", typ="definitions", file=config_file
    )

    for filename, content in data.items():
        if filename in expected_datafiles:
            assert content == get_plot(result, "workspace", file=filename)
        else:
            assert filename not in get_plot(result, "workspace")


def test_show_plots_defined_with_native_os_path(tmp_dir, dvc, scm, capsys):
    """Regression test for #8689"""
    top_level_plot = os.path.join("subdir", "top_level_plot.csv")
    stage_plot = os.path.join("subdir", "stage_plot.csv")
    (tmp_dir / "subdir").mkdir()
    (tmp_dir / top_level_plot).write_text("foo,bar\n1,2")
    (tmp_dir / stage_plot).write_text("foo,bar\n1,2")
    (tmp_dir / "dvc.yaml").dump({"plots": [top_level_plot]})

    dvc.stage.add(name="foo", plots=[stage_plot], cmd="echo foo")

    plots = dvc.plots.show()

    # sources are in posixpath format
    sources = plots["workspace"]["sources"]["data"]
    assert sources["subdir/top_level_plot.csv"]["data"] == [{"foo": "1", "bar": "2"}]
    assert sources["subdir/stage_plot.csv"]["data"] == [{"foo": "1", "bar": "2"}]
    # definitions are in native os format
    definitions = plots["workspace"]["definitions"]["data"]
    assert top_level_plot in definitions["dvc.yaml"]["data"]
    assert stage_plot in definitions[""]["data"]

    capsys.readouterr()
    assert main(["plots", "show", "--json"]) == 0
    out, _ = capsys.readouterr()
    json_out = json.loads(out)
    assert "errors" not in json_out

    json_data = json_out["data"]
    assert json_data[f"dvc.yaml::{top_level_plot}"]
    assert json_data[stage_plot]




tests/func/utils/__init__.py




tests/func/utils/test_hydra.py
import pytest

from dvc.exceptions import InvalidArgumentError


@pytest.mark.parametrize("suffix", ["yaml", "toml", "json"])
@pytest.mark.parametrize(
    "overrides, expected",
    [
        # Overriding
        (["foo=baz"], {"foo": "baz", "goo": {"bag": 3.0}, "lorem": False}),
        (["foo=baz", "goo=bar"], {"foo": "baz", "goo": "bar", "lorem": False}),
        (
            ["foo.0=bar"],
            {"foo": ["bar", {"baz": 2}], "goo": {"bag": 3.0}, "lorem": False},
        ),
        (
            ["foo.1.baz=3"],
            {
                "foo": [{"bar": 1}, {"baz": 3}],
                "goo": {"bag": 3.0},
                "lorem": False,
            },
        ),
        (
            ["goo.bag=4.0"],
            {
                "foo": [{"bar": 1}, {"baz": 2}],
                "goo": {"bag": 4.0},
                "lorem": False,
            },
        ),
        (
            ["++goo={bag: 1, b: 2}"],
            {
                "foo": [{"bar": 1}, {"baz": 2}],
                "goo": {"bag": 1, "b": 2},
                "lorem": False,
            },
        ),
        # 6129
        (
            ["lorem="],
            {
                "foo": [{"bar": 1}, {"baz": 2}],
                "goo": {"bag": 3.0},
                "lorem": "",
            },
        ),
        # 6129
        (
            ["lorem=null"],
            {
                "foo": [{"bar": 1}, {"baz": 2}],
                "goo": {"bag": 3.0},
                "lorem": None,
            },
        ),
        # 5868
        (
            ["lorem=1992-11-20"],
            {
                "foo": [{"bar": 1}, {"baz": 2}],
                "goo": {"bag": 3.0},
                "lorem": "1992-11-20",
            },
        ),
        # 5868
        (
            ["lorem='1992-11-20'"],
            {
                "foo": [{"bar": 1}, {"baz": 2}],
                "goo": {"bag": 3.0},
                "lorem": "1992-11-20",
            },
        ),
        # Appending
        (
            ["+a=1"],
            {
                "foo": [{"bar": 1}, {"baz": 2}],
                "goo": {"bag": 3.0},
                "lorem": False,
                "a": 1,
            },
        ),
        # Removing
        (["~foo"], {"goo": {"bag": 3.0}, "lorem": False}),
    ],
)
def test_apply_overrides(tmp_dir, suffix, overrides, expected):
    from dvc.utils.hydra import apply_overrides

    if suffix == "toml" and overrides in [
        ["foo=baz"],
        ["foo.0=bar"],
        ["foo=baz", "goo=bar"],
        ["lorem=null"],
    ]:
        pytest.skip(
            "TOML dumper breaks when overriding a list/dict with other type or"
            " when handling `null` values."
        )

    params_file = tmp_dir / f"params.{suffix}"
    params_file.dump(
        {"foo": [{"bar": 1}, {"baz": 2}], "goo": {"bag": 3.0}, "lorem": False}
    )
    apply_overrides(path=params_file.name, overrides=overrides)
    assert params_file.parse() == expected


@pytest.mark.parametrize(
    "overrides",
    [["foobar=2"], ["lorem=3,2"], ["+lorem=3"], ["foo[0]=bar"]],
)
def test_invalid_overrides(tmp_dir, overrides):
    from dvc.utils.hydra import apply_overrides

    params_file = tmp_dir / "params.yaml"
    params_file.dump(
        {"foo": [{"bar": 1}, {"baz": 2}], "goo": {"bag": 3.0}, "lorem": False}
    )
    with pytest.raises(InvalidArgumentError):
        apply_overrides(path=params_file.name, overrides=overrides)


def hydra_setup(tmp_dir, config_dir, config_name):
    config_dir = tmp_dir / config_dir
    (config_dir / "db").mkdir(parents=True)
    (config_dir / f"{config_name}.yaml").dump({"defaults": [{"db": "mysql"}]})
    (config_dir / "db" / "mysql.yaml").dump(
        {"driver": "mysql", "user": "omry", "pass": "secret"}
    )
    (config_dir / "db" / "postgresql.yaml").dump(
        {"driver": "postgresql", "user": "foo", "pass": "bar", "timeout": 10}
    )
    return str(config_dir)


@pytest.mark.parametrize("suffix", ["yaml", "toml", "json"])
@pytest.mark.parametrize(
    "overrides,expected",
    [
        ([], {"db": {"driver": "mysql", "user": "omry", "pass": "secret"}}),
        (
            ["db=postgresql"],
            {
                "db": {
                    "driver": "postgresql",
                    "user": "foo",
                    "pass": "bar",
                    "timeout": 10,
                }
            },
        ),
        (
            ["db=postgresql", "db.timeout=20"],
            {
                "db": {
                    "driver": "postgresql",
                    "user": "foo",
                    "pass": "bar",
                    "timeout": 20,
                }
            },
        ),
    ],
)
def test_compose_and_dump(tmp_dir, suffix, overrides, expected):
    from dvc.utils.hydra import compose_and_dump

    config_name = "config"
    output_file = tmp_dir / f"params.{suffix}"
    config_dir = hydra_setup(tmp_dir, "conf", "config")
    compose_and_dump(output_file, config_dir, config_name, overrides)
    assert output_file.parse() == expected


def test_compose_and_dump_yaml_handles_string(tmp_dir):
    """Regression test for https://github.com/iterative/dvc/issues/8583"""
    from dvc.utils.hydra import compose_and_dump

    config = tmp_dir / "conf" / "config.yaml"
    config.parent.mkdir()
    config.write_text("foo: 'no'\n")
    output_file = tmp_dir / "params.yaml"
    compose_and_dump(output_file, str(config.parent), "config", [])
    assert output_file.read_text() == "foo: 'no'\n"


def test_compose_and_dump_resolves_interpolation(tmp_dir):
    """Regression test for https://github.com/iterative/dvc/issues/9196"""
    from dvc.utils.hydra import compose_and_dump

    config = tmp_dir / "conf" / "config.yaml"
    config.parent.mkdir()
    config.dump({"data": {"root": "path/to/root", "raw": "${.root}/raw"}})
    output_file = tmp_dir / "params.yaml"
    compose_and_dump(output_file, str(config.parent), "config", [])
    assert output_file.parse() == {
        "data": {"root": "path/to/root", "raw": "path/to/root/raw"}
    }


@pytest.mark.parametrize(
    "overrides, expected",
    [
        (
            {"params.yaml": ["defaults/foo=1,2"]},
            [
                {"params.yaml": ["defaults/foo=1"]},
                {"params.yaml": ["defaults/foo=2"]},
            ],
        ),
        (
            {"params.yaml": ["+foo=1,2", "~bar", "++foobar=5,6"]},
            [
                {"params.yaml": ["+foo=1", "~bar=null", "++foobar=5"]},
                {"params.yaml": ["+foo=1", "~bar=null", "++foobar=6"]},
                {"params.yaml": ["+foo=2", "~bar=null", "++foobar=5"]},
                {"params.yaml": ["+foo=2", "~bar=null", "++foobar=6"]},
            ],
        ),
        (
            {"params.yaml": ["foo=1,2", "bar=3,4"]},
            [
                {"params.yaml": ["foo=1", "bar=3"]},
                {"params.yaml": ["foo=1", "bar=4"]},
                {"params.yaml": ["foo=2", "bar=3"]},
                {"params.yaml": ["foo=2", "bar=4"]},
            ],
        ),
        (
            {"params.yaml": ["foo=choice(1,2)"]},
            [{"params.yaml": ["foo=1"]}, {"params.yaml": ["foo=2"]}],
        ),
        (
            {"params.yaml": ["foo=range(1, 3)"]},
            [{"params.yaml": ["foo=1"]}, {"params.yaml": ["foo=2"]}],
        ),
        (
            {"params.yaml": ["foo=1,2"], "others.yaml": ["bar=3"]},
            [
                {"params.yaml": ["foo=1"], "others.yaml": ["bar=3"]},
                {"params.yaml": ["foo=2"], "others.yaml": ["bar=3"]},
            ],
        ),
        (
            {"params.yaml": ["foo=1,2"], "others.yaml": ["bar=3,4"]},
            [
                {"params.yaml": ["foo=1"], "others.yaml": ["bar=3"]},
                {"params.yaml": ["foo=1"], "others.yaml": ["bar=4"]},
                {"params.yaml": ["foo=2"], "others.yaml": ["bar=3"]},
                {"params.yaml": ["foo=2"], "others.yaml": ["bar=4"]},
            ],
        ),
    ],
)
def test_hydra_sweeps(overrides, expected):
    from dvc.utils.hydra import get_hydra_sweeps

    assert get_hydra_sweeps(overrides) == expected


def test_invalid_sweep():
    from dvc.utils.hydra import get_hydra_sweeps

    with pytest.raises(InvalidArgumentError):
        get_hydra_sweeps({"params.yaml": ["foo=glob(*)"]})




tests/func/utils/test_strict_yaml.py
import os

import pytest
from ruamel.yaml import __with_libyaml__ as ruamel_clib

from dvc.cli import main

DUPLICATE_KEYS = """\
stages:
  stage1:
    cmd: python train.py
    cmd: python train.py
"""

DUPLICATE_KEYS_OUTPUT = """\
'./dvc.yaml' is invalid.

While constructing a mapping, in line 3, column 5
  3 ‚îÇ   cmd: python train.py

Found duplicate key "cmd" with value "python train.py" (original value:\
 "python\ntrain.py"), in line 4, column 5
  4 ‚îÇ   cmd: python train.py"""


MAPPING_VALUES_NOT_ALLOWED = """\
stages:
  stage1
    cmd: python script.py
"""

MAPPING_VALUES_NOT_ALLOWED_OUTPUT = """\
'./dvc.yaml' is invalid.

Mapping values are not allowed {}, in line 3, column 8
  3 ‚îÇ   cmd: python script.py""".format(
    "in this context" if ruamel_clib else "here"
)


NO_HYPHEN_INDICATOR_IN_BLOCK = """\
stages:
  stage1:
    cmd: python script.py
    outs:
      - logs:
          cache: false
      metrics:
"""

NO_HYPHEN_INDICATOR_IN_BLOCK_OUTPUT = """\
'./dvc.yaml' is invalid.

While parsing a block collection, in line 5, column 7
  5 ‚îÇ     - logs:

{}, in line 7, column 7
  7 ‚îÇ     metrics:""".format(
    "Did not find expected '-' indicator"
    if ruamel_clib
    else "Expected <block end>, but found '?'"
)


UNCLOSED_SCALAR = """\
stages:
  stage1:
    cmd: python script.py
    desc: "this is my stage one
"""

UNCLOSED_SCALAR_OUTPUT = """\
'./dvc.yaml' is invalid.

While scanning a quoted scalar, in line 4, column 11
  4 ‚îÇ   desc: "this is my stage one

Found unexpected end of stream, in line 5, column 1
  5"""


NOT_A_DICT = "3"
NOT_A_DICT_OUTPUT = "'./dvc.yaml' validation failed: expected a dictionary.\n"


EMPTY_STAGE = """\
stages:
  stage1:
"""

EMPTY_STAGE_OUTPUT = """\
'./dvc.yaml' validation failed.

expected a dictionary, in stages -> stage1, line 2, column 3
  1 stages:
  2   stage1:
  3"""


MISSING_CMD = """\
stages:
  stage1:
    cmd: {}
"""

MISSING_CMD_OUTPUT = """\
'./dvc.yaml' validation failed.

expected str, in stages -> stage1 -> cmd, line 3, column 10
  2   stage1:
  3 ‚îÇ   cmd: {}"""


DEPS_AS_DICT = """\
stages:
  stage1:
    cmd: python script.py
    deps:
      - src:
"""

DEPS_AS_DICT_OUTPUT = """\
'./dvc.yaml' validation failed.

expected str, in stages -> stage1 -> deps -> 0, line 5, column 9
  4 ‚îÇ   deps:
  5 ‚îÇ     - src:
"""

OUTS_AS_STR = """\
stages:
  train:
    cmd:
      - python train.py
    deps:
      - config.cfg
    outs:
      models/"""

OUTS_AS_STR_OUTPUT = """\
'./dvc.yaml' validation failed.

expected a list, in stages -> train -> outs, line 3, column 5
  2   train:
  3 ‚îÇ   cmd:
  4 ‚îÇ     - python train.py"""


NULL_VALUE_ON_OUTS = """\
stages:
  stage1:
    cmd: python script.py
    outs:
    - logs:
        cache: false
        persist: true
        remote:
"""

NULL_VALUE_ON_OUTS_OUTPUT = """\
'./dvc.yaml' validation failed.

expected str, in stages -> stage1 -> outs -> 0 -> logs -> remote, line 6, \
column\n9
  5 ‚îÇ   - logs:
  6 ‚îÇ   ‚îÇ   cache: false
  7 ‚îÇ   ‚îÇ   persist: true"""

ADDITIONAL_KEY_ON_OUTS = """\
stages:
  stage1:
    cmd: python script.py
    outs:
    - logs:
        cache: false
        not_existing_key: false
"""

ADDITIONAL_KEY_ON_OUTS_OUTPUT = """\
'./dvc.yaml' validation failed.

extra keys not allowed, in stages -> stage1 -> outs -> 0 -> logs ->
not_existing_key, line 6, column 9
  5 ‚îÇ   - logs:
  6 ‚îÇ   ‚îÇ   cache: false
  7 ‚îÇ   ‚îÇ   not_existing_key: false"""


FOREACH_SCALAR_VALUE = """\
stages:
  group:
    foreach: 3
    do:
      cmd: python script${i}.py
"""

FOREACH_SCALAR_VALUE_OUTPUT = """\
'./dvc.yaml' validation failed.

expected dict, in stages -> group -> foreach, line 3, column 5
  2   group:
  3 ‚îÇ   foreach: 3
  4 ‚îÇ   do:"""

FOREACH_DO_NULL = """\
stages:
  stage1:
    foreach: [1,2,3]
    do:
"""


FOREACH_DO_NULL_OUTPUT = """\
'./dvc.yaml' validation failed.

expected a dictionary, in stages -> stage1 -> do, line 3, column 5
  2   stage1:
  3 ‚îÇ   foreach: [1,2,3]
  4 ‚îÇ   do:"""


FOREACH_DO_MISSING_CMD = """\
stages:
  stage1:
    foreach: [1,2,3]
    do:
      outs:
      - ${item}
"""


FOREACH_WITH_CMD_DO_MISSING = """\
stages:
  stage1:
    foreach: [1,2,3]
    cmd: python script${item}.py
"""


FOREACH_WITH_CMD_DO_MISSING_OUTPUT = """\
'./dvc.yaml' validation failed: 2 errors.

extra keys not allowed, in stages -> stage1 -> cmd, line 3, column 5
  2   stage1:
  3 ‚îÇ   foreach: [1,2,3]
  4 ‚îÇ   cmd: python script${item}.py

required key not provided, in stages -> stage1 -> do, line 3, column 5
  2   stage1:
  3 ‚îÇ   foreach: [1,2,3]
  4 ‚îÇ   cmd: python script${item}.py"""


FOREACH_DO_MISSING_CMD_OUTPUT = """\
'./dvc.yaml' validation failed.

required key not provided, in stages -> stage1 -> do -> cmd, line 5, column 7
  4 ‚îÇ   do:
  5 ‚îÇ     outs:
  6 ‚îÇ     - ${item}"""


MERGE_CONFLICTS = """\
stages:
  load_data:
<<<<<<< HEAD
    cmd: python src/load_data.py
    deps:
    - src/load_data.py
=======
    cmd: python load_data.py
    deps:
    - load_data.py
>>>>>>> branch
    outs:
    - data
"""

MERGE_CONFLICTS_OUTPUT = """\
'./dvc.yaml' is invalid (possible merge conflicts).

While scanning a simple key, in line 3, column 1
  3 <<<<<<< HEAD

Could not find expected ':', in line 4, column 8
  4 ‚îÇ   cmd: python src/load_data.py"""


examples = {
    # on parse errors
    "duplicate_keys": (DUPLICATE_KEYS, DUPLICATE_KEYS_OUTPUT),
    "mapping_values_not_allowed": (
        MAPPING_VALUES_NOT_ALLOWED,
        MAPPING_VALUES_NOT_ALLOWED_OUTPUT,
    ),
    "no_hyphen_block": (
        NO_HYPHEN_INDICATOR_IN_BLOCK,
        NO_HYPHEN_INDICATOR_IN_BLOCK_OUTPUT,
    ),
    "unclosed_scalar": (UNCLOSED_SCALAR, UNCLOSED_SCALAR_OUTPUT),
    # schema validation errors
    "not_a_dict": (NOT_A_DICT, NOT_A_DICT_OUTPUT),
    "empty_stage": (EMPTY_STAGE, EMPTY_STAGE_OUTPUT),
    "missing_cmd": (MISSING_CMD, MISSING_CMD_OUTPUT),
    "deps_as_dict": (DEPS_AS_DICT, DEPS_AS_DICT_OUTPUT),
    "outs_as_str": (OUTS_AS_STR, OUTS_AS_STR_OUTPUT),
    "null_value_on_outs": (NULL_VALUE_ON_OUTS, NULL_VALUE_ON_OUTS_OUTPUT),
    "additional_key_on_outs": (
        ADDITIONAL_KEY_ON_OUTS,
        ADDITIONAL_KEY_ON_OUTS_OUTPUT,
    ),
    "foreach_scalar": (FOREACH_SCALAR_VALUE, FOREACH_SCALAR_VALUE_OUTPUT),
    "foreach_do_do_null": (FOREACH_DO_NULL, FOREACH_DO_NULL_OUTPUT),
    "foreach_do_missing_cmd": (
        FOREACH_DO_MISSING_CMD,
        FOREACH_DO_MISSING_CMD_OUTPUT,
    ),
    "foreach_unknown_cmd_missing_do": (
        FOREACH_WITH_CMD_DO_MISSING,
        FOREACH_WITH_CMD_DO_MISSING_OUTPUT,
    ),
    # merge conflicts
    "merge_conflicts": (MERGE_CONFLICTS, MERGE_CONFLICTS_OUTPUT),
}


@pytest.fixture
def force_posixpath(mocker):
    # make it always return posix path, easier for validating error messages
    mocker.patch(
        "dvc.utils.strictyaml.make_relpath",
        return_value="./dvc.yaml",
    )


@pytest.fixture
def fixed_width_term(mocker):
    """Fixed width console."""
    from rich.console import Console

    mocker.patch.object(
        Console, "width", new_callable=mocker.PropertyMock(return_value=80)
    )


@pytest.mark.parametrize("text, expected", examples.values(), ids=examples.keys())
def test_exceptions(
    tmp_dir,
    dvc,
    capsys,
    force_posixpath,
    fixed_width_term,
    text,
    expected,
):
    tmp_dir.gen("dvc.yaml", text)

    capsys.readouterr()  # clear outputs
    assert main(["stage", "list"]) != 0
    out, err = capsys.readouterr()

    assert not out

    # strip whitespace on the right: output is always left-justified
    # by rich.syntax.Syntax:
    for expected_line, err_line in zip(expected.splitlines(), err.splitlines()):
        assert expected_line == err_line.rstrip(" ")


@pytest.mark.parametrize(
    "text, expected",
    [
        (DUPLICATE_KEYS, "'./dvc.yaml' is invalid in revision '{short_rev}'."),
        (
            MISSING_CMD,
            "'./dvc.yaml' validation failed in revision '{short_rev}'.",
        ),
    ],
)
def test_on_revision(
    tmp_dir,
    scm,
    dvc,
    force_posixpath,
    fixed_width_term,
    capsys,
    text,
    expected,
):
    tmp_dir.scm_gen("dvc.yaml", text, commit="add dvc.yaml")
    capsys.readouterr()  # clear outputs

    assert main(["ls", f"file://{tmp_dir.as_posix()}", "--rev", "HEAD"]) != 0

    out, err = capsys.readouterr()
    assert not out
    assert expected.format(short_rev=scm.get_rev()[:7]) in err


def test_make_relpath(tmp_dir, monkeypatch):
    from dvc.utils.strictyaml import make_relpath

    path = tmp_dir / "dvc.yaml"
    expected_path = "./dvc.yaml" if os.name == "posix" else ".\\dvc.yaml"
    assert make_relpath(path) == expected_path

    (tmp_dir / "dir").mkdir(exist_ok=True)
    monkeypatch.chdir("dir")

    expected_path = "../dvc.yaml" if os.name == "posix" else "..\\dvc.yaml"
    assert make_relpath(path) == expected_path


def test_fallback_exception_message(tmp_dir, dvc, mocker, caplog):
    # When trying to pretty print exception messages, we fallback to old way
    # of printing things.
    mocker.patch(
        "dvc.utils.strictyaml.YAMLSyntaxError.__pretty_exc__",
        side_effect=ValueError,
    )
    mocker.patch(
        "dvc.utils.strictyaml.YAMLValidationError.__pretty_exc__",
        side_effect=ValueError,
    )

    # syntax errors
    dvc_file = tmp_dir / "dvc.yaml"
    dvc_file.write_text(MAPPING_VALUES_NOT_ALLOWED)
    assert main(["stage", "list"]) != 0
    assert "unable to read: 'dvc.yaml', YAML file structure is corrupted" in caplog.text

    caplog.clear()
    # validation error
    dvc_file.dump({"stages": {"stage1": None}})
    assert main(["stage", "list"]) != 0
    assert "dvc.yaml' validation failed" in caplog.text




tests/integration/__init__.py




tests/integration/conftest.py
# pylint disable=unused-argument
from tests.unit.repo.experiments.conftest import exp_stage, test_queue  # noqa: F401




tests/integration/test_studio_live_experiments.py
import pytest
from dvc_studio_client import env, post_live_metrics
from funcy import first

from dvc.env import (
    DVC_STUDIO_OFFLINE,
    DVC_STUDIO_REPO_URL,
    DVC_STUDIO_TOKEN,
    DVC_STUDIO_URL,
)


@pytest.mark.parametrize("tmp", [True, False])
@pytest.mark.parametrize("offline", [True, False])
def test_post_to_studio(
    tmp_dir, dvc, scm, exp_stage, mocker, monkeypatch, tmp, offline
):
    valid_response = mocker.MagicMock()
    valid_response.status_code = 200
    live_metrics = mocker.spy(post_live_metrics, "post_live_metrics")
    mocked_post = mocker.patch("requests.post", return_value=valid_response)

    monkeypatch.setenv(DVC_STUDIO_REPO_URL, "STUDIO_REPO_URL")
    monkeypatch.setenv(DVC_STUDIO_TOKEN, "STUDIO_TOKEN")
    monkeypatch.setenv(DVC_STUDIO_URL, "https://0.0.0.0")
    monkeypatch.setenv(DVC_STUDIO_OFFLINE, offline)

    baseline_sha = scm.get_rev()
    exp_rev = first(
        dvc.experiments.run(exp_stage.addressing, params=["foo=1"], tmp_dir=tmp)
    )
    name = dvc.experiments.get_exact_name([exp_rev])[exp_rev]

    assert live_metrics.call_count == 2
    start_call, done_call = live_metrics.call_args_list

    if offline:
        assert mocked_post.call_count == 0

    else:
        start_call, done_call = live_metrics.call_args_list
        assert start_call.kwargs["dvc_studio_config"]["token"] == "STUDIO_TOKEN"
        assert start_call.kwargs["dvc_studio_config"]["repo_url"] == "STUDIO_REPO_URL"

        assert mocked_post.call_count == 2

        start_call, done_call = mocked_post.call_args_list

        assert start_call.kwargs["json"] == {
            "type": "start",
            "repo_url": "STUDIO_REPO_URL",
            "baseline_sha": baseline_sha,
            "name": name,
            "params": {"params.yaml": {"foo": 1}},
            "client": "dvc",
        }

        assert done_call.kwargs["json"] == {
            "type": "done",
            "repo_url": "STUDIO_REPO_URL",
            "baseline_sha": baseline_sha,
            "name": name,
            "client": "dvc",
            "experiment_rev": exp_rev,
            "metrics": {"metrics.yaml": {"data": {"foo": 1}}},
        }


@pytest.mark.parametrize("tmp", [True, False])
def test_post_to_studio_custom_message(
    tmp_dir, dvc, scm, exp_stage, mocker, monkeypatch, tmp
):
    valid_response = mocker.MagicMock()
    valid_response.status_code = 200
    mocked_post = mocker.patch("requests.post", return_value=valid_response)

    monkeypatch.setenv(env.STUDIO_ENDPOINT, "https://0.0.0.0")
    monkeypatch.setenv(env.STUDIO_REPO_URL, "STUDIO_REPO_URL")
    monkeypatch.setenv(env.STUDIO_TOKEN, "STUDIO_TOKEN")

    baseline_sha = scm.get_rev()
    exp_rev = first(
        dvc.experiments.run(
            exp_stage.addressing, params=["foo=1"], tmp_dir=tmp, message="foo"
        )
    )
    name = dvc.experiments.get_exact_name([exp_rev])[exp_rev]
    assert mocked_post.call_count == 2

    start_call = mocked_post.call_args_list[0]

    assert start_call.kwargs["json"] == {
        "type": "start",
        "repo_url": "STUDIO_REPO_URL",
        "baseline_sha": baseline_sha,
        "name": name,
        "params": {"params.yaml": {"foo": 1}},
        "client": "dvc",
        "message": "foo",
    }




tests/integration/plots/__init__.py




tests/integration/plots/conftest.py
import pytest


@pytest.fixture
def repo_with_plots(tmp_dir, scm, dvc, run_copy_metrics):
    def make():
        linear_v1 = [
            {"x": 1, "y": 0.1},
            {"x": 2, "y": 0.2},
            {"x": 3, "y": 0.3},
        ]

        confusion_v1 = [
            {"actual": 0, "predicted": 1},
            {"actual": 0, "predicted": 1},
            {"actual": 1, "predicted": 0},
            {"actual": 1, "predicted": 0},
        ]

        image_v1 = b"content"

        (tmp_dir / "linear_src.json").dump_json(linear_v1)
        (tmp_dir / "confusion_src.json").dump_json(confusion_v1)
        (tmp_dir / "image_src.png").write_bytes(image_v1)

        scm.add(["linear_src.json", "confusion_src.json", "image_src.png"])
        scm.commit("add data sources")

        run_copy_metrics(
            "linear_src.json",
            "linear.json",
            name="linear",
            plots=["linear.json"],
            commit="linear",
        )
        run_copy_metrics(
            "confusion_src.json",
            "confusion.json",
            name="confusion",
            plots=["confusion.json"],
            commit="confusion",
        )
        linear_props = {"title": "linear", "x": "x"}
        dvc.plots.modify("linear.json", linear_props)
        confusion_props = {
            "title": "confusion matrix",
            "x": "predicted",
            "y": "actual",
            "template": "confusion",
        }
        dvc.plots.modify("confusion.json", confusion_props)
        run_copy_metrics(
            "image_src.png",
            "image.png",
            name="image",
            plots=["image.png"],
            commit="image",
        )

        scm.add(["dvc.yaml", "dvc.lock"])
        scm.commit("commit dvc files")
        yield image_v1, linear_v1, confusion_v1, confusion_props
        linear_v2 = [
            {"x": 1, "y": 0.2},
            {"x": 2, "y": 0.3},
            {"x": 3, "y": 0.4},
        ]
        confusion_v2 = [
            {"actual": 0, "predicted": 0},
            {"actual": 0, "predicted": 0},
            {"actual": 1, "predicted": 1},
            {"actual": 1, "predicted": 1},
        ]
        image_v2 = b"content2"

        (tmp_dir / "linear_src.json").dump_json(linear_v2)
        (tmp_dir / "confusion_src.json").dump_json(confusion_v2)
        (tmp_dir / "image_src.png").write_bytes(image_v2)

        dvc.reproduce()
        yield image_v2, linear_v2, confusion_v2, confusion_props

    return make


@pytest.fixture
def repo_with_config_plots(tmp_dir, scm, dvc, run_copy_metrics):
    def make():
        linear_train_v1 = [
            {"x": 1, "y": 0.1},
            {"x": 2, "y": 0.2},
            {"x": 3, "y": 0.3},
        ]
        linear_test_v1 = [
            {"x": 1, "y": 0.2},
            {"x": 2, "y": 0.3},
            {"x": 3, "y": 0.4},
        ]

        confusion_train_v1 = [
            {"actual": 0, "predicted": 1},
            {"actual": 0, "predicted": 1},
            {"actual": 1, "predicted": 0},
            {"actual": 1, "predicted": 0},
        ]
        confusion_test_v1 = [
            {"actual": 0, "predicted": 1},
            {"actual": 0, "predicted": 0},
            {"actual": 1, "predicted": 1},
            {"actual": 1, "predicted": 0},
        ]

        (tmp_dir / "linear_train_src.json").dump_json(linear_train_v1)
        (tmp_dir / "linear_test_src.json").dump_json(linear_test_v1)
        (tmp_dir / "confusion_train_src.json").dump_json(confusion_train_v1)
        (tmp_dir / "confusion_test_src.json").dump_json(confusion_test_v1)

        scm.add(
            [
                "linear_train_src.json",
                "linear_test_src.json",
                "confusion_train_src.json",
                "confusion_test_src.json",
            ]
        )
        scm.commit("add data sources")

        run_copy_metrics(
            "linear_train_src.json",
            "linear_train.json",
            name="linear_train",
            outs=["linear_train.json"],
            commit="linear_train",
        )
        run_copy_metrics(
            "linear_test_src.json",
            "linear_test.json",
            name="linear_test",
            outs=["linear_test.json"],
            commit="linear_test",
        )
        run_copy_metrics(
            "confusion_train_src.json",
            "confusion_train.json",
            name="confusion_train",
            outs=["confusion_train.json"],
            commit="confusion_train",
        )
        run_copy_metrics(
            "confusion_test_src.json",
            "confusion_test.json",
            name="confusion_test",
            outs=["confusion_test.json"],
            commit="confusion_test",
        )
        plots_config = [
            {
                "linear_train_vs_test": {
                    "x": "x",
                    "y": {"linear_train.json": "y", "linear_test.json": "y"},
                    "title": "linear plot",
                }
            },
            {
                "confusion_train_vs_test": {
                    "x": "actual",
                    "y": {
                        "confusion_train.json": "predicted",
                        "confusion_test.json": "predicted",
                    },
                    "template": "confusion",
                }
            },
        ]

        from dvc.utils.serialize import modify_yaml

        with modify_yaml("dvc.yaml") as dvcfile_content:
            dvcfile_content["plots"] = plots_config

        scm.add(["dvc.yaml", "dvc.lock"])
        scm.commit("commit dvc files")
        yield {
            "data": {
                "linear_train.json": linear_train_v1,
                "linear_test.json": linear_test_v1,
                "confusion_train.json": confusion_train_v1,
                "confusion_test.json": confusion_test_v1,
            },
            "configs": {"dvc.yaml": plots_config},
        }

    return make




tests/integration/plots/test_plots.py
import json
import os
from copy import deepcopy
from typing import Dict, List
from urllib.parse import urlparse
from urllib.request import url2pathname

import dpath
import pytest
from bs4 import BeautifulSoup
from funcy import first

from dvc.cli import main
from dvc.render import REVISION_FIELD, VERSION_FIELD

JSON_OUT = "vis_data"


def call(capsys, subcommand="show"):
    capsys.readouterr()
    assert main(["plots", subcommand, "--json", "-o", JSON_OUT, "--split"]) == 0
    split_json_out, _ = capsys.readouterr()

    split_json_result = json.loads(split_json_out)

    capsys.readouterr()
    assert main(["plots", subcommand, "--json", "-o", JSON_OUT]) == 0
    json_out, _ = capsys.readouterr()

    json_result = json.loads(json_out)

    assert main(["plots", subcommand]) == 0
    html_path_out, _ = capsys.readouterr()

    parsed = urlparse(html_path_out.strip())
    abspath = url2pathname(parsed.path)
    return abspath, json_result, split_json_result


def extract_vega_specs(html_path, plots_ids):
    from dvc_render.base import Renderer

    result = {}

    with open(html_path, encoding="utf-8") as fd:
        content = fd.read()

    reader = BeautifulSoup(content, features="html.parser")
    for plot_id in plots_ids:
        script = _remove_blanks(
            reader.find("div", id=Renderer.remove_special_chars(plot_id)).script.text
        )
        result[plot_id] = json.loads(
            script.split("; vegaEmbed")[0].replace("var spec = ", "")
        )

    return result


def drop_fields(datapoints: List[Dict], fields: List[str]):
    tmp = deepcopy(datapoints)
    for datapoint in tmp:
        keys = set(datapoint.keys())
        for key in keys:
            if key in fields:
                datapoint.pop(key)
    return tmp


def verify_image(path, version, filename, content, html_path, json_result):
    assert os.path.exists(html_path)
    with open(html_path, encoding="utf-8") as fd:
        html_content = fd.read()

    image_data = {}
    for datapoint in json_result[filename]:
        if datapoint["revisions"] == [version]:
            image_data = datapoint
            break

    assert image_data, f"{version} data for {filename} was not found"
    assert image_data["type"] == "image"
    output_filename = filename.replace("/", "_")
    output_name = f"{version}_{output_filename}"
    assert image_data["url"] == str(path / JSON_OUT / output_name)
    assert (path / JSON_OUT / output_name).read_bytes() == content

    assert os.path.join("static", output_name) in html_content

    # there should be no absolute paths in produced HTML
    # TODO uncomment once dvc-render is adjusted
    # assert str(path) not in html_content
    assert (path / "dvc_plots" / "static" / output_name).read_bytes() == content


def _remove_blanks(text: str):
    return " ".join(text.replace("\t", "").replace("\n", "").split())


def verify_vega(
    versions,
    html_result,
    json_result,
    split_json_result,
):
    if isinstance(versions, str):
        versions = [versions]

    for j in [json_result, split_json_result]:
        assert len(j) == 1
        assert j[0]["type"] == "vega"
        assert set(j[0]["revisions"]) == set(versions)

    assert json_result[0]["datapoints"] == split_json_result[0]["datapoints"]
    assert set(versions) == set(json_result[0]["datapoints"].keys())

    assert json_result[0]["content"]["data"]["values"]
    assert html_result["data"]["values"]
    assert split_json_result[0]["content"]["data"]["values"] == "<DVC_METRIC_DATA>"

    def _assert_templates_equal(html_template, filled_template, split_template):
        # besides data, json and split json should be equal
        path = ["data", "values"]
        tmp1 = deepcopy(html_template)
        tmp2 = deepcopy(filled_template)
        tmp3 = deepcopy(split_template)
        dpath.set(tmp1, path, {})
        dpath.set(tmp2, path, {})
        dpath.set(tmp3, path, {})

        assert tmp1 == tmp2 == tmp3

    _assert_templates_equal(
        html_result, json_result[0]["content"], split_json_result[0]["content"]
    )


def verify_vega_props(plot_id, json_result, title, x, y, **kwargs):
    data = json_result[plot_id]
    assert len(data) == 1
    data = first(data)

    assert dpath.get(data, ["content", "title"]) == title

    try:
        # TODO confusion_matrix_plot - need to find better way of asserting
        #      encoding as its place is not constant in vega
        plot_x = dpath.get(data, ["content", "spec", "encoding", "x", "field"])
        plot_y = dpath.get(data, ["content", "spec", "encoding", "y", "field"])
    except KeyError:
        # default plot
        plot_x = dpath.get(data, ["content", "layer", 0, "encoding", "x", "field"])
        plot_y = dpath.get(data, ["content", "layer", 0, "encoding", "y", "field"])

    assert plot_x == x
    assert plot_y == y


def _update_datapoints(datapoints: List, update: Dict):
    result = []
    for dp in datapoints:
        tmp = dp.copy()
        tmp.update(update)
        result.append(tmp)
    return result


@pytest.mark.vscode
def test_no_plots(tmp_dir, scm, dvc, capsys):
    html_path, json_result, split_json_result = call(capsys)
    assert not os.path.exists(html_path)
    assert json_result == {}
    assert split_json_result == {}


@pytest.mark.vscode
def test_repo_with_plots(tmp_dir, scm, dvc, capsys, run_copy_metrics, repo_with_plots):
    repo_state = repo_with_plots()

    image_v1, linear_v1, confusion_v1, confusion_props = next(repo_state)

    html_path, json_result, split_json_result = call(capsys)
    html_result = extract_vega_specs(html_path, ["linear.json", "confusion.json"])

    assert "errors" not in json_result
    assert "errors" not in split_json_result

    json_data = json_result["data"]
    split_json_data = split_json_result["data"]

    assert json_data["linear.json"][0]["content"]["data"][
        "values"
    ] == _update_datapoints(
        linear_v1,
        {
            VERSION_FIELD: {
                "revision": "workspace",
                "filename": "linear.json",
                "field": "y",
            },
        },
    )
    assert html_result["linear.json"]["data"]["values"] == _update_datapoints(
        linear_v1,
        {
            REVISION_FIELD: "workspace",
        },
    )
    assert json_data["confusion.json"][0]["content"]["data"][
        "values"
    ] == _update_datapoints(
        confusion_v1,
        {
            VERSION_FIELD: {
                "revision": "workspace",
                "filename": "confusion.json",
                "field": "actual",
            },
        },
    )
    assert html_result["confusion.json"]["data"]["values"] == _update_datapoints(
        confusion_v1,
        {
            REVISION_FIELD: "workspace",
        },
    )
    verify_image(tmp_dir, "workspace", "image.png", image_v1, html_path, json_data)

    for plot in ["linear.json", "confusion.json"]:
        verify_vega(
            "workspace",
            html_result[plot],
            json_data[plot],
            split_json_data[plot],
        )

    verify_vega_props("confusion.json", json_data, **confusion_props)

    image_v2, linear_v2, confusion_v2, confusion_props = next(repo_state)

    html_path, json_result, split_json_result = call(capsys, subcommand="diff")
    html_result = extract_vega_specs(html_path, ["linear.json", "confusion.json"])

    assert "errors" not in json_result
    assert "errors" not in split_json_result

    json_data = json_result["data"]
    split_json_data = split_json_result["data"]

    verify_image(tmp_dir, "workspace", "image.png", image_v2, html_path, json_data)
    verify_image(tmp_dir, "HEAD", "image.png", image_v1, html_path, json_data)

    for plot in ["linear.json", "confusion.json"]:
        verify_vega(
            ["HEAD", "workspace"],
            html_result[plot],
            json_data[plot],
            split_json_data[plot],
        )
    verify_vega_props("confusion.json", json_data, **confusion_props)
    path = tmp_dir / "subdir"
    path.mkdir()
    with path.chdir():
        html_path, json_result, split_json_result = call(capsys, subcommand="diff")
        html_result = extract_vega_specs(
            html_path,
            ["../linear.json", "../confusion.json"],
        )

        assert "errors" not in json_result
        assert "errors" not in split_json_result

        json_data = json_result["data"]
        split_json_data = split_json_result["data"]
        assert json_data["../linear.json"][0]["content"]["data"][
            "values"
        ] == _update_datapoints(
            linear_v2,
            {
                VERSION_FIELD: {
                    "revision": "workspace",
                    "filename": "../linear.json",
                    "field": "y",
                },
            },
        ) + _update_datapoints(
            linear_v1,
            {
                VERSION_FIELD: {
                    "revision": "HEAD",
                    "filename": "../linear.json",
                    "field": "y",
                },
            },
        )
        assert html_result["../linear.json"]["data"]["values"] == _update_datapoints(
            linear_v2,
            {
                REVISION_FIELD: "workspace",
            },
        ) + _update_datapoints(
            linear_v1,
            {
                REVISION_FIELD: "HEAD",
            },
        )
        assert json_data["../confusion.json"][0]["content"]["data"][
            "values"
        ] == _update_datapoints(
            confusion_v2,
            {
                VERSION_FIELD: {
                    "revision": "workspace",
                    "filename": "../confusion.json",
                    "field": "actual",
                },
            },
        ) + _update_datapoints(
            confusion_v1,
            {
                VERSION_FIELD: {
                    "revision": "HEAD",
                    "filename": "../confusion.json",
                    "field": "actual",
                },
            },
        )
        assert html_result["../confusion.json"]["data"]["values"] == _update_datapoints(
            confusion_v2,
            {
                REVISION_FIELD: "workspace",
            },
        ) + _update_datapoints(
            confusion_v1,
            {
                REVISION_FIELD: "HEAD",
            },
        )

        for plot in [
            "../linear.json",
            "../confusion.json",
        ]:
            verify_vega(
                ["HEAD", "workspace"],
                html_result[plot],
                json_data[plot],
                split_json_data[plot],
            )
        verify_image(
            path,
            "workspace",
            "../image.png",
            image_v2,
            html_path,
            json_data,
        )
        verify_image(
            path,
            "HEAD",
            "../image.png",
            image_v1,
            html_path,
            json_data,
        )


@pytest.mark.vscode
def test_repo_with_removed_plots(tmp_dir, capsys, repo_with_plots):
    from dvc.utils.fs import remove

    next(repo_with_plots())

    # even if there is no data, call should be successful
    remove(tmp_dir / ".dvc" / "cache")
    remove("linear.json")
    remove("confusion.json")
    remove("image.png")

    for s in {"show", "diff"}:
        _, json_result, split_json_result = call(capsys, subcommand=s)
        errors = [
            {
                "name": p,
                "source": p,
                "rev": "workspace",
                "type": "FileNotFoundError",
                "msg": "",
            }
            for p in [
                "linear.json",
                "confusion.json",
                "image.png",
            ]
        ]
        expected_result = {
            "errors": errors,
            "data": {
                "image.png": [],
                "confusion.json": [],
                "linear.json": [],
            },
        }
        assert json_result == expected_result
        assert split_json_result == expected_result


def test_config_output_dir(tmp_dir, dvc, capsys):
    subdir = tmp_dir / "subdir"
    ret = main(["config", "plots.out_dir", os.fspath(subdir)])
    assert ret == 0

    metric = [{"first_val": 100, "val": 2}, {"first_val": 200, "val": 3}]
    (tmp_dir / "metric.json").dump_json(metric, sort_keys=True)

    assert main(["plots", "show", "metric.json"]) == 0

    out, _ = capsys.readouterr()
    assert subdir.as_uri() in out
    assert subdir.is_dir()
    assert (subdir / "index.html").is_file()

    cli_arg_subdir = tmp_dir / "cli_option"
    assert main(["plots", "show", "-o", os.fspath(cli_arg_subdir), "metric.json"]) == 0

    out, _ = capsys.readouterr()
    assert cli_arg_subdir.as_uri() in out
    assert cli_arg_subdir.is_dir()
    assert (cli_arg_subdir / "index.html").is_file()


@pytest.mark.vscode
def test_repo_with_config_plots(tmp_dir, capsys, repo_with_config_plots):
    repo_state = repo_with_config_plots()
    plots = next(repo_state)

    html_path, _, __ = call(capsys)

    assert os.path.exists(html_path)
    html_result = extract_vega_specs(
        html_path,
        [
            "dvc.yaml::linear_train_vs_test",
            "dvc.yaml::confusion_train_vs_test",
        ],
    )
    ble = _update_datapoints(
        plots["data"]["linear_train.json"],
        {
            REVISION_FIELD: "linear_train.json",
        },
    ) + _update_datapoints(
        plots["data"]["linear_test.json"],
        {
            REVISION_FIELD: "linear_test.json",
        },
    )

    assert html_result["dvc.yaml::linear_train_vs_test"]["data"]["values"] == ble
    # TODO check json results once vscode is able to handle flexible plots




tests/integration/plots/test_repo_plots_api.py
import pytest
from funcy import merge

from tests.utils.plots import get_plot


@pytest.mark.studio
def test_api(tmp_dir, dvc, repo_with_plots):
    repo_state = repo_with_plots()
    image_v1, linear_v1, confusion_v1, confusion_params = next(repo_state)

    workspace_data = next(dvc.plots.collect())

    assert get_plot(workspace_data, "workspace", file="image.png", endkey="props") == {}
    image_source = get_plot(
        workspace_data, "workspace", file="image.png", endkey="data_source"
    )
    assert callable(image_source)
    assert image_source() == {"data": image_v1}

    assert get_plot(
        workspace_data, "workspace", file="linear.json", endkey="props"
    ) == {"title": "linear", "x": "x"}
    linear_source = get_plot(
        workspace_data, "workspace", file="linear.json", endkey="data_source"
    )
    assert callable(linear_source)
    assert linear_source() == {"data": linear_v1}

    assert (
        get_plot(workspace_data, "workspace", file="confusion.json", endkey="props")
        == confusion_params
    )
    confusion_source = get_plot(
        workspace_data,
        "workspace",
        file="confusion.json",
        endkey="data_source",
    )
    assert callable(confusion_source)
    assert confusion_source() == {"data": confusion_v1}

    image_v2, linear_v2, confusion_v2, _ = next(repo_state)
    data_generator = dvc.plots.collect(revs=["workspace", "HEAD"])

    workspace_data = next(data_generator)

    assert get_plot(workspace_data, "workspace", file="image.png", endkey="props") == {}
    image_source = get_plot(
        workspace_data, "workspace", file="image.png", endkey="data_source"
    )
    assert callable(image_source)
    assert image_source() == {"data": image_v2}

    assert get_plot(
        workspace_data, "workspace", file="linear.json", endkey="props"
    ) == {"title": "linear", "x": "x"}
    linear_source = get_plot(
        workspace_data, "workspace", file="linear.json", endkey="data_source"
    )
    assert callable(linear_source)
    assert linear_source() == {"data": linear_v2}

    assert (
        get_plot(workspace_data, "workspace", file="confusion.json", endkey="props")
        == confusion_params
    )
    confusion_source = get_plot(
        workspace_data,
        "workspace",
        file="confusion.json",
        endkey="data_source",
    )
    assert callable(confusion_source)
    assert confusion_source() == {"data": confusion_v2}

    head_data = next(data_generator)

    assert get_plot(head_data, "HEAD", file="image.png", endkey="props") == {}
    image_source = get_plot(head_data, "HEAD", file="image.png", endkey="data_source")
    assert callable(image_source)
    assert image_source() == {"data": image_v1}

    assert get_plot(head_data, "HEAD", file="linear.json", endkey="props") == {
        "title": "linear",
        "x": "x",
    }
    linear_source = get_plot(
        head_data, "HEAD", file="linear.json", endkey="data_source"
    )
    assert callable(linear_source)
    assert linear_source() == {"data": linear_v1}

    assert (
        get_plot(head_data, "HEAD", file="confusion.json", endkey="props")
        == confusion_params
    )
    confusion_source = get_plot(
        head_data, "HEAD", file="confusion.json", endkey="data_source"
    )
    assert callable(confusion_source)
    assert confusion_source() == {"data": confusion_v1}


@pytest.mark.studio
def test_api_with_config_plots(tmp_dir, dvc, capsys, repo_with_config_plots):
    repo_state = repo_with_config_plots()
    plots_state = next(repo_state)

    plots_data = next(dvc.plots.collect())

    assert get_plot(
        plots_data, "workspace", typ="definitions", file="dvc.yaml"
    ) == merge(*plots_state["configs"]["dvc.yaml"])

    for file in plots_state["data"]:
        data_source = get_plot(plots_data, "workspace", file=file, endkey="data_source")
        assert callable(data_source)
        assert data_source() == {"data": plots_state["data"][file]}




tests/remotes/__init__.py
from dvc_ssh.tests.fixtures import (  # noqa, pylint: disable=unused-import
    make_ssh,
    ssh,
    ssh_server,
)

from .git_server import git_server, git_ssh  # noqa: F401

TEST_REMOTE = "upstream"
TEST_CONFIG = {
    "cache": {},
    "core": {"remote": TEST_REMOTE},
    "remote": {TEST_REMOTE: {"url": ""}},
}




tests/remotes/git_server.py
import pytest
from dvc_ssh.tests.cloud import SSH, TEST_SSH_KEY_PATH, TEST_SSH_USER


class GitSSH(SSH):
    @staticmethod
    def get_url(host, port):
        return f"ssh://{host}:{port}/tmp/data/git"


@pytest.fixture
def git_server(request, test_config):
    import asyncssh
    from sshfs import SSHFileSystem

    test_config.requires("ssh")
    docker_services = request.getfixturevalue("docker_services")
    conn_info = {
        "host": "127.0.0.1",
        "port": docker_services.port_for("git-server", 2222),
    }

    def get_fs():
        return SSHFileSystem(
            **conn_info,
            username=TEST_SSH_USER,
            client_keys=[TEST_SSH_KEY_PATH],
        )

    def _check():
        try:
            fs = get_fs()
            fs.exists("/")
            fs.execute("git --version")
        except asyncssh.Error:
            return False
        else:
            return True

    docker_services.wait_until_responsive(timeout=30.0, pause=1, check=_check)
    return conn_info


@pytest.fixture
def git_ssh_connection(git_server):
    from sshfs import SSHFileSystem

    return SSHFileSystem(
        host=git_server["host"],
        port=git_server["port"],
        username=TEST_SSH_USER,
        client_keys=[TEST_SSH_KEY_PATH],
    )


@pytest.fixture
def git_ssh(git_server, monkeypatch):
    url = GitSSH(GitSSH.get_url(**git_server))
    url.mkdir(exist_ok=True, parents=True)
    return url




tests/remotes/user.key
-----BEGIN RSA PRIVATE KEY-----
MIIEpgIBAAKCAQEA/HlMR0chgFLx6A/BmYi9Ypj9nr0kZ3Wo6n3ZdSXORJaH6e7d
LSw+0eoD1NDCxDORZlwYNAg45zndU5sPN4IvVcAvC/FD0qSK1H0ku1p4QV82y4a1
SOnzRlDUIVhdQxnlQa8gI+zDO8AyzOJ4oZ9LL7Hy+mIDqGxRwHeKfDYXuHkE+aM9
CCkidPqR/Uqxag03+y51MEphC07mr7mrDb4lOmeqy8Xq0ZcUjIkmKiGCVVRJO6g4
kYFwr0UwpKCwLzOfw4Fy2SpzWfe3IdVUqIro97d1AGC+9OUQETiVpBYObBI1qfQo
ef6MT+CFcTV40cyKdCG6x57f6gpPMYqYfdHTyQIDAQABAoIBAQDkzgTL3/HDeugS
WB0qyFphvaazMlSIkn/3qv/lA9MQI5+e6MN3Cc8Qq9S3DE5GQzm1GycwGHeBTdZ/
y1maA5hkTRwV5ZuCjW3nrlYYmJ+9Fs3w2u712leHVP86DPvQMOqsgUpOZGZ2gvNG
7MNILbWUzt8V/Le17hyUoYFWmisbGH0UyiRJsUHoAdZFdRq+sDVsFmu49fhcEsLI
5B0fB2v5FSwgSuIfYNjPNEJG9xK+qZzjEXd5g7XVs5mbrMflbyOwDDf6/nMILGnO
ZvtCagqaH0SaxnEzucsdFdnGBhzGEbPXxDVxuS7kcdWpEN+f0JksYaVaLetCsD3s
xudTxI3RAoGBAP85vkIaG2WEurONLURLAiM3dNxP4KXonADqn8RnJ5TNpn4Il9g5
wiuANjja7Cvf6GrvDRl7lrphAQ/c/6iRl6tPv8Hvertd7RXmG9QoLl/qG3ghjNDl
WUexvloZ7NXqKq9PSj7vq76cxIfSklAEQ+x1ldalVaS9iR+ZqKryY8plAoGBAP09
atBWJPapFoci7Mrml4N8TNY+n7unD/yyoLabmm0YfPbBy9x9SyCbefVwB5F1dWeV
MZc3oG6EMKg/BH/vnKpiNtXk4OubIbqJD3jHrqeT6wAkSItgVnlU6t6FLuKjLXoD
Buw3oM/+i2pDy8oSsxueBv1GyMZG0ixcShkeXPuVAoGBAMFFHMo5st1hcXBeTBUX
J/s7F4duBZQdXWVkRrAX3WVVheqS30miE2OVp3nObmGbIQk5FRZi/HUO2BsHI6Km
/c+AiJl3m90e91ZJ9nDmLJf9U+fYoCXgR4d/FcJtN2eV99ThmjumisvBMyIXVyy4
zibVtC3i7cPes2P2nD83ZlHxAoGBAN+VSCckx4HXjAJH/ZSuvnriVdyaceD2ARF0
jJxtCYzkoAAk3l6PaLMjUiw2exgcAkov2RbPkB/DKkqBSPHDliiAijWS3FpoHwFY
XYaflj5yRHtdjYcwyWhaZvuLzvdeZppg7c3E14CMFn792IFSvTvW7AjWZBFbGdj8
qpc+zY15AoGBANvsGFHmurP+VUu1ibegREBySts0WGNyq5VOTCrkN9S7AKR1pZNI
I6W7KrRcXEvbBM7B47ykbE40hAachxN1Rpk+9qEom6etTaw/yMewgFNjZXYJw06z
4bq6ofjKK8VqCWx41pWcmXj7Fa2A43RvZWg8TlX7Q8uc4wTBpTkuzfZC
-----END RSA PRIVATE KEY-----




tests/remotes/user.key.pub
sh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQD8eUxHRyGAUvHoD8GZiL1imP2evSRndajqfdl1Jc5Elofp7t0tLD7R6gPU0MLEM5FmXBg0CDjnOd1Tmw83gi9VwC8L8UPSpIrUfSS7WnhBXzbLhrVI6fNGUNQhWF1DGeVBryAj7MM7wDLM4nihn0svsfL6YgOobFHAd4p8Nhe4eQT5oz0IKSJ0+pH9SrFqDTf7LnUwSmELTuavuasNviU6Z6rLxerRlxSMiSYqIYJVVEk7qDiRgXCvRTCkoLAvM5/DgXLZKnNZ97ch1VSoiuj3t3UAYL705RAROJWkFg5sEjWp9Ch5/oxP4IVxNXjRzIp0IbrHnt/qCk8xiph90dPJ user@unicorn




tests/remotes/git-init/git.sh
#!/bin/bash
apk add --no-cache git




tests/unit/__init__.py




tests/unit/test_analytics.py
import json
import platform

import pytest
from voluptuous import Any, Schema

from dvc import analytics
from dvc.cli import parse_args


@pytest.fixture
def tmp_global_dir(mocker, tmp_path):
    """
    Fixture to prevent modifying the actual global config
    """

    def _user_config_dir(appname, *_args, **_kwargs):
        return str(tmp_path / appname)

    mocker.patch("iterative_telemetry.user_config_dir", _user_config_dir)


def test_collect_and_send_report(mocker, tmp_global_dir):
    mock_json = mocker.patch("json.dump")
    mock_daemon = mocker.patch("dvc.daemon._spawn")
    analytics.collect_and_send_report()
    report = mock_json.call_args[0][0]

    assert not report.get("cmd_class")
    assert not report.get("cmd_return_code")

    args = parse_args(["add", "foo"])
    return_code = 0

    analytics.collect_and_send_report(args, return_code)
    report = mock_json.call_args[0][0]

    assert report["cmd_class"] == "CmdAdd"
    assert report["cmd_return_code"] == return_code

    assert mock_daemon.call_count == 2


def test_runtime_info(tmp_global_dir):
    schema = Schema(
        {
            "dvc_version": str,
            "is_binary": bool,
            "scm_class": Any("Git", None),
            "user_id": str,
            "system_info": dict,
            "group_id": Any(str, None),
        },
        required=True,
    )

    assert schema(analytics._runtime_info())


def test_send(mocker, tmp_path):
    mock_post = mocker.patch("requests.post")

    import requests

    url = "https://analytics.dvc.org"
    report = {"name": "dummy report"}
    report_file = tmp_path / "report"

    report_file.write_text(json.dumps(report))
    mock_post.side_effect = requests.exceptions.RequestException

    analytics.send(str(report_file))
    assert mock_post.called
    assert mock_post.call_args[0][0] == url
    assert not report_file.exists()


@pytest.mark.parametrize(
    "config, result",
    [
        ({}, True),
        ({"analytics": "false"}, False),
        ({"analytics": "true"}, True),
        ({"unknown": "broken"}, True),
        ({"analytics": "false", "unknown": "broken"}, False),
    ],
)
def test_is_enabled(dvc, config, result, monkeypatch, tmp_global_dir):
    with dvc.config.edit(validate=False) as conf:
        conf["core"] = config

    # reset DVC_TEST env var, which affects `is_enabled()`
    monkeypatch.delenv("DVC_TEST")
    monkeypatch.delenv("DVC_NO_ANALYTICS", raising=False)

    assert result == analytics.is_enabled()


@pytest.mark.parametrize(
    "config, env, result",
    [
        (None, None, True),
        (None, "true", False),
        (None, "false", False),  # only checking if env is set
        ("false", None, False),
        ("false", "true", False),
        ("false", "false", False),
        ("true", None, True),
        ("true", "true", False),
        ("true", "false", False),  # we checking if env is set
    ],
)
def test_is_enabled_env_neg(dvc, config, env, result, monkeypatch, tmp_global_dir):
    # reset DVC_TEST env var, which affects `is_enabled()`
    monkeypatch.delenv("DVC_TEST")
    monkeypatch.delenv("DVC_NO_ANALYTICS", raising=False)

    with dvc.config.edit() as conf:
        conf["core"] = {}

    assert analytics.is_enabled()

    if config is not None:
        with dvc.config.edit() as conf:
            conf["core"] = {"analytics": config}

    if env is not None:
        monkeypatch.setenv("DVC_NO_ANALYTICS", env)

    assert result == analytics.is_enabled()


def test_system_info():
    schema = Schema({"os": Any("windows", "mac", "linux")}, required=True)

    system = platform.system()

    if system == "Windows":
        schema = schema.extend(
            {
                "windows_version_build": int,
                "windows_version_major": int,
                "windows_version_minor": int,
                "windows_version_service_pack": str,
            }
        )

    if system == "Darwin":
        schema = schema.extend({"mac_version": str})

    if system == "Linux":
        schema = schema.extend(
            {
                "linux_distro": str,
                "linux_distro_like": str,
                "linux_distro_version": str,
            }
        )

    assert schema(analytics._system_info())




tests/unit/test_api.py
import pytest

from dvc import api


def test_open_raises_error_if_no_context(tmp_dir, dvc):
    tmp_dir.dvc_gen("foo", "foo-text")

    fd = api.open("foo")
    with pytest.raises(AttributeError, match="should be used in a with statement."):
        fd.read()


def test_open_rev_raises_error_on_wrong_mode(tmp_dir, dvc):
    tmp_dir.dvc_gen("foo", "foo-text")

    with pytest.raises(ValueError, match="Only reading `mode` is supported."):
        with api.open("foo", mode="w"):
            pass




tests/unit/test_collect.py
from dvc.repo.collect import collect


def test_collect_duplicates(tmp_dir, scm, dvc):
    tmp_dir.gen("params.yaml", "foo: 1\nbar: 2")
    tmp_dir.gen("foobar", "")

    dvc.run(name="stage-1", cmd="echo stage-1", params=["foo"])
    dvc.run(name="stage-2", cmd="echo stage-2", params=["bar"])

    outs, _ = collect(dvc, deps=True, targets=["params.yaml"])
    assert len(outs) == 1

    outs, _ = collect(dvc, deps=True, targets=["params.yaml"], duplicates=True)
    assert len(outs) == 2

    outs, _ = collect(dvc, deps=True, targets=["foobar"], duplicates=True)
    assert not outs




tests/unit/test_compare.py
import textwrap

import pytest

from dvc.compare import diff_table, metrics_table, show_diff, show_metrics
from dvc.utils.serialize import YAMLFileCorruptedError


@pytest.mark.parametrize("title", ["Metric", "Param"])
def test_diff_table(title):
    td = diff_table(
        {"metrics.json": {"a.b.c": {"old": 1, "new": 2, "diff": 3}}},
        title=title,
    )
    assert td.as_dict() == [
        {
            "Path": "metrics.json",
            title: "a.b.c",
            "HEAD": "1",
            "workspace": "2",
            "Change": "3",
        }
    ]


def test_diff_table_with_value_column():
    td = diff_table(
        {"metrics.json": {"a.b.c": {"old": 1, "new": 2, "diff": 3}}},
        title="Metric",
        old=False,
    )
    assert td.as_dict() == [
        {
            "Path": "metrics.json",
            "Metric": "a.b.c",
            "Value": "2",
            "Change": "3",
        }
    ]


def test_no_path():
    td = diff_table(
        {"metrics.json": {"a.b.c": {"old": 1, "new": 2, "diff": 3}}},
        title="Metric",
        no_path=True,
    )
    assert td.as_dict() == [
        {"Metric": "a.b.c", "HEAD": "1", "workspace": "2", "Change": "3"}
    ]


def test_do_not_show_changes():
    td = diff_table(
        {"metrics.json": {"a.b.c": {"old": 1, "new": 2, "diff": 3}}},
        title="Metric",
        show_changes=False,
    )
    assert td.as_dict() == [
        {
            "Path": "metrics.json",
            "Metric": "a.b.c",
            "HEAD": "1",
            "workspace": "2",
        }
    ]


def test_diff_table_precision():
    diff = {"metrics.json": {"a.b.c": {"old": 1.1234, "new": 2.2345, "diff": 3.3456}}}
    td = diff_table(diff, title="Metric", precision=3)
    assert td.as_dict() == [
        {
            "Path": "metrics.json",
            "Metric": "a.b.c",
            "HEAD": "1.12",
            "workspace": "2.23",
            "Change": "3.35",
        }
    ]


def test_diff_table_rounding():
    diff = {"metrics.json": {"a.b.c": {"old": 1.1234, "new": 2.2345, "diff": 3.3456}}}
    td = diff_table(diff, title="Metric", precision=3, round_digits=True)
    assert td.as_dict() == [
        {
            "Path": "metrics.json",
            "Metric": "a.b.c",
            "HEAD": "1.123",
            "workspace": "2.235",
            "Change": "3.346",
        }
    ]


@pytest.mark.parametrize(
    "extra, expected", [({"on_empty_diff": "no diff"}, "no diff"), ({}, "-")]
)
def test_diff_unsupported_diff_message(extra, expected):
    td = diff_table(
        {"metrics.json": {"": {"old": "1", "new": "2"}}},
        title="Metric",
        **extra,
    )
    assert td.as_dict() == [
        {
            "Path": "metrics.json",
            "Metric": "",
            "HEAD": "1",
            "workspace": "2",
            "Change": expected,
        }
    ]


def test_diff_new():
    td = diff_table(
        {"param.json": {"a.b.d": {"old": None, "new": "new"}}}, title="Param"
    )
    assert td.as_dict() == [
        {
            "Path": "param.json",
            "Param": "a.b.d",
            "HEAD": "-",
            "workspace": "new",
            "Change": "-",
        }
    ]


def test_diff_old_deleted():
    td = diff_table(
        {"metric.json": {"a.b.d": {"old": "old", "new": None}}}, title="Metric"
    )
    assert td.as_dict() == [
        {
            "Path": "metric.json",
            "Metric": "a.b.d",
            "HEAD": "old",
            "workspace": "-",
            "Change": "-",
        }
    ]


def test_diff_sorted():
    td = diff_table(
        {
            "metrics.yaml": {
                "x.b": {"old": 5, "new": 6, "diff": 1},
                "a.d.e": {"old": 3, "new": 4, "diff": 1},
                "a.b.c": {"old": 1, "new": 2, "diff": 1},
            }
        },
        "Metric",
    )
    assert list(td) == [
        ["metrics.yaml", "a.b.c", "1", "2", "1"],
        ["metrics.yaml", "a.d.e", "3", "4", "1"],
        ["metrics.yaml", "x.b", "5", "6", "1"],
    ]


def test_diff_falsey_values():
    diff = {"metrics.yaml": {"x.b": {"old": 0, "new": 0.0, "diff": 0.0}}}
    td = diff_table(diff, "Metric")
    assert td.as_dict() == [
        {
            "Path": "metrics.yaml",
            "Metric": "x.b",
            "HEAD": "0",
            "workspace": "0.0",
            "Change": "0.0",
        }
    ]


@pytest.mark.parametrize(
    "composite, expected",
    [([2, 3], "[2, 3]"), ({"foo": 3, "bar": 3}, "{'foo': 3, 'bar': 3}")],
)
def test_diff_list(composite, expected):
    td = diff_table({"params.yaml": {"a.b.c": {"old": 1, "new": composite}}}, "Param")
    assert td.as_dict() == [
        {
            "Path": "params.yaml",
            "Param": "a.b.c",
            "HEAD": "1",
            "workspace": expected,
            "Change": "-",
        }
    ]


@pytest.mark.parametrize("markdown", [True, False])
def test_diff_mocked(mocker, markdown):
    ret = mocker.MagicMock()
    m = mocker.patch("dvc.compare.diff_table", return_value=ret)

    show_diff({}, "metrics", markdown=markdown)

    m.assert_called_once_with(
        {},
        title="metrics",
        old=True,
        no_path=False,
        precision=None,
        on_empty_diff=None,
        show_changes=True,
        round_digits=False,
        a_rev=None,
        b_rev=None,
    )
    ret.render.assert_called_once_with(markdown=markdown)


def test_diff_default(capsys):
    show_diff(
        {
            "metrics.yaml": {
                "x.b": {"old": 5, "new": 6},
                "a.d.e": {"old": 3, "new": 4, "diff": 1},
                "a.b.c": {"old": 1, "new": 2, "diff": 1},
            }
        },
        "Metric",
    )
    out, _ = capsys.readouterr()

    assert out == textwrap.dedent(
        """\
        Path          Metric    HEAD    workspace    Change
        metrics.yaml  a.b.c     1       2            1
        metrics.yaml  a.d.e     3       4            1
        metrics.yaml  x.b       5       6            -
        """
    )


def test_metrics_diff_md(capsys):
    show_diff(
        {
            "metrics.yaml": {
                "x.b": {"old": 5, "new": 6},
                "a.d.e": {"old": 3, "new": 4, "diff": 1},
                "a.b.c": {"old": 1, "new": 2, "diff": 1},
            }
        },
        "Metric",
        markdown=True,
    )
    out, _ = capsys.readouterr()

    assert out == textwrap.dedent(
        """\
        | Path         | Metric   | HEAD   | workspace   | Change   |
        |--------------|----------|--------|-------------|----------|
        | metrics.yaml | a.b.c    | 1      | 2           | 1        |
        | metrics.yaml | a.d.e    | 3      | 4           | 1        |
        | metrics.yaml | x.b      | 5      | 6           | -        |

        """
    )


def test_metrics_show_with_valid_falsey_values():
    td = metrics_table(
        {
            "branch_1": {
                "data": {
                    "metrics.json": {"data": {"a": 0, "b": {"ad": 0.0, "bc": 0.0}}}
                }
            }
        },
        all_branches=True,
    )
    assert td.as_dict() == [
        {
            "Revision": "branch_1",
            "Path": "metrics.json",
            "a": "0",
            "b.ad": "0.0",
            "b.bc": "0.0",
        }
    ]


def test_metrics_show_with_no_revision():
    td = metrics_table(
        {
            "branch_1": {
                "data": {
                    "metrics.json": {"data": {"a": 0, "b": {"ad": 0.0, "bc": 0.0}}}
                }
            }
        },
        all_branches=False,
    )
    assert td.as_dict() == [
        {"Path": "metrics.json", "a": "0", "b.ad": "0.0", "b.bc": "0.0"}
    ]


def test_metrics_show_with_non_dict_values():
    td = metrics_table(
        {"branch_1": {"data": {"metrics.json": {"data": 1}}}},
        all_branches=True,
    )
    assert td.as_dict() == [{"Revision": "branch_1", "Path": "metrics.json", "": "1"}]


def test_metrics_show_with_multiple_revision():
    td = metrics_table(
        {
            "branch_1": {
                "data": {"metrics.json": {"data": {"a": 1, "b": {"ad": 1, "bc": 2}}}}
            },
            "branch_2": {
                "data": {"metrics.json": {"data": {"a": 1, "b": {"ad": 3, "bc": 4}}}}
            },
        },
        all_branches=True,
    )
    assert td.as_dict() == [
        {
            "Revision": "branch_1",
            "Path": "metrics.json",
            "a": "1",
            "b.ad": "1",
            "b.bc": "2",
        },
        {
            "Revision": "branch_2",
            "Path": "metrics.json",
            "a": "1",
            "b.ad": "3",
            "b.bc": "4",
        },
    ]


def test_metrics_show_with_one_revision_multiple_paths():
    td = metrics_table(
        {
            "branch_1": {
                "data": {
                    "metrics.json": {"data": {"a": 1, "b": {"ad": 0.1, "bc": 1.03}}},
                    "metrics_1.json": {"data": {"a": 2.3, "b": {"ad": 6.5, "bc": 7.9}}},
                }
            }
        },
        all_branches=True,
    )
    assert td.as_dict() == [
        {
            "Revision": "branch_1",
            "Path": "metrics.json",
            "a": "1",
            "b.ad": "0.1",
            "b.bc": "1.03",
        },
        {
            "Revision": "branch_1",
            "Path": "metrics_1.json",
            "a": "2.3",
            "b.ad": "6.5",
            "b.bc": "7.9",
        },
    ]


def test_metrics_show_with_different_metrics_header():
    td = metrics_table(
        {
            "branch_1": {
                "data": {"metrics.json": {"data": {"b": {"ad": 1, "bc": 2}, "c": 4}}}
            },
            "branch_2": {
                "data": {"metrics.json": {"data": {"a": 1, "b": {"ad": 3, "bc": 4}}}}
            },
        },
        all_branches=True,
    )
    assert td.as_dict() == [
        {
            "Revision": "branch_1",
            "Path": "metrics.json",
            "a": "-",
            "b.ad": "1",
            "b.bc": "2",
            "c": "4",
        },
        {
            "Revision": "branch_2",
            "Path": "metrics.json",
            "a": "1",
            "b.ad": "3",
            "b.bc": "4",
            "c": "-",
        },
    ]


def test_metrics_show_precision():
    metrics = {
        "branch_1": {
            "data": {
                "metrics.json": {
                    "data": {
                        "a": 1.098765366365355,
                        "b": {"ad": 1.5342673, "bc": 2.987725527},
                    }
                }
            }
        }
    }

    td = metrics_table(metrics, all_branches=True, precision=4)
    assert td.as_dict() == [
        {
            "Revision": "branch_1",
            "Path": "metrics.json",
            "a": "1.099",
            "b.ad": "1.534",
            "b.bc": "2.988",
        }
    ]

    td = metrics_table(metrics, all_branches=True, precision=4, round_digits=True)
    assert td.as_dict() == [
        {
            "Revision": "branch_1",
            "Path": "metrics.json",
            "a": "1.0988",
            "b.ad": "1.5343",
            "b.bc": "2.9877",
        }
    ]

    td = metrics_table(metrics, all_branches=True, precision=7)
    assert td.as_dict() == [
        {
            "Revision": "branch_1",
            "Path": "metrics.json",
            "a": "1.098765",
            "b.ad": "1.534267",
            "b.bc": "2.987726",
        }
    ]


@pytest.mark.parametrize("markdown", [True, False])
def test_metrics_show_mocked(mocker, markdown):
    ret = mocker.MagicMock()
    m = mocker.patch("dvc.compare.metrics_table", return_value=ret)

    show_metrics({}, markdown=markdown)

    m.assert_called_once_with(
        {},
        all_branches=False,
        all_tags=False,
        all_commits=False,
        precision=None,
        round_digits=False,
    )
    ret.render.assert_called_once_with(markdown=markdown)


def test_metrics_show_default(capsys):
    show_metrics(
        metrics={
            "branch_1": {
                "data": {"metrics.json": {"data": {"b": {"ad": 1, "bc": 2}, "c": 4}}},
                "error": Exception("Failed just a little bit"),
            },
            "branch_2": {
                "data": {"metrics.json": {"data": {"a": 1, "b": {"ad": 3, "bc": 4}}}}
            },
        },
        all_branches=True,
    )
    out, _ = capsys.readouterr()
    assert out == textwrap.dedent(
        """\
        Revision    Path          a    b.ad    b.bc    c
        branch_1    metrics.json  -    1       2       4
        branch_2    metrics.json  1    3       4       -
        """
    )


def test_metrics_show_markdown(capsys):
    show_metrics(
        metrics={
            "branch_1": {
                "data": {"metrics.json": {"data": {"b": {"ad": 1, "bc": 2}, "c": 4}}}
            },
            "branch_2": {
                "data": {"metrics.json": {"data": {"a": 1, "b": {"ad": 3, "bc": 4}}}}
            },
            "branch_3": {"error": YAMLFileCorruptedError("failed")},
        },
        all_branches=True,
        markdown=True,
    )
    out, _ = capsys.readouterr()
    assert out == textwrap.dedent(
        """\
        | Revision   | Path         | a   | b.ad   | b.bc   | c   |
        |------------|--------------|-----|--------|--------|-----|
        | branch_1   | metrics.json | -   | 1      | 2      | 4   |
        | branch_2   | metrics.json | 1   | 3      | 4      | -   |

        """
    )




tests/unit/test_config.py
import logging
import os
import textwrap

import pytest

from dvc.config import Config, ConfigError
from dvc.fs import LocalFileSystem


@pytest.mark.parametrize(
    "path, expected",
    [
        ("cache", "../cache"),
        (os.path.join("..", "cache"), "../../cache"),
        (os.getcwd(), os.getcwd()),
        ("ssh://some/path", "ssh://some/path"),
    ],
)
def test_to_relpath(path, expected):
    assert Config._to_relpath(os.path.join(".", "config"), path) == expected


def test_get_fs(tmp_dir, scm):
    tmp_dir.scm_gen("foo", "foo", commit="add foo")

    fs = scm.get_fs("master")
    config = Config.from_cwd(fs=fs)

    assert config.fs == fs
    assert config.wfs != fs
    assert isinstance(config.wfs, LocalFileSystem)

    assert config._get_fs("repo") == fs
    assert config._get_fs("local") == config.wfs
    assert config._get_fs("global") == config.wfs
    assert config._get_fs("system") == config.wfs


def test_s3_ssl_verify(tmp_dir, dvc):
    config = Config.from_cwd(validate=False)
    with config.edit() as conf:
        conf["remote"]["remote-name"] = {"url": "s3://bucket/dvc"}

    assert "ssl_verify" not in config["remote"]["remote-name"]

    with config.edit() as conf:
        section = conf["remote"]["remote-name"]
        section["ssl_verify"] = False

    assert (tmp_dir / ".dvc" / "config").read_text() == textwrap.dedent(
        """\
        [core]
            no_scm = True
        ['remote "remote-name"']
            url = s3://bucket/dvc
            ssl_verify = False
        """
    )

    with config.edit() as conf:
        section = conf["remote"]["remote-name"]
        section["ssl_verify"] = "/path/to/custom/cabundle.pem"

    assert (tmp_dir / ".dvc" / "config").read_text() == textwrap.dedent(
        """\
        [core]
            no_scm = True
        ['remote "remote-name"']
            url = s3://bucket/dvc
            ssl_verify = /path/to/custom/cabundle.pem
        """
    )


def test_load_unicode_error(tmp_dir, dvc, mocker):
    config = Config.from_cwd(validate=False)
    mocker.patch(
        "configobj.ConfigObj",
        side_effect=UnicodeDecodeError("", b"", 0, 0, ""),
    )
    with pytest.raises(ConfigError):
        with config.edit():
            pass


def test_load_configob_error(tmp_dir, dvc, mocker):
    from configobj import ConfigObjError

    config = Config.from_cwd(validate=False)
    mocker.patch(
        "configobj.ConfigObj",
        side_effect=ConfigObjError(),
    )
    with pytest.raises(ConfigError):
        with config.edit():
            pass


def test_feature_section_supports_arbitrary_values(caplog):
    with caplog.at_level(logging.WARNING, logger="dvc.config_schema"):
        data = Config.validate(
            {
                "feature": {
                    "random_key_1": "random_value_1",
                    "random_key_2": 42,
                }
            }
        )

    assert "random_key_1" not in data
    assert "random_key_2" not in data
    assert (
        "'feature.random_key_1', 'feature.random_key_2' "
        "config options are unsupported"
    ) in caplog.text




tests/unit/test_context.py
from dataclasses import asdict
from math import pi

import pytest

from dvc.fs import LocalFileSystem
from dvc.parsing import DEFAULT_PARAMS_FILE
from dvc.parsing.context import (
    Context,
    CtxDict,
    CtxList,
    KeyNotInContext,
    MergeError,
    ParamsLoadError,
    Value,
    recurse_not_a_node,
)
from dvc.utils import relpath
from dvc.utils.serialize import dumps_yaml


def test_context():
    context = Context({"foo": "bar"})
    assert context["foo"] == Value("bar")

    context = Context(foo="bar")
    assert context["foo"] == Value("bar")

    context["foobar"] = "foobar"
    assert context["foobar"] == Value("foobar")

    del context["foobar"]
    assert "foobar" not in context
    assert "foo" in context

    with pytest.raises(KeyError):
        _ = context["foobar"]


def test_context_dict_ignores_keys_except_str():
    c = Context({"one": 1, 3: 3})
    assert "one" in c
    assert 3 not in c

    c[3] = 3
    assert 3 not in c


def test_context_list():
    lst = ["foo", "bar", "baz"]
    context = Context(lst=lst)

    assert context["lst"] == CtxList(lst)
    assert context["lst"][0] == Value("foo")
    del context["lst"][-1]

    assert "baz" not in context

    with pytest.raises(IndexError):
        _ = context["lst"][3]

    context["lst"].insert(0, "baz")
    assert context["lst"] == CtxList(["baz"] + lst[:2])


def test_context_setitem_getitem():
    context = Context()
    lst = [1, 2, "three", True, pi, b"bytes", None]
    context["list"] = lst

    assert isinstance(context["list"], CtxList)
    assert context["list"] == CtxList(lst)
    for i, val in enumerate(lst):
        assert context["list"][i] == Value(val)

    d = {
        "foo": "foo",
        "bar": "bar",
        "list": [
            {"foo0": "foo0", "bar0": "bar0"},
            {"foo1": "foo1", "bar1": "bar1"},
        ],
    }
    context["data"] = d

    assert isinstance(context["data"], CtxDict)
    assert context["data"] == CtxDict(d)
    assert context["data"]["foo"] == Value("foo")
    assert context["data"]["bar"] == Value("bar")

    assert isinstance(context["data"]["list"], CtxList)
    assert context["data"]["list"] == CtxList(d["list"])

    for i, val in enumerate(d["list"]):
        c = context["data"]["list"][i]
        assert isinstance(c, CtxDict)
        assert c == CtxDict(val)
        assert c[f"foo{i}"] == Value(f"foo{i}")
        assert c[f"bar{i}"] == Value(f"bar{i}")

    with pytest.raises(TypeError):
        context["set"] = {1, 2, 3}


def test_loop_context():
    context = Context({"foo": "foo", "bar": "bar", "lst": [1, 2, 3]})

    assert list(context) == ["foo", "bar", "lst"]
    assert len(context) == 3

    assert list(context["lst"]) == [Value(i) for i in [1, 2, 3]]
    assert len(context["lst"]) == 3

    assert list(context.items()) == [
        ("foo", Value("foo")),
        ("bar", Value("bar")),
        ("lst", CtxList([1, 2, 3])),
    ]


def test_repr():
    data = {"foo": "foo", "bar": "bar", "lst": [1, 2, 3]}
    context = Context(data)

    assert repr(context) == repr(data)
    assert str(context) == str(data)


def test_select():
    context = Context(foo="foo", bar="bar", lst=[1, 2, 3])

    assert context.select("foo") == Value("foo")
    assert context.select("bar") == Value("bar")
    assert context.select("lst") == CtxList([1, 2, 3])
    assert context.select("lst.0") == Value(1)

    with pytest.raises(KeyNotInContext):
        context.select("baz")

    d = {
        "lst": [
            {"foo0": "foo0", "bar0": "bar0"},
            {"foo1": "foo1", "bar1": "bar1"},
        ]
    }
    context = Context(d)
    assert context.select("lst") == CtxList(d["lst"])
    assert context.select("lst.0") == CtxDict(d["lst"][0])
    assert context.select("lst.1") == CtxDict(d["lst"][1])

    with pytest.raises(KeyNotInContext):
        context.select("lst.2")

    for i, _ in enumerate(d["lst"]):
        assert context.select(f"lst.{i}.foo{i}") == Value(f"foo{i}")
        assert context.select(f"lst.{i}.bar{i}") == Value(f"bar{i}")


def test_select_unwrap():
    context = Context({"dct": {"foo": "bar"}}, lst=[1, 2, 3], foo="foo")

    assert context.select("dct.foo", unwrap=True) == "bar"
    assert context.select("lst.0", unwrap=True) == 1
    assert context.select("foo", unwrap=True) == "foo"

    node = context.select("dct", unwrap=True)
    assert isinstance(node, dict)
    assert recurse_not_a_node(node)
    assert node == {"foo": "bar"}

    node = context.select("lst", unwrap=True)
    assert isinstance(node, list)
    assert recurse_not_a_node(node)
    assert node == [1, 2, 3]


def test_merge_dict():
    d1 = {"Train": {"us": {"lr": 10}}}
    d2 = {"Train": {"us": {"layers": 100}}}

    c1 = Context(d1)
    c2 = Context(d2)

    c1.merge_update(c2)
    assert c1.select("Train.us") == CtxDict(lr=10, layers=100)

    with pytest.raises(MergeError):
        # cannot overwrite by default
        c1.merge_update({"Train": {"us": {"lr": 15}}})

    c1.merge_update({"Train": {"us": {"lr": 15}}}, overwrite=True)
    node = c1.select("Train.us")
    assert node == {"lr": 15, "layers": 100}
    assert isinstance(node, CtxDict)
    assert node["lr"] == Value(15)
    assert node["layers"] == Value(100)


def test_merge_list():
    c1 = Context(lst=[1, 2, 3])
    with pytest.raises(MergeError):
        # cannot overwrite by default
        c1.merge_update({"lst": [10, 11, 12]})

    # lists are never merged
    c1.merge_update({"lst": [10, 11, 12]}, overwrite=True)
    node = c1.select("lst")
    assert node == [10, 11, 12]
    assert isinstance(node, CtxList)
    assert node[0] == Value(10)


def test_overwrite_with_setitem():
    context = Context(foo="foo", d={"bar": "bar", "baz": "baz"})
    context["d"] = "overwrite"
    assert "d" in context
    assert context["d"] == Value("overwrite")


def test_load_from(mocker):
    d = {"x": {"y": {"z": 5}, "lst": [1, 2, 3]}, "foo": "foo"}
    fs = mocker.Mock(
        open=mocker.mock_open(read_data=dumps_yaml(d)),
        **{"exists.return_value": True, "isdir.return_value": False},
    )
    file = "params.yaml"
    c = Context.load_from(fs, file)

    assert asdict(c["x"].meta) == {
        "source": file,
        "dpaths": ["x"],
        "local": False,
    }
    assert asdict(c["foo"].meta) == {
        "source": file,
        "local": False,
        "dpaths": ["foo"],
    }
    assert asdict(c["x"]["y"].meta) == {
        "source": file,
        "dpaths": ["x", "y"],
        "local": False,
    }
    assert asdict(c["x"]["y"]["z"].meta) == {
        "source": file,
        "dpaths": ["x", "y", "z"],
        "local": False,
    }
    assert asdict(c["x"]["lst"].meta) == {
        "source": file,
        "dpaths": ["x", "lst"],
        "local": False,
    }
    assert asdict(c["x"]["lst"][0].meta) == {
        "source": file,
        "dpaths": ["x", "lst", "0"],
        "local": False,
    }


def test_clone():
    d = {
        "dct": {
            "foo0": "foo0",
            "bar0": "bar0",
            "foo1": "foo1",
            "bar1": "bar1",
        },
        "lst": [1, 2, 3],
    }
    c1 = Context(d)
    c2 = Context.clone(c1)

    c2["dct"]["foo0"] = "foo"
    del c2["dct"]["foo1"]

    assert c1 != c2
    assert c1 == Context(d)
    assert c2.select("lst.0") == Value(1)
    with pytest.raises(KeyNotInContext):
        c2.select("lst.1.not_existing_key")


def test_track(tmp_dir):
    d = {
        "lst": [
            {"foo0": "foo0", "bar0": "bar0"},
            {"foo1": "foo1", "bar1": "bar1"},
        ],
        "dct": {"foo": "foo", "bar": "bar", "baz": "baz"},
    }
    fs = LocalFileSystem()
    (tmp_dir / "params.yaml").dump(d, fs=fs)

    context = Context.load_from(fs, "params.yaml")

    def key_tracked(d, key):
        assert len(d) == 1
        return key in d["params.yaml"]

    with context.track() as tracked:
        context.select("lst")
        assert key_tracked(tracked, "lst")

        context.select("dct")
        assert not key_tracked(tracked, "dct")

        context.select("dct.foo")
        assert key_tracked(tracked, "dct.foo")

        # Currently, it's unable to track dictionaries, as it can be merged
        # from multiple sources.
        context.select("lst.0")
        assert not key_tracked(tracked, "lst.0")

        # FIXME: either support tracking list values in ParamsDependency
        # or, prevent this from being tracked.
        context.select("lst.0.foo0")
        assert key_tracked(tracked, "lst.0.foo0")


def test_track_from_multiple_files(tmp_dir):
    d1 = {"Train": {"us": {"lr": 10}}}
    d2 = {"Train": {"us": {"layers": 100}}}

    fs = LocalFileSystem()
    path1 = "params.yaml"
    path2 = "params2.yaml"
    (tmp_dir / path1).dump(d1, fs=fs)
    (tmp_dir / path2).dump(d2, fs=fs)

    context = Context.load_from(fs, path1)
    c = Context.load_from(fs, path2)
    context.merge_update(c)

    def key_tracked(d, path, key):
        return key in d[relpath(path)]

    with context.track() as tracked:
        context.select("Train")
        assert not key_tracked(tracked, path1, "Train")
        assert not key_tracked(tracked, path2, "Train")

        context.select("Train.us")
        assert not key_tracked(tracked, path1, "Train.us")
        assert not key_tracked(tracked, path2, "Train.us")

        context.select("Train.us.lr")
        assert key_tracked(tracked, path1, "Train.us.lr")
        assert not key_tracked(tracked, path2, "Train.us.lr")
        context.select("Train.us.layers")
        assert not key_tracked(tracked, path1, "Train.us.layers")
        assert key_tracked(tracked, path2, "Train.us.layers")

    context = Context.clone(context)
    assert not context._tracked_data

    # let's see with an alias
    context["us"] = context["Train"]["us"]
    with context.track() as tracked:
        context.select("us")
        assert not key_tracked(tracked, path1, "Train.us")
        assert not key_tracked(tracked, path2, "Train.us")

        context.select("us.lr")
        assert key_tracked(tracked, path1, "Train.us.lr")
        assert not key_tracked(tracked, path2, "Train.us.lr")
        context.select("Train.us.layers")
        assert not key_tracked(tracked, path1, "Train.us.layers")
        assert key_tracked(tracked, path2, "Train.us.layers")


def test_node_value():
    d = {"dct": {"foo": "bar"}, "lst": [1, 2, 3], "foo": "foo"}
    context = Context(d)
    assert isinstance(context, (Context, CtxDict))
    assert isinstance(context["dct"], CtxDict)
    assert isinstance(context["lst"], CtxList)
    assert isinstance(context["foo"], Value)
    assert isinstance(context["dct"]["foo"], Value)
    assert isinstance(context["lst"][0], Value)

    assert context.value == d
    assert recurse_not_a_node(context.value)
    assert isinstance(context.value["dct"], dict)
    assert isinstance(context.value["lst"], list)
    assert isinstance(context.value["foo"], str)
    assert isinstance(context.value["dct"]["foo"], str)
    assert isinstance(context.value["lst"][0], int)

    assert isinstance(context["dct"].value, dict)
    assert context["dct"]["foo"].value == "bar"

    assert isinstance(context["lst"].value, list)
    assert context["lst"][1].value == 2

    assert context["foo"].value == "foo"


def test_resolve_resolves_dict_keys():
    d = {"dct": {"foo": "foobar", "persist": True}}

    context = Context(d)
    assert context.resolve({"${dct.foo}": {"persist": "${dct.persist}"}}) == {
        "foobar": {"persist": True}
    }


def test_resolve_resolves_boolean_value():
    d = {"enabled": True, "disabled": False}
    context = Context(d)

    assert context.resolve_str("${enabled}") is True
    assert context.resolve_str("${disabled}") is False

    assert context.resolve_str("--flag ${enabled}") == "--flag true"
    assert context.resolve_str("--flag ${disabled}") == "--flag false"


def test_load_from_raises_if_file_not_exist(tmp_dir, dvc):
    with pytest.raises(ParamsLoadError) as exc_info:
        Context.load_from(dvc.fs, DEFAULT_PARAMS_FILE)

    assert str(exc_info.value) == "'params.yaml' does not exist"


def test_load_from_raises_if_file_is_directory(tmp_dir, dvc):
    (tmp_dir / "data").mkdir()

    with pytest.raises(ParamsLoadError) as exc_info:
        Context.load_from(dvc.fs, "data")

    assert str(exc_info.value) == "'data' is a directory"




tests/unit/test_daemon.py
import inspect
import os

from dvc import daemon


def test_daemon(mocker):
    mock_windows = mocker.patch("dvc.daemon._spawn_windows")
    mock_posix = mocker.patch("dvc.daemon._spawn_posix")
    daemon.daemon(["updater"])

    if os.name == "nt":
        mock_posix.assert_not_called()
        mock_windows.assert_called()
        args = mock_windows.call_args[0]
    else:
        mock_windows.assert_not_called()
        mock_posix.assert_called()
        args = mock_posix.call_args[0]

    env = args[1]
    assert "PYTHONPATH" in env

    file_path = os.path.abspath(inspect.stack()[0][1])
    file_dir = os.path.dirname(file_path)
    test_dir = os.path.dirname(file_dir)
    dvc_dir = os.path.dirname(test_dir)
    assert env["PYTHONPATH"] == dvc_dir
    assert env[daemon.DVC_DAEMON] == "1"


def test_no_recursive_spawn(mocker):
    mocker.patch.dict(os.environ, {daemon.DVC_DAEMON: "1"})
    mock_spawn = mocker.patch("dvc.daemon._spawn")
    daemon.daemon(["updater"])
    mock_spawn.assert_not_called()




tests/unit/test_dvcfile.py
import pytest

from dvc.dvcfile import (
    LOCK_FILE,
    PROJECT_FILE,
    FileIsGitIgnored,
    ProjectFile,
    SingleStageFile,
    load_file,
)
from dvc.stage import PipelineStage
from dvc.stage.exceptions import StageFileDoesNotExistError, StageFileIsNotDvcFileError
from dvc.utils.fs import remove
from dvc.utils.serialize import EncodingError
from dvc.utils.strictyaml import YAMLValidationError


@pytest.mark.parametrize(
    "path",
    [
        "pipelines.yaml",
        "pipelines.yml",
        "custom-pipelines.yml",
        "custom-pipelines.yaml",
        "../models/pipelines.yml",
    ],
)
def test_pipelines_file(path):
    file_obj = load_file(object(), path)
    assert isinstance(file_obj, ProjectFile)


@pytest.mark.parametrize("path", ["Dvcfile", "stage.dvc", "../models/stage.dvc"])
def test_pipelines_single_stage_file(path):
    file_obj = load_file(object(), path)
    assert isinstance(file_obj, SingleStageFile)


@pytest.mark.parametrize("file", ["stage.dvc", "dvc.yaml"])
@pytest.mark.parametrize("is_dvcignored", [True, False])
def test_stage_load_on_not_existing_file(tmp_dir, dvc, file, is_dvcignored):
    dvcfile = load_file(dvc, file)
    if is_dvcignored:
        (tmp_dir / ".dvcignore").write_text(file)

    assert not dvcfile.exists()
    with pytest.raises(StageFileDoesNotExistError) as exc_info:
        assert dvcfile.stages.values()

    assert str(exc_info.value) == f"'{file}' does not exist"


@pytest.mark.parametrize("file", ["stage.dvc", "dvc.yaml"])
def test_stage_load_on_non_file(tmp_dir, dvc, file):
    (tmp_dir / file).mkdir()
    dvcfile = load_file(dvc, file)
    with pytest.raises(StageFileIsNotDvcFileError):
        assert dvcfile.stages.values()


@pytest.mark.parametrize("file", ["stage.dvc", "dvc.yaml"])
def test_stage_load_on_invalid_data(tmp_dir, dvc, file):
    data = {"is_this_a_valid_dvcfile": False}
    (tmp_dir / file).dump(data)
    dvcfile = load_file(dvc, file)
    with pytest.raises(YAMLValidationError):
        assert dvcfile.stages
    with pytest.raises(YAMLValidationError):
        assert dvcfile.validate(data, file)


def test_dump_stage(tmp_dir, dvc):
    stage = PipelineStage(dvc, cmd="command", name="stage_name", path="dvc.yaml")
    dvcfile = load_file(dvc, "dvc.yaml")

    dvcfile.dump(stage, update_lock=False, update_pipeline=False)
    assert not (tmp_dir / PROJECT_FILE).exists()
    assert not (tmp_dir / LOCK_FILE).exists()

    dvcfile.dump(stage, update_pipeline=False)
    assert not (tmp_dir / PROJECT_FILE).exists()
    assert (tmp_dir / LOCK_FILE).exists()
    assert dvcfile._lockfile.load()

    remove(tmp_dir / LOCK_FILE)

    dvcfile.dump(stage)
    assert (tmp_dir / PROJECT_FILE).exists()
    assert (tmp_dir / LOCK_FILE).exists()
    assert list(dvcfile.stages.values()) == [stage]


@pytest.mark.parametrize("file", ["stage.dvc", "dvc.yaml"])
def test_stage_load_file_exists_but_dvcignored(tmp_dir, dvc, scm, file):
    (tmp_dir / file).write_text("")
    (tmp_dir / ".dvcignore").write_text(file)

    dvc._reset()
    dvcfile = load_file(dvc, file)
    with pytest.raises(StageFileDoesNotExistError) as exc_info:
        assert dvcfile.stages.values()

    assert str(exc_info.value) == f"'{file}' is dvc-ignored"


@pytest.mark.parametrize("file", ["foo.dvc", "dvc.yaml"])
def test_try_loading_dvcfile_that_is_gitignored(tmp_dir, dvc, scm, file):
    with open(tmp_dir / ".gitignore", "a+", encoding="utf-8") as fd:
        fd.write(file)

    # create a file just to avoid other checks
    (tmp_dir / file).write_text("")
    scm._reset()

    dvcfile = load_file(dvc, file)
    with pytest.raises(FileIsGitIgnored) as exc_info:
        dvcfile._load()

    assert str(exc_info.value) == f"bad DVC file name '{file}' is git-ignored."


def test_dvcfile_encoding_error(tmp_dir, dvc):
    tmp_dir.gen(PROJECT_FILE, b"\x80some: stuff")

    dvcfile = load_file(dvc, PROJECT_FILE)
    with pytest.raises(EncodingError):
        dvcfile._load()




tests/unit/test_hashinfo.py
from dvc_data.hashfile.hash_info import HashInfo


def test_as_raw():
    hash_info = HashInfo("md5", "a1d0c6e83f027327d8461063f4ac58a6.dir", "objname")

    raw = hash_info.as_raw()

    assert hash_info.name == "md5"
    assert hash_info.value == "a1d0c6e83f027327d8461063f4ac58a6.dir"
    assert hash_info.obj_name == "objname"

    assert raw.name == "md5"
    assert raw.value == "a1d0c6e83f027327d8461063f4ac58a6"
    assert raw.obj_name == "objname"




tests/unit/test_ignore.py
import os
from unittest.mock import MagicMock, mock_open, patch

import pytest

from dvc.ignore import DvcIgnorePatterns


def mock_dvcignore(dvcignore_path, patterns):
    from dvc.fs import localfs

    fs = MagicMock()
    fs.path = localfs.path
    fs.sep = localfs.sep
    with patch.object(fs, "open", mock_open(read_data="\n".join(patterns))):
        return DvcIgnorePatterns.from_file(dvcignore_path, fs, "mocked")


@pytest.mark.parametrize(
    "file_to_ignore_relpath, patterns,  expected_match",
    [
        # all rules from https://git-scm.com/docs/gitignore
        ("to_ignore", ["to_ignore"], True),
        ("dont_ignore.txt", ["dont_ignore"], False),
        # A blank line matches no files, so it can serve as a separator for
        # readability.
        ("to_ignore", ["", "to_ignore"], True),
        # A line starting with # serves as a comment.
        # Put a backslash ("\") in front of the first hash for patterns
        # that begin with a hash.
        ("#to_ignore", ["\\#to_ignore"], True),
        ("#to_ignore", ["#to_ignore"], False),
        # Trailing spaces are ignored unless they are quoted with
        # backslash ("\").
        (" to_ignore", [" to_ignore"], False),
        (" to_ignore", ["\\ to_ignore"], True),
        # An optional prefix "!" which negates the pattern; any matching file
        # excluded by a previous pattern will become included again.
        ("to_ignore.txt", ["to_ignore*"], True),
        ("to_ignore.txt", ["to_ignore*", "!to_ignore.txt"], False),
        ("to_ignore.txt", ["!to_ignore.txt", "to_ignore*"], True),
        # It is not possible to re-include a file if a parent directory of
        # that file is excluded.
        # Git doesn't list excluded directories for performance reasons,
        # so any patterns on contained files have no effect,
        # no matter where they are defined.
        # see (`tests/func/test_ignore.py::test_ignore_parent_path`)
        # Put a backslash ("\") in front of the first "!"
        # for patterns that begin with a literal "!",
        # for example, "\!important!.txt".
        ("!to_ignore.txt", ["\\!to_ignore.txt"], True),
        # The slash / is used as the directory separator.
        # Separators may occur at the beginning, middle or end of the
        # .gitignore search pattern.
        # If there is a separator at the beginning or middle (or both)
        # of the pattern, then the pattern is relative to the directory
        # level of the particular .gitignore file itself.
        # Otherwise the pattern may also match at any level below
        # the .gitignore level.
        ("file", ["/file"], True),
        (os.path.join("data", "file"), ["/file"], False),
        (os.path.join("data", "file"), ["data/file"], True),
        (os.path.join("other", "data", "file"), ["data/file"], False),
        (
            os.path.join(
                os.path.sep,
                "full",
                "path",
                "to",
                "ignore",
                "file",
                "to_ignore",
            ),
            ["to_ignore"],
            True,
        ),
        # If there is a separator at the end of the pattern then the pattern
        # will only match directories,
        # otherwise the pattern can match both files and directories.
        # For example, a pattern doc/frotz/ matches doc/frotz directory,
        # but not a/doc/frotz directory;
        # see (`tests/func/test_ignore.py::test_ignore_sub_directory`)
        # however frotz/ matches frotz and a/frotz that is a directory
        # (all paths are relative from the .gitignore file).
        # see (`tests/func/test_ignore.py::test_ignore_directory`)
        # An asterisk "*" matches anything except a slash.
        ("to_ignore.txt", ["/*.txt"], True),
        (os.path.join("path", "to_ignore.txt"), ["/*.txt"], False),
        (os.path.join("data", "file.txt"), ["data/*"], True),
        (os.path.join("data", "subdir", "file.txt"), ["data/*"], True),
        (os.path.join("data", "file.txt"), ["data/"], True),
        (os.path.join("data", "subdir", "file.txt"), ["data/"], True),
        (os.path.join("data", "subdir", "file.txt"), ["subdir/"], True),
        (os.path.join("data", "subdir", "file.txt"), ["/subdir/"], False),
        (os.path.join("data", "path"), ["path/"], False),
        (os.path.join(".git", "file.txt"), [".git/"], True),
        (os.path.join("data", ".dvc", "file.txt"), [".dvc/"], True),
        # wait for Git
        # (os.path.join("data", "sub", "file.txt"), ["data/*"], True),
        (
            os.path.join("rel", "path", "path2", "to_ignore"),
            ["rel/*/to_ignore"],
            False,
        ),
        ("file.txt", ["file.*"], True),
        # The character "?" matches any one character except "/".
        ("file.txt", ["fi?e.t?t"], True),
        ("fi/e.txt", ["fi?e.t?t"], False),
        # The range notation, e.g. [a-zA-Z], can be used
        # to match one of the characters in a range. See fnmatch(3) and
        # the FNM_PATHNAME flag for a more detailed description.
        ("file.txt", ["[a-zA-Z]ile.txt"], True),
        ("2ile.txt", ["[a-zA-Z]ile.txt"], False),
        # Two consecutive asterisks ("**") in patterns matched against
        # full pathname may have special meaning:
        # A leading "**" followed by a slash means match in all directories.
        # For example, "**/foo" matches file or directory "foo" anywhere, the
        # same as pattern "foo".
        # "**/foo/bar" matches file or directory "bar" anywhere that is
        # directly under directory "foo".
        (os.path.join("rel", "p", "p2", "to_ignore"), ["**/to_ignore"], True),
        (
            os.path.join("rel", "p", "p2", "to_ignore"),
            ["**/p2/to_ignore"],
            True,
        ),
        (
            os.path.join("rel", "path", "path2", "dont_ignore"),
            ["**/to_ignore"],
            False,
        ),
        # A trailing "/**" matches everything inside.
        # For example, "abc/**" matches all files inside directory "abc",
        # relative to the location of the .gitignore file, with infinite depth.
        (os.path.join("rel", "p", "p2", "to_ignore"), ["rel/**"], True),
        (os.path.join("rel", "p", "p2", "to_ignore"), ["p/**"], False),
        (
            os.path.join("rel", "path", "path2", "dont_ignore"),
            ["rel/**"],
            True,
        ),
        # A slash followed by two consecutive asterisks then a slash matches
        # zero or more directories.
        # For example, "a/**/b" matches "a/b", "a/x/b", "a/x/y/b" and so on.
        (os.path.join("rel", "p", "to_ignore"), ["rel/**/to_ignore"], True),
        (
            os.path.join("rel", "p", "p2", "to_ignore"),
            ["rel/**/to_ignore"],
            True,
        ),
        (
            os.path.join("rel", "path", "path2", "dont_ignore"),
            ["rel/**/to_ignore"],
            False,
        ),
        (
            os.path.join("rel", "path", "path2", "dont_ignore"),
            ["path/**/dont_ignore"],
            False,
        ),
        # Other consecutive asterisks are considered regular asterisks
        # and will match according to the previous rules.
        ("to_ignore.txt", ["/***.txt"], True),
        (os.path.join("path", "to_ignore.txt"), ["/****.txt"], False),
        (os.path.join("path", "to_ignore.txt"), ["****.txt"], True),
        (os.path.join("data", "file.txt"), ["data/***"], True),
        # bug from PathSpec
        # (os.path.join("data", "p", "file.txt"), ["data/***"], False),
        (os.path.join("data", "p", "file.txt"), ["***/file.txt"], False),
        (
            os.path.join("rel", "path", "path2", "to_ignore"),
            ["rel/***/to_ignore"],
            False,
        ),
    ],
)
def test_match_ignore_from_file(file_to_ignore_relpath, patterns, expected_match):
    dvcignore_path = os.path.join(
        os.path.sep, "full", "path", "to", "ignore", "file", ".dvcignore"
    )
    dvcignore_dirname = os.path.dirname(dvcignore_path)

    ignore_file = mock_dvcignore(dvcignore_path, patterns)

    assert (
        ignore_file.matches(dvcignore_dirname, file_to_ignore_relpath) == expected_match
    )


@pytest.mark.parametrize("sub_dir", ["", "dir"])
@pytest.mark.parametrize("omit_dir", [".git", ".hg", ".dvc"])
def test_should_ignore_dir(omit_dir, sub_dir):
    root = os.path.join(os.path.sep, "walk", "dir", "root")
    ignore = DvcIgnorePatterns([".git/", ".hg/", ".dvc/"], root, os.sep)

    dirs = [omit_dir, "dir1", "dir2"]
    files = [omit_dir, "file1", "file2"]

    if sub_dir:
        current = os.path.join(root, sub_dir)
    else:
        current = root

    new_dirs, new_files = ignore(current, dirs, files)

    assert set(new_dirs) == {"dir1", "dir2"}
    assert set(new_files) == {"file1", "file2", omit_dir}




tests/unit/test_imports.py
import subprocess
import sys


def test_no_remote_imports():
    remote_modules = {
        "boto3",
        "botocore",
        "google.cloud.storage",
        "azure.storage.blob",
        "oss2",
        "pydrive2",
        "paramiko",
        "pyarrow",
    }

    code = "import dvc.cli, sys; print(' '.join(sys.modules))"
    res = subprocess.run(
        [sys.executable, "-c", code], stdout=subprocess.PIPE, check=True
    )
    modules = res.stdout.decode().split()
    assert not set(modules) & remote_modules




tests/unit/test_info.py
import os
import re
import shutil

import pytest

from dvc.info import SUBPROJECTS, get_dvc_info

# Python's version is in the shape of:
# <major>.<minor>.<patch>[{a|b|rc}N][.postN][.devN]
# `patch` is more than enough for the tests.
# Refer PEP-0440 for complete regex just in-case.
PYTHON_VERSION_REGEX = r"Python \d\.\d+\.\d+\S*"
DVC_VERSION_REGEX = r"\d+\.\d+\.(\d+\.)?.*"


def find_supported_remotes(string):
    lines = string.splitlines()
    index = 0

    for index, line in enumerate(lines):
        if line == "Supports:":
            index += 1
            break
    else:
        return []

    remotes = {}
    for line in lines[index:]:
        if not line.startswith("\t"):
            break

        remote_name, _, raw_dependencies = line.strip().strip(",").partition(" ")
        remotes[remote_name] = {
            dependency: version
            for dependency, _, version in [
                dependency.partition(" = ")
                for dependency in raw_dependencies[1:-1].split(", ")
            ]
        }
    return remotes


@pytest.mark.parametrize("scm_init", [True, False])
def test_info_in_repo(scm_init, tmp_dir):
    tmp_dir.init(scm=scm_init, dvc=True)
    # Create `.dvc/cache`, that is needed to check supported link types.
    os.mkdir(tmp_dir.dvc.cache.local.path)

    dvc_info = get_dvc_info()

    assert re.search(rf"DVC version: {DVC_VERSION_REGEX}", dvc_info)
    assert re.search(f"Platform: {PYTHON_VERSION_REGEX} on .*", dvc_info)
    for subproject in SUBPROJECTS:
        assert re.search(rf"{subproject} = .*", dvc_info)

    assert find_supported_remotes(dvc_info)
    assert re.search(r"Cache types: .*", dvc_info)

    if scm_init:
        assert "Repo: dvc, git" in dvc_info
    else:
        assert "Repo: dvc (no_scm)" in dvc_info


def test_info_in_subdir(tmp_dir, scm, caplog):
    dvc_subdir = tmp_dir / "subdir"
    dvc_subdir.mkdir()

    with dvc_subdir.chdir():
        dvc_subdir.init(scm=False, dvc=True)
        with dvc_subdir.dvc.config.edit() as conf:
            del conf["core"]["no_scm"]

        dvc_info = get_dvc_info()

    assert "Repo: dvc (subdir), git" in dvc_info


def test_info_in_broken_git_repo(tmp_dir, dvc, scm, caplog):
    shutil.rmtree(dvc.scm.dir)
    dvc_info = get_dvc_info()

    assert "Repo: dvc, git (broken)" in dvc_info


def test_caches(tmp_dir, dvc, caplog):
    tmp_dir.add_remote(name="sshcache", url="ssh://example.com/path", default=False)
    with tmp_dir.dvc.config.edit() as conf:
        conf["cache"]["ssh"] = "sshcache"

    dvc_info = get_dvc_info()

    # Order of cache types is runtime dependent
    assert re.search("Caches: (local, ssh|ssh, local)", dvc_info)


def test_remotes_empty(tmp_dir, dvc, caplog):
    # No remotes are configured
    dvc_info = get_dvc_info()

    assert "Remotes: None" in dvc_info


def test_remotes(tmp_dir, dvc, caplog):
    tmp_dir.add_remote(name="server", url="ssh://localhost", default=False)
    tmp_dir.add_remote(name="r1", url="azure://example.com/path", default=False)
    tmp_dir.add_remote(name="r2", url="remote://server/path", default=False)

    dvc_info = get_dvc_info()

    assert re.search("Remotes: (ssh, azure|azure, ssh)", dvc_info)


def test_fs_info_in_repo(tmp_dir, dvc, caplog):
    os.mkdir(dvc.cache.local.path)
    dvc_info = get_dvc_info()

    assert re.search(r"Cache directory: .* on .*", dvc_info)
    assert re.search(r"Workspace directory: .* on .*", dvc_info)


def test_info_outside_of_repo(tmp_dir, caplog):
    dvc_info = get_dvc_info()

    assert re.search(rf"DVC version: {DVC_VERSION_REGEX}", dvc_info)
    assert re.search(f"Platform: {PYTHON_VERSION_REGEX} on .*", dvc_info)
    assert find_supported_remotes(dvc_info)
    assert not re.search(r"Cache types: .*", dvc_info)
    assert "Repo:" not in dvc_info


def test_fs_info_outside_of_repo(tmp_dir, caplog):
    dvc_info = get_dvc_info()
    assert re.search(rf"DVC version: {DVC_VERSION_REGEX}", dvc_info)
    assert re.search(f"Platform: {PYTHON_VERSION_REGEX} on .*", dvc_info)
    assert find_supported_remotes(dvc_info)


def test_plugin_versions(tmp_dir, dvc):
    from dvc.fs import registry

    dvc_info = get_dvc_info()
    remotes = find_supported_remotes(dvc_info)

    for remote, dependencies in remotes.items():
        assert dependencies.keys() == registry[remote].REQUIRES.keys()




tests/unit/test_interpolate.py
from math import inf, pi

import pytest

from dvc.parsing.context import Context, recurse_not_a_node


@pytest.mark.parametrize(
    "template, var", [("${value}", "value"), ("${ item }", "item")]
)
@pytest.mark.parametrize(
    "data", [True, 12, pi, None, False, 0, "0", "123", "Foobar", "", inf, 3e4]
)
def test_resolve_primitive_values(data, template, var):
    context = Context({var: data})
    assert context.resolve(template) == data


@pytest.mark.parametrize(
    "template, expected",
    [
        (r"\${value}", "${value}"),
        (r"\${ value }", "${ value }"),
        (r"\${ value } days", "${ value } days"),
        (r"Month of \${value}", "Month of ${value}"),
        (r"May the \${value} be with you", "May the ${value} be with you"),
        (
            r"Great shot kid, that was \${value} in a ${value}",
            "Great shot kid, that was ${value} in a value",
        ),
    ],
)
def test_escape(template, expected):
    context = Context({"value": "value"})
    assert context.resolve(template) == expected


def test_resolve_str():
    template = "My name is ${last}, ${first} ${last}"
    expected = "My name is Bond, James Bond"
    context = Context({"first": "James", "last": "Bond"})
    assert context.resolve(template) == expected


def test_resolve_primitives_dict_access():
    data = {
        "dict": {
            "num": 5,
            "string": "foo",
            "nested": {"float": pi, "string": "bar"},
        }
    }
    context = Context(data)

    assert context.resolve("${dict.num}") == 5
    assert context.resolve("${dict.string}") == "foo"
    assert context.resolve("${dict.nested.float}") == pi
    assert context.resolve("${dict.nested.string}") == "bar"

    assert context.resolve("Number ${dict.num}") == "Number 5"


def test_resolve_primitives_list_access():
    context = Context(
        {
            "dict": [
                {"f": "f"},
                {"fo": "fo"},
                {"foo": "foo"},
                {"foo": ["f", "o", "o"]},
            ]
        }
    )

    assert context.resolve("${dict[0].f}") == "f"
    assert context.resolve("${dict[1].fo}") == "fo"
    assert context.resolve("${dict[2].foo}") == "foo"
    assert context.resolve("${dict[3].foo[0]}") == "f"

    assert context.resolve("${ dict.1.fo}${dict.3.foo.1}bar") == "foobar"


def test_resolve_collection():
    from tests.func.parsing import (
        CONTEXT_DATA,
        RESOLVED_DVC_YAML_DATA,
        TEMPLATED_DVC_YAML_DATA,
    )

    context = Context(CONTEXT_DATA)
    resolved = context.resolve(TEMPLATED_DVC_YAML_DATA)
    assert resolved == RESOLVED_DVC_YAML_DATA
    assert recurse_not_a_node(resolved)


def test_resolve_unicode():
    context = Context({"‡§®‡•á‡§™‡§æ‡§≤‡•Ä": {"‡§ö‡§ø‡§Ø‡§æ": ["‡§ö‡§ø", "‡§Ø‡§æ"]}})
    assert context.resolve_str("${‡§®‡•á‡§™‡§æ‡§≤‡•Ä.‡§ö‡§ø‡§Ø‡§æ[0]}${‡§®‡•á‡§™‡§æ‡§≤‡•Ä.‡§ö‡§ø‡§Ø‡§æ[1]}") == "‡§ö‡§ø‡§Ø‡§æ"
    assert context.resolve_str("${‡§®‡•á‡§™‡§æ‡§≤‡•Ä[‡§ö‡§ø‡§Ø‡§æ][0]}${‡§®‡•á‡§™‡§æ‡§≤‡•Ä[‡§ö‡§ø‡§Ø‡§æ][1]}") == "‡§ö‡§ø‡§Ø‡§æ"




tests/unit/test_lockfile.py
import pytest

from dvc.dvcfile import FileIsGitIgnored, Lockfile
from dvc.stage import PipelineStage
from dvc.utils.strictyaml import YAMLValidationError


def test_stage_dump_no_outs_deps(tmp_dir, dvc):
    stage = PipelineStage(name="s1", repo=dvc, path="path", cmd="command")
    lockfile = Lockfile(dvc, "path.lock")
    lockfile.dump(stage)
    assert lockfile.load() == {
        "schema": "2.0",
        "stages": {"s1": {"cmd": "command"}},
    }


def test_stage_dump_when_already_exists(tmp_dir, dvc):
    data = {"s1": {"cmd": "command", "deps": [], "outs": []}}
    (tmp_dir / "path.lock").dump({"schema": "2.0", "stages": data})
    stage = PipelineStage(name="s2", repo=dvc, path="path", cmd="command2")
    lockfile = Lockfile(dvc, "path.lock")
    lockfile.dump(stage)
    assert lockfile.load() == {
        "schema": "2.0",
        "stages": {**data, "s2": {"cmd": "command2"}},
    }


def test_stage_dump_with_deps_and_outs(tmp_dir, dvc):
    data = {
        "s1": {
            "cmd": "command",
            "deps": [{"md5": "1.txt", "path": "checksum"}],
            "outs": [{"md5": "2.txt", "path": "checksum"}],
        }
    }
    (tmp_dir / "path.lock").dump({"schema": "2.0", "stages": data})
    lockfile = Lockfile(dvc, "path.lock")
    stage = PipelineStage(name="s2", repo=dvc, path="path", cmd="command2")
    lockfile.dump(stage)
    assert lockfile.load() == {
        "schema": "2.0",
        "stages": {**data, "s2": {"cmd": "command2"}},
    }


def test_stage_overwrites_if_already_exists(tmp_dir, dvc):
    lockfile = Lockfile(dvc, "path.lock")
    stage = PipelineStage(name="s2", repo=dvc, path="path", cmd="command2")
    lockfile.dump(stage)
    stage = PipelineStage(name="s2", repo=dvc, path="path", cmd="command3")
    lockfile.dump(stage)
    assert lockfile.load() == {
        "schema": "2.0",
        "stages": {"s2": {"cmd": "command3"}},
    }


def test_load_when_lockfile_does_not_exist(tmp_dir, dvc):
    assert {} == Lockfile(dvc, "pipelines.lock").load()


@pytest.mark.parametrize(
    "corrupt_data",
    [
        {"s1": {"outs": []}},
        {"s1": {}},
        {
            "s1": {
                "cmd": "command",
                "outs": [{"md5": "checksum", "path": "path", "random": "value"}],
            }
        },
        {"s1": {"cmd": "command", "deps": [{"md5": "checksum"}]}},
    ],
)
def test_load_when_lockfile_is_corrupted(tmp_dir, dvc, corrupt_data):
    (tmp_dir / "Dvcfile.lock").dump(corrupt_data)
    lockfile = Lockfile(dvc, "Dvcfile.lock")
    with pytest.raises(YAMLValidationError) as exc_info:
        lockfile.load()
    assert "Dvcfile.lock" in str(exc_info.value)


@pytest.mark.parametrize("dvcignored", [True, False])
@pytest.mark.parametrize("file_exists", [True, False])
def test_try_loading_lockfile_that_is_gitignored(
    tmp_dir, dvc, scm, dvcignored, file_exists
):
    # it should raise error if the file is git-ignored, even if:
    #   1. The file does not exist at all.
    #   2. Or, is dvc-ignored.
    files = [".gitignore"]
    if dvcignored:
        files.append(".dvcignore")

    for file in files:
        with (tmp_dir / file).open(mode="a+") as fd:
            fd.write("dvc.lock")

    if file_exists:
        (tmp_dir / "dvc.lock").write_text("")

    scm._reset()

    with pytest.raises(FileIsGitIgnored) as exc_info:
        Lockfile(dvc, "dvc.lock").load()

    assert str(exc_info.value) == "'dvc.lock' is git-ignored."




tests/unit/test_logger.py
import logging
import time
import traceback
from datetime import datetime

import colorama
import pytest

import dvc.logger
from dvc.exceptions import DvcException

logger = logging.getLogger("dvc")
formatter = dvc.logger.ColorFormatter()
colors = {
    "blue": colorama.Fore.BLUE,
    "green": colorama.Fore.GREEN,
    "red": colorama.Fore.RED,
    "yellow": colorama.Fore.YELLOW,
    "nc": colorama.Fore.RESET,
}


@pytest.fixture
def dt(mocker):
    mocker.patch(
        "time.time", return_value=time.mktime(datetime(2020, 2, 2).timetuple())
    )
    return "2020-02-02 00:00:00,000"


class TestColorFormatter:
    def test_debug(self, caplog, dt):
        with caplog.at_level(logging.DEBUG, logger="dvc"):
            logger.debug("message")

            expected = "{blue}{datetime}{nc} {blue}DEBUG{nc}: message".format(
                **colors, datetime=dt
            )

            assert expected == formatter.format(caplog.records[0])

    def test_info(self, caplog):
        with caplog.at_level(logging.INFO, logger="dvc"):
            logger.info("message")

            assert formatter.format(caplog.records[0]) == "message"

    def test_warning(self, caplog):
        with caplog.at_level(logging.INFO, logger="dvc"):
            logger.warning("message")

            expected = "{yellow}WARNING{nc}: message".format(**colors)

            assert expected == formatter.format(caplog.records[0])

    def test_error(self, caplog):
        with caplog.at_level(logging.INFO, logger="dvc"):
            logger.error("message")

            expected = "{red}ERROR{nc}: message".format(**colors)

            assert expected == formatter.format(caplog.records[0])

    def test_exception(self, caplog):
        with caplog.at_level(logging.INFO, logger="dvc"):
            try:
                raise ValueError
            except Exception:
                logger.exception("message")

            expected = "{red}ERROR{nc}: message".format(**colors)

            assert expected == formatter.format(caplog.records[0])

    def test_exception_with_description_and_without_message(self, caplog):
        with caplog.at_level(logging.INFO, logger="dvc"):
            try:
                raise Exception("description")
            except Exception:
                logger.exception("")

            expected = "{red}ERROR{nc}: description".format(**colors)

            assert expected == formatter.format(caplog.records[0])

    def test_exception_with_description_and_message(self, caplog):
        with caplog.at_level(logging.INFO, logger="dvc"):
            try:
                raise Exception("description")
            except Exception:
                logger.exception("message")

            expected = "{red}ERROR{nc}: message - description".format(**colors)

            assert expected == formatter.format(caplog.records[0])

    def test_exception_under_verbose(self, caplog, dt):
        with caplog.at_level(logging.DEBUG, logger="dvc"):
            try:
                raise Exception("description")
            except Exception:
                stack_trace = traceback.format_exc()
                logger.exception("")

            expected = (
                "{red}{datetime}{nc} "
                "{red}ERROR{nc}: description\n"
                "{stack_trace}".format(
                    stack_trace=stack_trace,
                    **colors,
                    datetime=dt,
                )
            )

            assert expected == formatter.format(caplog.records[0])

    def test_exc_info_on_other_record_types(self, caplog, dt):
        with caplog.at_level(logging.DEBUG, logger="dvc"):
            try:
                raise Exception("description")
            except Exception:  # noqa: BLE001
                stack_trace = traceback.format_exc()
                logger.debug("", exc_info=True)

            expected = (
                "{blue}{datetime}{nc} "
                "{blue}DEBUG{nc}: description\n"
                "{stack_trace}".format(
                    stack_trace=stack_trace,
                    datetime=dt,
                    **colors,
                )
            )

            assert expected == formatter.format(caplog.records[0])

    def test_tb_only(self, caplog, dt):
        with caplog.at_level(logging.DEBUG, logger="dvc"):
            try:
                raise Exception("description")
            except Exception:
                stack_trace = traceback.format_exc()
                logger.exception("something", extra={"tb_only": True})

            expected = (
                "{red}{datetime}{nc} "
                "{red}ERROR{nc}: something\n"
                "{stack_trace}".format(
                    stack_trace=stack_trace,
                    **colors,
                    datetime=dt,
                )
            )

            assert expected == formatter.format(caplog.records[0])

    def test_nested_exceptions(self, caplog, dt):
        with caplog.at_level(logging.DEBUG, logger="dvc"):
            try:
                raise Exception("first")
            except Exception as exc:  # noqa: BLE001
                try:
                    raise DvcException("second") from exc
                except DvcException:
                    stack_trace = traceback.format_exc()
                    logger.exception("message")

            expected = (
                "{red}{datetime}{nc} "
                "{red}ERROR{nc}: message - second: first\n"
                "{stack_trace}".format(
                    stack_trace=stack_trace,
                    **colors,
                    datetime=dt,
                )
            )
            assert expected == formatter.format(caplog.records[0])
            assert "Exception: first" in stack_trace
            assert "dvc.exceptions.DvcException: second" in stack_trace

    def test_progress_awareness(self, mocker, capsys, caplog):
        from dvc.progress import Tqdm

        mocker.patch("sys.stdout.isatty", return_value=True)
        with Tqdm(total=100, desc="progress") as pbar:
            pbar.update()

            # logging an invisible message should not break
            # the progress bar output
            with caplog.at_level(logging.INFO, logger="dvc"):
                debug_record = logging.LogRecord(
                    name="dvc",
                    level=logging.DEBUG,
                    pathname=__name__,
                    lineno=1,
                    msg="debug",
                    args=(),
                    exc_info=None,
                )

                formatter.format(debug_record)
                captured = capsys.readouterr()
                assert not captured.out

            #  when the message is actually visible
            with caplog.at_level(logging.INFO, logger="dvc"):
                logger.info("some info")
                captured = capsys.readouterr()
                assert not captured.out


def test_handlers():
    out, deb, vrb, err = logger.handlers

    assert out.level == logging.INFO
    assert deb.level == logging.DEBUG
    assert vrb.level == logging.TRACE
    assert err.level == logging.WARNING


def test_logging_debug_with_datetime(caplog, dt):
    with caplog.at_level(logging.DEBUG, logger="dvc"):
        logger.warning("WARNING")
        logger.debug("DEBUG")
        logger.trace("TRACE")
        logger.error("ERROR")

        for record in caplog.records:
            assert dt in formatter.format(record)
            assert record.levelname == record.message


def test_info_with_debug_loglevel_shows_no_datetime(caplog, dt):
    with caplog.at_level(logging.DEBUG, logger="dvc"):
        logger.info("message")

        assert formatter.format(caplog.records[0]) == "message"


def test_add_existing_level(caplog, dt):
    # Common pattern to configure logging level in external libraries
    # eg:
    # https://github.com/bokeh/bokeh/blob/04bb30fef2e72e64baaa8b2f330806d5bfdd3b11/
    # bokeh/util/logconfig.py#L79-L85
    TRACE2 = 4  # noqa: N806
    logging.addLevelName(TRACE2, "TRACE2")
    logging.TRACE2 = TRACE2

    dvc.logger.add_logging_level("TRACE2", 2)

    # DVC sets all expected entrypoints, but doesn't override the level
    assert logging.TRACE2 == 4
    assert hasattr(logging, "trace2")
    assert hasattr(logger, "trace2")
    assert logging.getLevelName("TRACE2") == 4

    # The TRACE2 logging level uses the original, higher logging level
    with caplog.at_level(logging.TRACE2, logger="dvc"):
        logger.trace2("TRACE2")
    assert len(caplog.records) == 1

    (record,) = caplog.records
    assert record.levelno == 4
    assert record.levelname == "TRACE2"
    assert record.message == "TRACE2"




tests/unit/test_metrics.py
import json
import os


def test_metrics_order(tmp_dir, dvc):
    tmp_dir.gen(
        {
            "p.json": json.dumps({"p1": 1}),
            "p1.json": json.dumps({"p2": 1}),
            "sub": {
                "p3.json": json.dumps({"p3": 1}),
                "p4.json": json.dumps({"p4": 1}),
            },
        }
    )

    dvc.stage.add(
        metrics=["p.json", str(tmp_dir / "sub" / "p4.json")],
        cmd="cmd1",
        name="stage1",
    )
    with (tmp_dir / "sub").chdir():
        dvc.stage.add(
            metrics=[str(tmp_dir / "p1.json"), "p3.json"],
            cmd="cmd2",
            name="stage2",
        )

    assert list(dvc.metrics.show()[""]["data"]) == [
        "p.json",
        os.path.join("sub", "p4.json"),
        "p1.json",
        os.path.join("sub", "p3.json"),
    ]




tests/unit/test_params.py
import os

from dvc.utils.serialize import dumps_yaml


def test_params_order(tmp_dir, dvc):
    tmp_dir.gen(
        {
            "params.yaml": dumps_yaml({"p": 1}),
            "params1.yaml": dumps_yaml({"p1": 1}),
            "sub": {"params2.yaml": dumps_yaml({"p2": 1})},
        }
    )

    params_path = os.path.join("..", "params.yaml")
    p2_path = os.path.join("sub", "params2.yaml")
    dvc.stage.add(
        params=[{p2_path: ["p2"]}, {"params1.yaml": ["p1"]}],
        cmd="cmd1",
        name="stage1",
    )
    with (tmp_dir / "sub").chdir():
        dvc.stage.add(params=[{params_path: ["p"]}], cmd="cmd2", name="stage2")

    # params are sorted during dumping, therefore p1 is first
    assert list(dvc.params.show()[""]["data"]) == [
        "params1.yaml",
        p2_path,
        "params.yaml",
    ]


def test_repro_unicode(tmp_dir, dvc):
    tmp_dir.gen({"settings.json": '{"Œ©_value": 1}'})
    stage = dvc.stage.add(
        params=[{"settings.json": ["Œ©_value"]}], cmd="cmd", name="stage1"
    )
    assert dvc.reproduce(dry=True) == [stage]

    stage.cmd = "foo"
    stage.dump()

    dvc.remove(stage.name)
    assert not (tmp_dir / "dvc.yaml").exists()
    assert not (tmp_dir / "dvc.lock").exists()




tests/unit/test_pathspec_math.py
import pytest

from dvc.pathspec_math import PatternInfo, _change_dirname


@pytest.mark.parametrize(
    "patterns, dirname, changed",
    [
        # A line starting with # serves as a comment.
        ("#comment", "/dir", "#comment"),
        # Put a backslash ("\") in front of the first hash for patterns that
        # begin with a hash.
        ("\\#hash", "/dir", "dir/**/#hash"),
        ("\\#hash", "/#dir", "#dir/**/#hash"),
        # Trailing spaces are ignored unless they are quoted with
        # backslash ("\").
        (" space", "/dir", "dir/**/space"),
        ("\\ space", "/dir", "dir/**/ space"),
        # An optional prefix "!" which negates the pattern;
        ("!include", "/dir", "!/dir/**/include"),
        # Put a backslash ("\") in front of the first "!" for patterns that
        # begin with a literal "!", for example, "\!important!.txt".
        ("\\!important!.txt", "/dir", "dir/**/!important!.txt"),
        # If there is a separator at the beginning or middle (or both) of the
        # pattern, then the pattern is relative to the directory level of the
        # particular .gitignore file itself.
        ("/separator.txt", "/dir", "dir/separator.txt"),
        ("subdir/separator.txt", "/dir", "dir/subdir/separator.txt"),
        # Otherwise the pattern may also match at any level below
        # the .gitignore level.
        ("no_sep", "/dir", "dir/**/no_sep"),
        # If there is a separator at the end of the pattern then the pattern
        # will only match directories, otherwise the pattern can match both
        # files and directories.
        ("doc/fortz/", "/dir", "dir/doc/fortz/"),
        ("fortz/", "/dir", "dir/**/fortz/"),
        # An asterisk "*" matches anything except a slash.
        ("*aste*risk*", "/dir", "dir/**/*aste*risk*"),
        # The character "?" matches any one character except "/".
        ("?fi?le?", "/dir", "dir/**/?fi?le?"),
        # The range notation, e.g. [a-zA-Z], can be used to match one of the
        # characters in a range. See fnmatch(3) and the FNM_PATHNAME flag
        # for a more detailed description.
        ("[a-zA-Z]file[a-zA-Z]", "/dir", "dir/**/[a-zA-Z]file[a-zA-Z]"),
        # Two consecutive asterisks ("**") in patterns matched against full
        # pathname may have special meaning:
        # A leading "**" followed by a slash means match in all directories.
        # For example, "**/foo" matches file or directory "foo" anywhere,
        # the same as pattern "foo".
        ("**/foo", "/dir", "dir/**/foo"),
        # "**/foo/bar" matches file or directory "bar" anywhere that is
        # directly under directory "foo".
        ("**/foo/bar", "/dir", "dir/**/foo/bar"),
        # A trailing "/**" matches everything inside.
        # For example, "abc/**" matches all files inside directory "abc",
        # relative to the location of the .gitignore file, with infinite depth.
        ("abc/**", "/dir", "dir/abc/**"),
        # A slash followed by two consecutive asterisks then a slash matches
        # zero or more directories. For example, "a/**/b"
        # matches "a/b", "a/x/b", "a/x/y/b" and so on.
        ("a/**/b", "/dir", "dir/a/**/b"),
        # Other consecutive asterisks are considered regular asterisks and
        # will match according to the previous rules.
        ("/***.txt", "/dir", "dir/***.txt"),
        ("data/***", "/dir", "dir/data/***"),
        ("***/file.txt", "/dir", "dir/***/file.txt"),
        ("***file", "/dir", "dir/**/***file"),
        ("a/***/b", "/dir", "dir/a/***/b"),
    ],
)
def test_dvcignore_pattern_change_dir(tmp_dir, patterns, dirname, changed):
    assert _change_dirname(dirname, [PatternInfo(patterns, "")], "/") == [
        PatternInfo(changed, "")
    ]




tests/unit/test_progress.py
import logging

from dvc.progress import Tqdm
from dvc.utils import env2bool


def test_quiet_logging(caplog, capsys):
    with caplog.at_level(logging.CRITICAL, logger="dvc"):
        for _ in Tqdm(range(10)):
            pass
        out_err = capsys.readouterr()
        assert not out_err.out
        assert not out_err.err


def test_quiet_logging_disable_false(caplog, capsys, mocker):
    # simulate interactive terminal
    mocker.patch("sys.stdout.isatty", return_value=True)
    with caplog.at_level(logging.CRITICAL, logger="dvc"):
        for _ in Tqdm(range(10), disable=False):
            pass
        out_err = capsys.readouterr()
        assert not out_err.out
        assert not out_err.err


def test_quiet_notty(caplog, capsys):
    with caplog.at_level(logging.INFO, logger="dvc"):
        for _ in Tqdm(range(10)):
            pass
        out_err = capsys.readouterr()
        assert not out_err.out
        if env2bool("DVC_IGNORE_ISATTY"):
            assert "0/10" in out_err.err
        else:
            assert not out_err.err


def test_default(caplog, capsys, mocker):
    # simulate interactive terminal
    mocker.patch("sys.stdout.isatty", return_value=True)
    with caplog.at_level(logging.INFO, logger="dvc"):
        for _ in Tqdm(range(10)):
            pass

        out_err = capsys.readouterr()
        assert not out_err.out
        assert "0/10" in out_err.err




tests/unit/test_prompt.py
from dvc.prompt import confirm


def test_confirm_in_tty_if_stdin_is_closed(mocker):
    mock_input = mocker.patch("dvc.prompt.input", side_effect=EOFError)
    mock_isatty = mocker.patch("sys.stdout.isatty", return_value=True)
    ret = confirm("message")
    mock_isatty.assert_called()
    mock_input.assert_called()
    assert not ret




tests/unit/test_run.py
import pytest

from dvc.stage.utils import is_valid_name


@pytest.mark.parametrize("name", ["copy_name", "copy-name", "copyName", "12"])
def test_valid_stage_names(name):
    assert is_valid_name(name)


@pytest.mark.parametrize("name", ["copy$name", "copy-name?", "copy-name@v1"])
def test_invalid_stage_names(name):
    assert not is_valid_name(name)




tests/unit/test_rwlock.py
import json
import os

import pytest

from dvc.fs import localfs
from dvc.lock import LockError
from dvc.rwlock import (
    RWLockFileCorruptedError,
    RWLockFileFormatError,
    _edit_rwlock,
    rwlock,
)


def test_rwlock(tmp_path):
    path = os.fspath(tmp_path)
    foo = "foo"

    with rwlock(path, localfs, "cmd1", [foo], [], False):
        with pytest.raises(LockError):
            with rwlock(path, localfs, "cmd2", [], [foo], False):
                pass

    with rwlock(path, localfs, "cmd1", [], [foo], False):
        with pytest.raises(LockError):
            with rwlock(path, localfs, "cmd2", [foo], [], False):
                pass

    with rwlock(path, localfs, "cmd1", [], [foo], False):
        with pytest.raises(LockError):
            with rwlock(path, localfs, "cmd2", [], [foo], False):
                pass


def test_rwlock_reentrant(tmp_path):
    path = os.fspath(tmp_path)
    foo = "foo"

    with rwlock(path, localfs, "cmd1", [], [foo], False):
        with rwlock(path, localfs, "cmd1", [], [foo], False):
            pass
        with _edit_rwlock(path, localfs, False) as lock:
            assert lock == {
                "read": {},
                "write": {"foo": {"cmd": "cmd1", "pid": os.getpid()}},
            }

    with rwlock(path, localfs, "cmd", [foo], [], False):
        with rwlock(path, localfs, "cmd", [foo], [], False):
            pass
        with _edit_rwlock(path, localfs, False) as lock:
            assert lock == {
                "read": {"foo": [{"cmd": "cmd", "pid": os.getpid()}]},
                "write": {},
            }


def test_rwlock_edit_is_guarded(tmp_path, mocker):
    # patching to speedup tests
    mocker.patch("dvc.lock.DEFAULT_TIMEOUT", 0.01)

    path = os.fspath(tmp_path)

    with _edit_rwlock(path, localfs, False):
        with pytest.raises(LockError):
            with _edit_rwlock(path, localfs, False):
                pass


def test_rwlock_subdirs(tmp_path):
    path = os.fspath(tmp_path)
    foo = "foo"
    subfoo = os.path.join("foo", "subfoo")

    with rwlock(path, localfs, "cmd1", [foo], [], False):
        with pytest.raises(LockError, match=r"subfoo(.|\n)*cmd1"):
            with rwlock(path, localfs, "cmd2", [], [subfoo], False):
                pass

    with rwlock(path, localfs, "cmd1", [], [subfoo], False):
        with pytest.raises(LockError, match=r"'foo'(.|\n)*cmd1"):
            with rwlock(path, localfs, "cmd2", [foo], [], False):
                pass

    with rwlock(path, localfs, "cmd1", [], [subfoo], False):
        with pytest.raises(LockError):
            with rwlock(path, localfs, "cmd2", [], [foo], False):
                pass

    with rwlock(path, localfs, "cmd1", [subfoo], [], False):
        with rwlock(path, localfs, "cmd2", [foo], [], False):
            pass


def test_broken_rwlock(tmp_path):
    dir_path = os.fspath(tmp_path)
    path = tmp_path / "rwlock"

    path.write_text('{"broken": "format"}', encoding="utf-8")
    with pytest.raises(RWLockFileFormatError):
        with _edit_rwlock(dir_path, localfs, False):
            pass

    path.write_text("{broken json", encoding="utf-8")
    with pytest.raises(RWLockFileCorruptedError):
        with _edit_rwlock(dir_path, localfs, False):
            pass


@pytest.mark.parametrize("return_value", [True, False])
def test_corrupted_rwlock(tmp_path, mocker, return_value):
    dir_path = os.fspath(tmp_path)
    path = tmp_path / "rwlock"

    foo = "foo"
    bar = "bar"
    cmd_foo = "cmd_foo"
    cmd_bar = "cmd_bar"
    mocker.patch("psutil.pid_exists", return_value=return_value)

    corrupted_rwlock = {
        "write": {foo: {"pid": 1234, "cmd": cmd_foo}},
        "read": {
            foo: [{"pid": 5555, "cmd": cmd_foo}],
            bar: [
                {"pid": 6666, "cmd": cmd_bar},
                {"pid": 7777, "cmd": cmd_bar},
            ],
        },
    }

    path.write_text(json.dumps(corrupted_rwlock), encoding="utf-8")

    if return_value:
        with pytest.raises(LockError):
            with rwlock(dir_path, localfs, "cmd_other", [], [foo, bar], False):
                pass
    else:
        with rwlock(dir_path, localfs, "cmd_other", [], [foo, bar], False):
            pass
        assert path.read_text() == """{"read": {}}"""




tests/unit/test_scm.py
import pytest

from dvc.exceptions import DvcException
from dvc.scm import resolve_rev


def test_resolve_rev_empty_git_repo(scm):
    with pytest.raises(DvcException, match="unknown Git revision 'HEAD'"):
        resolve_rev(scm, "HEAD")




tests/unit/test_tabular_data.py
import pytest

from dvc.compare import TabularData


def test_table_empty(capsys):
    td = TabularData(["Col1", "Col2", "Col3"])
    assert dict(td.items()) == {"Col1": [], "Col2": [], "Col3": []}
    assert td.columns == [[], [], []]
    assert td.keys() == ["Col1", "Col2", "Col3"]
    assert list(td) == []
    assert td.Col1 == []
    assert td.Col2 == []
    assert td.Col3 == []

    assert td[1:] == []
    with pytest.raises(IndexError):
        _ = td[1]

    assert len(td) == 0
    assert td.shape == (3, 0)
    assert td.to_csv() == """Col1,Col2,Col3\r\n"""

    td.render()
    assert capsys.readouterr() == ("", "")

    td.render(rich_table=True)
    assert capsys.readouterr() == ("", "")

    td.render(markdown=True)
    assert capsys.readouterr() == (
        "| Col1   | Col2   | Col3   |\n|--------|--------|--------|\n\n",
        "",
    )

    td.rename("Col1", "Col11")
    assert td.keys() == ["Col11", "Col2", "Col3"]

    td.project("Col3", "Col11")
    assert td.keys() == ["Col3", "Col11"]


def test_list_operations():
    td = TabularData(["col1", "col2", "col3"])
    td.append(["1", "2", "3"])

    assert list(td) == [["1", "2", "3"]]
    td.extend((["11", "12", "13"], ["21", "22", "23"]))
    assert list(td) == [
        ["1", "2", "3"],
        ["11", "12", "13"],
        ["21", "22", "23"],
    ]
    td.insert(1, ["01", "02", "03"])
    assert list(td) == [
        ["1", "2", "3"],
        ["01", "02", "03"],
        ["11", "12", "13"],
        ["21", "22", "23"],
    ]
    assert td.shape == (3, 4)
    assert len(td) == 4
    assert td[1] == ["01", "02", "03"]
    assert td[1:] == [
        ["01", "02", "03"],
        ["11", "12", "13"],
        ["21", "22", "23"],
    ]
    assert td[::-1] == [
        ["21", "22", "23"],
        ["11", "12", "13"],
        ["01", "02", "03"],
        ["1", "2", "3"],
    ]
    del td[1]
    assert list(td) == [
        ["1", "2", "3"],
        ["11", "12", "13"],
        ["21", "22", "23"],
    ]
    assert td.shape == (3, 3)
    td[1:3] = [["51", "52", "53"], ["61", "62", "63"]]
    assert list(td) == [
        ["1", "2", "3"],
        ["51", "52", "53"],
        ["61", "62", "63"],
    ]
    td[1] = ["41", "42", "43"]
    assert td[1] == ["41", "42", "43"]

    del td[1:3]
    assert td.shape == (3, 1)

    assert td.to_csv() == "col1,col2,col3\r\n1,2,3\r\n"


def test_dict_like_interfaces():
    td = TabularData(["col-1", "col-2"])

    td.extend([["foo", "bar"], ["foobar", "foobar"]])
    assert td.keys() == ["col-1", "col-2"]
    assert dict(td.items()) == {
        "col-1": ["foo", "foobar"],
        "col-2": ["bar", "foobar"],
    }
    assert td.as_dict() == [
        {"col-1": "foo", "col-2": "bar"},
        {"col-1": "foobar", "col-2": "foobar"},
    ]
    assert td.as_dict(["col-1"]) == [{"col-1": "foo"}, {"col-1": "foobar"}]


def test_fill_value():
    td = TabularData(["col-1", "col-2", "col-3"], fill_value="?")
    td.append(["foo"])
    assert list(td) == [["foo", "?", "?"]]

    td.extend(
        [
            ["bar"],
            ["foobar", "foobar2"],
            ["f", "fo", "foo", "foob", "fooba", "foobar"],
        ]
    )
    assert list(td) == [
        ["foo", "?", "?"],
        ["bar", "?", "?"],
        ["foobar", "foobar2", "?"],
        ["f", "fo", "foo"],
    ]

    td.insert(1, ["lorem"])
    assert td[1] == ["lorem", "?", "?"]

    td[0] = ["lorem", "ipsum"]
    assert td[0] == ["lorem", "ipsum", "?"]

    td[1:2] = [["f", "fo"]]
    assert td[1:2] == [["f", "fo", "?"]]

    td.add_column("col-4")
    assert td.keys() == ["col-1", "col-2", "col-3", "col-4"]
    assert td[0][3] == "?"


def test_drop():
    td = TabularData(["col1", "col2", "col3", "other"])
    td.append(["foo", "bar", "baz", "other_val"])
    assert list(td) == [["foo", "bar", "baz", "other_val"]]
    td.drop("col2")
    assert td.keys() == ["col1", "col3", "other"]
    assert list(td) == [["foo", "baz", "other_val"]]


def test_protected():
    td = TabularData(["col1", "col2", "col3", "other"])
    td.append(["foo", "bar", "baz", "other_val"])
    td.protect("col1", "col2")

    td.drop("col1", "col2", "col3", "other")
    assert td.keys() == ["col1", "col2"]
    assert list(td) == [["foo", "bar"]]

    td.unprotect("col2")

    td.drop("col1", "col2")
    assert td.keys() == ["col1"]
    assert list(td) == [["foo"]]


def test_row_from_dict():
    td = TabularData(["col1", "col2"])
    td.row_from_dict({"col3": "value3", "col4": "value4"})
    assert td.keys() == ["col1", "col2", "col3", "col4"]
    assert dict(td.items()) == {
        "col1": [""],
        "col2": [""],
        "col3": ["value3"],
        "col4": ["value4"],
    }
    td.row_from_dict({"col3": "value3", "col5": "value5", "col6": "value6"})
    assert td.keys() == ["col1", "col2", "col3", "col4", "col5", "col6"]
    assert dict(td.items()) == {
        "col1": ["", ""],
        "col2": ["", ""],
        "col3": ["value3", "value3"],
        "col4": ["value4", ""],
        "col5": ["", "value5"],
        "col6": ["", "value6"],
    }
    assert td.shape == (6, 2)
    assert list(td) == [
        ["", "", "value3", "value4", "", ""],
        ["", "", "value3", "", "value5", "value6"],
    ]


@pytest.mark.parametrize(
    "axis,how,data,expected",
    [
        (
            "rows",
            "any",
            [["foo"], ["foo", "bar"], ["foo", "bar", "foobar"]],
            [
                ["foo", "bar", "foobar"],
            ],
        ),
        (
            "rows",
            "all",
            [["foo"], ["foo", "bar"], ["", "", ""]],
            [
                ["foo", "", ""],
                ["foo", "bar", ""],
            ],
        ),
        (
            "cols",
            "any",
            [["foo"], ["foo", "bar"], ["foo", "bar", "foobar"]],
            [["foo"], ["foo"], ["foo"]],
        ),
        (
            "cols",
            "all",
            [["foo"], ["foo", "bar"], ["", "", ""]],
            [["foo", ""], ["foo", "bar"], ["", ""]],
        ),
    ],
)
def test_dropna(axis, how, data, expected):
    td = TabularData(["col-1", "col-2", "col-3"])
    td.extend(data)
    td.dropna(axis, how)
    assert list(td) == expected


@pytest.mark.parametrize(
    "axis,expected",
    [
        ("cols", [["foo", ""], ["foo", ""], ["foo", "foobar"]]),
        ("rows", [["foo", "bar", ""], ["foo", "bar", "foobar"]]),
    ],
)
def test_dropna_subset(axis, expected):
    td = TabularData(["col-1", "col-2", "col-3"])
    td.extend([["foo"], ["foo", "bar"], ["foo", "bar", "foobar"]])
    td.dropna(axis, subset=["col-1", "col-2"])
    assert list(td) == expected


@pytest.mark.parametrize(
    "axis,expected,ignore_empty",
    [
        (
            "rows",
            [
                ["foo", "-", "-"],
                ["foo", "foo", "-"],
                ["foo", "bar", "foobar"],
            ],
            True,
        ),
        ("cols", [["-"], ["foo"], ["foo"], ["bar"]], True),
        (
            "cols",
            [
                ["-", "-"],
                ["foo", "-"],
                ["foo", "-"],
                ["bar", "foobar"],
            ],
            False,
        ),
    ],
)
def test_drop_duplicates(axis, expected, ignore_empty):
    td = TabularData(["col-1", "col-2", "col-3"], fill_value="-")
    td.extend([["foo"], ["foo", "foo"], ["foo", "foo"], ["foo", "bar", "foobar"]])

    assert list(td) == [
        ["foo", "-", "-"],
        ["foo", "foo", "-"],
        ["foo", "foo", "-"],
        ["foo", "bar", "foobar"],
    ]

    td.drop_duplicates(axis, ignore_empty=ignore_empty)

    assert list(td) == expected


def test_drop_duplicates_rich_text():
    from dvc.ui import ui

    td = TabularData(["col-1", "col-2", "col-3"], fill_value="-")

    td.extend(
        [
            ["foo", None, ui.rich_text("-")],
            ["foo", "foo"],
            ["foo", "foo"],
            ["foo", "bar", "foobar"],
        ]
    )

    assert list(td) == [
        ["foo", "-", ui.rich_text("-")],
        ["foo", "foo", "-"],
        ["foo", "foo", "-"],
        ["foo", "bar", "foobar"],
    ]

    td.drop_duplicates("cols")

    assert list(td) == [["-"], ["foo"], ["foo"], ["bar"]]


@pytest.mark.parametrize(
    "axis,subset,expected",
    [
        (
            "rows",
            ["col-1"],
            [["foo", "foo", "foo", "bar"]],
        ),
        (
            "rows",
            ["col-1", "col-3"],
            [
                ["foo", "foo", "foo", "bar"],
                ["foo", "bar", "foobar", "bar"],
            ],
        ),
        (
            "cols",
            ["col-1", "col-3"],
            [
                ["foo", "foo", "bar"],
                ["bar", "foo", "bar"],
                ["bar", "foobar", "bar"],
            ],
        ),
    ],
)
def test_drop_duplicates_subset(axis, subset, expected):
    td = TabularData(["col-1", "col-2", "col-3", "col-4"])
    td.extend(
        [
            ["foo", "foo", "foo", "bar"],
            ["foo", "bar", "foo", "bar"],
            ["foo", "bar", "foobar", "bar"],
        ]
    )
    assert list(td) == [
        ["foo", "foo", "foo", "bar"],
        ["foo", "bar", "foo", "bar"],
        ["foo", "bar", "foobar", "bar"],
    ]
    td.drop_duplicates(axis, subset=subset)
    assert list(td) == expected


def test_dropna_invalid_axis():
    td = TabularData(["col-1", "col-2", "col-3"])

    with pytest.raises(ValueError, match="Invalid 'axis' value foo."):
        td.dropna("foo")


def test_drop_duplicates_invalid_axis():
    td = TabularData(["col-1", "col-2", "col-3"])

    with pytest.raises(ValueError, match="Invalid 'axis' value foo."):
        td.drop_duplicates("foo")




tests/unit/test_updater.py
import json
import logging
import os
import time

import pytest

from dvc import __version__
from dvc.updater import Updater
from tests.func.parsing.test_errors import escape_ansi


@pytest.fixture
def tmp_global_dir(mocker, tmp_path):
    """
    Fixture to prevent modifying the actual global config
    """
    mocker.patch("dvc.config.Config.get_dir", return_value=str(tmp_path))


@pytest.fixture(autouse=True)
def mock_env(monkeypatch):
    monkeypatch.delenv("CI", raising=False)
    monkeypatch.setenv("DVC_TEST", "False")


@pytest.fixture
def updater(tmp_path, tmp_global_dir):
    return Updater(tmp_path)


@pytest.fixture
def mock_tty(mocker):
    return mocker.patch("sys.stdout.isatty", return_value=True)


def test_fetch(mocker, updater):
    mock_get = mocker.patch("requests.get")
    mock_get.return_value.status_code = 200
    mock_get.return_value.json.return_value = {"version": __version__}

    assert not os.path.exists(updater.updater_file)

    updater.fetch(detach=False)

    mock_get.assert_called_once_with(Updater.URL, timeout=Updater.TIMEOUT_GET)
    assert os.path.isfile(updater.updater_file)

    with open(updater.updater_file, encoding="utf-8") as fobj:
        info = json.load(fobj)

    assert info["version"] == __version__


@pytest.mark.parametrize(
    "config, result",
    [
        ({}, True),
        ({"check_update": "true"}, True),
        ({"check_update": "false"}, False),
    ],
)
def test_is_enabled(dvc, updater, config, result):
    with dvc.config.edit(validate=False) as conf:
        conf["core"] = config

    assert result == updater.is_enabled()


@pytest.mark.parametrize("result", [True, False])
def test_check_update_respect_config(result, updater, mocker):
    mock_check = mocker.patch("dvc.updater.Updater._check")
    mocker.patch.object(updater, "is_enabled", return_value=result)
    updater.check()
    assert result == mock_check.called


@pytest.mark.parametrize(
    "current,latest,notify",
    [
        ("0.0.2", "0.0.2", False),
        ("0.0.2", "0.0.3", True),
        ("0.0.2", "0.0.1", False),
    ],
    ids=["uptodate", "behind", "ahead"],
)
def test_check_updates(mocker, capsys, updater, current, latest, notify):
    mocker.patch("sys.stdout.isatty", return_value=True)

    updater.current = current
    with open(updater.updater_file, "w+", encoding="utf-8") as f:
        json.dump({"version": latest}, f)

    updater.check()
    out, err = capsys.readouterr()
    expected_message = (
        f"You are using dvc version {current}; "
        f"however, version {latest} is available.\n"
        if notify
        else ""
    )

    assert expected_message in escape_ansi(err)
    assert not out


def test_check_refetches_each_day(mock_tty, updater, caplog, mocker):
    updater.current = "0.0.8"
    with open(updater.updater_file, "w+", encoding="utf-8") as f:
        json.dump({"version": "0.0.9"}, f)
    fetch = mocker.patch.object(updater, "fetch")

    time_value = time.time() + 24 * 60 * 60 + 10
    mock_time = mocker.patch("time.time", return_value=time_value)

    caplog.clear()
    with caplog.at_level(logging.INFO, logger="dvc.updater"):
        updater.check()
    assert not caplog.text
    fetch.assert_called_once()
    mock_time.assert_called()


def test_check_fetches_on_invalid_data_format(mock_tty, updater, caplog, mocker):
    updater.current = "0.0.5"
    with open(updater.updater_file, "w+", encoding="utf-8") as f:
        f.write('"{"version: "0.0.6"')
    fetch = mocker.patch.object(updater, "fetch")
    caplog.clear()
    with caplog.at_level(logging.INFO, logger="dvc.updater"):
        updater.check()
    assert not caplog.text
    fetch.assert_called_once()


def test_check(mocker, updater):
    mock_check = mocker.patch("dvc.updater.Updater._check")
    updater.check()
    updater.check()
    updater.check()

    assert mock_check.call_count == 3


@pytest.mark.parametrize(
    "pkg, instruction",
    [
        ("pip", "To upgrade, run 'pip install --upgrade dvc'."),
        ("rpm", "To upgrade, run 'yum update dvc'."),
        ("brew", "To upgrade, run 'brew upgrade dvc'."),
        ("deb", "To upgrade, run 'apt-get install --only-upgrade dvc'."),
        ("conda", "To upgrade, run 'conda update dvc'."),
        ("choco", "To upgrade, run 'choco upgrade dvc'."),
        (
            "osxpkg",
            "To upgrade, uninstall dvc and reinstall from https://dvc.org.",
        ),
        (
            "exe",
            "To upgrade, uninstall dvc and reinstall from https://dvc.org.",
        ),
        (
            "binary",
            "To upgrade, uninstall dvc and reinstall from https://dvc.org.",
        ),
        (
            None,
            (
                "Find the latest release at "
                "https://github.com/iterative/dvc/releases/latest."
            ),
        ),
        (
            "unknown",
            (
                "Find the latest release at "
                "https://github.com/iterative/dvc/releases/latest."
            ),
        ),
    ],
)
def test_notify_message(updater, pkg, instruction):
    update_message = (
        "You are using dvc version 0.0.2; however, version 0.0.3 is available."
    )

    message = updater._get_message("0.0.3", current="0.0.2", pkg=pkg)
    assert message.plain.splitlines() == ["", update_message, instruction]




tests/unit/cli/__init__.py




tests/unit/cli/test_main.py
from argparse import Namespace

import pytest
from funcy import raiser

from dvc.cli import main
from dvc_data.hashfile.build import IgnoreInCollectedDirError
from dvc_data.hashfile.cache import DiskError
from dvc_objects.fs.base import FileSystem, RemoteMissingDepsError


def test_state_pickle_errors_are_correctly_raised(tmp_dir, caplog, mocker):
    path = tmp_dir / "dir" / "test"
    mocker.patch(
        "dvc.cli.parse_args",
        return_value=Namespace(
            func=raiser(DiskError(path, "md5s")),
            quiet=False,
            verbose=True,
        ),
    )

    assert main() == 255
    assert (
        "Could not open pickled 'md5s' cache.\n"
        f"Remove the '{path.relative_to(tmp_dir)}' directory "
        "and then retry this command.\n"
        "See <https://error.dvc.org/pickle> for more information." in caplog.text
    )


@pytest.mark.parametrize(
    "pkg, msg",
    [
        (None, "Please report this bug to"),
        ("pip", "pip install 'dvc[proto]'"),
        ("conda", "conda install -c conda-forge dvc-proto"),
    ],
)
def test_remote_missing_deps_are_correctly_reported(tmp_dir, caplog, mocker, pkg, msg):
    error = RemoteMissingDepsError(FileSystem(), "proto", "proto://", ["deps"])
    mocker.patch("dvc.utils.pkg.PKG", pkg)
    mocker.patch(
        "dvc.cli.parse_args",
        return_value=Namespace(
            func=raiser(error),
            quiet=False,
            verbose=True,
        ),
    )

    assert main() == 255
    expected = (
        "URL 'proto://' is supported but requires these missing dependencies: "
        "['deps']. "
    )
    if pkg:
        expected += (
            "To install dvc with those dependencies, run:\n\n"
            f"\t{msg}\n\n"
            "See <https://dvc.org/doc/install> for more info."
        )
    else:
        expected += (
            "\nPlease report this bug to "
            "<https://github.com/iterative/dvc/issues>. "
            "Thank you!"
        )
    assert expected in caplog.text


def test_ignore_in_collected_dir_error_is_logged(tmp_dir, caplog, mocker):
    error = IgnoreInCollectedDirError(".dvcignore", "dir")
    mocker.patch(
        "dvc.cli.parse_args",
        return_value=Namespace(
            func=raiser(error),
            quiet=False,
            verbose=True,
        ),
    )
    assert main() == 255
    expected = ".dvcignore file should not be in collected dir path: 'dir'"
    assert expected in caplog.text




tests/unit/command/__init__.py




tests/unit/command/test_add.py
import logging

from dvc.cli import parse_args
from dvc.commands.add import CmdAdd


def test_add(mocker, dvc):
    cli_args = parse_args(
        [
            "add",
            "--no-commit",
            "--external",
            "--glob",
            "--file",
            "file",
            "data",
        ]
    )
    assert cli_args.func == CmdAdd

    cmd = cli_args.func(cli_args)
    m = mocker.patch.object(cmd.repo, "add", autospec=True)

    assert cmd.run() == 0

    m.assert_called_once_with(
        ["data"],
        no_commit=True,
        glob=True,
        file="file",
        external=True,
        out=None,
        remote=None,
        to_remote=False,
        jobs=None,
        force=False,
    )


def test_add_to_remote(mocker):
    cli_args = parse_args(
        [
            "add",
            "s3://bucket/foo",
            "--to-remote",
            "--out",
            "bar",
            "--remote",
            "remote",
        ]
    )
    assert cli_args.func == CmdAdd

    cmd = cli_args.func(cli_args)
    m = mocker.patch.object(cmd.repo, "add", autospec=True)

    assert cmd.run() == 0

    m.assert_called_once_with(
        ["s3://bucket/foo"],
        no_commit=False,
        glob=False,
        file=None,
        external=False,
        out="bar",
        remote="remote",
        to_remote=True,
        jobs=None,
        force=False,
    )


def test_add_to_remote_invalid_combinations(mocker, caplog):
    cli_args = parse_args(["add", "s3://bucket/foo", "s3://bucket/bar", "--to-remote"])
    assert cli_args.func == CmdAdd

    cmd = cli_args.func(cli_args)
    with caplog.at_level(logging.ERROR, logger="dvc"):
        assert cmd.run() == 1
        expected_msg = "multiple targets can't be used with --to-remote"
        assert expected_msg in caplog.text

    for option, value in (("--remote", "remote"), ("--remote-jobs", "4")):
        cli_args = parse_args(["add", "foo", option, value])

        cmd = cli_args.func(cli_args)
        with caplog.at_level(logging.ERROR, logger="dvc"):
            assert cmd.run() == 1
            expected_msg = f"{option} can't be used without --to-remote"
            assert expected_msg in caplog.text


def test_add_to_cache_invalid_combinations(mocker, caplog):
    cli_args = parse_args(["add", "s3://bucket/foo", "s3://bucket/bar", "-o", "foo"])
    assert cli_args.func == CmdAdd

    cmd = cli_args.func(cli_args)
    with caplog.at_level(logging.ERROR, logger="dvc"):
        assert cmd.run() == 1
        expected_msg = "multiple targets can't be used with -o"
        assert expected_msg in caplog.text




tests/unit/command/test_cache.py
import os
import textwrap

from dvc.cli import main


def test_cache_dir_local(tmp_dir, dvc, capsys, caplog):
    (tmp_dir / ".dvc" / "config.local").write_text(
        textwrap.dedent(
            """\
            [cache]
                dir = some/path
            """
        )
    )
    path = os.path.join(dvc.dvc_dir, "some", "path")

    assert main(["cache", "dir", "--local"]) == 0

    out, _ = capsys.readouterr()
    assert path in out

    assert main(["cache", "dir"]) == 0
    out, _ = capsys.readouterr()
    assert path in out

    assert main(["cache", "dir", "--project"]) == 251
    assert "option 'dir' doesn't exist in section 'cache'" in caplog.text




tests/unit/command/test_checkout.py
from dvc.cli import parse_args
from dvc.commands.checkout import CmdCheckout, log_changes


def test_checkout(tmp_dir, dvc, mocker):
    cli_args = parse_args(["checkout", "foo.dvc", "bar.dvc", "--relink", "--with-deps"])
    assert cli_args.func == CmdCheckout

    cmd = cli_args.func(cli_args)
    m = mocker.patch("dvc.repo.Repo.checkout")

    assert cmd.run() == 0
    m.assert_called_once_with(
        targets=["foo.dvc", "bar.dvc"],
        force=False,
        recursive=False,
        relink=True,
        with_deps=True,
    )


def test_log_changes(capsys):
    stats = {
        "added": ["file1", "dir1/"],
        "deleted": ["dir2/"],
        "modified": ["file2"],
    }

    from itertools import zip_longest

    def _assert_output(stats, expected_outs):
        log_changes(stats)
        out, _ = capsys.readouterr()
        actual_output = out.splitlines()
        for out, line in zip_longest(expected_outs, actual_output):
            assert out.expandtabs() in line

    _assert_output(stats, ["M\tfile2", "A\tfile1", "A\tdir1/", "D\tdir2/"])

    del stats["deleted"][0]
    _assert_output(stats, ["M\tfile2", "A\tfile1", "A\tdir1/"])

    del stats["modified"]
    _assert_output(stats, ["A\tfile1", "A\tdir1/"])




tests/unit/command/test_compat_flag.py
from itertools import takewhile

import pytest

from dvc.cli import parse_args


def _id_gen(val) -> str:
    if isinstance(val, list):
        return "-".join(takewhile(lambda v: not v.startswith("-"), val))
    return str(val)


@pytest.mark.parametrize(
    "args, key",
    [
        (["exp", "list", "--names-only"], "name_only"),
        (["stage", "list", "--names-only"], "name_only"),
    ],
    ids=_id_gen,
)
def test_backward_compat_flags(args, key):
    """Test support for flags kept for backward compatibility."""
    cli_args = parse_args(args)
    d = vars(cli_args)
    assert d[key] is True




tests/unit/command/test_config.py
import pytest

from dvc.cli import DvcParserError, parse_args
from dvc.commands.config import CmdConfig


def test_config_formatter():
    example_config = {
        "section_foo": {"option_bar": True, "option_baz": False},
        "section_foo2": {
            "option_bar2": {"option_baz2": True},
            "option_baz3": {"option_baz4": False},
        },
        "section_foo3": {},
    }

    config_lines = tuple(CmdConfig._format_config(example_config))
    assert config_lines == (
        "section_foo.option_bar=True",
        "section_foo.option_baz=False",
        "section_foo2.option_bar2.option_baz2=True",
        "section_foo2.option_baz3.option_baz4=False",
    )


@pytest.mark.parametrize("name", ["way.too.long", "no_option", "remote.way.too.long"])
def test_config_bad_name(name):
    with pytest.raises(DvcParserError):
        parse_args(["config", name])




tests/unit/command/test_dag.py
import networkx as nx
import pytest

from dvc.cli import main, parse_args
from dvc.commands.dag import CmdDAG, _build, _show_ascii, _show_dot, _show_mermaid


@pytest.mark.parametrize(
    "fmt, formatter",
    [
        (None, "_show_ascii"),
        ("--dot", "_show_dot"),
        ("--mermaid", "_show_mermaid"),
        ("--md", "_show_mermaid"),
    ],
)
def test_dag(tmp_dir, dvc, mocker, fmt, formatter):
    from dvc.commands import dag

    tmp_dir.dvc_gen("foo", "foo")

    args = ["dag", "--full", "foo.dvc"]
    if fmt:
        args.append(fmt)
    cli_args = parse_args(args)
    assert cli_args.func == CmdDAG

    fmt_func = mocker.spy(dag, formatter)

    cmd = cli_args.func(cli_args)

    mocker.patch("dvc.commands.dag._build", return_value=dvc.index.graph)

    assert cmd.run() == 0

    assert fmt_func.called


@pytest.fixture
def repo(tmp_dir, dvc):
    tmp_dir.dvc_gen("a", "a")
    tmp_dir.dvc_gen("b", "b")

    dvc.run(no_exec=True, deps=["a", "c"], outs=["d", "e"], cmd="cmd1", name="1")
    dvc.run(no_exec=True, deps=["b", "c"], outs=["f", "g"], cmd="cmd2", name="2")
    dvc.run(
        no_exec=True,
        deps=["a", "b", "c"],
        outs=["h", "i"],
        cmd="cmd3",
        name="3",
    )
    dvc.run(no_exec=True, deps=["a", "h"], outs=["j"], cmd="cmd4", name="4")

    return dvc


def test_build(repo):
    assert nx.is_isomorphic(_build(repo), repo.index.graph)


def test_build_target(repo):
    graph = _build(repo, target="3")
    assert set(graph.nodes()) == {"3", "b.dvc", "a.dvc"}
    assert set(graph.edges()) == {("3", "a.dvc"), ("3", "b.dvc")}


def test_build_target_with_outs(repo):
    graph = _build(repo, target="3", outs=True)
    assert set(graph.nodes()) == {"a", "b", "h", "i"}
    assert set(graph.edges()) == {("i", "a"), ("i", "b"), ("h", "a"), ("h", "b")}


def test_build_granular_target_with_outs(repo):
    graph = _build(repo, target="h", outs=True)
    assert set(graph.nodes()) == {"a", "b", "h"}
    assert set(graph.edges()) == {("h", "a"), ("h", "b")}


def test_build_full(repo):
    graph = _build(repo, target="3", full=True)
    assert nx.is_isomorphic(graph, repo.index.graph)


# NOTE: granular or not, full outs DAG should be the same
@pytest.mark.parametrize("granular", [True, False])
def test_build_full_outs(repo, granular):
    target = "h" if granular else "3"
    graph = _build(repo, target=target, outs=True, full=True)
    assert set(graph.nodes()) == {"j", "i", "d", "b", "g", "f", "e", "a", "h"}
    assert set(graph.edges()) == {
        ("d", "a"),
        ("e", "a"),
        ("f", "b"),
        ("g", "b"),
        ("h", "a"),
        ("h", "b"),
        ("i", "a"),
        ("i", "b"),
        ("j", "a"),
        ("j", "h"),
    }


def test_show_ascii(repo):
    assert [line.rstrip() for line in _show_ascii(repo.index.graph).splitlines()] == [
        "                        +----------------+                          +----------------+",  # noqa: E501
        "                        | stage: 'a.dvc' |                          | stage: 'b.dvc' |",  # noqa: E501
        "                       *+----------------+****                      +----------------+",  # noqa: E501
        "                  *****           *           *****                  ***           ***",  # noqa: E501
        "              ****                *                *****           **                 **",  # noqa: E501
        "           ***                     *                    ***      **                     **",  # noqa: E501
        "+------------+                     **                   +------------+              +------------+",  # noqa: E501
        "| stage: '1' |                       **                 | stage: '3' |              | stage: '2' |",  # noqa: E501
        "+------------+                         ***              +------------+              +------------+",  # noqa: E501
        "                                          **           ***",
        "                                            **       **",
        "                                              **   **",
        "                                          +------------+",
        "                                          | stage: '4' |",
        "                                          +------------+",
    ]


def test_show_dot(repo):
    # dot file rendering is not deterministic though graph
    # output doesn't depend upon order of lines. Use sorted values
    # https://github.com/iterative/dvc/pull/7725
    expected = [
        "\"stage: '1'\";",
        "\"stage: '2'\";",
        "\"stage: '3'\" -> \"stage: '4'\";",
        "\"stage: '3'\";",
        "\"stage: '4'\";",
        "\"stage: 'a.dvc'\" -> \"stage: '1'\";",
        "\"stage: 'a.dvc'\" -> \"stage: '3'\";",
        "\"stage: 'a.dvc'\" -> \"stage: '4'\";",
        "\"stage: 'a.dvc'\";",
        "\"stage: 'b.dvc'\" -> \"stage: '2'\";",
        "\"stage: 'b.dvc'\" -> \"stage: '3'\";",
        "\"stage: 'b.dvc'\";",
        "strict digraph  {",
        "}",
    ]
    actual = sorted(line.rstrip() for line in _show_dot(repo.index.graph).splitlines())
    assert actual == expected


def test_show_dot_properly_escapes():
    graph = nx.DiGraph(
        [
            ("evaluate", "trainüöÑ"),  # emoji
            ("evaluate", "featurize"),
            ("featurize", "prepare:1"),  # colon
            ("prepare:1", "data/raw/1.dvc"),  # posix path
            ("prepare:1", "data\\raw\\2.dvc"),  # windows path
            ("prepare", "4"),  # just a number
        ]
    )

    expected = {
        "strict digraph  {",
        '"data\\raw\\2.dvc";',
        '"prepare";',
        '"4";',
        '"data/raw/1.dvc";',
        '"trainüöÑ";',
        '"evaluate";',
        '"prepare:1";',
        '"featurize";',
        '"data\\raw\\2.dvc" -> "prepare:1";',
        '"4" -> "prepare";',
        '"data/raw/1.dvc" -> "prepare:1";',
        '"trainüöÑ" -> "evaluate";',
        '"prepare:1" -> "featurize";',
        '"featurize" -> "evaluate";',
        "}",
    }
    actual = {line.rstrip() for line in _show_dot(graph).splitlines()}
    assert actual == expected


def test_show_mermaid(repo):
    assert [line.rstrip() for line in _show_mermaid(repo.index.graph).splitlines()] == [
        "flowchart TD",
        "\tnode1[\"stage: '1'\"]",
        "\tnode2[\"stage: '2'\"]",
        "\tnode3[\"stage: '3'\"]",
        "\tnode4[\"stage: '4'\"]",
        "\tnode5[\"stage: 'a.dvc'\"]",
        "\tnode6[\"stage: 'b.dvc'\"]",
        "\tnode3-->node4",
        "\tnode5-->node1",
        "\tnode5-->node3",
        "\tnode5-->node4",
        "\tnode6-->node2",
        "\tnode6-->node3",
    ]


def test_show_mermaid_markdown(repo, dvc, capsys, mocker):
    mocker.patch("dvc.commands.dag._build", return_value=dvc.index.graph)

    capsys.readouterr()
    assert main(["dag", "--md"]) == 0
    assert [line.rstrip() for line in capsys.readouterr().out.splitlines()] == [
        "```mermaid",
        "flowchart TD",
        "\tnode1[\"stage: '1'\"]",
        "\tnode2[\"stage: '2'\"]",
        "\tnode3[\"stage: '3'\"]",
        "\tnode4[\"stage: '4'\"]",
        "\tnode5[\"stage: 'a.dvc'\"]",
        "\tnode6[\"stage: 'b.dvc'\"]",
        "\tnode3-->node4",
        "\tnode5-->node1",
        "\tnode5-->node3",
        "\tnode5-->node4",
        "\tnode6-->node2",
        "\tnode6-->node3",
        "```",
    ]




tests/unit/command/test_data_status.py
import json

import pytest
from funcy import omit

from dvc.cli import main, parse_args
from dvc.commands.data import CmdDataStatus
from dvc.repo import Repo
from dvc.repo.data import Status
from tests.func.parsing.test_errors import escape_ansi


@pytest.fixture
def mocked_status():
    return Status(
        not_in_cache=["notincache"],
        committed={
            "added": ["dir/bar", "dir/foo"],
            "deleted": ["dir/baz"],
            "modified": ["dir/foobar"],
            "unknown": ["dir/unknown1"],
        },
        uncommitted={
            "added": ["dir/baz"],
            "modified": ["dir/bar"],
            "deleted": ["dir/foobar"],
            "unknown": ["dir2/unknown2"],
        },
        untracked=["untracked"],
        unchanged=["dir/foo"],
        git={"is_dirty": True, "is_empty": False},
    )


def test_cli(dvc, mocker, mocked_status):
    status = mocker.patch("dvc.repo.Repo.data_status", return_value=mocked_status)

    cli_args = parse_args(
        [
            "data",
            "status",
            "--json",
            "--unchanged",
            "--untracked-files",
            "--granular",
        ]
    )

    assert cli_args.func == CmdDataStatus
    cmd = cli_args.func(cli_args)
    assert cmd.run() == 0
    status.assert_called_once_with(
        untracked_files="all",
        not_in_remote=False,
        remote_refresh=True,
        granular=True,
    )


@pytest.mark.parametrize(
    "args, to_omit",
    [
        ([], ["untracked", "unchanged"]),
        (["--unchanged"], ["untracked"]),
        (["--unchanged", "--untracked-files"], []),
    ],
)
def test_json(dvc, mocker, capsys, mocked_status, args, to_omit):
    mocker.patch("dvc.repo.Repo.data_status", return_value=mocked_status)
    assert main(["data", "status", "--json", *args]) == 0
    out, err = capsys.readouterr()
    assert out.rstrip() == json.dumps(omit(mocked_status, [*to_omit, "git"]))
    assert not err


def test_no_changes_repo(dvc, scm, capsys):
    assert main(["data", "status"]) == 0
    assert capsys.readouterr() == ("No changes.\n", "")


def test_empty_scm_repo(tmp_dir, capsys):
    tmp_dir.init(scm=True)
    Repo.init()

    assert main(["data", "status"]) == 0
    out, err = capsys.readouterr()
    assert (
        out
        == """\
No changes in an empty git repo.
(there are changes not tracked by dvc, use "git status" to see)
"""
    )
    assert not err


@pytest.mark.parametrize(
    "args",
    [
        ("--untracked-files",),
        ("--unchanged",),
        ("--untracked-files", "--unchanged"),
    ],
)
@pytest.mark.parametrize("is_dirty", [True, False])
def test_show_status(dvc, scm, mocker, capsys, mocked_status, args, is_dirty):
    mocked_status["git"]["is_dirty"] = is_dirty
    mocker.patch("dvc.repo.Repo.data_status", return_value=mocked_status)
    assert main(["data", "status", *args]) == 0
    out, err = capsys.readouterr()
    expected_out = """\
Not in cache:
  (use "dvc fetch <file>..." to download files)
        notincache

DVC committed changes:
  (git commit the corresponding dvc files to update the repo)
        added: dir/bar
        added: dir/foo
        deleted: dir/baz
        modified: dir/foobar
        unknown: dir/unknown1

DVC uncommitted changes:
  (use "dvc commit <file>..." to track changes)
  (use "dvc checkout <file>..." to discard changes)
        added: dir/baz
        modified: dir/bar
        deleted: dir/foobar
        unknown: dir2/unknown2
"""
    if "--untracked-files" in args:
        expected_out += """
Untracked files:
  (use "git add <file> ..." or "dvc add <file>..." to commit to git or to dvc)
        untracked
"""
    if "--unchanged" in args:
        expected_out += """
DVC unchanged files:
        dir/foo
"""

    if is_dirty:
        expected_out += """\
(there are other changes not tracked by dvc, use "git status" to see)
"""
    assert escape_ansi(out) == expected_out
    assert not err




tests/unit/command/test_data_sync.py
from dvc.cli import parse_args
from dvc.commands.data_sync import CmdDataFetch, CmdDataPull, CmdDataPush


def test_fetch(mocker):
    cli_args = parse_args(
        [
            "fetch",
            "target1",
            "target2",
            "--jobs",
            "2",
            "--remote",
            "remote",
            "--all-branches",
            "--all-tags",
            "--all-commits",
            "--with-deps",
            "--recursive",
            "--run-cache",
        ]
    )
    assert cli_args.func == CmdDataFetch

    cmd = cli_args.func(cli_args)
    m = mocker.patch.object(cmd.repo, "fetch", autospec=True, return_value=0)

    assert cmd.run() == 0

    m.assert_called_once_with(
        targets=["target1", "target2"],
        jobs=2,
        remote="remote",
        all_branches=True,
        all_tags=True,
        all_commits=True,
        with_deps=True,
        recursive=True,
        run_cache=True,
    )


def test_pull(mocker):
    cli_args = parse_args(
        [
            "pull",
            "target1",
            "target2",
            "--jobs",
            "2",
            "--remote",
            "remote",
            "--all-branches",
            "--all-tags",
            "--all-commits",
            "--with-deps",
            "--force",
            "--recursive",
            "--run-cache",
            "--glob",
            "--allow-missing",
        ]
    )
    assert cli_args.func == CmdDataPull

    cmd = cli_args.func(cli_args)
    m = mocker.patch.object(cmd.repo, "pull", autospec=True)

    assert cmd.run() == 0

    m.assert_called_once_with(
        targets=["target1", "target2"],
        jobs=2,
        remote="remote",
        all_branches=True,
        all_tags=True,
        all_commits=True,
        with_deps=True,
        force=True,
        recursive=True,
        run_cache=True,
        glob=True,
        allow_missing=True,
    )


def test_push(mocker):
    cli_args = parse_args(
        [
            "push",
            "target1",
            "target2",
            "--jobs",
            "2",
            "--remote",
            "remote",
            "--all-branches",
            "--all-tags",
            "--all-commits",
            "--with-deps",
            "--recursive",
            "--run-cache",
            "--glob",
        ]
    )
    assert cli_args.func == CmdDataPush

    cmd = cli_args.func(cli_args)
    m = mocker.patch.object(cmd.repo, "push", autospec=True, return_value=0)

    assert cmd.run() == 0

    m.assert_called_once_with(
        targets=["target1", "target2"],
        jobs=2,
        remote="remote",
        all_branches=True,
        all_tags=True,
        all_commits=True,
        with_deps=True,
        recursive=True,
        run_cache=True,
        glob=True,
    )




tests/unit/command/test_diff.py
import collections
import os

import pytest

from dvc.cli import parse_args
from dvc.commands.diff import _digest, _show_markdown


@pytest.mark.parametrize(
    "checksum, expected",
    [
        ("wxyz1234pq", "wxyz1234"),
        ({"old": "1234567890", "new": "0987654321"}, "12345678..09876543"),
    ],
    ids=["str", "dict"],
)
def test_digest(checksum, expected):
    assert expected == _digest(checksum)


def test_default(mocker, capsys):
    args = parse_args(["diff"])
    cmd = args.func(args)
    diff = {
        "added": [{"path": "file", "hash": "00000000"}],
        "deleted": [],
        "modified": [],
        "renamed": [
            {
                "path": {
                    "old": os.path.join("data", "file_old"),
                    "new": os.path.join("data", "file_new"),
                },
                "hash": "11111111",
            }
        ],
        "not in cache": [],
    }
    mocker.patch("dvc.repo.Repo.diff", return_value=diff)

    assert cmd.run() == 0
    assert (
        "Added:\n"
        "    file\n"
        "\n"
        "Renamed:\n"
        "    data{sep}file_old -> data{sep}file_new\n"
        "\n"
        "files summary: 1 added, 1 renamed"
    ).format(sep=os.path.sep) in capsys.readouterr()[0]


def test_show_hash(mocker, capsys):
    args = parse_args(["diff", "--show-hash"])
    cmd = args.func(args)
    diff = {
        "added": [],
        "deleted": [
            {"path": os.path.join("data", ""), "hash": "XXXXXXXX.dir"},
            {"path": os.path.join("data", "foo"), "hash": "11111111"},
            {"path": os.path.join("data", "bar"), "hash": "00000000"},
        ],
        "modified": [
            {"path": "file2", "hash": {"old": "AAAAAAAA", "new": "BBBBBBBB"}},
            {"path": "file1", "hash": {"old": "CCCCCCCC", "new": "DDDDDDDD"}},
        ],
        "renamed": [
            {
                "path": {
                    "old": os.path.join("data", "file_old"),
                    "new": os.path.join("data", "file_new"),
                },
                "hash": "11111111",
            }
        ],
        "not in cache": [],
    }
    mocker.patch("dvc.repo.Repo.diff", return_value=diff)
    assert cmd.run() == 0

    out, _ = capsys.readouterr()
    assert (
        "Deleted:\n    XXXXXXXX  "
        + os.path.join("data", "")
        + "\n    00000000  "
        + os.path.join("data", "bar")
        + "\n    11111111  "
        + os.path.join("data", "foo")
        + "\n\nRenamed:\n    11111111  "
        + os.path.join("data", "file_old")
        + " -> "
        + os.path.join("data", "file_new")
        + "\n"
        "\n"
        "Modified:\n"
        "    CCCCCCCC..DDDDDDDD  file1\n"
        "    AAAAAAAA..BBBBBBBB  file2\n"
        "\n"
        "files summary: 2 deleted, 1 renamed, 2 modified"
    ) in out


def test_show_json(mocker, capsys):
    args = parse_args(["diff", "--json"])
    cmd = args.func(args)
    diff = {
        "added": [
            {"path": "file2", "hash": "22222222"},
            {"path": "file1", "hash": "11111111"},
        ],
        "deleted": [],
        "modified": [],
        "not in cache": [],
    }
    mocker.patch("dvc.repo.Repo.diff", return_value=diff)

    assert cmd.run() == 0
    out, _ = capsys.readouterr()
    assert '"added": [{"path": "file1"}, {"path": "file2"}]' in out
    assert '"deleted": []' in out
    assert '"modified": []' in out
    assert '"not in cache": []' in out


def test_show_json_and_hash(mocker, capsys):
    args = parse_args(["diff", "--json", "--show-hash"])
    cmd = args.func(args)

    diff = {
        "added": [
            # py35: maintain a consistent key order for tests purposes
            collections.OrderedDict([("path", "file2"), ("hash", "22222222")]),
            collections.OrderedDict([("path", "file1"), ("hash", "11111111")]),
        ],
        "deleted": [],
        "modified": [],
        "renamed": [
            {
                "path": {"old": "file_old", "new": "file_new"},
                "hash": "11111111",
            }
        ],
        "not in cache": [],
    }
    mocker.patch("dvc.repo.Repo.diff", return_value=diff)

    assert cmd.run() == 0
    out, _ = capsys.readouterr()
    assert (
        '"added": [{"path": "file1", "hash": "11111111"}, '
        '{"path": "file2", "hash": "22222222"}]' in out
    )
    assert '"deleted": []' in out
    assert '"modified": []' in out
    assert (
        '"renamed": [{"path": {"old": "file_old", '
        '"new": "file_new"}, "hash": "11111111"}]' in out
    )
    assert '"not in cache": []' in out


def test_show_json_hide_missing(mocker, capsys):
    args = parse_args(["diff", "--json", "--hide-missing"])
    cmd = args.func(args)
    diff = {
        "added": [
            {"path": "file2", "hash": "22222222"},
            {"path": "file1", "hash": "11111111"},
        ],
        "deleted": [],
        "modified": [],
        "renamed": [
            {
                "path": {"old": "file_old", "new": "file_new"},
                "hash": "11111111",
            }
        ],
        "not in cache": [],
    }
    mocker.patch("dvc.repo.Repo.diff", return_value=diff)

    assert cmd.run() == 0
    out, _ = capsys.readouterr()
    assert '"added": [{"path": "file1"}, {"path": "file2"}]' in out
    assert '"deleted": []' in out
    assert '"renamed": [{"path": {"old": "file_old", "new": "file_new"}' in out
    assert '"modified": []' in out
    assert '"not in cache": []' not in out


@pytest.mark.parametrize("show_hash", [None, True, False])
def test_diff_show_markdown_and_hash(mocker, show_hash):
    options = ["diff", "--md"] + (["--show-hash"] if show_hash else [])
    args = parse_args(options)
    cmd = args.func(args)

    diff = {}
    show_hash = show_hash if show_hash else False
    mock_show_markdown = mocker.patch("dvc.commands.diff._show_markdown")
    mocker.patch("dvc.repo.Repo.diff", return_value=diff.copy())

    assert cmd.run() == 0
    mock_show_markdown.assert_called_once_with(diff, show_hash, False)


@pytest.mark.parametrize(
    "opts",
    (
        [],
        ["a_rev", "b_rev"],
        ["--targets", "."],
        ["--hide-missing"],
    ),
)
@pytest.mark.parametrize(
    "show, expected",
    (
        ([], ""),
        (["--json"], "{}"),
        (["--md"], "| Status   | Path   |\n|----------|--------|"),
    ),
)
def test_no_changes(mocker, capsys, opts, show, expected):
    args = parse_args(["diff", *opts, *show])
    cmd = args.func(args)
    mocker.patch("dvc.repo.Repo.diff", return_value={})

    assert cmd.run() == 0
    out, _ = capsys.readouterr()
    assert expected == out.strip()


def test_show_markdown(capsys):
    diff = {
        "deleted": [
            {"path": "zoo"},
            {"path": os.path.join("data", "")},
            {"path": os.path.join("data", "foo")},
            {"path": os.path.join("data", "bar")},
        ],
        "modified": [{"path": "file"}],
        "added": [{"path": "file"}],
        "renamed": [{"path": {"old": "file_old", "new": "file_new"}}],
        "not in cache": [{"path": "file2"}],
    }

    _show_markdown(diff)
    out, _ = capsys.readouterr()
    assert out == (
        "| Status       | Path                 |\n"
        "|--------------|----------------------|\n"
        "| added        | file                 |\n"
        "| deleted      | zoo                  |\n"
        "| deleted      | data{sep}                |\n"
        "| deleted      | data{sep}foo             |\n"
        "| deleted      | data{sep}bar             |\n"
        "| renamed      | file_old -> file_new |\n"
        "| modified     | file                 |\n"
        "| not in cache | file2                |\n"
        "\n"
    ).format(sep=os.path.sep)


def test_show_markdown_with_hash(capsys):
    diff = {
        "deleted": [
            {"path": "zoo", "hash": "22222"},
            {"path": os.path.join("data", ""), "hash": "XXXXXXXX.dir"},
            {"path": os.path.join("data", "foo"), "hash": "11111111"},
            {"path": os.path.join("data", "bar"), "hash": "00000000"},
        ],
        "modified": [{"path": "file", "hash": {"old": "AAAAAAAA", "new": "BBBBBBBB"}}],
        "added": [{"path": "file", "hash": "00000000"}],
        "renamed": [
            {
                "path": {"old": "file_old", "new": "file_new"},
                "hash": "11111111",
            }
        ],
        "not in cache": [{"path": "file2", "hash": "12345678"}],
    }

    _show_markdown(diff, show_hash=True)

    out, _ = capsys.readouterr()
    assert out == (
        "| Status       | Hash               | Path                 |\n"
        "|--------------|--------------------|----------------------|\n"
        "| added        | 00000000           | file                 |\n"
        "| deleted      | 22222              | zoo                  |\n"
        "| deleted      | XXXXXXXX           | data{sep}                |\n"
        "| deleted      | 11111111           | data{sep}foo             |\n"
        "| deleted      | 00000000           | data{sep}bar             |\n"
        "| renamed      | 11111111           | file_old -> file_new |\n"
        "| modified     | AAAAAAAA..BBBBBBBB | file                 |\n"
        "| not in cache | 12345678           | file2                |\n"
        "\n"
    ).format(sep=os.path.sep)


def test_show_markdown_hide_missing(capsys):
    diff = {
        "deleted": [
            {"path": "zoo"},
            {"path": os.path.join("data", "")},
            {"path": os.path.join("data", "foo")},
            {"path": os.path.join("data", "bar")},
        ],
        "modified": [{"path": "file"}],
        "added": [{"path": "file"}],
        "renamed": [{"path": {"old": "file_old", "new": "file_new"}}],
        "not in cache": [{"path": "file2"}],
    }

    _show_markdown(diff, hide_missing=True)

    out, _ = capsys.readouterr()
    assert out == (
        "| Status   | Path                 |\n"
        "|----------|----------------------|\n"
        "| added    | file                 |\n"
        "| deleted  | zoo                  |\n"
        "| deleted  | data{sep}                |\n"
        "| deleted  | data{sep}foo             |\n"
        "| deleted  | data{sep}bar             |\n"
        "| renamed  | file_old -> file_new |\n"
        "| modified | file                 |\n"
        "\n"
    ).format(sep=os.path.sep)


def test_hide_missing(mocker, capsys):
    args = parse_args(["diff", "--hide-missing"])
    cmd = args.func(args)
    diff = {
        "added": [{"path": "file", "hash": "00000000"}],
        "deleted": [],
        "modified": [],
        "renamed": [
            {
                "path": {"old": "file_old", "new": "file_new"},
                "hash": "11111111",
            }
        ],
        "not in cache": [],
    }
    mocker.patch("dvc.repo.Repo.diff", return_value=diff)

    assert cmd.run() == 0
    out, _ = capsys.readouterr()
    assert (
        "Added:\n"
        "    file\n"
        "\n"
        "Renamed:\n"
        "    file_old -> file_new\n"
        "\n"
        "files summary: 1 added, 1 renamed" in out
    )
    assert "not in cache" not in out




tests/unit/command/test_experiments.py
import pytest

from dvc.cli import parse_args
from dvc.commands.experiments.apply import CmdExperimentsApply
from dvc.commands.experiments.branch import CmdExperimentsBranch
from dvc.commands.experiments.clean import CmdExperimentsClean
from dvc.commands.experiments.diff import CmdExperimentsDiff
from dvc.commands.experiments.ls import CmdExperimentsList
from dvc.commands.experiments.pull import CmdExperimentsPull
from dvc.commands.experiments.push import CmdExperimentsPush
from dvc.commands.experiments.remove import CmdExperimentsRemove
from dvc.commands.experiments.run import CmdExperimentsRun
from dvc.commands.experiments.save import CmdExperimentsSave
from dvc.commands.experiments.show import CmdExperimentsShow
from dvc.exceptions import InvalidArgumentError

from .test_repro import common_arguments as repro_arguments


def test_experiments_apply(dvc, scm, mocker):
    cli_args = parse_args(["experiments", "apply", "exp_rev"])
    assert cli_args.func == CmdExperimentsApply

    cmd = cli_args.func(cli_args)
    m = mocker.patch("dvc.repo.experiments.apply.apply", return_value={})

    assert cmd.run() == 0

    m.assert_called_once_with(cmd.repo, "exp_rev")


def test_experiments_diff(dvc, scm, mocker):
    cli_args = parse_args(
        [
            "experiments",
            "diff",
            "HEAD~10",
            "HEAD~1",
            "--all",
            "--param-deps",
            "--json",
            "--md",
            "--precision",
            "10",
        ]
    )
    assert cli_args.func == CmdExperimentsDiff

    cmd = cli_args.func(cli_args)
    m = mocker.patch("dvc.repo.experiments.diff.diff", return_value={})

    assert cmd.run() == 0

    m.assert_called_once_with(
        cmd.repo, a_rev="HEAD~10", b_rev="HEAD~1", all=True, param_deps=True
    )


def test_experiments_diff_revs(mocker, capsys):
    mocker.patch(
        "dvc.repo.experiments.diff.diff",
        return_value={
            "params": {"params.yaml": {"foo": {"diff": 1, "old": 1, "new": 2}}},
            "metrics": {"metrics.yaml": {"foo": {"diff": 1, "old": 1, "new": 2}}},
        },
    )

    cli_args = parse_args(["exp", "diff", "exp_a", "exp_b"])
    cmd = cli_args.func(cli_args)

    capsys.readouterr()
    assert cmd.run() == 0
    cap = capsys.readouterr()
    assert "exp_a" in cap.out
    assert "exp_b" in cap.out


def test_experiments_show(dvc, scm, mocker):
    cli_args = parse_args(
        [
            "experiments",
            "show",
            "--all-tags",
            "--all-branches",
            "--all-commits",
            "--hide-queued",
            "--hide-failed",
            "--sha",
            "--param-deps",
            "-n",
            "1",
            "--rev",
            "foo",
            "--force",
        ]
    )
    assert cli_args.func == CmdExperimentsShow

    cmd = cli_args.func(cli_args)

    m = mocker.patch("dvc.repo.experiments.show.show", return_value={})
    assert cmd.run() == 0

    m.assert_called_once_with(
        cmd.repo,
        all_tags=True,
        all_branches=True,
        all_commits=True,
        hide_queued=True,
        hide_failed=True,
        num=1,
        revs=["foo"],
        sha_only=True,
        param_deps=True,
        fetch_running=True,
        force=True,
    )


def test_experiments_run(dvc, scm, mocker):
    default_arguments = {
        "params": [],
        "name": None,
        "queue": False,
        "run_all": False,
        "jobs": 1,
        "tmp_dir": False,
        "machine": None,
        "copy_paths": [],
        "message": None,
    }
    default_arguments.update(repro_arguments)

    cmd = CmdExperimentsRun(parse_args(["exp", "run"]))
    mocker.patch.object(cmd.repo, "reproduce")
    mocker.patch.object(cmd.repo.experiments, "run")
    cmd.run()
    cmd.repo.experiments.run.assert_called_with(**default_arguments)


def test_experiments_branch(dvc, scm, mocker):
    m = mocker.patch("dvc.repo.experiments.branch.branch", return_value={})

    cli_args = parse_args(["experiments", "branch", "expname"])
    assert cli_args.func == CmdExperimentsBranch

    cmd = cli_args.func(cli_args)
    assert cmd.run() == 0

    m.assert_called_with(cmd.repo, "expname", None)

    cli_args = parse_args(["experiments", "branch", "expname", "branchname"])
    cmd = cli_args.func(cli_args)
    assert cmd.run() == 0

    m.assert_called_with(cmd.repo, "expname", "branchname")


def test_experiments_list(dvc, scm, mocker):
    cli_args = parse_args(
        [
            "experiments",
            "list",
            "origin",
            "--all-commits",
            "-n",
            "-1",
            "--rev",
            "foo",
            "--name-only",
        ]
    )
    assert cli_args.func == CmdExperimentsList

    cmd = cli_args.func(cli_args)
    m = mocker.patch("dvc.repo.experiments.ls.ls", return_value={})

    assert cmd.run() == 0

    m.assert_called_once_with(
        cmd.repo,
        git_remote="origin",
        rev=["foo"],
        all_commits=True,
        num=-1,
    )


@pytest.mark.parametrize(
    "args,expected",
    [
        ([], "main:\n\tsha-a [exp-a]\n"),
        (["--name-only"], "exp-a\n"),
        (["--sha-only"], "sha-a\n"),
    ],
)
def test_experiments_list_format(mocker, capsys, args, expected):
    mocker.patch(
        "dvc.repo.experiments.ls.ls",
        return_value={
            "main": [
                ("exp-a", "sha-a"),
            ]
        },
    )
    raw_args = ["experiments", "list", *args]
    cli_args = parse_args(raw_args)

    cmd = cli_args.func(cli_args)

    capsys.readouterr()
    assert cmd.run() == 0
    cap = capsys.readouterr()
    assert cap.out == expected


def test_experiments_list_remote(mocker, capsys):
    mocker.patch(
        "dvc.repo.experiments.ls.ls",
        return_value={
            "main": [
                ("exp-a", None),
            ]
        },
    )
    cli_args = parse_args(["experiments", "list", "git_remote"])

    cmd = cli_args.func(cli_args)

    capsys.readouterr()
    assert cmd.run() == 0
    cap = capsys.readouterr()
    assert cap.out == "main:\n\texp-a\n"

    cli_args = parse_args(["experiments", "list", "git_remote", "--sha-only"])

    cmd = cli_args.func(cli_args)

    capsys.readouterr()

    with pytest.raises(InvalidArgumentError):
        cmd.run()


def test_experiments_push(dvc, scm, mocker):
    cli_args = parse_args(
        [
            "experiments",
            "push",
            "origin",
            "experiment1",
            "experiment2",
            "--all-commits",
            "-n",
            "2",
            "--rev",
            "foo",
            "--force",
            "--no-cache",
            "--remote",
            "my-remote",
            "--jobs",
            "1",
            "--run-cache",
        ]
    )
    assert cli_args.func == CmdExperimentsPush

    cmd = cli_args.func(cli_args)
    m = mocker.patch("dvc.repo.experiments.push.push", return_value={})

    assert cmd.run() == 0

    m.assert_called_once_with(
        cmd.repo,
        "origin",
        ["experiment1", "experiment2"],
        rev=["foo"],
        all_commits=True,
        num=2,
        force=True,
        push_cache=False,
        dvc_remote="my-remote",
        jobs=1,
        run_cache=True,
    )

    cli_args = parse_args(
        [
            "experiments",
            "push",
            "origin",
        ]
    )
    cmd = cli_args.func(cli_args)

    with pytest.raises(InvalidArgumentError) as exp_info:
        cmd.run()
    assert (
        str(exp_info.value) == "Either provide an `experiment` argument"
        ", or use the `--rev` or `--all-commits` flag."
    )


def test_experiments_pull(dvc, scm, mocker):
    cli_args = parse_args(
        [
            "experiments",
            "pull",
            "origin",
            "experiment",
            "--all-commits",
            "--rev",
            "foo",
            "--force",
            "--no-cache",
            "--remote",
            "my-remote",
            "--jobs",
            "1",
            "--run-cache",
        ]
    )
    assert cli_args.func == CmdExperimentsPull

    cmd = cli_args.func(cli_args)
    m = mocker.patch("dvc.repo.experiments.pull.pull", return_value={})

    assert cmd.run() == 0

    m.assert_called_once_with(
        cmd.repo,
        "origin",
        ["experiment"],
        rev=["foo"],
        all_commits=True,
        num=1,
        force=True,
        pull_cache=False,
        dvc_remote="my-remote",
        jobs=1,
        run_cache=True,
    )

    cli_args = parse_args(
        [
            "experiments",
            "pull",
            "origin",
        ]
    )
    cmd = cli_args.func(cli_args)

    with pytest.raises(InvalidArgumentError) as exp_info:
        cmd.run()
    assert (
        str(exp_info.value) == "Either provide an `experiment` argument"
        ", or use the `--rev` or `--all-commits` flag."
    )


def test_experiments_remove_flag(dvc, scm, mocker, capsys, caplog):
    cli_args = parse_args(
        [
            "experiments",
            "remove",
            "--all-commits",
            "--rev",
            "foo",
            "--num",
            "2",
            "--git-remote",
            "myremote",
        ]
    )
    assert cli_args.func == CmdExperimentsRemove
    cmd = cli_args.func(cli_args)
    m = mocker.patch("dvc.repo.experiments.remove.remove", return_value={})
    assert cmd.run() == 0
    m.assert_called_once_with(
        cmd.repo,
        exp_names=[],
        all_commits=True,
        rev=["foo"],
        num=2,
        queue=False,
        git_remote="myremote",
    )


def test_experiments_remove_special(dvc, scm, mocker, capsys, caplog):
    cli_args = parse_args(
        [
            "experiments",
            "remove",
            "--git-remote",
            "myremote",
            "exp-123",
            "exp-234",
        ]
    )
    assert cli_args.func == CmdExperimentsRemove
    cmd = cli_args.func(cli_args)
    m = mocker.patch("dvc.repo.experiments.remove.remove", return_value={})
    assert cmd.run() == 0
    m.assert_called_once_with(
        cmd.repo,
        exp_names=["exp-123", "exp-234"],
        all_commits=False,
        rev=None,
        num=1,
        queue=False,
        git_remote="myremote",
    )


def test_experiments_remove_invalid(dvc, scm, mocker, capsys, caplog):
    cmd = CmdExperimentsRemove(
        parse_args(["exp", "remove", "--all-commits", "exp-1", "exp-2"])
    )
    with pytest.raises(InvalidArgumentError):
        cmd.run()
    cmd = CmdExperimentsRemove(parse_args(["exp", "remove"]))
    with pytest.raises(InvalidArgumentError) as excinfo:
        cmd.run()
    assert (
        str(excinfo.value) == "Either provide an `experiment` argument"
        ", or use the `--rev` or `--all-commits` or `--queue` flag."
    )


def test_experiments_save(dvc, scm, mocker):
    cli_args = parse_args(["exp", "save", "--name", "exp-name", "--force"])
    assert cli_args.func == CmdExperimentsSave

    cmd = cli_args.func(cli_args)
    m = mocker.patch("dvc.repo.experiments.save.save", return_value="acabb")

    assert cmd.run() == 0

    m.assert_called_once_with(
        cmd.repo, name="exp-name", force=True, include_untracked=[], message=None
    )


def test_experiments_clean(dvc, scm, mocker):
    cli_args = parse_args(["experiments", "clean"])
    assert cli_args.func == CmdExperimentsClean

    cmd = cli_args.func(cli_args)
    m = mocker.patch("dvc.repo.experiments.clean.clean", return_value={})

    assert cmd.run() == 0

    m.assert_called_once_with(cmd.repo)




tests/unit/command/test_gc.py
import pytest

from dvc.cli import parse_args
from dvc.commands.gc import CmdGC
from dvc.exceptions import InvalidArgumentError


def test_(dvc, scm, mocker):
    cli_args = parse_args(
        [
            "gc",
            "--workspace",
            "--all-tags",
            "--all-branches",
            "--all-commits",
            "--all-experiments",
            "--date",
            "2022-06-30",
            "--cloud",
            "--remote",
            "origin",
            "--force",
            "--jobs",
            "3",
            "--projects",
            "project1",
            "project2",
        ]
    )
    assert cli_args.func == CmdGC

    cmd = cli_args.func(cli_args)
    m = mocker.patch("dvc.repo.Repo.gc", return_value={})

    assert cmd.run() == 0

    m.assert_called_once_with(
        workspace=True,
        all_tags=True,
        all_branches=True,
        all_commits=True,
        all_experiments=True,
        commit_date="2022-06-30",
        cloud=True,
        remote="origin",
        force=True,
        jobs=3,
        repos=["project1", "project2"],
        rev=None,
        num=None,
        not_in_remote=False,
    )

    cli_args = parse_args(["gc"])
    cmd = cli_args.func(cli_args)
    with pytest.raises(InvalidArgumentError):
        cmd.run()

    cli_args = parse_args(["gc", "--num", "2"])
    cmd = cli_args.func(cli_args)
    with pytest.raises(InvalidArgumentError):
        cmd.run()

    cli_args = parse_args(["gc", "--cloud", "--not-in-remote"])
    cmd = cli_args.func(cli_args)
    with pytest.raises(InvalidArgumentError):
        cmd.run()

    cli_args = parse_args(["gc", "--remote", "myremote"])
    cmd = cli_args.func(cli_args)
    with pytest.raises(InvalidArgumentError):
        cmd.run()




tests/unit/command/test_get.py
from dvc.cli import parse_args
from dvc.commands.get import CmdGet


def test_get(mocker):
    cli_args = parse_args(
        [
            "get",
            "repo_url",
            "src",
            "--out",
            "out",
            "--rev",
            "version",
            "--jobs",
            "4",
        ]
    )
    assert cli_args.func == CmdGet

    cmd = cli_args.func(cli_args)
    m = mocker.patch("dvc.repo.Repo.get")

    assert cmd.run() == 0

    m.assert_called_once_with(
        "repo_url", path="src", out="out", rev="version", jobs=4, force=False
    )


def test_get_url(mocker, capsys):
    cli_args = parse_args(["get", "repo_url", "src", "--rev", "version", "--show-url"])
    assert cli_args.func == CmdGet

    cmd = cli_args.func(cli_args)
    m = mocker.patch("dvc.api.get_url", return_value="resource_url")

    assert cmd.run() == 0
    out, _ = capsys.readouterr()
    assert "resource_url" in out

    m.assert_called_once_with("src", repo="repo_url", rev="version")




tests/unit/command/test_get_url.py
from dvc.cli import parse_args
from dvc.commands.get_url import CmdGetUrl


def test_get_url(mocker):
    cli_args = parse_args(["get-url", "src", "out", "-j", "5"])
    assert cli_args.func == CmdGetUrl

    cmd = cli_args.func(cli_args)
    m = mocker.patch("dvc.repo.Repo.get_url")

    assert cmd.run() == 0

    m.assert_called_once_with("src", out="out", jobs=5, force=False)




tests/unit/command/test_git_hook.py
import pytest

from dvc.cli import parse_args
from dvc.commands.git_hook import CmdPostCheckout, CmdPreCommit, CmdPrePush


@pytest.mark.parametrize(
    "hook, cls",
    [
        ("pre-commit", CmdPreCommit),
        ("post-checkout", CmdPostCheckout),
        ("pre-push", CmdPrePush),
    ],
)
def test_out_of_repo(tmp_dir, hook, cls, mocker):
    cli_args = parse_args(["git-hook", hook])
    assert cli_args.func == cls
    cmd = cli_args.func(cli_args)
    mock_main = mocker.patch("dvc.cli.main")
    assert cmd.run() == 0
    assert not mock_main.called




tests/unit/command/test_imp.py
from dvc.cli import parse_args
from dvc.commands.imp import CmdImport


def test_import(mocker):
    cli_args = parse_args(
        [
            "import",
            "repo_url",
            "src",
            "--out",
            "out",
            "--file",
            "file",
            "--rev",
            "version",
            "--jobs",
            "3",
        ]
    )
    assert cli_args.func == CmdImport

    cmd = cli_args.func(cli_args)
    m = mocker.patch.object(cmd.repo, "imp", autospec=True)

    assert cmd.run() == 0

    m.assert_called_once_with(
        "repo_url",
        path="src",
        out="out",
        fname="file",
        rev="version",
        no_exec=False,
        no_download=False,
        jobs=3,
    )


def test_import_no_exec(mocker):
    cli_args = parse_args(
        [
            "import",
            "repo_url",
            "src",
            "--out",
            "out",
            "--file",
            "file",
            "--rev",
            "version",
            "--no-exec",
        ]
    )

    cmd = cli_args.func(cli_args)
    m = mocker.patch.object(cmd.repo, "imp", autospec=True)

    assert cmd.run() == 0

    m.assert_called_once_with(
        "repo_url",
        path="src",
        out="out",
        fname="file",
        rev="version",
        no_exec=True,
        no_download=False,
        jobs=None,
    )


def test_import_no_download(mocker):
    cli_args = parse_args(
        [
            "import",
            "repo_url",
            "src",
            "--out",
            "out",
            "--file",
            "file",
            "--rev",
            "version",
            "--no-download",
        ]
    )

    cmd = cli_args.func(cli_args)
    m = mocker.patch.object(cmd.repo, "imp", autospec=True)

    assert cmd.run() == 0

    m.assert_called_once_with(
        "repo_url",
        path="src",
        out="out",
        fname="file",
        rev="version",
        no_exec=False,
        no_download=True,
        jobs=None,
    )




tests/unit/command/test_imp_url.py
import logging

import pytest

from dvc.cli import parse_args
from dvc.commands.imp_url import CmdImportUrl
from dvc.exceptions import DvcException


def test_import_url(mocker):
    cli_args = parse_args(
        [
            "import-url",
            "src",
            "out",
            "--file",
            "file",
            "--jobs",
            "4",
        ]
    )
    assert cli_args.func == CmdImportUrl

    cmd = cli_args.func(cli_args)
    m = mocker.patch.object(cmd.repo, "imp_url", autospec=True)

    assert cmd.run() == 0

    m.assert_called_once_with(
        "src",
        out="out",
        fname="file",
        no_exec=False,
        no_download=False,
        remote=None,
        to_remote=False,
        jobs=4,
        force=False,
        version_aware=False,
    )


def test_failed_import_url(mocker, caplog):
    cli_args = parse_args(["import-url", "http://somesite.com/file_name"])
    assert cli_args.func == CmdImportUrl

    cmd = cli_args.func(cli_args)
    mocker.patch.object(cmd.repo, "imp_url", side_effect=DvcException("error"))
    with caplog.at_level(logging.ERROR, logger="dvc"):
        assert cmd.run() == 1
        expected_error = (
            "failed to import http://somesite.com/file_name. "
            "You could also try downloading it manually, and "
            "adding it with `dvc add`."
        )
        assert expected_error in caplog.text


@pytest.mark.parametrize(
    "flag,expected",
    [
        ("--no-exec", {"no_exec": True, "no_download": False}),
        ("--no-download", {"no_download": True, "no_exec": False}),
    ],
)
def test_import_url_no_exec_download_flags(mocker, flag, expected):
    cli_args = parse_args(
        [
            "import-url",
            flag,
            "src",
            "out",
            "--file",
            "file",
        ]
    )

    cmd = cli_args.func(cli_args)
    m = mocker.patch.object(cmd.repo, "imp_url", autospec=True)

    assert cmd.run() == 0

    m.assert_called_once_with(
        "src",
        out="out",
        fname="file",
        remote=None,
        to_remote=False,
        jobs=None,
        force=False,
        version_aware=False,
        **expected,
    )


def test_import_url_to_remote(mocker):
    cli_args = parse_args(
        [
            "import-url",
            "s3://bucket/foo",
            "bar",
            "--to-remote",
            "--remote",
            "remote",
        ]
    )
    assert cli_args.func == CmdImportUrl

    cmd = cli_args.func(cli_args)
    m = mocker.patch.object(cmd.repo, "imp_url", autospec=True)

    assert cmd.run() == 0

    m.assert_called_once_with(
        "s3://bucket/foo",
        out="bar",
        fname=None,
        no_exec=False,
        no_download=False,
        remote="remote",
        to_remote=True,
        jobs=None,
        force=False,
        version_aware=False,
    )


@pytest.mark.parametrize("flag", ["--no-exec", "--no-download", "--version-aware"])
def test_import_url_to_remote_invalid_combination(dvc, mocker, caplog, flag):
    cli_args = parse_args(
        [
            "import-url",
            "s3://bucket/foo",
            "bar",
            "--to-remote",
            "--remote",
            "remote",
            flag,
        ]
    )
    assert cli_args.func == CmdImportUrl

    cmd = cli_args.func(cli_args)
    with caplog.at_level(logging.ERROR, logger="dvc"):
        assert cmd.run() == 1
        expected_msg = (
            "--no-exec/--no-download/--version-aware cannot be combined with "
            "--to-remote"
        )
        assert expected_msg in caplog.text


def test_import_url_to_remote_flag(dvc, mocker, caplog):
    cli_args = parse_args(
        ["import-url", "s3://bucket/foo", "bar", "--remote", "remote"]
    )

    cmd = cli_args.func(cli_args)
    with caplog.at_level(logging.ERROR, logger="dvc"):
        assert cmd.run() == 1
        expected_msg = "--remote can't be used without --to-remote"
        assert expected_msg in caplog.text




tests/unit/command/test_ls_url.py
from dvc.cli import parse_args
from dvc.commands.ls_url import CmdListUrl


def test_ls_url(mocker):
    cli_args = parse_args(["ls-url", "src"])
    assert cli_args.func == CmdListUrl
    cmd = cli_args.func(cli_args)
    m = mocker.patch("dvc.repo.Repo.ls_url", autospec=True)

    assert cmd.run() == 0

    m.assert_called_once_with("src", recursive=False)


def test_recursive(mocker):
    cli_args = parse_args(["ls-url", "-R", "src"])
    assert cli_args.func == CmdListUrl
    cmd = cli_args.func(cli_args)
    m = mocker.patch("dvc.repo.Repo.ls_url", autospec=True)

    assert cmd.run() == 0

    m.assert_called_once_with("src", recursive=True)




tests/unit/command/test_machine.py
import os
from unittest.mock import call

import configobj
import pytest

from dvc.cli import parse_args
from dvc.commands.machine import (
    CmdMachineAdd,
    CmdMachineCreate,
    CmdMachineDestroy,
    CmdMachineList,
    CmdMachineModify,
    CmdMachineRemove,
    CmdMachineRename,
    CmdMachineSsh,
    CmdMachineStatus,
)

DATA = {
    ".dvc": {
        "config": (
            "[feature]\n"
            "  machine = true\n"
            "['machine \"foo\"']\n"
            "  cloud = aws\n"
            "['machine \"myaws\"']\n"
            "  cloud = aws"
        )
    }
}


def test_add(tmp_dir):
    tmp_dir.gen({".dvc": {"config": "[feature]\n  machine = true"}})
    cli_args = parse_args(["machine", "add", "foo", "aws"])
    assert cli_args.func == CmdMachineAdd
    cmd = cli_args.func(cli_args)
    assert cmd.run() == 0
    config = configobj.ConfigObj(str(tmp_dir / ".dvc" / "config"))
    assert config['machine "foo"']["cloud"] == "aws"


def test_remove(tmp_dir):
    tmp_dir.gen(DATA)
    cli_args = parse_args(["machine", "remove", "foo"])
    assert cli_args.func == CmdMachineRemove
    cmd = cli_args.func(cli_args)
    assert cmd.run() == 0
    config = configobj.ConfigObj(str(tmp_dir / ".dvc" / "config"))
    assert list(config.keys()) == ["feature", 'machine "myaws"']


def test_create(tmp_dir, dvc, mocker):
    cli_args = parse_args(["machine", "create", "foo"])
    assert cli_args.func == CmdMachineCreate

    cmd = cli_args.func(cli_args)
    m = mocker.patch.object(cmd.repo.machine, "create", autospec=True, return_value=0)

    assert cmd.run() == 0
    m.assert_called_once_with("foo")


def test_status(tmp_dir, scm, dvc, mocker):
    tmp_dir.gen(DATA)
    cli_args = parse_args(["machine", "status", "foo"])
    assert cli_args.func == CmdMachineStatus

    cmd = cli_args.func(cli_args)
    m = mocker.patch.object(cmd.repo.machine, "status", autospec=True, return_value=[])
    assert cmd.run() == 0
    m.assert_called_once_with("foo")

    cli_args = parse_args(["machine", "status"])
    cmd = cli_args.func(cli_args)
    m = mocker.patch.object(cmd.repo.machine, "status", autospec=True, return_value=[])
    assert cmd.run() == 0
    assert m.call_count == 2
    m.assert_has_calls([call("foo"), call("myaws")])


def test_destroy(tmp_dir, dvc, mocker):
    cli_args = parse_args(["machine", "destroy", "foo"])
    assert cli_args.func == CmdMachineDestroy

    cmd = cli_args.func(cli_args)
    m = mocker.patch.object(cmd.repo.machine, "destroy", autospec=True, return_value=0)

    assert cmd.run() == 0
    m.assert_called_once_with("foo")


def test_ssh(tmp_dir, dvc, mocker):
    cli_args = parse_args(["machine", "ssh", "foo"])
    assert cli_args.func == CmdMachineSsh

    cmd = cli_args.func(cli_args)
    m = mocker.patch.object(
        cmd.repo.machine, "run_shell", autospec=True, return_value=0
    )

    assert cmd.run() == 0
    m.assert_called_once_with("foo")


@pytest.mark.parametrize("show_origin", [["--show-origin"], []])
def test_list(tmp_dir, mocker, show_origin):
    from dvc.compare import TabularData
    from dvc.ui import ui

    tmp_dir.gen(DATA)
    cli_args = parse_args(["machine", "list", *show_origin, "foo"])
    assert cli_args.func == CmdMachineList
    cmd = cli_args.func(cli_args)
    if show_origin:
        m = mocker.patch.object(ui, "write", autospec=True)
    else:
        m = mocker.patch.object(TabularData, "render", autospec=True)
    assert cmd.run() == 0
    if show_origin:
        m.assert_called_once_with(f".dvc{os.sep}config	cloud=aws")
    else:
        m.assert_called_once()


def test_modified(tmp_dir):
    tmp_dir.gen(DATA)
    cli_args = parse_args(["machine", "modify", "foo", "cloud", "azure"])
    assert cli_args.func == CmdMachineModify
    cmd = cli_args.func(cli_args)
    assert cmd.run() == 0
    config = configobj.ConfigObj(str(tmp_dir / ".dvc" / "config"))
    assert config['machine "foo"']["cloud"] == "azure"


def test_rename(tmp_dir, scm, dvc):
    tmp_dir.gen(DATA)
    cli_args = parse_args(["machine", "rename", "foo", "bar"])
    assert cli_args.func == CmdMachineRename
    cmd = cli_args.func(cli_args)
    assert cmd.run() == 0
    config = configobj.ConfigObj(str(tmp_dir / ".dvc" / "config"))
    assert config['machine "bar"']["cloud"] == "aws"


@pytest.mark.parametrize(
    "cmd,use_config",
    [
        ("add", True),
        ("default", True),
        ("remove", True),
        ("list", True),
        ("modify", True),
        ("create", False),
        ("destroy", False),
        ("rename", True),
        ("status", False),
        ("ssh", False),
    ],
)
def test_help_message(tmp_dir, scm, dvc, cmd, use_config, capsys):
    try:
        parse_args(["machine", cmd, "--help"])
    except SystemExit:
        pass
    cap = capsys.readouterr()
    assert ("--global" in cap.out) is use_config




tests/unit/command/test_metrics.py
import json

from dvc.cli import parse_args
from dvc.commands.metrics import CmdMetricsDiff, CmdMetricsShow


def test_metrics_diff(dvc, mocker, capsys):
    cli_args = parse_args(
        [
            "metrics",
            "diff",
            "HEAD~10",
            "HEAD~1",
            "-R",
            "--all",
            "--md",
            "--targets",
            "target1",
            "target2",
            "--no-path",
        ]
    )

    assert cli_args.func == CmdMetricsDiff

    cmd = cli_args.func(cli_args)
    diff = {"metrics.yaml": {"": {"old": 1, "new": 3}}}
    metrics_diff = mocker.patch("dvc.repo.metrics.diff.diff", return_value=diff)
    show_diff_mock = mocker.patch("dvc.compare.show_diff")

    assert cmd.run() == 0

    metrics_diff.assert_called_once_with(
        cmd.repo,
        targets=["target1", "target2"],
        a_rev="HEAD~10",
        b_rev="HEAD~1",
        recursive=True,
        all=True,
    )
    show_diff_mock.assert_called_once_with(
        diff,
        title="Metric",
        no_path=True,
        precision=5,
        markdown=True,
        round_digits=True,
        a_rev="HEAD~10",
        b_rev="HEAD~1",
    )


def test_metrics_diff_json(dvc, mocker, capsys):
    cli_args = parse_args(
        [
            "metrics",
            "diff",
            "HEAD~10",
            "HEAD~1",
            "-R",
            "--all",
            "--json",
            "--targets",
            "target1",
            "target2",
            "--no-path",
            "--precision",
            "10",
        ]
    )

    assert cli_args.func == CmdMetricsDiff
    cmd = cli_args.func(cli_args)

    diff = {"metrics.yaml": {"": {"old": 1, "new": 3}}}
    metrics_diff = mocker.patch("dvc.repo.metrics.diff.diff", return_value=diff)
    show_diff_mock = mocker.patch("dvc.compare.show_diff")

    assert cmd.run() == 0
    out, _ = capsys.readouterr()
    metrics_diff.assert_called_once_with(
        cmd.repo,
        targets=["target1", "target2"],
        a_rev="HEAD~10",
        b_rev="HEAD~1",
        recursive=True,
        all=True,
    )
    show_diff_mock.assert_not_called()
    assert json.dumps(diff) in out


def test_metrics_show(dvc, mocker):
    cli_args = parse_args(
        [
            "metrics",
            "show",
            "-R",
            "--all-tags",
            "--all-branches",
            "--all-commits",
            "target1",
            "target2",
            "--precision",
            "8",
        ]
    )
    assert cli_args.func == CmdMetricsShow

    cmd = cli_args.func(cli_args)
    m1 = mocker.patch("dvc.repo.metrics.show.show", return_value={})
    m2 = mocker.patch("dvc.compare.show_metrics", return_value="")

    assert cmd.run() == 0

    m1.assert_called_once_with(
        cmd.repo,
        ["target1", "target2"],
        recursive=True,
        all_tags=True,
        all_branches=True,
        all_commits=True,
    )
    m2.assert_called_once_with(
        {},
        markdown=False,
        all_tags=True,
        all_branches=True,
        all_commits=True,
        precision=8,
        round_digits=True,
    )


def test_metrics_show_json(dvc, mocker, capsys):
    cli_args = parse_args(
        [
            "metrics",
            "show",
            "--json",
            "-R",
            "--all-tags",
            "--all-branches",
            "--all-commits",
            "target1",
            "target2",
            "--precision",
            "8",
        ]
    )

    assert cli_args.func == CmdMetricsShow
    cmd = cli_args.func(cli_args)
    d = {
        "branch_1": {"metrics.json": {"b": {"ad": 1, "bc": 2}, "c": 4}},
        "branch_2": {"metrics.json": {"a": 1, "b": {"ad": 3, "bc": 4}}},
    }
    metrics_show = mocker.patch("dvc.repo.metrics.show.show", return_value=d)
    show_metrics_mock = mocker.patch("dvc.compare.show_metrics")

    assert cmd.run() == 0
    out, _ = capsys.readouterr()
    metrics_show.assert_called_once_with(
        cmd.repo,
        ["target1", "target2"],
        recursive=True,
        all_tags=True,
        all_branches=True,
        all_commits=True,
    )
    show_metrics_mock.assert_not_called()
    assert json.dumps(d) in out




tests/unit/command/test_params.py
from dvc.cli import parse_args
from dvc.commands.params import CmdParamsDiff


def test_params_diff(dvc, mocker):
    cli_args = parse_args(
        [
            "params",
            "diff",
            "HEAD~10",
            "HEAD~1",
            "--targets",
            "target",
            "--all",
            "--json",
            "--md",
            "--no-path",
            "--deps",
        ]
    )
    assert cli_args.func == CmdParamsDiff

    cmd = cli_args.func(cli_args)
    params_diff = mocker.patch("dvc.repo.params.diff.diff", return_value={})
    show_diff_mock = mocker.patch("dvc.compare.show_diff")

    assert cmd.run() == 0

    params_diff.assert_called_once_with(
        cmd.repo,
        a_rev="HEAD~10",
        b_rev="HEAD~1",
        targets=["target"],
        all=True,
        deps=True,
    )
    show_diff_mock.assert_not_called()


def test_params_diff_from_cli(dvc, mocker):
    cli_args = parse_args(["params", "diff"])
    assert cli_args.func == CmdParamsDiff

    cmd = cli_args.func(cli_args)
    params_diff = mocker.patch("dvc.repo.params.diff.diff", return_value={})
    show_diff_mock = mocker.patch("dvc.compare.show_diff")

    assert cmd.run() == 0

    params_diff.assert_called_once_with(
        cmd.repo,
        a_rev=None,
        b_rev=None,
        all=False,
        targets=None,
        deps=False,
    )
    show_diff_mock.assert_called_once_with(
        {},
        title="Param",
        markdown=False,
        no_path=False,
        show_changes=False,
        a_rev=None,
        b_rev=None,
    )


def test_params_diff_show_json(dvc, mocker, capsys):
    cli_args = parse_args(["params", "diff", "HEAD~10", "HEAD~1", "--json"])
    cmd = cli_args.func(cli_args)
    mocker.patch("dvc.repo.params.diff.diff", return_value={"params.yaml": {"a": "b"}})
    show_diff_mock = mocker.patch("dvc.compare.show_diff")

    assert cmd.run() == 0
    out, _ = capsys.readouterr()
    assert '{"params.yaml": {"a": "b"}}\n' in out
    show_diff_mock.assert_not_called()




tests/unit/command/test_plots.py
import json
import os
import posixpath
from pathlib import Path

import pytest
from funcy import set_in

from dvc.cli import parse_args
from dvc.commands.plots import CmdPlotsDiff, CmdPlotsShow, CmdPlotsTemplates
from dvc.render.match import RendererWithErrors
from dvc.utils.serialize import YAMLFileCorruptedError


@pytest.fixture
def plots_data():
    return {
        "revision": {
            "sources": {
                "data": {
                    "plot.csv": {
                        "data": [{"val": 1}, {"val": 2}],
                        "props": {},
                    },
                    "other.jpg": {"data": b"content"},
                }
            },
            "definitions": {"data": {"dvc.yaml": {"data": {"plot.csv": {}}}}},
        }
    }


def test_plots_diff(dvc, mocker, plots_data):
    cli_args = parse_args(
        [
            "plots",
            "diff",
            "--out",
            "result.extension",
            "-t",
            "template",
            "--targets",
            "datafile",
            "--show-vega",
            "-x",
            "x_field",
            "-y",
            "y_field",
            "--title",
            "my_title",
            "--x-label",
            "x_title",
            "--y-label",
            "y_title",
            "--experiment",
            "HEAD",
            "tag1",
            "tag2",
        ]
    )
    assert cli_args.func == CmdPlotsDiff

    cmd = cli_args.func(cli_args)
    m = mocker.patch("dvc.repo.plots.diff.diff", return_value=plots_data)
    render_mock = mocker.patch("dvc_render.render_html", return_value="html_path")

    assert cmd.run() == 0

    m.assert_called_once_with(
        cmd.repo,
        targets=["datafile"],
        revs=["HEAD", "tag1", "tag2"],
        props={
            "template": "template",
            "x": "x_field",
            "y": "y_field",
            "title": "my_title",
            "x_label": "x_title",
            "y_label": "y_title",
        },
        experiment=True,
    )
    render_mock.assert_not_called()


def test_plots_show_vega(dvc, mocker, plots_data):
    cli_args = parse_args(
        [
            "plots",
            "show",
            "-o",
            "result.extension",
            "-t",
            "template",
            "--show-vega",
            "--no-header",
            "datafile",
        ]
    )
    assert cli_args.func == CmdPlotsShow

    cmd = cli_args.func(cli_args)

    m = mocker.patch(
        "dvc.repo.plots.Plots.show",
        return_value=plots_data,
    )
    render_mock = mocker.patch("dvc_render.render_html", return_value="html_path")

    assert cmd.run() == 0

    m.assert_called_once_with(
        targets=["datafile"],
        props={"template": "template", "header": False},
    )
    render_mock.assert_not_called()


def test_plots_diff_vega(dvc, mocker, capsys, plots_data):
    cli_args = parse_args(
        [
            "plots",
            "diff",
            "HEAD~10",
            "HEAD~1",
            "--show-vega",
            "--targets",
            "plot.csv",
        ]
    )
    cmd = cli_args.func(cli_args)
    mocker.patch("dvc.repo.plots.diff.diff", return_value=plots_data)
    mocker.patch(
        "dvc_render.VegaRenderer.get_filled_template",
        return_value={"this": "is vega json"},
    )
    render_mock = mocker.patch("dvc_render.render_html")
    assert cmd.run() == 0

    out, _ = capsys.readouterr()

    assert json.dumps({"this": "is vega json"}) in out
    render_mock.assert_not_called()


@pytest.mark.parametrize("auto_open", [True, False])
def test_plots_diff_open(tmp_dir, dvc, mocker, capsys, plots_data, auto_open):
    mocked_open = mocker.patch("webbrowser.open", return_value=True)

    args = ["plots", "diff", "--targets", "plots.csv"]

    if auto_open:
        with dvc.config.edit() as conf:
            conf["plots"]["auto_open"] = True
    else:
        args.append("--open")

    cli_args = parse_args(args)
    cmd = cli_args.func(cli_args)
    mocker.patch("dvc.repo.plots.diff.diff", return_value=plots_data)

    index_path = tmp_dir / "dvc_plots" / "index.html"
    mocker.patch("dvc_render.render_html", return_value=index_path)

    assert cmd.run() == 0
    mocked_open.assert_called_once_with(index_path.as_uri())

    out, _ = capsys.readouterr()
    assert index_path.as_uri() in out


def test_plots_diff_open_wsl(tmp_dir, dvc, mocker, plots_data):
    mocked_open = mocker.patch("webbrowser.open", return_value=True)
    mocked_uname_result = mocker.MagicMock()
    mocked_uname_result.release = "microsoft"
    mocker.patch("platform.uname", return_value=mocked_uname_result)

    cli_args = parse_args(["plots", "diff", "--targets", "plots.csv", "--open"])
    cmd = cli_args.func(cli_args)
    mocker.patch("dvc.repo.plots.diff.diff", return_value=plots_data)

    index_path = tmp_dir / "dvc_plots" / "index.html"
    mocker.patch("dvc_render.render_html", return_value=index_path)

    assert cmd.run() == 0
    mocked_open.assert_called_once_with(str(Path("dvc_plots") / "index.html"))


def test_plots_diff_open_failed(tmp_dir, dvc, mocker, capsys, plots_data):
    mocked_open = mocker.patch("webbrowser.open", return_value=False)
    cli_args = parse_args(["plots", "diff", "--targets", "plots.csv", "--open"])
    cmd = cli_args.func(cli_args)
    mocker.patch("dvc.repo.plots.diff.diff", return_value=plots_data)

    assert cmd.run() == 1
    expected_url = tmp_dir / "dvc_plots" / "index.html"
    mocked_open.assert_called_once_with(expected_url.as_uri())

    error_message = (
        f"Failed to open {expected_url.as_uri()}. Please try opening it manually."
    )

    out, err = capsys.readouterr()
    assert expected_url.as_uri() in out
    assert error_message in err


@pytest.mark.parametrize(
    "output, expected_url_path",
    [
        (
            "plots file with spaces",
            posixpath.join("plots%20file%20with%20spaces", "index.html"),
        ),
        (
            os.path.join("dir", "..", "plots"),
            posixpath.join("plots", "index.html"),
        ),
    ],
    ids=["quote", "resolve"],
)
def test_plots_path_is_quoted_and_resolved_properly(
    tmp_dir, dvc, mocker, capsys, output, expected_url_path, plots_data
):
    cli_args = parse_args(["plots", "diff", "--targets", "datafile", "--out", output])
    cmd = cli_args.func(cli_args)
    mocker.patch("dvc.repo.plots.diff.diff", return_value=plots_data)

    assert cmd.run() == 0
    expected_url = posixpath.join(tmp_dir.as_uri(), expected_url_path)

    out, _ = capsys.readouterr()
    assert expected_url in out


def test_should_pass_template_dir(tmp_dir, dvc, mocker, capsys):
    cli_args = parse_args(
        [
            "plots",
            "diff",
            "HEAD~1",
            "--json",
            "--targets",
            "plot.csv",
        ]
    )
    cmd = cli_args.func(cli_args)

    data = mocker.MagicMock()
    mocker.patch("dvc.repo.plots.diff.diff", return_value=data)

    renderers = mocker.MagicMock()
    match_renderers = mocker.patch(
        "dvc.render.match.match_defs_renderers", return_value=renderers
    )

    assert cmd.run() == 0

    match_renderers.assert_called_once_with(
        data=data,
        out="dvc_plots",
        templates_dir=str(tmp_dir / ".dvc/plots"),
    )


@pytest.mark.parametrize("output", ("some_out", os.path.join("to", "subdir"), None))
def test_should_call_render(tmp_dir, mocker, capsys, plots_data, output):
    cli_args = parse_args(["plots", "diff", "--targets", "plots.csv", "--out", output])
    cmd = cli_args.func(cli_args)
    mocker.patch("dvc.repo.plots.diff.diff", return_value=plots_data)

    output = output or "dvc_plots"
    index_path = tmp_dir / output / "index.html"
    renderer = mocker.MagicMock()
    mocker.patch(
        "dvc.render.match.match_defs_renderers",
        return_value=[RendererWithErrors(renderer, {}, {})],
    )
    render_mock = mocker.patch("dvc_render.render_html", return_value=index_path)

    assert cmd.run() == 0

    out, _ = capsys.readouterr()
    assert index_path.as_uri() in out

    render_mock.assert_called_once_with(
        renderers=[renderer],
        output_file=Path(tmp_dir / output / "index.html"),
        html_template=None,
    )


def test_plots_diff_json(dvc, mocker, capsys):
    cli_args = parse_args(
        [
            "plots",
            "diff",
            "HEAD~10",
            "HEAD~1",
            "--json",
            "--split",
            "--targets",
            "plot.csv",
            "-o",
            "out",
        ]
    )
    cmd = cli_args.func(cli_args)

    data = mocker.MagicMock()
    mocker.patch("dvc.repo.plots.diff.diff", return_value=data)

    renderers = mocker.MagicMock()
    mocker.patch("dvc.render.match.match_defs_renderers", return_value=renderers)
    render_mock = mocker.patch("dvc_render.render_html")

    show_json_mock = mocker.patch("dvc.commands.plots._show_json")

    assert cmd.run() == 0

    show_json_mock.assert_called_once_with(renderers, True, errors={})

    render_mock.assert_not_called()


@pytest.mark.parametrize(
    "target,expected_out,expected_rtn",
    (("t1", "\"{'t1'}\"", 0), (None, "t1\nt2", 0), ("t3", "", 1)),
)
def test_plots_templates(dvc, mocker, capsys, target, expected_out, expected_rtn):
    t1 = mocker.Mock()
    t1.DEFAULT_NAME = "t1"
    t1.DEFAULT_CONTENT = "{'t1'}"

    t2 = mocker.Mock()
    t2.DEFAULT_NAME = "t2"
    t2.DEFAULT_CONTENT = "{'t2'}"

    mocker.patch("dvc_render.vega_templates.TEMPLATES", [t1, t2])

    arguments = ["plots", "templates"]
    if target:
        arguments += [target]

    cli_args = parse_args(arguments)
    assert cli_args.func == CmdPlotsTemplates

    cmd = cli_args.func(cli_args)

    rtn = cmd.run()

    out, _ = capsys.readouterr()

    assert out.strip() == expected_out
    assert rtn == expected_rtn


@pytest.mark.parametrize("split", (True, False))
def test_show_json(split, mocker, capsys):
    import dvc.commands.plots

    renderer = mocker.MagicMock()
    renderer_obj = RendererWithErrors(renderer, {}, {})
    renderer.name = "rname"
    to_json_mock = mocker.patch(
        "dvc.render.convert.to_json", return_value={"renderer": "json"}
    )

    dvc.commands.plots._show_json([renderer_obj], split)

    to_json_mock.assert_called_once_with(renderer, split)

    out, _ = capsys.readouterr()
    assert json.dumps({"rname": {"renderer": "json"}}) in out


def test_show_json_no_renderers(capsys):
    import dvc.commands.plots

    dvc.commands.plots._show_json([])

    out, _ = capsys.readouterr()
    assert json.dumps({}) in out


def test_show_json_with_error(dvc, mocker, capsys):
    cli_args = parse_args(["plots", "show", "--json"])
    cmd = cli_args.func(cli_args)

    e = YAMLFileCorruptedError("dvc.yaml")
    data = set_in({}, ["workspace", "definitions", "error"], e)
    cmd._func = mocker.MagicMock(return_value=data)

    cmd.run()
    out, _ = capsys.readouterr()
    assert json.loads(out) == {
        "errors": [
            {
                "rev": "workspace",
                "type": type(e).__name__,
                "msg": e.args[0],
            }
        ]
    }




tests/unit/command/test_queue.py
import pytest

from dvc.cli import parse_args
from dvc.commands.queue.kill import CmdQueueKill
from dvc.commands.queue.logs import CmdQueueLogs
from dvc.commands.queue.remove import CmdQueueRemove
from dvc.commands.queue.start import CmdQueueStart
from dvc.commands.queue.status import CmdQueueStatus
from dvc.commands.queue.stop import CmdQueueStop
from dvc.exceptions import InvalidArgumentError


def test_experiments_remove_flags(dvc, scm, mocker):
    cli_args = parse_args(
        [
            "queue",
            "remove",
            "--queued",
            "--success",
            "--failed",
        ]
    )
    assert cli_args.func == CmdQueueRemove
    cmd = cli_args.func(cli_args)
    remove_mocker = mocker.patch(
        "dvc.repo.experiments.queue.celery.LocalCeleryQueue.clear",
        return_value={},
    )
    assert cmd.run() == 0
    remove_mocker.assert_called_once_with(
        success=True,
        failed=True,
        queued=True,
    )
    cli_args = parse_args(
        [
            "queue",
            "remove",
            "--all",
        ]
    )
    cmd = cli_args.func(cli_args)
    remove_mocker.reset_mock()
    assert cmd.run() == 0
    remove_mocker.assert_called_once_with(
        success=True,
        failed=True,
        queued=True,
    )


def test_experiments_remove_invalid(dvc, scm, mocker):
    cli_args = parse_args(["queue", "remove", "--queued", ["exp1", "exp2"]])
    cmd = cli_args.func(cli_args)
    with pytest.raises(InvalidArgumentError):
        assert cmd.run() == 0

    cli_args = parse_args(
        [
            "queue",
            "remove",
        ]
    )
    cmd = cli_args.func(cli_args)
    with pytest.raises(InvalidArgumentError):
        assert cmd.run() == 0


def test_experiments_remove_name(dvc, scm, mocker):
    cli_args = parse_args(
        [
            "queue",
            "remove",
            "exp1",
            "exp2",
        ]
    )
    assert cli_args.func == CmdQueueRemove
    cmd = cli_args.func(cli_args)
    remove_mocker = mocker.patch(
        "dvc.repo.experiments.queue.celery.LocalCeleryQueue.remove",
        return_value={},
    )
    assert cmd.run() == 0
    remove_mocker.assert_called_once_with(
        revs=["exp1", "exp2"],
    )


def test_experiments_kill(dvc, scm, mocker):
    cli_args = parse_args(
        [
            "queue",
            "kill",
            "--force",
            "exp1",
            "exp2",
        ]
    )
    assert cli_args.func == CmdQueueKill

    cmd = cli_args.func(cli_args)
    m = mocker.patch(
        "dvc.repo.experiments.queue.celery.LocalCeleryQueue.kill",
        return_value={},
    )

    assert cmd.run() == 0
    m.assert_called_once_with(revs=["exp1", "exp2"], force=True)


def test_experiments_start(dvc, scm, mocker):
    cli_args = parse_args(["queue", "start", "-j", "3"])
    assert cli_args.func == CmdQueueStart

    cmd = cli_args.func(cli_args)
    m = mocker.patch(
        "dvc.repo.experiments.queue.celery.LocalCeleryQueue._spawn_worker",
    )

    assert cmd.run() == 0
    assert m.call_count == 3


def test_experiments_stop(dvc, scm, mocker):
    cli_args = parse_args(
        [
            "queue",
            "stop",
            "--kill",
        ]
    )
    assert cli_args.func == CmdQueueStop

    cmd = cli_args.func(cli_args)
    m = mocker.patch(
        "dvc.repo.experiments.queue.celery.LocalCeleryQueue.shutdown",
    )

    assert cmd.run() == 0
    m.assert_called_once_with(kill=True)


@pytest.mark.parametrize(
    "worker_status, output",
    [
        (
            {"worker1": [], "worker2": []},
            "Worker status: 0 active, 2 idle",
        ),
        (
            {
                "worker1": [{"id": "1"}],
                "worker2": [{"id": "2"}],
                "worker3": [],
            },
            "Worker status: 2 active, 1 idle",
        ),
        (
            {"worker1": [{"id": "1"}]},
            "Worker status: 1 active, 0 idle",
        ),
    ],
)
def test_worker_status(dvc, scm, worker_status, output, mocker, capsys):
    cli_args = parse_args(
        [
            "queue",
            "status",
        ]
    )
    assert cli_args.func == CmdQueueStatus

    cmd = cli_args.func(cli_args)
    mocker.patch(
        "dvc.repo.experiments.queue.celery.LocalCeleryQueue.status",
        return_value=[],
    )
    m = mocker.patch(
        "dvc.repo.experiments.queue.celery.LocalCeleryQueue.worker_status",
        return_value=worker_status,
    )

    assert cmd.run() == 0
    m.assert_called_once_with()
    log, _ = capsys.readouterr()
    assert "No experiment tasks in the queue." in log
    assert output in log


def test_experiments_status(dvc, scm, mocker, capsys):
    from datetime import datetime

    cli_args = parse_args(
        [
            "queue",
            "status",
        ]
    )
    assert cli_args.func == CmdQueueStatus

    cmd = cli_args.func(cli_args)
    status_result = [
        {
            "rev": "c61a525a4ff39007301b4516fb6e54b323a0587b",
            "name": "I40",
            "timestamp": datetime(2022, 6, 9, 20, 49, 48),
            "status": "Queued",
        },
        {
            "rev": "8da9c339da30636261a3491a90aafdb760a4168f",
            "name": "I60",
            "timestamp": datetime(2022, 6, 9, 20, 49, 43),
            "status": "Running",
        },
    ]
    m = mocker.patch(
        "dvc.repo.experiments.queue.celery.LocalCeleryQueue.status",
        return_value=status_result,
    )

    assert cmd.run() == 0
    m.assert_called_once_with()
    log, _ = capsys.readouterr()
    assert "Task     Name    Created       Status" in log
    assert "c61a525  I40     Jun 09, 2022  Queued" in log
    assert "8da9c33  I60     Jun 09, 2022  Running" in log


def test_queue_logs(dvc, scm, mocker):
    cli_args = parse_args(["queue", "logs", "exp1", "-e", "utf8", "-f"])
    assert cli_args.func == CmdQueueLogs

    cmd = cli_args.func(cli_args)
    m = mocker.patch(
        "dvc.repo.experiments.queue.celery.LocalCeleryQueue.logs",
        return_value={},
    )

    assert cmd.run() == 0
    m.assert_called_once_with(rev="exp1", encoding="utf8", follow=True)




tests/unit/command/test_repro.py
from dvc.cli import parse_args
from dvc.commands.repro import CmdRepro

common_arguments = {
    "all_pipelines": False,
    "downstream": False,
    "dry": False,
    "force": False,
    "interactive": False,
    "pipeline": False,
    "single_item": False,
    "recursive": False,
    "force_downstream": False,
    "pull": False,
    "allow_missing": False,
    "targets": [],
}
repro_arguments = {
    "run_cache": True,
    "no_commit": False,
    "glob": False,
}


def test_default_arguments(dvc, mocker):
    cmd = CmdRepro(parse_args(["repro"]))
    mocker.patch.object(cmd.repo, "reproduce")
    cmd.run()
    cmd.repo.reproduce.assert_called_with(**common_arguments, **repro_arguments)


def test_downstream(dvc, mocker):
    cmd = CmdRepro(parse_args(["repro", "--downstream"]))
    mocker.patch.object(cmd.repo, "reproduce")
    cmd.run()
    arguments = common_arguments.copy()
    arguments.update(repro_arguments)
    arguments.update({"downstream": True})
    cmd.repo.reproduce.assert_called_with(**arguments)




tests/unit/command/test_stage.py
import pytest

from dvc.cli import parse_args
from dvc.commands.stage import CmdStageAdd
from tests.utils.asserts import called_once_with_subset


@pytest.mark.parametrize(
    "command, parsed_command",
    [
        (["echo", "foo", "bar"], "echo foo bar"),
        (["echo", '"foo bar"'], 'echo "foo bar"'),
        (["echo", "foo bar"], 'echo "foo bar"'),
        (["cmd", "--flag", ""], 'cmd --flag ""'),
    ],
)
def test_stage_add(mocker, dvc, command, parsed_command):
    cli_args = parse_args(
        [
            "stage",
            "add",
            "--name",
            "name",
            "--deps",
            "deps",
            "--outs",
            "outs",
            "--outs-no-cache",
            "outs-no-cache",
            "--metrics",
            "metrics",
            "--metrics-no-cache",
            "metrics-no-cache",
            "--plots",
            "plots",
            "--plots-no-cache",
            "plots-no-cache",
            "--wdir",
            "wdir",
            "--force",
            "--outs-persist",
            "outs-persist",
            "--outs-persist-no-cache",
            "outs-persist-no-cache",
            "--always-changed",
            "--params",
            "file:param1,param2",
            "--params",
            "param3",
            "--external",
            "--desc",
            "description",
            "--force",
            *command,
        ]
    )
    assert cli_args.func == CmdStageAdd

    cmd = cli_args.func(cli_args)
    m = mocker.patch.object(cmd.repo.stage, "add")

    assert cmd.run() == 0
    assert called_once_with_subset(
        m,
        name="name",
        deps=["deps"],
        outs=["outs"],
        outs_no_cache=["outs-no-cache"],
        params=[
            {"file": ["param1", "param2"]},
            {"params.yaml": ["param3"]},
        ],
        metrics=["metrics"],
        metrics_no_cache=["metrics-no-cache"],
        plots=["plots"],
        plots_no_cache=["plots-no-cache"],
        wdir="wdir",
        outs_persist=["outs-persist"],
        outs_persist_no_cache=["outs-persist-no-cache"],
        always_changed=True,
        external=True,
        desc="description",
        cmd=parsed_command,
        force=True,
    )


def test_stage_add_and_run(mocker, dvc):
    cli_args = parse_args(["stage", "add", "--run", "-n", "foo", "-o", "foo", "cmd"])
    cmd = cli_args.func(cli_args)
    add_mock = mocker.patch.object(cmd.repo.stage, "add")

    assert cmd.run() == 0

    assert called_once_with_subset(add_mock, name="foo", outs=["foo"], cmd="cmd")
    add_mock.return_value.run.assert_called_once()
    add_mock.return_value.dump.assert_called_once_with(update_pipeline=False)




tests/unit/command/test_status.py
import json

import pytest

from dvc.cli import parse_args
from dvc.commands.status import CmdDataStatus


def test_cloud_status(tmp_dir, dvc, mocker):
    cli_args = parse_args(
        [
            "status",
            "--cloud",
            "target1",
            "target2",
            "--jobs",
            "2",
            "--remote",
            "remote",
            "--all-branches",
            "--all-tags",
            "--all-commits",
            "--with-deps",
            "--recursive",
        ]
    )
    assert cli_args.func == CmdDataStatus

    cmd = cli_args.func(cli_args)
    m = mocker.patch.object(cmd.repo, "status", autospec=True, return_value={})

    assert cmd.run() == 0

    m.assert_called_once_with(
        cloud=True,
        targets=["target1", "target2"],
        jobs=2,
        remote="remote",
        all_branches=True,
        all_tags=True,
        all_commits=True,
        with_deps=True,
        recursive=True,
    )


@pytest.mark.parametrize("status", [{}, {"a": "b", "c": [1, 2, 3]}, [1, 2, 3]])
def test_status_show_json(dvc, mocker, capsys, status):
    cli_args = parse_args(["status", "--json"])
    assert cli_args.func == CmdDataStatus

    cmd = cli_args.func(cli_args)

    mocker.patch.object(cmd.repo, "status", autospec=True, return_value=status)

    assert cmd.run() == 0
    out, _ = capsys.readouterr()
    assert json.dumps(status) in out


@pytest.mark.parametrize(
    "status, ret", [({}, 0), ({"a": "b", "c": [1, 2, 3]}, 1), ([1, 2, 3], 1)]
)
def test_status_quiet(dvc, mocker, caplog, capsys, status, ret):
    cli_args = parse_args(["status", "-q"])
    assert cli_args.func == CmdDataStatus

    cmd = cli_args.func(cli_args)

    mocker.patch.object(cmd.repo, "status", autospec=True, return_value=status)
    caplog.clear()
    assert cmd.run() == ret
    assert not caplog.messages
    captured = capsys.readouterr()
    assert not captured.out


def test_status_empty(dvc, mocker, capsys):
    from dvc.repo.index import Index

    cli_args = parse_args(["status"])
    assert cli_args.func == CmdDataStatus

    cmd = cli_args.func(cli_args)

    spy = mocker.spy(Index, "from_repo")

    assert cmd.run() == 0

    captured = capsys.readouterr()
    assert "no data or pipelines tracked" in captured.out
    # stages should only be collected once
    assert spy.call_count == 1


@pytest.mark.parametrize(
    "cloud_opts, expected_message",
    [
        (["--cloud"], "Cache and remote 'default' are in sync"),
        (["--remote", "remote1"], "Cache and remote 'remote1' are in sync"),
        ([], "Data and pipelines are up to date"),
    ],
)
def test_status_up_to_date(dvc, mocker, capsys, cloud_opts, expected_message):
    from dvc.repo.index import Index

    cli_args = parse_args(["status", *cloud_opts])
    assert cli_args.func == CmdDataStatus

    cmd = cli_args.func(cli_args)

    mocker.patch.dict(cmd.repo.config, {"core": {"remote": "default"}})
    mocker.patch.object(cmd.repo, "status", autospec=True, return_value={})
    mocker.patch("dvc.repo.Repo.index", return_value=Index(dvc, [object()]))
    cmd.repo._reset = mocker.Mock()

    assert cmd.run() == 0
    captured = capsys.readouterr()
    assert expected_message in captured.out




tests/unit/command/test_update.py
from dvc.cli import parse_args
from dvc.commands.update import CmdUpdate


def test_update(dvc, mocker):
    cli_args = parse_args(
        [
            "update",
            "target1",
            "target2",
            "--rev",
            "REV",
            "--recursive",
            "-j",
            "8",
        ]
    )
    assert cli_args.func == CmdUpdate
    cmd = cli_args.func(cli_args)
    m = mocker.patch("dvc.repo.Repo.update")

    assert cmd.run() == 0

    m.assert_called_once_with(
        targets=["target1", "target2"],
        rev="REV",
        recursive=True,
        to_remote=False,
        no_download=False,
        remote=None,
        jobs=8,
    )


def test_update_to_remote(dvc, mocker):
    cli_args = parse_args(
        [
            "update",
            "target1",
            "target2",
            "--to-remote",
            "-j",
            "5",
            "-r",
            "remote",
            "--recursive",
        ]
    )
    assert cli_args.func == CmdUpdate
    cmd = cli_args.func(cli_args)
    m = mocker.patch("dvc.repo.Repo.update")

    assert cmd.run() == 0

    m.assert_called_once_with(
        targets=["target1", "target2"],
        rev=None,
        recursive=True,
        to_remote=True,
        no_download=False,
        remote="remote",
        jobs=5,
    )




tests/unit/command/ls/__init__.py




tests/unit/command/ls/test_ls.py
import json

from dvc.cli import parse_args
from dvc.commands.ls import CmdList


def _test_cli(mocker, *args):
    cli_args = parse_args(["list", *args])
    assert cli_args.func == CmdList

    cmd = cli_args.func(cli_args)
    m = mocker.patch("dvc.repo.Repo.ls")

    assert cmd.run() == 0
    return m


def test_list(mocker):
    url = "local_dir"
    m = _test_cli(mocker, url)
    m.assert_called_once_with(url, None, recursive=False, rev=None, dvc_only=False)


def test_list_recursive(mocker):
    url = "local_dir"
    m = _test_cli(mocker, url, "-R")
    m.assert_called_once_with(url, None, recursive=True, rev=None, dvc_only=False)


def test_list_git_ssh_rev(mocker):
    url = "git@github.com:repo"
    m = _test_cli(mocker, url, "--rev", "123")
    m.assert_called_once_with(url, None, recursive=False, rev="123", dvc_only=False)


def test_list_targets(mocker):
    url = "local_dir"
    target = "subdir"
    m = _test_cli(mocker, url, target)
    m.assert_called_once_with(url, target, recursive=False, rev=None, dvc_only=False)


def test_list_outputs_only(mocker):
    url = "local_dir"
    m = _test_cli(mocker, url, None, "--dvc-only")
    m.assert_called_once_with(url, None, recursive=False, rev=None, dvc_only=True)


def test_show_json(mocker, capsys):
    cli_args = parse_args(["list", "local_dir", "--json"])
    assert cli_args.func == CmdList

    cmd = cli_args.func(cli_args)

    result = [{"key": "val"}]
    mocker.patch("dvc.repo.Repo.ls", return_value=result)

    assert cmd.run() == 0
    out, _ = capsys.readouterr()
    assert json.dumps(result) in out


def test_show_colors(mocker, capsys, monkeypatch):
    cli_args = parse_args(["list", "local_dir"])
    assert cli_args.func == CmdList
    cmd = cli_args.func(cli_args)

    monkeypatch.setenv("LS_COLORS", "ex=01;32:rs=0:di=01;34:*.xml=01;31:*.dvc=01;33:")
    result = [
        {"isdir": False, "isexec": 0, "isout": False, "path": ".dvcignore"},
        {"isdir": False, "isexec": 0, "isout": False, "path": ".gitignore"},
        {"isdir": False, "isexec": 0, "isout": False, "path": "README.md"},
        {"isdir": True, "isexec": 0, "isout": True, "path": "data"},
        {"isdir": False, "isexec": 0, "isout": True, "path": "structure.xml"},
        {
            "isdir": False,
            "isexec": 0,
            "isout": False,
            "path": "structure.xml.dvc",
        },
        {"isdir": True, "isexec": 0, "isout": False, "path": "src"},
        {"isdir": False, "isexec": 1, "isout": False, "path": "run.sh"},
    ]
    mocker.patch("dvc.repo.Repo.ls", return_value=result)

    assert cmd.run() == 0
    out, _ = capsys.readouterr()
    entries = out.splitlines()

    assert entries == [
        ".dvcignore",
        ".gitignore",
        "README.md",
        "\x1b[01;34mdata\x1b[0m",
        "\x1b[01;31mstructure.xml\x1b[0m",
        "\x1b[01;33mstructure.xml.dvc\x1b[0m",
        "\x1b[01;34msrc\x1b[0m",
        "\x1b[01;32mrun.sh\x1b[0m",
    ]


def test_list_alias():
    cli_args = parse_args(["ls", "local_dir"])
    assert cli_args.func == CmdList




tests/unit/command/ls/test_ls_colors.py
from dvc.commands.ls.ls_colors import LsColors


def colorize(ls_colors):
    def _colorize(f, spec=""):
        fs_path = {
            "path": f,
            "isexec": "e" in spec,
            "isdir": "d" in spec,
            "isout": "o" in spec,
        }
        return ls_colors.format(fs_path)

    return _colorize


def test_ls_colors_out_file():
    ls_colors = LsColors(LsColors.default)
    assert colorize(ls_colors)("file", "o") == "file"


def test_ls_colors_out_dir():
    ls_colors = LsColors(LsColors.default)
    assert colorize(ls_colors)("dir", "do") == "\x1b[01;34mdir\x1b[0m"


def test_ls_colors_out_exec():
    ls_colors = LsColors(LsColors.default)
    assert colorize(ls_colors)("script.sh", "eo") == "\x1b[01;32mscript.sh\x1b[0m"


def test_ls_colors_out_ext():
    ls_colors = LsColors(LsColors.default + ":*.xml=01;33")
    assert colorize(ls_colors)("file.xml", "o") == "\x1b[01;33mfile.xml\x1b[0m"


def test_ls_colors_file():
    ls_colors = LsColors(LsColors.default)
    assert colorize(ls_colors)("file") == "file"


def test_ls_colors_dir():
    ls_colors = LsColors(LsColors.default)
    assert colorize(ls_colors)("dir", "d") == "\x1b[01;34mdir\x1b[0m"


def test_ls_colors_exec():
    ls_colors = LsColors(LsColors.default)
    assert colorize(ls_colors)("script.sh", "e") == "\x1b[01;32mscript.sh\x1b[0m"


def test_ls_colors_ext():
    ls_colors = LsColors(LsColors.default + ":*.xml=01;33")
    assert colorize(ls_colors)("file.xml") == "\x1b[01;33mfile.xml\x1b[0m"


def test_ls_repo_with_custom_color_env_defined(monkeypatch):
    monkeypatch.setenv("LS_COLORS", "rs=0:di=01;34:*.xml=01;31:*.dvc=01;33:")
    ls_colors = LsColors()
    colorizer = colorize(ls_colors)

    assert colorizer(".dvcignore") == ".dvcignore"
    assert colorizer(".gitignore") == ".gitignore"
    assert colorizer("README.md") == "README.md"
    assert colorizer("data", "d") == "\x1b[01;34mdata\x1b[0m"
    assert colorizer("structure.xml") == "\x1b[01;31mstructure.xml\x1b[0m"
    assert colorizer("structure.xml.dvc") == "\x1b[01;33mstructure.xml.dvc\x1b[0m"




tests/unit/data/__init__.py




tests/unit/data/db/__init__.py




tests/unit/data/db/test_local.py
import errno
import os

import pytest

from dvc.fs import LocalFileSystem
from dvc_data.hashfile.db.local import LocalHashFileDB
from dvc_data.hashfile.hash_info import HashInfo


def test_status_download_optimization(mocker, dvc):
    """When comparing the status to pull a remote cache,
    And the desired files to fetch are already on the local cache,
    Don't check the existence of the desired files on the remote cache
    """
    from dvc_data.hashfile.status import compare_status

    odb = LocalHashFileDB(LocalFileSystem(), os.getcwd())
    obj_ids = {
        HashInfo("md5", "acbd18db4cc2f85cedef654fccc4a4d8"),
        HashInfo("md5", "37b51d194a7513e45b56f6524f2d51f2"),
    }

    local_exists = [hash_info.value for hash_info in obj_ids]
    mocker.patch.object(odb, "oids_exist", return_value=local_exists)

    src_odb = mocker.Mock()

    compare_status(src_odb, odb, obj_ids, check_deleted=False)
    assert src_odb.oids_exist.call_count == 0


@pytest.mark.parametrize("link_name", ["hardlink", "symlink"])
def test_is_protected(tmp_dir, dvc, link_name):
    odb = dvc.cache.local
    fs = odb.fs
    link_method = getattr(fs, link_name)

    (tmp_dir / "foo").write_text("foo")

    foo = tmp_dir / "foo"
    link = tmp_dir / "link"

    link_method(foo, link)

    assert not odb.is_protected(foo)
    assert not odb.is_protected(link)

    odb.protect(foo)

    assert odb.is_protected(foo)
    assert odb.is_protected(link)

    odb.unprotect(link)

    assert not odb.is_protected(link)
    if os.name == "nt" and link_name == "hardlink":
        # NOTE: NTFS doesn't allow deleting read-only files, which forces us to
        # set write perms on the link, which propagates to the source.
        assert not odb.is_protected(foo)
    else:
        assert odb.is_protected(foo)


@pytest.mark.parametrize("err", [errno.EPERM, errno.EACCES, errno.EROFS])
def test_protect_ignore_errors(tmp_dir, dvc, mocker, err):
    tmp_dir.gen("foo", "foo")

    mock_chmod = mocker.patch("os.chmod", side_effect=OSError(err, "something"))
    dvc.cache.local.protect("foo")
    assert mock_chmod.called


@pytest.mark.parametrize("err", [errno.EPERM, errno.EACCES, errno.EROFS])
def test_set_exec_ignore_errors(tmp_dir, dvc, mocker, err):
    tmp_dir.gen("foo", "foo")

    mock_chmod = mocker.patch("os.chmod", side_effect=OSError(err, "something"))
    dvc.cache.local.set_exec("foo")
    assert mock_chmod.called


def test_staging_file(tmp_dir, dvc):
    from dvc_data.hashfile import check
    from dvc_data.hashfile.build import build
    from dvc_data.hashfile.transfer import transfer

    tmp_dir.gen("foo", "foo")
    fs = LocalFileSystem()

    local_odb = dvc.cache.local
    staging_odb, _, obj = build(local_odb, (tmp_dir / "foo").fs_path, fs, "md5")

    assert not local_odb.exists(obj.hash_info.value)
    assert staging_odb.exists(obj.hash_info.value)

    with pytest.raises(FileNotFoundError):
        check(local_odb, obj)
    check(staging_odb, obj)

    transfer(staging_odb, local_odb, {obj.hash_info}, hardlink=True)
    check(local_odb, obj)
    check(staging_odb, obj)

    path = local_odb.oid_to_path(obj.hash_info.value)
    assert fs.exists(path)


def test_staging_dir(tmp_dir, dvc):
    from dvc_data.hashfile import check
    from dvc_data.hashfile.build import build
    from dvc_data.hashfile.transfer import transfer

    tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})
    fs = LocalFileSystem()
    local_odb = dvc.cache.local

    staging_odb, _, obj = build(local_odb, (tmp_dir / "dir").fs_path, fs, "md5")

    assert not local_odb.exists(obj.hash_info.value)
    assert staging_odb.exists(obj.hash_info.value)

    with pytest.raises(FileNotFoundError):
        check(local_odb, obj)
    check(staging_odb, obj)

    transfer(staging_odb, local_odb, {obj.hash_info}, shallow=False, hardlink=True)
    check(local_odb, obj)
    check(staging_odb, obj)

    path = local_odb.oid_to_path(obj.hash_info.value)
    assert fs.exists(path)




tests/unit/dependency/__init__.py




tests/unit/dependency/test_dependency.py
import pytest

from dvc.dependency import Dependency
from dvc.stage import Stage


def test_save_missing(dvc, mocker):
    stage = Stage(dvc)
    dep = Dependency(stage, "path")
    mocker.patch.object(dep.fs, "exists", return_value=False)
    with pytest.raises(dep.DoesNotExistError):
        dep.save()




tests/unit/dependency/test_params.py
import pytest

from dvc.dependency import ParamsDependency, loadd_from, loads_params
from dvc.dependency.param import BadParamFileError, MissingParamsError
from dvc.stage import Stage
from dvc.utils.serialize import dump_toml, dump_yaml, load_yaml

PARAMS = {"foo": 1, "bar": 53.135, "baz": "str", "qux": None}
DEFAULT_PARAMS_FILE = ParamsDependency.DEFAULT_PARAMS_FILE


def test_loads_params(dvc):
    stage = Stage(dvc)
    deps = loads_params(
        stage,
        [
            "foo",
            "bar",
            {"a_file": ["baz", "bat"]},
            {"b_file": ["cat"]},
            {},
            {"a_file": ["foobar"]},
        ],
    )
    assert len(deps) == 3

    assert isinstance(deps[0], ParamsDependency)
    assert deps[0].def_path == ParamsDependency.DEFAULT_PARAMS_FILE
    assert deps[0].params == ["foo", "bar"]
    assert not deps[0].hash_info

    assert isinstance(deps[1], ParamsDependency)
    assert deps[1].def_path == "a_file"
    assert deps[1].params == ["baz", "bat", "foobar"]
    assert not deps[1].hash_info

    assert isinstance(deps[2], ParamsDependency)
    assert deps[2].def_path == "b_file"
    assert deps[2].params == ["cat"]
    assert not deps[2].hash_info


def test_loads_params_without_any_specific_targets(dvc):
    stage = Stage(dvc)
    deps = loads_params(
        stage,
        [
            "foo",
            {"params.yaml": None},
            {"a_file": []},
            {"b_file": ["baz"]},
            {"b_file": ["bat"]},
            {"a_file": ["foobar"]},
        ],
    )
    assert len(deps) == 3

    assert isinstance(deps[0], ParamsDependency)
    assert deps[0].def_path == ParamsDependency.DEFAULT_PARAMS_FILE
    assert deps[0].params == []
    assert not deps[0].hash_info

    assert isinstance(deps[1], ParamsDependency)
    assert deps[1].def_path == "a_file"
    assert deps[1].params == []
    assert not deps[1].hash_info

    assert isinstance(deps[2], ParamsDependency)
    assert deps[2].def_path == "b_file"
    assert deps[2].params == ["baz", "bat"]
    assert not deps[2].hash_info


@pytest.mark.parametrize(
    "params, errmsg",
    [
        ([3], "Only list of str/dict is supported. Got: 'int'"),
        (
            [{"b_file": "cat"}],
            "Expected list of params for custom params file 'b_file', got 'str'.",
        ),
    ],
)
def test_params_error(dvc, params, errmsg):
    with pytest.raises(ValueError, match=errmsg):
        loads_params(Stage(dvc), params)


def test_loadd_from(dvc):
    stage = Stage(dvc)
    deps = loadd_from(stage, [{"params": PARAMS}])
    assert len(deps) == 1
    assert isinstance(deps[0], ParamsDependency)
    assert deps[0].def_path == ParamsDependency.DEFAULT_PARAMS_FILE
    assert deps[0].params == list(PARAMS.keys())
    assert deps[0].hash_info.value == PARAMS


def test_dumpd_with_info(dvc):
    dep = ParamsDependency(Stage(dvc), None, PARAMS)
    assert dep.dumpd() == {"path": DEFAULT_PARAMS_FILE, "params": PARAMS}


def test_dumpd_without_info(dvc):
    dep = ParamsDependency(Stage(dvc), None, list(PARAMS.keys()))
    assert dep.dumpd() == {
        "path": DEFAULT_PARAMS_FILE,
        "params": list(PARAMS.keys()),
    }


def test_read_params_nonexistent_file(dvc):
    dep = ParamsDependency(Stage(dvc), None, ["foo"])
    assert dep.read_params() == {}


def test_read_params_unsupported_format(tmp_dir, dvc):
    tmp_dir.gen(DEFAULT_PARAMS_FILE, b"\0\1\2\3\4\5\6\7")
    dep = ParamsDependency(Stage(dvc), None, ["foo"])
    with pytest.raises(BadParamFileError):
        dep.read_params()


def test_read_params_nested(tmp_dir, dvc):
    dump_yaml(DEFAULT_PARAMS_FILE, {"some": {"path": {"foo": ["val1", "val2"]}}})
    dep = ParamsDependency(Stage(dvc), None, ["some.path.foo"])
    assert dep.read_params() == {"some.path.foo": ["val1", "val2"]}


def test_read_params_default_loader(tmp_dir, dvc):
    parameters_file = "parameters.foo"
    dump_yaml(parameters_file, {"some": {"path": {"foo": ["val1", "val2"]}}})
    dep = ParamsDependency(Stage(dvc), parameters_file, ["some.path.foo"])
    assert dep.read_params() == {"some.path.foo": ["val1", "val2"]}


def test_read_params_wrong_suffix(tmp_dir, dvc):
    parameters_file = "parameters.toml"
    dump_yaml(parameters_file, {"some": {"path": {"foo": ["val1", "val2"]}}})
    dep = ParamsDependency(Stage(dvc), parameters_file, ["some.path.foo"])
    with pytest.raises(BadParamFileError):
        dep.read_params()


def test_read_params_toml(tmp_dir, dvc):
    parameters_file = "parameters.toml"
    dump_toml(parameters_file, {"some": {"path": {"foo": ["val1", "val2"]}}})
    dep = ParamsDependency(Stage(dvc), parameters_file, ["some.path.foo"])
    assert dep.read_params() == {"some.path.foo": ["val1", "val2"]}


def test_read_params_py(tmp_dir, dvc):
    parameters_file = "parameters.py"
    tmp_dir.gen(
        parameters_file,
        (
            "INT: int = 5\n"
            "FLOAT = 0.001\n"
            "STR = 'abc'\n"
            "BOOL: bool = True\n"
            "DICT = {'a': 1}\n"
            "LIST = [1, 2, 3]\n"
            "SET = {4, 5, 6}\n"
            "TUPLE = (10, 100)\n"
            "NONE = None\n"
        ),
    )
    dep = ParamsDependency(
        Stage(dvc),
        parameters_file,
        [
            "INT",
            "FLOAT",
            "STR",
            "BOOL",
            "DICT",
            "LIST",
            "SET",
            "TUPLE",
            "NONE",
        ],
    )
    assert dep.read_params() == {
        "INT": 5,
        "FLOAT": 0.001,
        "STR": "abc",
        "BOOL": True,
        "DICT": {"a": 1},
        "LIST": [1, 2, 3],
        "SET": {4, 5, 6},
        "TUPLE": (10, 100),
        "NONE": None,
    }

    tmp_dir.gen(parameters_file, "class Train:\n    foo = 'val1'\n    bar = 'val2'\n")
    dep = ParamsDependency(Stage(dvc), parameters_file, ["Train.foo"])
    assert dep.read_params() == {"Train.foo": "val1"}

    dep = ParamsDependency(Stage(dvc), parameters_file, ["Train"])
    assert dep.read_params() == {"Train": {"foo": "val1", "bar": "val2"}}

    tmp_dir.gen(
        parameters_file,
        (
            "x = 4\n"
            "config.x = 3\n"
            "class Klass:\n"
            "    def __init__(self):\n"
            "        self.a = 'val1'\n"
            "        container.a = 2\n"
            "        self.container.a = 1\n"
            "        a = 'val2'\n"
        ),
    )
    dep = ParamsDependency(Stage(dvc), parameters_file, ["x", "Klass.a"])
    assert dep.read_params() == {"x": 4, "Klass.a": "val1"}


def test_params_py_tuple_status(tmp_dir, dvc):
    """https://github.com/iterative/dvc/issues/8803"""
    parameters_file = "parameters.py"
    tmp_dir.gen(parameters_file, "TUPLE = (10, 100)\n")
    dep = ParamsDependency(Stage(dvc), parameters_file, ["TUPLE"])
    # lock file uses YAML so the tuple will be loaded as a list
    dep.fill_values({"TUPLE": [10, 100]})
    assert dep.status() == {}
    dep.fill_values({"TUPLE": [11, 100]})
    assert dep.status() == {"parameters.py": {"TUPLE": "modified"}}
    dep.fill_values({"TUPLE": [10]})
    assert dep.status() == {"parameters.py": {"TUPLE": "modified"}}
    dep.fill_values({"TUPLE": {10: "foo", 100: "bar"}})
    assert dep.status() == {"parameters.py": {"TUPLE": "modified"}}


def test_get_hash_missing_config(dvc):
    dep = ParamsDependency(Stage(dvc), None, ["foo"])
    with pytest.raises(MissingParamsError):
        dep.get_hash()


def test_get_hash_missing_param(tmp_dir, dvc):
    tmp_dir.gen(DEFAULT_PARAMS_FILE, "bar: baz")
    dep = ParamsDependency(Stage(dvc), None, ["foo"])
    with pytest.raises(MissingParamsError):
        dep.get_hash()


@pytest.mark.parametrize("param_value", ["", "false", "[]", "{}", "null"])
def test_params_with_false_values(tmp_dir, dvc, param_value):
    """These falsy params values should not ignored by `status` on loading."""
    key = "param"
    dep = ParamsDependency(Stage(dvc), DEFAULT_PARAMS_FILE, [key])
    (tmp_dir / DEFAULT_PARAMS_FILE).write_text(f"{key}: {param_value}")

    dep.fill_values(load_yaml(DEFAULT_PARAMS_FILE))

    assert dep.status() == {}


def test_params_status_without_targets(tmp_dir, dvc):
    params_file = tmp_dir / "params.yaml"
    dep = ParamsDependency(Stage(dvc), str(params_file), [])

    assert dep.hash_info.value is None
    assert dep.status() == {"params.yaml": "deleted"}

    params_file.dump({"foo": "foo", "bar": "bar"})

    assert dep.status() == {"params.yaml": "new"}

    dep.fill_values({})
    assert dep.hash_info.value == {}
    assert dep.status() == {"params.yaml": {"bar": "new", "foo": "new"}}

    dep.fill_values({"foo": "foobar", "lorem": "ipsum"})
    assert dep.hash_info.value == {"foo": "foobar", "lorem": "ipsum"}
    assert dep.status() == {
        "params.yaml": {"bar": "new", "foo": "modified", "lorem": "deleted"}
    }




tests/unit/fs/__init__.py




tests/unit/fs/test_azure.py
import pytest
from dvc_azure import AzureAuthError, AzureFileSystem

container_name = "container-name"
connection_string = (
    "DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;"
    "AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsu"
    "Fq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;"
    "BlobEndpoint=http://127.0.0.1:10000/devstoreaccount1;"
)


def test_strip_protocol_env_var(monkeypatch, dvc):
    monkeypatch.setenv("AZURE_STORAGE_CONTAINER_NAME", container_name)
    monkeypatch.setenv("AZURE_STORAGE_CONNECTION_STRING", connection_string)

    assert AzureFileSystem._strip_protocol("azure://") == container_name


def test_strip_protocol(dvc):
    assert (
        AzureFileSystem._strip_protocol(f"azure://{container_name}") == container_name
    )


def test_init(dvc):
    config = {"connection_string": connection_string}
    fs = AzureFileSystem(**config)
    assert fs.fs_args["connection_string"] == connection_string


def test_azure_login_methods():
    def get_login_method(config):
        fs = AzureFileSystem(**config)
        return fs.login_method

    with pytest.raises(AzureAuthError):
        get_login_method({})

    assert get_login_method({"connection_string": "test"}) == "connection string"
    assert get_login_method({"account_name": "test"}).startswith("default credentials")
    assert (
        get_login_method({"account_name": "test", "allow_anonymous_login": True})
        == "anonymous login"
    )

    with pytest.raises(AzureAuthError):
        get_login_method(
            {"tenant_id": "test", "client_id": "test", "client_secret": "test"}
        )

    assert (
        get_login_method(
            {
                "account_name": "test",
                "tenant_id": "test",
                "client_id": "test",
                "client_secret": "test",
            }
        )
        == "AD service principal"
    )

    assert (
        get_login_method({"account_name": "test", "account_key": "test"})
        == "account key"
    )
    assert (
        get_login_method({"account_name": "test", "sas_token": "test"}) == "SAS token"
    )
    assert (
        get_login_method(
            {
                "connection_string": "test",
                "account_name": "test",
                "sas_token": "test",
            }
        )
        == "connection string"
    )
    assert (
        get_login_method({"connection_string": "test", "sas_token": "test"})
        == "connection string"
    )




tests/unit/fs/test_base.py
import pytest

from dvc.fs import FileSystem, RemoteMissingDepsError


def test_missing_deps(mocker):
    requires = {"missing": "missing"}
    mocker.patch.object(FileSystem, "REQUIRES", requires)
    with pytest.raises(RemoteMissingDepsError, match="missing dependencies"):
        FileSystem()




tests/unit/fs/test_data.py
import posixpath
import shutil

import pytest

import dvc_data
from dvc.fs import localfs
from dvc.fs.data import DataFileSystem
from dvc.utils.fs import remove
from dvc_data.hashfile.build import build
from dvc_data.hashfile.hash_info import HashInfo


@pytest.mark.parametrize(
    "path, key",
    [
        ("", ()),
        (".", ()),
        ("/", ()),
        ("foo", ("foo",)),
        ("dir/foo", ("dir", "foo")),
    ],
)
def test_get_key(tmp_dir, dvc, path, key):
    fs = DataFileSystem(index=dvc.index.data["repo"])
    assert fs.fs._get_key(path) == key


def test_exists(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    dvc.add("foo")
    (tmp_dir / "foo").unlink()

    fs = DataFileSystem(index=dvc.index.data["repo"])
    assert fs.exists("foo")


def test_open(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    dvc.add("foo")
    (tmp_dir / "foo").unlink()

    fs = DataFileSystem(index=dvc.index.data["repo"])
    with fs.open("foo", "r") as fobj:
        assert fobj.read() == "foo"


def test_open_dirty_hash(tmp_dir, dvc):
    tmp_dir.dvc_gen("file", "file")
    (tmp_dir / "file").write_text("something")

    fs = DataFileSystem(index=dvc.index.data["repo"])
    with fs.open("file", "r") as fobj:
        # NOTE: Unlike DVCFileSystem, DataFileSystem should not
        # be affected by a dirty workspace.
        assert fobj.read() == "file"


def test_open_no_remote(tmp_dir, dvc):
    tmp_dir.dvc_gen("file", "file")
    (tmp_dir / "file").unlink()
    remove(dvc.cache.local.path)

    fs = DataFileSystem(index=dvc.index.data["repo"])
    with pytest.raises(FileNotFoundError):
        with fs.open("file", "r"):
            pass


def test_open_dirty_no_hash(tmp_dir, dvc):
    tmp_dir.gen("file", "file")
    (tmp_dir / "file.dvc").write_text("outs:\n- path: file\n")

    fs = DataFileSystem(index=dvc.index.data["repo"])
    # NOTE: Unlike DVCFileSystem, DataFileSystem should not
    # be affected by a dirty workspace.
    with pytest.raises(FileNotFoundError):
        with fs.open("file", "r"):
            pass


def test_open_in_history(tmp_dir, scm, dvc):
    tmp_dir.gen("foo", "foo")
    dvc.add("foo")
    dvc.scm.add(["foo.dvc", ".gitignore"])
    dvc.scm.commit("foo")

    tmp_dir.gen("foo", "foofoo")
    dvc.add("foo")
    dvc.scm.add(["foo.dvc", ".gitignore"])
    dvc.scm.commit("foofoo")

    with dvc.switch("HEAD~1"):
        fs = DataFileSystem(index=dvc.index.data["repo"])
        with fs.open("foo", "r") as fobj:
            assert fobj.read() == "foo"


def test_isdir_isfile(tmp_dir, dvc):
    tmp_dir.gen({"datafile": "data", "datadir": {"foo": "foo", "bar": "bar"}})

    fs = DataFileSystem(index=dvc.index.data["repo"])
    assert not fs.isdir("datadir")
    assert not fs.isfile("datadir")
    assert not fs.isdir("datafile")
    assert not fs.isfile("datafile")

    dvc.add(["datadir", "datafile"])
    shutil.rmtree(tmp_dir / "datadir")
    (tmp_dir / "datafile").unlink()

    fs = DataFileSystem(index=dvc.index.data["repo"])
    assert fs.isdir("datadir")
    assert not fs.isfile("datadir")
    assert not fs.isdir("datafile")
    assert fs.isfile("datafile")


def test_isdir_mixed(tmp_dir, dvc):
    tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})

    dvc.add(str(tmp_dir / "dir" / "foo"))

    fs = DataFileSystem(index=dvc.index.data["repo"])
    assert fs.isdir("dir")
    assert not fs.isfile("dir")


def test_walk(tmp_dir, dvc):
    tmp_dir.gen(
        {
            "dir": {
                "subdir1": {"foo1": "foo1", "bar1": "bar1"},
                "subdir2": {"foo2": "foo2"},
                "foo": "foo",
                "bar": "bar",
            }
        }
    )

    dvc.add(localfs.find("dir"))
    fs = DataFileSystem(index=dvc.index.data["repo"])

    expected = [
        "dir/subdir1",
        "dir/subdir2",
        "dir/subdir1/foo1",
        "dir/subdir1/bar1",
        "dir/subdir2/foo2",
        "dir/foo",
        "dir/bar",
    ]

    actual = []
    for root, dirs, files in fs.walk("dir"):
        for entry in dirs + files:
            actual.append(posixpath.join(root, entry))

    assert set(actual) == set(expected)
    assert len(actual) == len(expected)


def test_walk_dir(tmp_dir, dvc):
    tmp_dir.gen(
        {
            "dir": {
                "subdir1": {"foo1": "foo1", "bar1": "bar1"},
                "subdir2": {"foo2": "foo2"},
                "foo": "foo",
                "bar": "bar",
            }
        }
    )

    dvc.add("dir")
    fs = DataFileSystem(index=dvc.index.data["repo"])

    expected = [
        "dir/subdir1",
        "dir/subdir2",
        "dir/subdir1/foo1",
        "dir/subdir1/bar1",
        "dir/subdir2/foo2",
        "dir/foo",
        "dir/bar",
    ]

    actual = []
    for root, dirs, files in fs.walk("dir"):
        for entry in dirs + files:
            actual.append(posixpath.join(root, entry))

    assert set(actual) == set(expected)
    assert len(actual) == len(expected)


def test_walk_missing(tmp_dir, dvc):
    fs = DataFileSystem(index=dvc.index.data["repo"])

    for _ in fs.walk("dir"):
        pass


def test_walk_not_a_dir(tmp_dir, dvc):
    tmp_dir.dvc_gen("foo", "foo")
    fs = DataFileSystem(index=dvc.index.data["repo"])

    for _ in fs.walk("foo"):
        pass


def test_get_hash_file(tmp_dir, dvc):
    tmp_dir.dvc_gen({"foo": "foo"})
    fs = DataFileSystem(index=dvc.index.data["repo"])
    assert fs.info("foo")["md5"] == "acbd18db4cc2f85cedef654fccc4a4d8"


def test_get_hash_dir(tmp_dir, dvc, mocker):
    tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar", "subdir": {"data": "data"}}})
    fs = DataFileSystem(index=dvc.index.data["repo"])
    hash_file_spy = mocker.spy(dvc_data.hashfile.hash, "hash_file")
    assert fs.info("dir")["md5"] == "8761c4e9acad696bee718615e23e22db.dir"
    assert not hash_file_spy.called


def test_get_hash_granular(tmp_dir, dvc):
    tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar", "subdir": {"data": "data"}}})
    fs = DataFileSystem(index=dvc.index.data["repo"])
    subdir = "dir/subdir"
    assert fs.info(subdir).get("md5") is None
    _, _, obj = build(dvc.cache.local, subdir, fs, "md5", dry_run=True)
    assert obj.hash_info == HashInfo("md5", "af314506f1622d107e0ed3f14ec1a3b5.dir")
    data = posixpath.join(subdir, "data")
    assert fs.info(data)["md5"] == "8d777f385d3dfec8815d20f7496026dc"
    _, _, obj = build(dvc.cache.local, data, fs, "md5", dry_run=True)
    assert obj.hash_info == HashInfo("md5", "8d777f385d3dfec8815d20f7496026dc")


def test_get_hash_dirty_file(tmp_dir, dvc):
    tmp_dir.dvc_gen("file", "file")
    (tmp_dir / "file").write_text("something")

    fs = DataFileSystem(index=dvc.index.data["repo"])
    expected = "8c7dd922ad47494fc02c388e12c00eac"
    assert fs.info("file").get("md5") == expected
    _, _, obj = build(dvc.cache.local, "file", fs, "md5", dry_run=True)
    assert obj.hash_info == HashInfo("md5", expected)


def test_get_hash_dirty_dir(tmp_dir, dvc):
    tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar"}})
    (tmp_dir / "dir" / "baz").write_text("baz")

    fs = DataFileSystem(index=dvc.index.data["repo"])
    expected = "5ea40360f5b4ec688df672a4db9c17d1.dir"
    assert fs.info("dir").get("md5") == expected
    _, _, obj = build(dvc.cache.local, "dir", fs, "md5", dry_run=True)
    assert obj.hash_info == HashInfo("md5", expected)




tests/unit/fs/test_dvc.py
import os
import posixpath
import shutil
from unittest import mock

import pytest

from dvc.fs import localfs
from dvc.fs.dvc import DVCFileSystem
from dvc.testing.tmp_dir import make_subrepo
from dvc_data.hashfile.build import build
from dvc_data.hashfile.hash_info import HashInfo


def test_exists(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    dvc.add("foo")
    (tmp_dir / "foo").unlink()

    fs = DVCFileSystem(repo=dvc)
    assert fs.exists("foo")


def test_open(tmp_dir, dvc):
    tmp_dir.gen("foo", "foo")
    dvc.add("foo")
    (tmp_dir / "foo").unlink()

    fs = DVCFileSystem(repo=dvc)
    with fs.open("foo", "r") as fobj:
        assert fobj.read() == "foo"


def test_open_dirty_hash(tmp_dir, dvc):
    tmp_dir.dvc_gen("file", "file")
    (tmp_dir / "file").write_text("something")

    fs = DVCFileSystem(repo=dvc)
    with fs.open("file", "r") as fobj:
        assert fobj.read() == "something"


def test_open_dirty_no_hash(tmp_dir, dvc):
    tmp_dir.gen("file", "file")
    (tmp_dir / "file.dvc").write_text("outs:\n- path: file\n")

    fs = DVCFileSystem(repo=dvc)
    with fs.open("file", "r") as fobj:
        assert fobj.read() == "file"


def test_open_in_history(tmp_dir, scm, dvc):
    tmp_dir.gen("foo", "foo")
    dvc.add("foo")
    dvc.scm.add(["foo.dvc", ".gitignore"])
    dvc.scm.commit("foo")

    tmp_dir.gen("foo", "foofoo")
    dvc.add("foo")
    dvc.scm.add(["foo.dvc", ".gitignore"])
    dvc.scm.commit("foofoo")

    with dvc.switch("HEAD~1"):
        fs = DVCFileSystem(repo=dvc)
        with fs.open("foo", "r") as fobj:
            assert fobj.read() == "foo"


def test_isdir_isfile(tmp_dir, dvc):
    tmp_dir.gen(
        {
            "datafile": "data",
            "datadir": {
                "foo": "foo",
                "bar": "bar",
            },
            "subdir": {
                "baz": "baz",
                "data": {
                    "abc": "abc",
                    "xyz": "xyz",
                },
            },
        },
    )

    fs = DVCFileSystem(repo=dvc)
    assert fs.isdir("datadir")
    assert not fs.isfile("datadir")
    assert not fs.isdvc("datadir")
    assert not fs.isdir("datafile")
    assert fs.isfile("datafile")
    assert not fs.isdvc("datafile")

    dvc.add(
        [
            "datadir",
            "datafile",
            os.path.join("subdir", "baz"),
            os.path.join("subdir", "data"),
        ]
    )
    shutil.rmtree(tmp_dir / "datadir")
    shutil.rmtree(tmp_dir / "subdir" / "data")
    (tmp_dir / "datafile").unlink()
    (tmp_dir / "subdir" / "baz").unlink()

    fs = DVCFileSystem(repo=dvc)
    assert fs.isdir("datadir")
    assert not fs.isfile("datadir")
    assert fs.isdvc("datadir")
    assert not fs.isdir("datafile")
    assert fs.isfile("datafile")
    assert fs.isdvc("datafile")

    assert fs.isdir("subdir")
    assert not fs.isfile("subdir")
    assert not fs.isdvc("subdir")
    assert fs.isfile("subdir/baz")
    assert fs.isdir("subdir/data")


def test_exists_isdir_isfile_dirty(tmp_dir, dvc):
    tmp_dir.dvc_gen({"datafile": "data", "datadir": {"foo": "foo", "bar": "bar"}})

    fs = DVCFileSystem(repo=dvc)
    shutil.rmtree(tmp_dir / "datadir")
    (tmp_dir / "datafile").unlink()

    assert fs.exists("datafile")
    assert fs.exists("datadir")
    assert fs.exists("datadir/foo")
    assert fs.isfile("datafile")
    assert not fs.isfile("datadir")
    assert fs.isfile("datadir/foo")
    assert not fs.isdir("datafile")
    assert fs.isdir("datadir")
    assert not fs.isdir("datadir/foo")

    # NOTE: creating file instead of dir and dir instead of file
    tmp_dir.gen({"datadir": "data", "datafile": {"foo": "foo", "bar": "bar"}})
    assert fs.exists("datafile")
    assert fs.exists("datadir")
    assert not fs.exists("datadir/foo")
    assert fs.exists("datafile/foo")
    assert not fs.isfile("datafile")
    assert fs.isfile("datadir")
    assert not fs.isfile("datadir/foo")
    assert fs.isfile("datafile/foo")
    assert fs.isdir("datafile")
    assert not fs.isdir("datadir")
    assert not fs.isdir("datadir/foo")
    assert not fs.isdir("datafile/foo")


def test_isdir_mixed(tmp_dir, dvc):
    tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})

    dvc.add(str(tmp_dir / "dir" / "foo"))

    fs = DVCFileSystem(repo=dvc)
    assert fs.isdir("dir")
    assert not fs.isfile("dir")


@pytest.mark.parametrize(
    "dvcfiles,extra_expected",
    [
        (False, []),
        (
            True,
            [
                "dir/subdir1/foo1.dvc",
                "dir/subdir1/bar1.dvc",
                "dir/subdir2/foo2.dvc",
            ],
        ),
    ],
)
def test_walk(tmp_dir, dvc, dvcfiles, extra_expected):
    tmp_dir.gen(
        {
            "dir": {
                "subdir1": {"foo1": "foo1", "bar1": "bar1"},
                "subdir2": {"foo2": "foo2"},
            }
        }
    )
    dvc.add(localfs.find("dir"))
    tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})
    fs = DVCFileSystem(repo=dvc)

    expected = [
        "dir/subdir1",
        "dir/subdir2",
        "dir/subdir1/foo1",
        "dir/subdir1/bar1",
        "dir/subdir2/foo2",
        "dir/foo",
        "dir/bar",
    ]

    actual = []
    for root, dirs, files in fs.walk("dir", dvcfiles=dvcfiles):
        for entry in dirs + files:
            actual.append(posixpath.join(root, entry))

    expected += extra_expected
    assert set(actual) == set(expected)
    assert len(actual) == len(expected)


def test_walk_dirty(tmp_dir, dvc):
    tmp_dir.dvc_gen(
        {
            "dir": {
                "foo": "foo",
                "subdir1": {"foo1": "foo1", "bar1": "bar1"},
                "subdir2": {"foo2": "foo2"},
            }
        }
    )
    tmp_dir.gen({"dir": {"bar": "bar", "subdir3": {"foo3": "foo3"}}})
    (tmp_dir / "dir" / "foo").unlink()

    fs = DVCFileSystem(repo=dvc)
    expected = [
        "dir/subdir1",
        "dir/subdir2",
        "dir/subdir3",
        "dir/subdir1/foo1",
        "dir/subdir1/bar1",
        "dir/subdir2/foo2",
        "dir/subdir3/foo3",
        "dir/bar",
        "dir/foo",
    ]

    actual = []
    for root, dirs, files in fs.walk("dir"):
        for entry in dirs + files:
            actual.append(posixpath.join(root, entry))

    assert set(actual) == set(expected)
    assert len(actual) == len(expected)


def test_walk_dirty_cached_dir(tmp_dir, scm, dvc):
    tmp_dir.dvc_gen({"data": {"foo": "foo", "bar": "bar"}}, commit="add data")
    (tmp_dir / "data" / "foo").unlink()

    fs = DVCFileSystem(repo=dvc)

    actual = []
    for root, dirs, files in fs.walk("data"):
        for entry in dirs + files:
            actual.append(posixpath.join(root, entry))

    expected = ["data/foo", "data/bar"]
    assert set(actual) == set(expected)
    assert len(actual) == len(expected)


def test_walk_mixed_dir(tmp_dir, scm, dvc):
    tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})
    tmp_dir.dvc.add(os.path.join("dir", "foo"))
    tmp_dir.scm.add(
        [
            os.path.join("dir", "bar"),
            os.path.join("dir", ".gitignore"),
            os.path.join("dir", "foo.dvc"),
        ]
    )
    tmp_dir.scm.commit("add dir")

    fs = DVCFileSystem(repo=dvc)

    expected = [
        "dir/foo",
        "dir/bar",
        "dir/.gitignore",
    ]
    actual = []
    for root, dirs, files in fs.walk("dir"):
        for entry in dirs + files:
            actual.append(posixpath.join(root, entry))

    assert set(actual) == set(expected)
    assert len(actual) == len(expected)


def test_walk_missing(tmp_dir, dvc):
    fs = DVCFileSystem(repo=dvc)

    for _ in fs.walk("dir"):
        pass


def test_walk_not_a_dir(tmp_dir, dvc):
    tmp_dir.dvc_gen("foo", "foo")
    fs = DVCFileSystem(repo=dvc)

    for _ in fs.walk("foo"):
        pass


def test_isdvc(tmp_dir, dvc):
    tmp_dir.gen({"foo": "foo", "bar": "bar", "dir": {"baz": "baz"}})
    dvc.add("foo")
    dvc.add("dir")
    fs = DVCFileSystem(repo=dvc)
    assert fs.isdvc("foo")
    assert not fs.isdvc("bar")
    assert fs.isdvc("dir")
    assert fs.isdvc("dir/baz")
    assert fs.isdvc("dir/baz", recursive=True)


def test_subrepos(tmp_dir, scm, dvc, mocker):
    tmp_dir.scm_gen(
        {"dir": {"repo.txt": "file to confuse DVCFileSystem"}},
        commit="dir/repo.txt",
    )

    subrepo1 = tmp_dir / "dir" / "repo"
    subrepo2 = tmp_dir / "dir" / "repo2"

    for repo in [subrepo1, subrepo2]:
        make_subrepo(repo, scm)

    with subrepo1.chdir():
        subrepo1.dvc_gen({"foo": "foo", "dir1": {"bar": "bar"}}, commit="FOO")
    with subrepo2.chdir():
        subrepo2.dvc_gen({"lorem": "lorem", "dir2": {"ipsum": "ipsum"}}, commit="BAR")

    dvc._reset()
    fs = DVCFileSystem(repo=dvc, subrepos=True)

    def assert_fs_belongs_to_repo(ret_val):
        method = fs.fs._get_repo

        def f(*args, **kwargs):
            r = method(*args, **kwargs)
            assert r.root_dir == ret_val.root_dir
            return r

        return f

    with mock.patch.object(
        fs.fs, "_get_repo", side_effect=assert_fs_belongs_to_repo(subrepo1.dvc)
    ):
        assert fs.exists("dir/repo/foo") is True
        assert fs.exists("dir/repo/bar") is False

        assert fs.isfile("dir/repo/foo") is True
        assert fs.isfile("dir/repo/dir1/bar") is True
        assert fs.isfile("dir/repo/dir1") is False

        assert fs.isdir("dir/repo/dir1") is True
        assert fs.isdir("dir/repo/dir1/bar") is False
        assert fs.isdvc("dir/repo/foo") is True

    with mock.patch.object(
        fs.fs, "_get_repo", side_effect=assert_fs_belongs_to_repo(subrepo2.dvc)
    ):
        assert fs.exists("dir/repo2/lorem") is True
        assert fs.exists("dir/repo2/ipsum") is False

        assert fs.isfile("dir/repo2/lorem") is True
        assert fs.isfile("dir/repo2/dir2/ipsum") is True
        assert fs.isfile("dir/repo2/dir2") is False

        assert fs.isdir("dir/repo2/dir2") is True
        assert fs.isdir("dir/repo2/dir2/ipsum") is False
        assert fs.isdvc("dir/repo2/lorem") is True


@pytest.mark.parametrize(
    "dvcfiles,extra_expected",
    [
        (False, []),
        (
            True,
            [
                "dir/repo/foo.dvc",
                "dir/repo/.dvcignore",
                "dir/repo/dir1.dvc",
                "dir/repo2/.dvcignore",
                "dir/repo2/lorem.dvc",
                "dir/repo2/dir2.dvc",
            ],
        ),
    ],
)
def test_subrepo_walk(tmp_dir, scm, dvc, dvcfiles, extra_expected):
    tmp_dir.scm_gen(
        {"dir": {"repo.txt": "file to confuse DVCFileSystem"}},
        commit="dir/repo.txt",
    )

    subrepo1 = tmp_dir / "dir" / "repo"
    subrepo2 = tmp_dir / "dir" / "repo2"

    subdirs = [subrepo1, subrepo2]
    for dir_ in subdirs:
        make_subrepo(dir_, scm)

    with subrepo1.chdir():
        subrepo1.dvc_gen({"foo": "foo", "dir1": {"bar": "bar"}}, commit="FOO")
    with subrepo2.chdir():
        subrepo2.dvc_gen({"lorem": "lorem", "dir2": {"ipsum": "ipsum"}}, commit="BAR")

    # using fs that does not have dvcignore
    dvc._reset()
    fs = DVCFileSystem(repo=dvc)
    expected = [
        "dir/repo",
        "dir/repo.txt",
        "dir/repo2",
        "dir/repo/.gitignore",
        "dir/repo/foo",
        "dir/repo/dir1",
        "dir/repo/dir1/bar",
        "dir/repo2/.gitignore",
        "dir/repo2/lorem",
        "dir/repo2/dir2",
        "dir/repo2/dir2/ipsum",
    ]

    actual = []
    for root, dirs, files in fs.walk(
        "dir",
        dvcfiles=dvcfiles,
        ignore_subrepos=False,
    ):
        for entry in dirs + files:
            actual.append(posixpath.join(root, entry))

    expected += extra_expected
    assert set(actual) == set(expected)
    assert len(actual) == len(expected)


def test_dvcfs_no_subrepos(tmp_dir, dvc, scm):
    tmp_dir.scm_gen(
        {"dir": {"repo.txt": "file to confuse DVCFileSystem"}},
        commit="dir/repo.txt",
    )
    tmp_dir.dvc_gen({"lorem": "lorem"}, commit="add foo")

    subrepo = tmp_dir / "dir" / "repo"
    make_subrepo(subrepo, scm)
    with subrepo.chdir():
        subrepo.dvc_gen({"foo": "foo", "dir1": {"bar": "bar"}}, commit="FOO")
        subrepo.scm_gen({"ipsum": "ipsum"}, commit="BAR")

    # using fs that does not have dvcignore
    dvc._reset()
    fs = DVCFileSystem(repo=dvc)
    expected = [
        "/.dvcignore",
        "/.gitignore",
        "/lorem",
        "/lorem.dvc",
        "/dir",
        "/dir/repo.txt",
    ]

    actual = []
    for root, dirs, files in fs.walk("/", dvcfiles=True):
        for entry in dirs + files:
            actual.append(posixpath.join(root, entry))

    assert set(actual) == set(expected)
    assert len(actual) == len(expected)

    assert fs.isfile("lorem") is True
    assert fs.isfile("dir/repo/foo") is False
    assert fs.isdir("dir/repo") is False
    assert fs.isdir("dir") is True

    assert fs.isdvc("lorem") is True
    assert fs.isdvc("dir/repo/dir1") is False

    assert fs.exists("dir/repo.txt") is True
    assert fs.exists("repo/ipsum") is False


def test_get_hash_cached_file(tmp_dir, dvc, mocker):
    tmp_dir.dvc_gen({"foo": "foo"})
    fs = DVCFileSystem(repo=dvc)
    expected = "acbd18db4cc2f85cedef654fccc4a4d8"
    assert fs.info("foo").get("md5") is None
    _, _, obj = build(dvc.cache.local, "foo", fs, "md5")
    assert obj.hash_info == HashInfo("md5", expected)
    (tmp_dir / "foo").unlink()
    assert fs.info("foo")["md5"] == expected


def test_get_hash_cached_dir(tmp_dir, dvc, mocker):
    tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar", "subdir": {"data": "data"}}})
    fs = DVCFileSystem(repo=dvc)
    expected = "8761c4e9acad696bee718615e23e22db.dir"
    assert fs.info("dir").get("md5") is None
    _, _, obj = build(dvc.cache.local, "dir", fs, "md5")
    assert obj.hash_info == HashInfo("md5", "8761c4e9acad696bee718615e23e22db.dir")

    shutil.rmtree(tmp_dir / "dir")
    assert fs.info("dir")["md5"] == expected
    _, _, obj = build(dvc.cache.local, "dir", fs, "md5")
    assert obj.hash_info == HashInfo("md5", "8761c4e9acad696bee718615e23e22db.dir")


def test_get_hash_cached_granular(tmp_dir, dvc, mocker):
    tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar", "subdir": {"data": "data"}}})
    fs = DVCFileSystem(repo=dvc)
    subdir = "dir/subdir"
    assert fs.info(subdir).get("md5") is None
    _, _, obj = build(dvc.cache.local, subdir, fs, "md5")
    assert obj.hash_info == HashInfo("md5", "af314506f1622d107e0ed3f14ec1a3b5.dir")
    assert fs.info(posixpath.join(subdir, "data")).get("md5") is None
    _, _, obj = build(dvc.cache.local, posixpath.join(subdir, "data"), fs, "md5")
    assert obj.hash_info == HashInfo("md5", "8d777f385d3dfec8815d20f7496026dc")
    (tmp_dir / "dir" / "subdir" / "data").unlink()
    assert (
        fs.info(posixpath.join(subdir, "data"))["md5"]
        == "8d777f385d3dfec8815d20f7496026dc"
    )


def test_get_hash_mixed_dir(tmp_dir, scm, dvc):
    tmp_dir.gen({"dir": {"foo": "foo", "bar": "bar"}})
    tmp_dir.dvc.add(os.path.join("dir", "foo"))
    tmp_dir.scm.add(
        [
            os.path.join("dir", "bar"),
            os.path.join("dir", ".gitignore"),
            os.path.join("dir", "foo.dvc"),
        ]
    )
    tmp_dir.scm.commit("add dir")

    fs = DVCFileSystem(repo=dvc)
    _, _, obj = build(dvc.cache.local, "dir", fs, "md5")
    assert obj.hash_info == HashInfo("md5", "e1d9e8eae5374860ae025ec84cfd85c7.dir")


def test_get_hash_dirty_file(tmp_dir, dvc):
    from dvc_data.hashfile import check
    from dvc_data.hashfile.hash import hash_file

    tmp_dir.dvc_gen("file", "file")
    file_hash_info = HashInfo("md5", "8c7dd922ad47494fc02c388e12c00eac")

    (tmp_dir / "file").write_text("something")
    something_hash_info = HashInfo("md5", "437b930db84b8079c2dd804a71936b5f")

    # file is modified in workspace
    # hash_file(file) should return workspace hash, not DVC cached hash
    fs = DVCFileSystem(repo=dvc)
    assert fs.info("file").get("md5") is None
    staging, _, obj = build(dvc.cache.local, "file", fs, "md5")
    assert obj.hash_info == something_hash_info
    check(staging, obj)

    # hash_file(file) should return DVC cached hash
    (tmp_dir / "file").unlink()
    assert fs.info("file")["md5"] == file_hash_info.value
    _, hash_info = hash_file("file", fs, "md5", state=dvc.state)
    assert hash_info == file_hash_info

    # tmp_dir/file can be built even though it is missing in workspace since
    # repofs will use the DVC cached hash (and refer to the local cache object)
    _, _, obj = build(dvc.cache.local, "file", fs, "md5")
    assert obj.hash_info == file_hash_info


def test_get_hash_dirty_dir(tmp_dir, dvc):
    tmp_dir.dvc_gen({"dir": {"foo": "foo", "bar": "bar"}})
    (tmp_dir / "dir" / "baz").write_text("baz")

    fs = DVCFileSystem(repo=dvc)
    _, meta, obj = build(dvc.cache.local, "dir", fs, "md5")
    assert obj.hash_info == HashInfo("md5", "ba75a2162ca9c29acecb7957105a0bc2.dir")
    assert meta.nfiles == 3


@pytest.mark.parametrize("traverse_subrepos", [True, False])
def test_walk_nested_subrepos(tmp_dir, dvc, scm, traverse_subrepos):
    # generate a dvc and fs structure, with suffix based on repo's basename
    def fs_structure(suffix):
        return {
            f"foo-{suffix}": f"foo-{suffix}",
            f"dir-{suffix}": {f"bar-{suffix}": f"bar-{suffix}"},
        }

    def dvc_structure(suffix):
        return {
            f"lorem-{suffix}": f"lorem-{suffix}",
            f"dvc-{suffix}": {f"ipsum-{suffix}": f"ipsum-{suffix}"},
        }

    paths = ["subrepo1", "subrepo2", os.path.join("subrepo1", "subrepo3")]
    subrepos = [tmp_dir / path for path in paths]
    for repo_dir in subrepos:
        make_subrepo(repo_dir, scm)

    extras = {".gitignore"}  # these files are always there
    expected = {}
    for repo_dir in [*subrepos, tmp_dir]:
        base = os.path.basename(repo_dir)
        scm_files = fs_structure(base)
        dvc_files = dvc_structure(base)
        with repo_dir.chdir():
            repo_dir.scm_gen(scm_files, commit=f"git add in {repo_dir}")
            repo_dir.dvc_gen(dvc_files, commit=f"dvc add in {repo_dir}")

        if traverse_subrepos or repo_dir == tmp_dir:
            repo_dir_path = (
                "/" + repo_dir.relative_to(tmp_dir).as_posix()
                if repo_dir != tmp_dir
                else "/"
            )
            expected[repo_dir_path] = set(scm_files.keys() | dvc_files.keys() | extras)
            # files inside a dvc directory
            expected[posixpath.join(repo_dir_path, f"dvc-{base}")] = {f"ipsum-{base}"}
            # files inside a git directory
            expected[posixpath.join(repo_dir_path, f"dir-{base}")] = {f"bar-{base}"}

    if traverse_subrepos:
        # update subrepos
        expected["/"].update(["subrepo1", "subrepo2"])
        expected["/subrepo1"].add("subrepo3")

    actual = {}
    fs = DVCFileSystem(repo=dvc)
    for root, dirs, files in fs.walk("/", ignore_subrepos=not traverse_subrepos):
        actual[root] = set(dirs + files)
    assert expected == actual




tests/unit/fs/test_dvc_info.py
import os

import pytest

from dvc.fs.dvc import DVCFileSystem
from dvc.testing.tmp_dir import make_subrepo


@pytest.fixture
def dvcfs(tmp_dir, dvc, scm):
    fs_structure = {
        "models": {  # mixed dvc + git directory
            "train.py": "train dot py",
            "test.py": "test dot py",
        },
        "README.md": "my little project",  # file
        "src": {  # repo-only directory
            "utils": {
                "__init__.py": "",
                "serve_model.py": "# this will serve a model `soon`",
            }
        },
    }
    dvc_structure = {
        "data": {  # dvc only directory
            "raw": {
                "raw-1.csv": "one, dot, csv",
                "raw-2.csv": "two, dot, csv",
            },
            "processed": {
                "processed-1.csv": "1, dot, csv",
                "processed-2.csv": "2, dot, csv",
            },
        },
        os.path.join("models", "transform.pickle"): "model model",  # file
    }

    tmp_dir.scm_gen(fs_structure, commit="repo init")
    tmp_dir.dvc_gen(dvc_structure, commit="use dvc")

    return DVCFileSystem(repo=dvc, subrepos=True)


def test_info_not_existing(dvcfs):
    with pytest.raises(FileNotFoundError):
        dvcfs.info("path/that/does/not/exist")


@pytest.mark.parametrize(
    "path",
    [
        "README.md",
        "models/train.py",
        "models/test.py",
        "src/utils/__init__.py",
        "src/utils/serve_model.py",
    ],
)
def test_info_git_tracked_file(dvcfs, path):
    info = dvcfs.info(path)

    assert info["repo"].root_dir == dvcfs.repo.root_dir
    assert "dvc_info" not in info
    assert info["type"] == "file"
    assert not info["isexec"]


@pytest.mark.parametrize(
    "path",
    [
        "data/raw/raw-1.csv",
        "data/raw/raw-2.csv",
        "data/processed/processed-1.csv",
        "data/processed/processed-2.csv",
        "models/transform.pickle",
    ],
)
def test_info_dvc_tracked_file(dvcfs, path):
    info = dvcfs.info(path)

    assert info["repo"].root_dir == dvcfs.repo.root_dir
    assert info["dvc_info"]["isdvc"]
    assert info["type"] == "file"
    assert not info["isexec"]


@pytest.mark.parametrize("path", ["src", "src/utils"])
def test_info_git_only_dirs(dvcfs, path):
    info = dvcfs.info(path)

    assert info["repo"].root_dir == dvcfs.repo.root_dir
    assert "dvc_info" not in info
    assert info["type"] == "directory"
    assert not info["isexec"]


@pytest.mark.parametrize("path", [".", "models"])
def test_info_git_dvc_mixed_dirs(dvcfs, path):
    info = dvcfs.info(path)

    assert info["repo"].root_dir == dvcfs.repo.root_dir
    assert not info["dvc_info"]["isdvc"]
    assert info["type"] == "directory"
    assert not info["isexec"]


@pytest.mark.parametrize(
    "path",
    [
        "data",
        "data/raw",
        "data/processed",
    ],
)
def test_info_dvc_only_dirs(dvcfs, path):
    info = dvcfs.info(path)

    assert info["repo"].root_dir == dvcfs.repo.root_dir
    assert info["dvc_info"]["isdvc"]
    assert info["type"] == "directory"
    assert not info["isexec"]


def test_info_on_subrepos(make_tmp_dir, tmp_dir, dvc, scm, dvcfs):
    subrepo = tmp_dir / "subrepo"
    make_subrepo(subrepo, scm)
    with subrepo.chdir():
        subrepo.scm_gen("foo", "foo", commit="add foo on subrepo")
        subrepo.dvc_gen("foobar", "foobar", commit="add foobar on subrepo")

    for path in [
        "subrepo",
        "subrepo/foo",
        "subrepo/foobar",
    ]:
        info = dvcfs.info(path)
        assert info["repo"].root_dir == str(
            subrepo
        ), f"repo root didn't match for {path}"




tests/unit/fs/test_fs.py
from dataclasses import dataclass

import pytest
from dvc_http import HTTPFileSystem, HTTPSFileSystem
from dvc_s3 import S3FileSystem
from dvc_ssh import SSHFileSystem

from dvc.config import RemoteNotFoundError
from dvc.fs import LocalFileSystem, get_cloud_fs, get_fs_cls, get_fs_config

url_cls_pairs = [
    ("s3://bucket/path", S3FileSystem),
    ("ssh://example.com:/dir/path", SSHFileSystem),
    ("http://example.com/path/to/file", HTTPFileSystem),
    ("https://example.com/path/to/file", HTTPSFileSystem),
    ("path/to/file", LocalFileSystem),
    ("path\\to\\file", LocalFileSystem),
    ("file", LocalFileSystem),
    ("./file", LocalFileSystem),
    (".\\file", LocalFileSystem),
    ("../file", LocalFileSystem),
    ("..\\file", LocalFileSystem),
    ("unknown://path", LocalFileSystem),
]


try:
    from dvc_hdfs import HDFSFileSystem

    url_cls_pairs += [("hdfs://example.com/dir/path", HDFSFileSystem)]
except ImportError:
    pass


@pytest.mark.parametrize("url, cls", url_cls_pairs)
def test_get_fs_cls(url, cls):
    assert get_fs_cls({"url": url}) == cls


def test_get_fs_config():
    result = get_fs_config({}, url="ssh://example.com:/dir/path")
    assert result == {"url": "ssh://example.com:/dir/path"}


def test_get_fs_config_error():
    with pytest.raises(RemoteNotFoundError):
        get_fs_config({"remote": {}}, name="myremote")


def test_remote_url():
    config = {
        "remote": {
            "base": {"url": "http://example.com"},
            "r1": {"url": "remote://base/r1", "user": "user"},
            "r2": {"url": "remote://r1/r2", "password": "123"},
        }
    }
    result = get_fs_config(config, url="remote://r2/foo")
    assert result == {
        "password": "123",
        "user": "user",
        "url": "http://example.com/r1/r2/foo",
    }


@dataclass
class FakeRepo:
    config: dict


def test_get_cloud_fs():
    repo = FakeRepo({})
    cls, config, path = get_cloud_fs(repo, url="ssh://example.com:/dir/path")
    assert cls is SSHFileSystem
    assert config == {"host": "example.com", "verify": False}
    assert path == "/dir/path"




tests/unit/fs/test_path.py
import pytest

from dvc.fs import Path


@pytest.mark.parametrize("prefix", ["", "/"])
@pytest.mark.parametrize("postfix", ["", "/"])
@pytest.mark.parametrize(
    "path,expected",
    [
        ("path", ("path",)),
        ("some/path", ("some", "path")),
    ],
)
def test_parts_posix(prefix, postfix, path, expected):
    assert Path("/").parts(prefix + path + postfix) == tuple(prefix) + expected


@pytest.mark.parametrize("postfix", ["", "\\"])
@pytest.mark.parametrize(
    "path,expected",
    [
        ("path", ("path",)),
        ("c:\\path", ("c:", "\\", "path")),
        ("some\\path", ("some", "path")),
    ],
)
def test_parts_nt(postfix, path, expected):
    assert Path("\\").parts(path + postfix) == expected




tests/unit/fs/test_s3.py
import os

import pytest
from dvc_s3 import S3FileSystem

from dvc.fs import ConfigError

bucket_name = "bucket-name"
prefix = "some/prefix"
url = f"s3://{bucket_name}/{prefix}"
key_id = "key-id"
key_secret = "key-secret"
session_token = "session-token"


@pytest.fixture(autouse=True)
def grants():
    return {
        "grant_read": "id=read-permission-id,id=other-read-permission-id",
        "grant_read_acp": "id=read-acp-permission-id",
        "grant_write_acp": "id=write-acp-permission-id",
        "grant_full_control": "id=full-control-permission-id",
    }


def test_verify_ssl_default_param(dvc):
    config = {"url": url}
    fs = S3FileSystem(**config)

    assert "client_kwargs" not in fs.fs_args

    config = {
        "url": url,
        "endpointurl": "https://my.custom.s3:1234",
    }
    fs = S3FileSystem(**config)

    assert "verify" not in fs.fs_args["client_kwargs"]


def test_s3_config_credentialpath(dvc, monkeypatch):
    environment = {}
    monkeypatch.setattr(os, "environ", environment)

    config = {"url": url, "credentialpath": "somewhere"}
    S3FileSystem(**config)
    assert environment["AWS_SHARED_CREDENTIALS_FILE"] == "somewhere"
    environment.clear()

    config = {"url": url, "configpath": "somewhere"}
    S3FileSystem(**config)
    assert environment["AWS_CONFIG_FILE"] == "somewhere"
    environment.clear()

    config = {
        "url": url,
        "credentialpath": "somewhere",
        "configpath": "elsewhere",
    }
    S3FileSystem(**config)
    assert environment["AWS_SHARED_CREDENTIALS_FILE"] == "somewhere"
    assert environment["AWS_CONFIG_FILE"] == "elsewhere"
    environment.clear()


def test_ssl_verify_bool_param(dvc):
    config = {"url": url, "ssl_verify": False}
    fs = S3FileSystem(**config)

    assert fs.fs_args["client_kwargs"]["verify"] == config["ssl_verify"]


def test_ssl_verify_path_param(dvc):
    config = {"url": url, "ssl_verify": "/path/to/custom/cabundle.pem"}
    fs = S3FileSystem(**config)

    assert fs.fs_args["client_kwargs"]["verify"] == config["ssl_verify"]


def test_ssl_verify_none_param(dvc):
    config = {"url": url, "ssl_verify": None}
    fs = S3FileSystem(**config)

    assert "client_kwargs" not in fs.fs_args

    config = {
        "url": url,
        "endpointurl": "https://my.custom.s3:1234",
        "ssl_verify": None,
    }
    fs = S3FileSystem(**config)

    assert "verify" not in fs.fs_args["client_kwargs"]


def test_grants(dvc):
    config = {
        "url": url,
        "grant_read": "id=read-permission-id,id=other-read-permission-id",
        "grant_read_acp": "id=read-acp-permission-id",
        "grant_write_acp": "id=write-acp-permission-id",
        "grant_full_control": "id=full-control-permission-id",
    }
    fs = S3FileSystem(**config)

    extra_args = fs.fs_args["s3_additional_kwargs"]
    assert (
        extra_args["GrantRead"] == "id=read-permission-id,id=other-read-permission-id"
    )
    assert extra_args["GrantReadACP"] == "id=read-acp-permission-id"
    assert extra_args["GrantWriteACP"] == "id=write-acp-permission-id"
    assert extra_args["GrantFullControl"] == "id=full-control-permission-id"


def test_grants_mutually_exclusive_acl_error(dvc, grants):
    for grant_option, grant_value in grants.items():
        config = {"url": url, "acl": "public-read", grant_option: grant_value}

        with pytest.raises(ConfigError):
            S3FileSystem(**config)


def test_sse_kms_key_id(dvc):
    fs = S3FileSystem(url=url, sse_kms_key_id="key")
    assert fs.fs_args["s3_additional_kwargs"]["SSEKMSKeyId"] == "key"


def test_key_id_and_secret(dvc):
    fs = S3FileSystem(
        url=url,
        access_key_id=key_id,
        secret_access_key=key_secret,
        session_token=session_token,
    )
    assert fs.fs_args["key"] == key_id
    assert fs.fs_args["secret"] == key_secret
    assert fs.fs_args["token"] == session_token




tests/unit/fs/test_tree.py
import pytest

from dvc.config import ConfigError
from dvc.fs import get_cloud_fs


def test_get_cloud_fs(tmp_dir, dvc):
    tmp_dir.add_remote(name="base", url="s3://bucket/path", default=False)
    tmp_dir.add_remote(name="first", url="remote://base/first", default=False)
    tmp_dir.add_remote(name="second", url="remote://first/second", default=False)

    base = "bucket/path"
    first = f"{base}/first"
    second = f"{first}/second"

    _, _, path = get_cloud_fs(dvc, name="base")
    assert path == base
    _, _, path = get_cloud_fs(dvc, name="first")
    assert path == first
    _, _, path = get_cloud_fs(dvc, name="second")
    assert path == second


def test_get_cloud_fs_validate(tmp_dir, dvc):
    tmp_dir.add_remote(name="base", url="ssh://example.com/path", default=False)
    tmp_dir.add_remote(
        name="first",
        config={"url": "remote://base/first", "type": "symlink"},
        default=False,
    )
    tmp_dir.add_remote(
        name="second",
        config={"url": "remote://first/second", "oss_key_id": "mykey"},
        default=False,
    )

    assert get_cloud_fs(dvc, name="base")[1]["host"] == "example.com"
    assert get_cloud_fs(dvc, name="first")[1]["host"] == "example.com"
    assert get_cloud_fs(dvc, name="first")[1]["type"] == ["symlink"]

    with pytest.raises(ConfigError):
        get_cloud_fs(dvc, name="second")




tests/unit/machine/__init__.py




tests/unit/machine/test_machine.py
import pytest


def test_validate_name():
    from dvc.exceptions import InvalidArgumentError
    from dvc.machine import RESERVED_NAMES, validate_name

    for name in RESERVED_NAMES:
        with pytest.raises(InvalidArgumentError):
            validate_name(name)


@pytest.mark.parametrize("cloud", ["aws", "azure"])
def test_get_config_and_backend(tmp_dir, dvc, cloud):
    from dvc.machine.backend.terraform import TerraformBackend

    name = "foo"
    with dvc.config.edit() as conf:
        conf["machine"][name] = {"cloud": cloud}
    config, backend = dvc.machine.get_config_and_backend(name)
    assert config == {"cloud": cloud, "name": name}
    assert isinstance(backend, TerraformBackend)


def test_get_config_and_backend_nonexistent(tmp_dir, dvc):
    from dvc.config import MachineNotFoundError

    with pytest.raises(MachineNotFoundError):
        dvc.machine.get_config_and_backend("foo")


def test_get_config_and_backend_default(tmp_dir, dvc):
    from dvc.config import NoMachineError

    with pytest.raises(NoMachineError):
        dvc.machine.get_config_and_backend()




tests/unit/output/__init__.py




tests/unit/output/test_annotations.py
import pytest

from dvc.annotations import Annotation


@pytest.mark.parametrize(
    "kwargs",
    [
        {"desc": "desc", "type": "type", "labels": ["label1", "label2"]},
        {"desc": "desc", "type": "type", "meta": {"key": "value"}},
    ],
)
def test_annotation_to_dict(kwargs):
    annot = Annotation(**kwargs)
    assert annot.to_dict() == kwargs




tests/unit/output/test_load.py
import pytest
from dvc_s3 import S3FileSystem

from dvc import output
from dvc.fs import LocalFileSystem
from dvc.output import Output
from dvc.stage import Stage


@pytest.mark.parametrize(
    "out_type,type_test_func",
    [
        ("outs", lambda o: not (o.metric or o.plot)),
        ("metrics", lambda o: o.metric and not o.plot),
        ("plots", lambda o: o.plot and not o.metric),
    ],
    ids=("outs", "metrics", "plots"),
)
def test_load_from_pipeline(dvc, out_type, type_test_func):
    outs = output.load_from_pipeline(
        Stage(dvc),
        [
            "file1",
            "file2",
            {"file3": {"cache": True}},
            {},
            {"file4": {"cache": False}},
            {"file5": {"persist": False}},
            {"file6": {"persist": True, "cache": False}},
        ],
        out_type,
    )
    cached_outs = {"file1", "file2", "file3", "file5"}
    persisted_outs = {"file6"}
    assert len(outs) == 6

    for i, out in enumerate(outs, start=1):
        assert isinstance(out, Output)
        assert isinstance(out.fs, LocalFileSystem)
        assert out.def_path == f"file{i}"
        assert out.use_cache == (out.def_path in cached_outs)
        assert out.persist == (out.def_path in persisted_outs)
        assert not out.hash_info
        assert type_test_func(out)


def test_load_from_pipeline_accumulates_flag(dvc):
    outs = output.load_from_pipeline(
        Stage(dvc),
        [
            "file1",
            {"file2": {"cache": False}},
            {"file1": {"persist": False}},
            {"file2": {"persist": True}},
        ],
        "outs",
    )
    for out in outs:
        assert isinstance(out, Output)
        assert isinstance(out.fs, LocalFileSystem)
        assert not out.plot
        assert not out.metric
        assert not out.hash_info

    assert outs[0].use_cache
    assert not outs[0].persist
    assert not outs[1].use_cache
    assert outs[1].persist


def test_load_remote_files_from_pipeline(dvc):
    stage = Stage(dvc)
    (out,) = output.load_from_pipeline(
        stage, [{"s3://dvc-test/file.txt": {"cache": False}}], typ="metrics"
    )
    assert isinstance(out, Output)
    assert isinstance(out.fs, S3FileSystem)
    assert not out.plot
    assert out.metric
    assert not out.persist
    assert not out.hash_info


def test_load_remote(dvc):
    stage = Stage(dvc)
    (foo, bar) = output.load_from_pipeline(
        stage,
        ["foo", {"bar": {"remote": "myremote"}}],
    )
    assert foo.remote is None
    assert bar.remote == "myremote"


@pytest.mark.parametrize("typ", [None, "", "illegal"])
def test_load_from_pipeline_error_on_typ(dvc, typ):
    with pytest.raises(
        ValueError, match=f"'{typ}' key is not allowed for pipeline files."
    ):
        output.load_from_pipeline(Stage(dvc), ["file1"], typ)


@pytest.mark.parametrize("key", [3, "list".split()])
def test_load_from_pipeline_illegal_type(dvc, key):
    stage = Stage(dvc)
    with pytest.raises(ValueError, match=f"'{type(key).__name__}' not supported."):
        output.load_from_pipeline(stage, [key], "outs")
    with pytest.raises(
        ValueError,
        match=f"Expected dict for 'key', got: '{type(key).__name__}'",
    ):
        output.load_from_pipeline(stage, [{"key": key}], "outs")


def test_plots_load_from_pipeline(dvc):
    outs = output.load_from_pipeline(
        Stage(dvc),
        [
            "file1",
            {
                "file2": {
                    "persist": True,
                    "cache": False,
                    "x": 3,
                    "random": "val",
                }
            },
        ],
        "plots",
    )
    assert isinstance(outs[0], Output)
    assert isinstance(outs[0].fs, LocalFileSystem)
    assert outs[0].use_cache
    assert outs[0].plot is True
    assert not outs[0].metric
    assert not outs[0].persist

    assert isinstance(outs[1], Output)
    assert isinstance(outs[1].fs, LocalFileSystem)
    assert not outs[1].use_cache
    assert outs[1].plot == {"x": 3}
    assert not outs[1].metric
    assert outs[1].persist




tests/unit/output/test_local.py
import os

from dvc.output import Output
from dvc.stage import Stage
from dvc.utils import relpath
from dvc_data.hashfile.hash_info import HashInfo
from dvc_data.hashfile.meta import Meta


def test_str_workdir_outside_repo(tmp_dir, erepo_dir):
    stage = Stage(erepo_dir.dvc)
    output = Output(stage, "path", cache=False)

    assert relpath("path", erepo_dir.dvc.root_dir) == str(output)


def test_str_workdir_inside_repo(dvc):
    stage = Stage(dvc)
    output = Output(stage, "path", cache=False)

    assert str(output) == "path"

    stage = Stage(dvc, wdir="some_folder")
    output = Output(stage, "path", cache=False)

    assert os.path.join("some_folder", "path") == str(output)


def test_str_on_local_absolute_path(dvc):
    stage = Stage(dvc)

    rel_path = os.path.join("path", "to", "file")
    abs_path = os.path.abspath(rel_path)
    output = Output(stage, abs_path, cache=False)

    assert output.def_path == rel_path
    assert output.fs_path == abs_path
    assert str(output) == rel_path


def test_str_on_external_absolute_path(dvc):
    stage = Stage(dvc)

    rel_path = os.path.join("..", "path", "to", "file")
    abs_path = os.path.abspath(rel_path)
    output = Output(stage, abs_path, cache=False)

    assert output.def_path == abs_path
    assert output.fs_path == abs_path
    assert str(output) == abs_path


def test_return_0_on_no_cache(dvc):
    o = Output(Stage(dvc), "path")
    o.use_cache = False
    assert o.get_files_number() == 0


def test_return_multiple_for_dir(dvc):
    o = Output(Stage(dvc), "path")
    o.hash_info = HashInfo("md5", "12345678.dir")
    o.meta = Meta(nfiles=2)
    assert o.get_files_number() == 2


def test_return_1_on_single_file_cache(mocker, dvc):
    mocker.patch.object(Output, "is_dir_checksum", False)
    o = Output(Stage(dvc), "path")
    o.hash_info = HashInfo("md5", "12345678")
    assert o.get_files_number() == 1




tests/unit/output/test_output.py
import logging
import os

import pytest
from funcy import first
from voluptuous import MultipleInvalid, Schema

from dvc.fs import RemoteMissingDepsError
from dvc.ignore import _no_match
from dvc.output import CHECKSUM_SCHEMA, Output
from dvc.stage import Stage
from dvc.utils.fs import remove


def test_save_missing(dvc, mocker):
    stage = Stage(dvc)
    out = Output(stage, "path", cache=False)
    mocker.patch.object(out.fs, "exists", return_value=False)
    with pytest.raises(out.DoesNotExistError):
        out.save()


@pytest.mark.parametrize(
    "value,expected",
    [
        ("", None),
        (None, None),
        (11111, "11111"),
        ("11111", "11111"),
        ("aAaBa", "aaaba"),
        (
            "3cc286c534a71504476da009ed174423",
            "3cc286c534a71504476da009ed174423",
        ),  # md5
        (
            "d41d8cd98f00b204e9800998ecf8427e-38",
            "d41d8cd98f00b204e9800998ecf8427e-38",
        ),  # etag
        (
            "000002000000000000000000c16859d1d071c6b1ffc9c8557d4909f1",
            "000002000000000000000000c16859d1d071c6b1ffc9c8557d4909f1",
        ),  # hdfs checksum
        # Not much we can do about hex and oct values without writing our own
        # parser. So listing these test cases just to acknowledge this.
        # See https://github.com/iterative/dvc/issues/3331.
        (0x3451, "13393"),
        (0o1244, "676"),
    ],
)
def test_checksum_schema(value, expected):
    assert Schema(CHECKSUM_SCHEMA)(value) == expected


@pytest.mark.parametrize("value", ["1", "11", {}, {"a": "b"}, [], [1, 2]])
def test_checksum_schema_fail(value):
    with pytest.raises(MultipleInvalid):
        assert Schema(CHECKSUM_SCHEMA)(value)


@pytest.mark.parametrize(
    "exists, expected_message",
    [
        (
            False,
            (
                "Output 'path'(stage: 'stage.dvc') is missing version info. "
                "Cache for it will not be collected. "
                "Use `dvc repro` to get your pipeline up to date."
            ),
        ),
        (
            True,
            (
                "Output 'path'(stage: 'stage.dvc') is missing version info. "
                "Cache for it will not be collected. "
                "Use `dvc repro` to get your pipeline up to date.\n"
                "You can also use `dvc commit stage.dvc` to associate "
                "existing 'path' with stage: 'stage.dvc'."
            ),
        ),
    ],
)
def test_get_used_objs(exists, expected_message, mocker, caplog):
    stage = mocker.MagicMock()
    mocker.patch.object(stage, "__str__", return_value="stage: 'stage.dvc'")
    mocker.patch.object(stage, "addressing", "stage.dvc")
    mocker.patch.object(stage, "wdir", os.getcwd())
    mocker.patch.object(stage.repo, "root_dir", os.getcwd())
    mocker.patch.object(stage.repo.dvcignore, "is_ignored", return_value=False)
    mocker.patch.object(
        stage.repo.dvcignore, "check_ignore", return_value=_no_match("path")
    )
    stage.repo.fs.version_aware = False
    stage.repo.fs.PARAM_CHECKSUM = "md5"

    output = Output(stage, "path")

    mocker.patch.object(output, "use_cache", True)
    mocker.patch.object(stage, "is_repo_import", False)
    mocker.patch.object(
        Output, "exists", new_callable=mocker.PropertyMock
    ).return_value = exists

    with caplog.at_level(logging.WARNING, logger="dvc"):
        assert {} == output.get_used_objs()
    assert first(caplog.messages) == expected_message


def test_remote_missing_dependency_on_dir_pull(tmp_dir, scm, dvc, mocker):
    tmp_dir.dvc_gen({"dir": {"subfile": "file2 content"}}, commit="add dir")
    with dvc.config.edit() as conf:
        conf["remote"]["s3"] = {"url": "s3://bucket/name"}
        conf["core"] = {"remote": "s3"}

    remove("dir")
    remove(dvc.cache.local.path)

    mocker.patch(
        "dvc.data_cloud.DataCloud.get_remote",
        side_effect=RemoteMissingDepsError(dvc.fs, "azure", "azure://", []),
    )
    with pytest.raises(RemoteMissingDepsError):
        dvc.pull()


def test_hash_info_cloud_versioning_dir(mocker):
    stage = mocker.MagicMock()
    stage.repo.fs.version_aware = False
    stage.repo.fs.PARAM_CHECKSUM = "etag"
    files = [
        {
            "size": 3,
            "version_id": "WYRG4BglP7pD.gEoJP6a4AqOhl.FRA.h",
            "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
            "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
            "relpath": "bar",
        },
        {
            "size": 3,
            "version_id": "0vL53tFVY5vVAoJ4HG2jCS1mEcohDPE0",
            "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
            "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
            "relpath": "foo",
        },
    ]
    out = Output(stage, "path", files=files)
    # `hash_info`` and `meta`` constructed from `files``
    assert out.hash_info.name == "md5"
    assert out.hash_info.value == "77e8000f532886eef8ee1feba82e9bad.dir"
    assert out.meta.isdir
    assert out.meta.nfiles == 2
    assert out.meta.size == 6


def test_dumpd_cloud_versioning_dir(mocker):
    stage = mocker.MagicMock()
    stage.repo.fs.version_aware = False
    stage.repo.fs.PARAM_CHECKSUM = "md5"
    files = [
        {
            "size": 3,
            "version_id": "WYRG4BglP7pD.gEoJP6a4AqOhl.FRA.h",
            "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
            "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
            "relpath": "bar",
        },
        {
            "size": 3,
            "version_id": "0vL53tFVY5vVAoJ4HG2jCS1mEcohDPE0",
            "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
            "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
            "relpath": "foo",
        },
    ]
    out = Output(stage, "path", files=files)

    dumpd = out.dumpd()
    assert dumpd == {"path": "path", "files": files}


def test_version_aware_is_set_based_on_files(mocker):
    import dvc.fs as dvc_fs

    get_fs_config = mocker.spy(dvc_fs, "get_fs_config")

    stage = mocker.MagicMock()
    stage.repo.fs.version_aware = False
    stage.repo.fs.PARAM_CHECKSUM = "etag"
    files = [
        {
            "size": 3,
            "version_id": "WYRG4BglP7pD.gEoJP6a4AqOhl.FRA.h",
            "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
            "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
            "relpath": "bar",
        }
    ]
    Output(stage, "path", files=files)
    # version_aware is passed as `True` if `files` is present`.
    # This will be intentionally ignored in filesystems that don't handle it
    # in `_prepare_credentials`.
    assert get_fs_config.call_args_list[0][1] == {
        "url": "path",
        "version_aware": True,
    }




tests/unit/remote/__init__.py




tests/unit/remote/test_oss.py
from dvc_oss import OSSFileSystem

bucket_name = "bucket-name"
endpoint = "endpoint"
key_id = "Fq2UVErCz4I6tq"
key_secret = "Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsu"


def test_init(dvc):
    prefix = "some/prefix"
    url = f"oss://{bucket_name}/{prefix}"
    config = {
        "url": url,
        "oss_key_id": key_id,
        "oss_key_secret": key_secret,
        "oss_endpoint": endpoint,
    }
    fs = OSSFileSystem(**config)
    assert fs.fs_args["endpoint"] == endpoint
    assert fs.fs_args["key"] == key_id
    assert fs.fs_args["secret"] == key_secret




tests/unit/remote/test_remote.py
import pytest
from dvc_gs import GSFileSystem
from dvc_s3 import S3FileSystem

from dvc.fs import get_cloud_fs


def test_remote_with_hash_jobs(dvc):
    dvc.config["remote"]["with_hash_jobs"] = {
        "url": "s3://bucket/name",
        "checksum_jobs": 100,
    }
    dvc.config["core"]["checksum_jobs"] = 200

    cls, config, _ = get_cloud_fs(dvc, name="with_hash_jobs")
    fs = cls(**config)
    assert fs.hash_jobs == 100


def test_remote_with_jobs(dvc):
    dvc.config["remote"]["with_jobs"] = {
        "url": "s3://bucket/name",
        "jobs": 100,
    }

    cls, config, _ = get_cloud_fs(dvc, name="with_jobs")
    fs = cls(**config)
    assert fs.jobs == 100


def test_remote_without_hash_jobs(dvc):
    dvc.config["remote"]["without_hash_jobs"] = {"url": "s3://bucket/name"}
    dvc.config["core"]["checksum_jobs"] = 200

    cls, config, _ = get_cloud_fs(dvc, name="without_hash_jobs")
    fs = cls(**config)
    assert fs.hash_jobs == 200


def test_remote_without_hash_jobs_default(dvc):
    dvc.config["remote"]["without_hash_jobs"] = {"url": "s3://bucket/name"}

    cls, config, _ = get_cloud_fs(dvc, name="without_hash_jobs")
    fs = cls(**config)
    assert fs.hash_jobs == fs.HASH_JOBS


@pytest.mark.parametrize("fs_cls", [GSFileSystem, S3FileSystem])
def test_makedirs_not_create_for_top_level_path(fs_cls, dvc, mocker):
    url = f"{fs_cls.protocol}://bucket/"
    fs = fs_cls(url=url)
    mocked_client = mocker.PropertyMock()
    mocker.patch.object(fs_cls, "fs", mocked_client)

    fs.makedirs(url)
    assert not mocked_client.called




tests/unit/remote/test_webdav.py
import pytest
from dvc_webdav import WebDAVFileSystem, WebDAVSFileSystem

from dvc.fs import get_cloud_fs
from tests.utils.asserts import issubset

url_fmt = "{scheme}://{user}@example.com/public.php/webdav"
url = "webdav://example.com/public.php/webdav"
user = "username"
password = "password"
token = "4MgjsNM5aSJjxIKM"
custom_auth_header = "Custom-Header"


def test_common():
    fs = WebDAVFileSystem(
        url=url,
        cert_path="cert/path",
        key_path="key/path",
        ssl_verify="bundle.pem",
        timeout=10,
        prefix="/public.php/webdav",
        user=None,
        password=None,
        ask_password=False,
        token=None,
        custom_auth_header=None,
    )
    assert issubset(
        {
            "headers": {},
            "auth": None,
            "base_url": url,
            "cert": ("cert/path", "key/path"),
            "verify": "bundle.pem",
            "timeout": 10,
        },
        fs.fs_args,
    )
    assert fs.prefix == "/public.php/webdav"


def test_user():
    fs = WebDAVFileSystem(url=url, user=user)
    assert issubset({"auth": (user, None), "headers": {}}, fs.fs_args)


def test_password():
    config = {"url": url, "user": user, "password": password}
    fs = WebDAVFileSystem(**config)
    assert issubset(
        {
            "headers": {},
            "auth": (user, password),
        },
        fs.fs_args,
    )


def test_token():
    config = {"token": token, "url": url}
    fs = WebDAVFileSystem(**config)
    assert issubset(
        {"headers": {"Authorization": f"Bearer {token}"}, "auth": None},
        fs.fs_args,
    )


def test_ask_password(mocker):
    ask_password_mocked = mocker.patch("dvc_webdav.ask_password", return_value="pass")
    host = "host"

    # it should not ask for password as password is set
    config = {
        "url": url,
        "user": user,
        "password": password,
        "ask_password": True,
        "host": host,
    }
    fs = WebDAVFileSystem(**config)
    assert issubset({"auth": (user, password), "headers": {}}, fs.fs_args)

    config.pop("password")
    fs = WebDAVFileSystem(**config)
    assert issubset({"auth": (user, "pass"), "headers": {}}, fs.fs_args)
    ask_password_mocked.assert_called_once_with(host, user)


def test_custom_auth_header():
    config = {
        "url": url,
        "custom_auth_header": custom_auth_header,
        "password": password,
    }
    fs = WebDAVFileSystem(**config)
    assert issubset(
        {"headers": {custom_auth_header: password}, "auth": None},
        fs.fs_args,
    )


def test_ask_password_custom_auth_header(mocker):
    ask_password_mocked = mocker.patch("dvc_webdav.ask_password", return_value="pass")
    host = "host"

    # it should not ask for password as password is set
    config = {
        "url": url,
        "custom_auth_header": custom_auth_header,
        "password": password,
        "ask_password": True,
        "host": host,
    }
    fs = WebDAVFileSystem(**config)
    assert issubset(
        {"headers": {custom_auth_header: password}, "auth": None}, fs.fs_args
    )

    config.pop("password")
    fs = WebDAVFileSystem(**config)
    assert issubset({"headers": {custom_auth_header: "pass"}, "auth": None}, fs.fs_args)
    ask_password_mocked.assert_called_once_with(host, custom_auth_header)


def test_ssl_verify_custom_cert():
    config = {
        "url": url,
        "ssl_verify": "/path/to/custom/cabundle.pem",
    }

    fs = WebDAVFileSystem(**config)
    assert fs.fs_args["verify"] == "/path/to/custom/cabundle.pem"


@pytest.mark.parametrize(
    "base_url, fs_cls",
    [
        (url_fmt.format(scheme="webdav", user=user), WebDAVFileSystem),
        (url_fmt.format(scheme="webdavs", user=user), WebDAVSFileSystem),
    ],
)
def test_remote_with_jobs(dvc, base_url, fs_cls):
    scheme = "http" + ("s" if fs_cls is WebDAVSFileSystem else "")
    remote_config = {"url": base_url}

    dvc.config["remote"]["dav"] = remote_config
    cls, config, _ = get_cloud_fs(dvc, name="dav")
    assert config["user"] == user
    assert f"{scheme}://{user}@example.com" in config["host"]
    assert cls is fs_cls

    # config from remote takes priority
    remote_config.update({"user": "admin"})
    cls, config, _ = get_cloud_fs(dvc, name="dav")
    assert config["user"] == "admin"
    assert f"{scheme}://{user}@example.com" in config["host"]
    assert cls is fs_cls




tests/unit/remote/test_webhdfs.py
import pytest
import requests
from dvc_webhdfs import WebHDFSFileSystem

host = "host"
kerberos = False
kerberos_principal = "principal"
port = 12345
proxy_to = "proxy"
ssl_verify = False
token = "token"
use_https = True
user = "test"


@pytest.fixture(name="webhdfs_config")
def fixture_webhdfs_config():
    url = f"webhdfs://{user}@{host}:{port}"
    url_config = WebHDFSFileSystem._get_kwargs_from_urls(url)
    return {
        "kerberos": kerberos,
        "kerberos_principal": kerberos_principal,
        "proxy_to": proxy_to,
        "ssl_verify": ssl_verify,
        "token": token,
        "use_https": use_https,
        **url_config,
    }


def test_init(dvc, webhdfs_config):
    fs = WebHDFSFileSystem(**webhdfs_config)
    assert fs.fs_args["host"] == host
    assert fs.fs_args["token"] == token
    assert fs.fs_args["user"] == user
    assert fs.fs_args["port"] == port
    assert fs.fs_args["kerberos"] == kerberos
    assert fs.fs_args["kerb_kwargs"] == {"principal": kerberos_principal}
    assert fs.fs_args["proxy_to"] == proxy_to
    assert fs.fs_args["use_https"] == use_https


def test_verify_ssl(dvc, webhdfs_config, monkeypatch, mocker):
    mock_session = mocker.create_autospec(requests.Session)
    monkeypatch.setattr(requests, "Session", mocker.Mock(return_value=mock_session))
    # can't have token at the same time as user or proxy_to
    del webhdfs_config["token"]
    fs = WebHDFSFileSystem(**webhdfs_config)
    # ssl verify can't be set until after the file system is instantiated
    assert fs.fs
    assert mock_session.verify == ssl_verify




tests/unit/render/__init__.py




tests/unit/render/test_convert.py
from dvc.render import REVISION_FIELD, REVISIONS_KEY, SRC_FIELD, TYPE_KEY, VERSION_FIELD
from dvc.render.convert import to_json


def test_to_json_vega(mocker):
    vega_renderer = mocker.MagicMock()
    vega_renderer.TYPE = "vega"
    vega_renderer.get_filled_template.return_value = {"this": "is vega"}
    vega_renderer.datapoints = [
        {
            "x": 1,
            "y": 2,
            VERSION_FIELD: {"revision": "foo"},
            "filename": "foo.json",
        },
        {
            "x": 2,
            "y": 1,
            VERSION_FIELD: {"revision": "bar"},
            "filename": "foo.json",
        },
    ]
    result = to_json(vega_renderer)
    assert result[0] == {
        TYPE_KEY: vega_renderer.TYPE,
        REVISIONS_KEY: ["bar", "foo"],
        "content": {"this": "is vega"},
        "datapoints": {
            "foo": [
                {
                    "x": 1,
                    "y": 2,
                    "filename": "foo.json",
                    VERSION_FIELD: {"revision": "foo"},
                },
            ],
            "bar": [
                {
                    "x": 2,
                    "y": 1,
                    "filename": "foo.json",
                    VERSION_FIELD: {"revision": "bar"},
                },
            ],
        },
    }
    vega_renderer.get_filled_template.assert_called()


def test_to_json_vega_split(mocker):
    vega_renderer = mocker.MagicMock()
    vega_renderer.TYPE = "vega"
    vega_renderer.get_filled_template.return_value = {"this": "is split vega"}
    vega_renderer.datapoints = [
        {
            "x": 1,
            "y": 2,
            VERSION_FIELD: {"revision": "foo"},
            "filename": "foo.json",
        },
        {
            "x": 2,
            "y": 1,
            VERSION_FIELD: {"revision": "bar"},
            "filename": "foo.json",
        },
    ]
    result = to_json(vega_renderer, split=True)
    assert result[0] == {
        TYPE_KEY: vega_renderer.TYPE,
        REVISIONS_KEY: ["bar", "foo"],
        "content": {"this": "is split vega"},
        "datapoints": {
            "foo": [
                {
                    "x": 1,
                    "y": 2,
                    "filename": "foo.json",
                    VERSION_FIELD: {"revision": "foo"},
                }
            ],
            "bar": [
                {
                    "x": 2,
                    "y": 1,
                    "filename": "foo.json",
                    VERSION_FIELD: {"revision": "bar"},
                }
            ],
        },
    }
    vega_renderer.get_filled_template.assert_called_with(
        as_string=False, skip_anchors=["data"]
    )


def test_to_json_image(mocker):
    image_renderer = mocker.MagicMock()
    image_renderer.TYPE = "image"
    image_renderer.datapoints = [
        {SRC_FIELD: "contentfoo", REVISION_FIELD: "foo"},
        {SRC_FIELD: "contentbar", REVISION_FIELD: "bar"},
    ]
    result = to_json(image_renderer)
    assert result[0] == {
        "url": image_renderer.datapoints[0].get(SRC_FIELD),
        REVISIONS_KEY: [image_renderer.datapoints[0].get(REVISION_FIELD)],
        TYPE_KEY: image_renderer.TYPE,
    }




tests/unit/render/test_image_converter.py
from dvc.render import REVISION_FIELD, SRC_FIELD
from dvc.render.converter.image import ImageConverter


def test_image_converter_no_out():
    data = {"image.png": b"content"}
    converter = ImageConverter("image.png", data)
    datapoints, _ = converter.flat_datapoints("r")

    assert datapoints[0] == {
        REVISION_FIELD: "r",
        "filename": "image.png",
        SRC_FIELD: converter._encode_image(b"content"),
    }


def test_image_converter_with_out(tmp_dir):
    data = {"image.png": b"content"}
    converter = ImageConverter("image.png", data, {"out": tmp_dir / "foo"})

    datapoints, _ = converter.flat_datapoints("r")

    assert datapoints[0] == {
        REVISION_FIELD: "r",
        "filename": "image.png",
        SRC_FIELD: str(tmp_dir / "foo" / "r_image.png"),
    }

    assert (tmp_dir / "foo" / "r_image.png").read_bytes() == b"content"


def test_image_converter_with_slash_in_revision(tmp_dir):
    """Regression test for #7934"""
    data = {"image.png": b"content"}
    converter = ImageConverter("image.png", data, {"out": tmp_dir / "foo"})

    datapoints, _ = converter.flat_datapoints("feature/r")

    assert datapoints[0] == {
        REVISION_FIELD: "feature/r",
        "filename": "image.png",
        SRC_FIELD: str(tmp_dir / "foo" / "feature_r_image.png"),
    }

    assert (tmp_dir / "foo" / "feature_r_image.png").read_bytes() == b"content"




tests/unit/render/test_match.py
from funcy import set_in

from dvc.render import VERSION_FIELD
from dvc.render.converter.vega import VegaConverter
from dvc.render.match import PlotsData, _squash_plots_properties, match_defs_renderers


def test_group_definitions():
    error = FileNotFoundError()
    data = {
        "v1": {
            "definitions": {
                "data": {
                    "config_file_1": {"data": {"plot_id_1": {}, "plot_id_2": {}}},
                    "config_file_2": {"data": {"plot_id_3": {}}},
                }
            }
        },
        "v2": {
            "definitions": {
                "data": {
                    "config_file_1": {"error": error},
                    "config_file_2": {"data": {"plot_id_3": {}}},
                }
            }
        },
    }

    grouped = PlotsData(data).group_definitions()

    assert grouped == {
        "config_file_1::plot_id_1": [("v1", "plot_id_1", {})],
        "config_file_1::plot_id_2": [("v1", "plot_id_2", {})],
        "config_file_2::plot_id_3": [
            ("v1", "plot_id_3", {}),
            ("v2", "plot_id_3", {}),
        ],
    }


def test_match_renderers(M):
    data = {
        "v1": {
            "definitions": {
                "data": {
                    "config_file_1": {
                        "data": {
                            "plot_id_1": {
                                "x": "x",
                                "y": {"file.json": "y"},
                            }
                        }
                    }
                },
            },
            "sources": {
                "data": {"file.json": {"data": [{"x": 1, "y": 1}, {"x": 2, "y": 2}]}}
            },
        },
        "errored_revision": {
            "definitions": {
                "data": {"config_file_1": {"error": FileNotFoundError()}},
            },
            "sources": {},
        },
        "revision_with_no_data": {
            "definitions": {
                "data": {
                    "config_file_1": {
                        "data": {
                            "plot_id_1": {
                                "x": "x",
                                "y": {"file.json": "y"},
                            }
                        }
                    }
                },
            },
            "sources": {"data": {"file.json": {"error": FileNotFoundError()}}},
        },
    }

    (renderer_with_errors,) = match_defs_renderers(data)
    renderer = renderer_with_errors[0]
    assert renderer.datapoints == [
        {
            VERSION_FIELD: {
                "revision": "v1",
                "filename": "file.json",
                "field": "y",
            },
            "x": 1,
            "y": 1,
        },
        {
            VERSION_FIELD: {
                "revision": "v1",
                "filename": "file.json",
                "field": "y",
            },
            "x": 2,
            "y": 2,
        },
    ]
    assert renderer.properties == {
        "title": "config_file_1::plot_id_1",
        "x": "x",
        "y": "y",
        "x_label": "x",
        "y_label": "y",
    }
    assert renderer_with_errors.source_errors == {
        "revision_with_no_data": {"file.json": M.instance_of(FileNotFoundError)}
    }
    assert not renderer_with_errors.definition_errors


def test_flat_datapoints_errors_are_caught(M, mocker):
    d = {}
    d = set_in(
        d,
        ["v1", "definitions", "data", "dvc.yaml", "data", "plot_id_1"],
        {"x": "x", "y": {"file.json": "y"}},
    )
    d = set_in(d, ["v1", "sources", "data", "file.json", "data"], [{"x": 1, "y": 1}])
    mocker.patch.object(VegaConverter, "flat_datapoints", side_effect=ValueError)
    (renderer_with_errors,) = match_defs_renderers(d)
    assert not renderer_with_errors.source_errors
    assert renderer_with_errors.definition_errors == {"v1": M.instance_of(ValueError)}


def test_squash_plots_properties():
    group = [
        ("v3", "config_file", "plot_id", {"foo": 1}),
        ("v2", "config_file", "plot_id", {"foo": 2, "bar": 2}),
        ("v1", "config_file", "plot_id", {"baz": 3}),
    ]

    plot_properties = _squash_plots_properties(group)

    assert plot_properties == {"foo": 1, "bar": 2, "baz": 3}




tests/unit/render/test_vega_converter.py
from collections import OrderedDict

import pytest

from dvc.exceptions import DvcException
from dvc.render import VERSION_FIELD
from dvc.render.converter.vega import FieldNotFoundError, VegaConverter, _lists


@pytest.mark.parametrize(
    "dictionary, expected_result",
    [
        ({}, []),
        ({"x": ["a", "b", "c"]}, [["a", "b", "c"]]),
        (
            OrderedDict([("x", {"y": ["a", "b"]}), ("z", {"w": ["c", "d"]})]),
            [["a", "b"], ["c", "d"]],
        ),
    ],
)
def test_finding_lists(dictionary, expected_result):
    result = _lists(dictionary)

    assert list(result) == expected_result


@pytest.mark.parametrize(
    "input_data,properties,expected_datapoints,expected_properties",
    [
        pytest.param(
            {"f": {"metric": [{"v": 1}, {"v": 2}]}},
            {},
            [
                {
                    "v": 1,
                    "step": 0,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v",
                    },
                },
                {
                    "v": 2,
                    "step": 1,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v",
                    },
                },
            ],
            {"x": "step", "y": "v", "x_label": "step", "y_label": "v"},
            id="default_x_y",
        ),
        pytest.param(
            {"f": {"metric": [{"v": 1, "v2": 0.1}, {"v": 2, "v2": 0.2}]}},
            {"x": "v", "y": "v2"},
            [
                {
                    "v": 1,
                    "v2": 0.1,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v2",
                    },
                },
                {
                    "v": 2,
                    "v2": 0.2,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v2",
                    },
                },
            ],
            {"x": "v", "y": "v2", "x_label": "v", "y_label": "v2"},
            id="choose_x_y",
        ),
        pytest.param(
            {
                "f": {
                    "some": "noise",
                    "very": {
                        "nested": {
                            "metric": [
                                {"v": 1, "v2": 0.1},
                                {"v": 2, "v2": 0.2},
                            ]
                        }
                    },
                }
            },
            {"x": "v", "y": "v2", "x_label": "x", "y_label": "y"},
            [
                {
                    "v": 1,
                    "v2": 0.1,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v2",
                    },
                },
                {
                    "v": 2,
                    "v2": 0.2,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v2",
                    },
                },
            ],
            {"x": "v", "y": "v2", "x_label": "x", "y_label": "y"},
            id="find_in_nested_structure",
        ),
        pytest.param(
            {"f": {"metric": [{"v": 1, "v2": 0.1}, {"v": 2, "v2": 0.2}]}},
            {"y": {"f": ["v", "v2"]}},
            [
                {
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v",
                    },
                    "dvc_inferred_y_value": 1,
                    "v": 1,
                    "v2": 0.1,
                    "step": 0,
                },
                {
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v",
                    },
                    "dvc_inferred_y_value": 2,
                    "v": 2,
                    "v2": 0.2,
                    "step": 1,
                },
                {
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v2",
                    },
                    "dvc_inferred_y_value": 0.1,
                    "v2": 0.1,
                    "v": 1,
                    "step": 0,
                },
                {
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v2",
                    },
                    "v": 2,
                    "v2": 0.2,
                    "dvc_inferred_y_value": 0.2,
                    "step": 1,
                },
            ],
            {
                "x": "step",
                "y": "dvc_inferred_y_value",
                "y_label": "y",
                "x_label": "step",
            },
            id="y_def_list",
        ),
        pytest.param(
            {
                "f": {
                    "metric": [{"v": 1}, {"v": 2}],
                    "other_metric": [{"z": 3}, {"z": 4}],
                }
            },
            {"y": {"f": ["v", "z"]}},
            [
                {
                    "dvc_inferred_y_value": 1,
                    "z": 3,
                    "v": 1,
                    "step": 0,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v",
                    },
                },
                {
                    "dvc_inferred_y_value": 2,
                    "z": 4,
                    "step": 1,
                    "v": 2,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v",
                    },
                },
                {
                    "dvc_inferred_y_value": 3,
                    "v": 1,
                    "z": 3,
                    "step": 0,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "z",
                    },
                },
                {
                    "dvc_inferred_y_value": 4,
                    "v": 2,
                    "z": 4,
                    "step": 1,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "z",
                    },
                },
            ],
            {
                "x": "step",
                "y": "dvc_inferred_y_value",
                "y_label": "y",
                "x_label": "step",
            },
            id="multi_source_json",
        ),
        pytest.param(
            {
                "f": {"metric": [{"v": 1, "v2": 0.1}, {"v": 2, "v2": 0.2}]},
                "f2": {"metric": [{"v": 3, "v2": 0.3}]},
            },
            {"x": "v", "y": {"f": "v2", "f2": "v2"}},
            [
                {
                    "v": 1,
                    "v2": 0.1,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v2",
                    },
                },
                {
                    "v": 2,
                    "v2": 0.2,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v2",
                    },
                },
                {
                    "v": 3,
                    "v2": 0.3,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f2",
                        "field": "v2",
                    },
                },
            ],
            {"x": "v", "y": "v2", "x_label": "v", "y_label": "v2"},
            id="multi_file_json",
        ),
        pytest.param(
            {"f": {"metric": [{"v": 1, "v2": 0.1}, {"v": 2, "v2": 0.2}]}},
            {"y": ["v", "v2"]},
            [
                {
                    "dvc_inferred_y_value": 1,
                    "v": 1,
                    "v2": 0.1,
                    "step": 0,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v",
                    },
                },
                {
                    "dvc_inferred_y_value": 2,
                    "v": 2,
                    "v2": 0.2,
                    "step": 1,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v",
                    },
                },
                {
                    "dvc_inferred_y_value": 0.1,
                    "v": 1,
                    "v2": 0.1,
                    "step": 0,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v2",
                    },
                },
                {
                    "dvc_inferred_y_value": 0.2,
                    "v": 2,
                    "v2": 0.2,
                    "step": 1,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v2",
                    },
                },
            ],
            {
                "x": "step",
                "y": "dvc_inferred_y_value",
                "x_label": "step",
                "y_label": "y",
            },
            id="y_list",
        ),
        pytest.param(
            {
                "f": {"metric": [{"v": 1, "v2": 0.1, "v3": 0.01}]},
                "f2": {"metric": [{"v": 1, "v2": 0.1}]},
            },
            {"y": {"f": ["v2", "v3"], "f2": ["v2"]}, "x": "v"},
            [
                {
                    "dvc_inferred_y_value": 0.1,
                    "v": 1,
                    "v2": 0.1,
                    "v3": 0.01,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v2",
                    },
                },
                {
                    "dvc_inferred_y_value": 0.01,
                    "v": 1,
                    "v2": 0.1,
                    "v3": 0.01,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v3",
                    },
                },
                {
                    "dvc_inferred_y_value": 0.1,
                    "v": 1,
                    "v2": 0.1,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f2",
                        "field": "v2",
                    },
                },
            ],
            {
                "x": "v",
                "y": "dvc_inferred_y_value",
                "x_label": "v",
                "y_label": "y",
            },
            id="multi_source_y_single_x",
        ),
        pytest.param(
            {
                "dir/f": {"metric": [{"v": 1, "v2": 0.1}]},
                "dir/f2": {"metric": [{"v": 1, "v2": 0.1}]},
            },
            {"y": {"dir/f": ["v2"], "dir/f2": ["v2"]}, "x": "v"},
            [
                {
                    "v": 1,
                    "v2": 0.1,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f",
                        "field": "v2",
                    },
                },
                {
                    "v": 1,
                    "v2": 0.1,
                    VERSION_FIELD: {
                        "revision": "r",
                        "filename": "f2",
                        "field": "v2",
                    },
                },
            ],
            {
                "x": "v",
                "y": "v2",
                "x_label": "v",
                "y_label": "v2",
            },
            id="multi_file_y_same_prefix",
        ),
    ],
)
def test_convert(
    input_data,
    properties,
    expected_datapoints,
    expected_properties,
):
    converter = VegaConverter("f", input_data, properties)
    datapoints, resolved_properties = converter.flat_datapoints("r")

    assert datapoints == expected_datapoints
    assert resolved_properties == expected_properties


@pytest.mark.parametrize(
    "input_data,properties,exc",
    [
        pytest.param(
            {
                "f": {
                    "metric": [
                        {"v": 1},
                        {"v": 2},
                    ]
                },
                "f2": {"metric": [{"v2": 0.1}]},
            },
            {"x": {"f": "v"}, "y": {"f2": "v2"}},
            DvcException,
            id="unequal_datapoints",
        ),
        pytest.param(
            {
                "f": {
                    "metric": [
                        {"v": 1, "v2": 0.1},
                        {"v": 2, "v2": 0.2},
                    ]
                },
                "f2": {
                    "metric": [
                        {"v": 3, "v2": 0.3},
                    ]
                },
            },
            {"x": {"f": "v", "f2": "v3"}, "y": {"f": "v2"}},
            FieldNotFoundError,
            id="unequal_x_y",
        ),
    ],
)
def test_convert_fail(input_data, properties, exc):
    converter = VegaConverter("f", input_data, properties)
    with pytest.raises(exc):
        converter.flat_datapoints("r")


@pytest.mark.parametrize(
    "properties,label",
    [
        ({"x": {"actual.csv": "actual"}}, "actual"),
        (
            {"x": {"train_actual.csv": "actual", "val_actual.csv": "actual"}},
            "actual",
        ),
        (
            {"x": {"actual.csv": "actual", "predicted.csv": "predicted"}},
            "x",
        ),
    ],
)
def test_infer_x_label(properties, label):
    assert VegaConverter.infer_x_label(properties) == label




tests/unit/repo/__init__.py




tests/unit/repo/test_open_repo.py
import os
from unittest.mock import call

import pytest

from dvc.repo.open_repo import _external_repo as external_repo
from dvc.testing.tmp_dir import make_subrepo


def test_hook_is_called(tmp_dir, erepo_dir, mocker):
    subrepo_paths = [
        "subrepo1",
        "subrepo2",
        os.path.join("dir", "subrepo3"),
        os.path.join("dir", "subrepo4"),
        "subrepo5",
        os.path.join("subrepo5", "subrepo6"),
    ]
    subrepos = [erepo_dir / path for path in subrepo_paths]
    for repo in subrepos:
        make_subrepo(repo, erepo_dir.scm)

    for repo in [*subrepos, erepo_dir]:
        with repo.chdir():
            repo.scm_gen("foo", "foo", commit=f"git add {repo}/foo")
            repo.dvc_gen("bar", "bar", commit=f"dvc add {repo}/bar")

    with external_repo(str(erepo_dir), subrepos=True, uninitialized=True) as repo:
        spy = mocker.spy(repo.dvcfs.fs, "repo_factory")

        list(repo.dvcfs.walk("", ignore_subrepos=False))  # drain
        assert spy.call_count == len(subrepos)

        paths = ["/" + path.replace("\\", "/") for path in subrepo_paths]
        spy.assert_has_calls(
            [
                call(
                    path,
                    fs=repo.fs,
                    scm=repo.scm,
                    repo_factory=repo.dvcfs.fs.repo_factory,
                )
                for path in paths
            ],
            any_order=True,
        )


@pytest.mark.parametrize("root_is_dvc", [False, True])
def test_subrepo_is_constructed_properly(
    tmp_dir, scm, mocker, make_tmp_dir, root_is_dvc
):
    if root_is_dvc:
        make_subrepo(tmp_dir, scm)

    subrepo = tmp_dir / "subrepo"
    make_subrepo(subrepo, scm)
    local_cache = subrepo.dvc.cache.local.path

    tmp_dir.scm_gen("bar", "bar", commit="add bar")
    subrepo.dvc_gen("foo", "foo", commit="add foo")

    cache_dir = make_tmp_dir("temp-cache")
    with external_repo(
        str(tmp_dir),
        subrepos=True,
        uninitialized=True,
        config={"cache": {"dir": str(cache_dir), "type": ["symlink"]}},
    ) as repo:
        spy = mocker.spy(repo.dvcfs.fs, "repo_factory")

        list(repo.dvcfs.walk("", ignore_subrepos=False))  # drain
        assert spy.call_count == 1
        subrepo = spy.spy_return

        assert repo.url == str(tmp_dir)
        assert repo.config["cache"]["dir"] == str(cache_dir)
        assert repo.cache.local.path == str(cache_dir)
        assert subrepo.cache.local.path == str(cache_dir)

        assert repo.config["cache"]["type"] == ["symlink"]
        assert repo.cache.local.cache_types == ["symlink"]
        assert subrepo.cache.local.cache_types == ["symlink"]

        assert subrepo.config["remote"]["auto-generated-upstream"]["url"] == local_cache
        if root_is_dvc:
            main_cache = tmp_dir.dvc.cache.local.path
            assert repo.config["remote"]["auto-generated-upstream"]["url"] == str(
                main_cache
            )




tests/unit/repo/test_repo.py
import os
import shutil

import pytest

from dvc.exceptions import OutputDuplicationError
from dvc.repo import NotDvcRepoError, Repo, locked
from dvc_data.hashfile.hash_info import HashInfo


def test_is_dvc_internal(dvc):
    assert dvc.is_dvc_internal(os.path.join("path", "to", ".dvc", "file"))
    assert not dvc.is_dvc_internal(os.path.join("path", "to-non-.dvc", "file"))


@pytest.mark.parametrize(
    "path",
    [
        os.path.join("dir", "subdir", "file"),
        os.path.join("dir", "subdir"),
        "dir",
    ],
)
def test_find_outs_by_path(tmp_dir, dvc, path):
    (stage,) = tmp_dir.dvc_gen({"dir": {"subdir": {"file": "file"}, "other": "other"}})

    outs = dvc.find_outs_by_path(path, strict=False)
    assert len(outs) == 1
    assert outs[0].fs_path == stage.outs[0].fs_path


def test_find_outs_by_path_does_graph_checks(tmp_dir, dvc):
    tmp_dir.dvc_gen("foo", "foo")
    shutil.copyfile("foo.dvc", "foo-2.dvc")

    dvc._reset()
    with pytest.raises(OutputDuplicationError):
        dvc.find_outs_by_path("foo")


@pytest.mark.parametrize(
    "path",
    [os.path.join("dir", "subdir", "file"), os.path.join("dir", "subdir")],
)
def test_used_objs(tmp_dir, dvc, path):
    tmp_dir.dvc_gen({"dir": {"subdir": {"file": "file"}, "other": "other"}})

    expected = {
        HashInfo("md5", "70922d6bf66eb073053a82f77d58c536.dir"),
        HashInfo("md5", "8c7dd922ad47494fc02c388e12c00eac"),
    }

    used = set()
    for _, obj_ids in dvc.used_objs([path]).items():
        used.update(obj_ids)

    assert used == expected


def test_locked(mocker):
    repo = mocker.MagicMock()
    repo._lock_depth = 0
    repo.method = locked(repo.method)

    args = ()
    kwargs = {}
    repo.method(repo, args, kwargs)

    assert repo.method_calls == [
        mocker.call._reset(),
        mocker.call.method(repo, args, kwargs),
        mocker.call._reset(),
    ]


def test_skip_graph_checks(tmp_dir, dvc, mocker, run_copy):
    # See https://github.com/iterative/dvc/issues/2671 for more info
    from dvc.repo.index import Index

    mock_build_graph = mocker.spy(Index.graph, "fget")

    # sanity check
    tmp_dir.gen("foo", "foo text")
    dvc.add("foo")
    run_copy("foo", "bar", single_stage=True)
    assert mock_build_graph.called

    # check that our hack can be enabled
    mock_build_graph.reset_mock()
    dvc._skip_graph_checks = True
    tmp_dir.gen("baz", "baz text")
    run_copy("baz", "qux", single_stage=True)
    assert not mock_build_graph.called

    # check that our hack can be disabled
    mock_build_graph.reset_mock()
    dvc._skip_graph_checks = False
    tmp_dir.gen("quux", "quux text")
    run_copy("quux", "quuz", single_stage=True)
    assert mock_build_graph.called


def test_branch_config(tmp_dir, scm):
    tmp_dir.scm_gen("foo", "foo", commit="init")

    # sanity check
    with pytest.raises(NotDvcRepoError):
        Repo().close()

    scm.checkout("branch", create_new=True)
    dvc = Repo.init()
    with dvc.config.edit() as conf:
        conf["remote"]["branch"] = {"url": "/some/path"}
    dvc.close()

    scm.add([os.path.join(".dvc", "config")])
    scm.commit("init dvc")
    scm.checkout("master")

    with pytest.raises(NotDvcRepoError):
        Repo(rev="master").close()

    dvc = Repo(rev="branch")
    try:
        assert dvc.config["remote"]["branch"]["url"] == "/some/path"
    finally:
        dvc.close()


def test_dynamic_cache_initalization(tmp_dir, scm):
    dvc = Repo.init()
    with dvc.config.edit() as conf:
        conf["cache"]["ssh"] = "foo"
        conf["remote"]["foo"] = {"url": "remote://bar/baz"}
    dvc.close()

    Repo(str(tmp_dir)).close()




tests/unit/repo/test_reproduce.py
import os

from dvc.repo.reproduce import _get_stage_files


def test_number_reproduces(tmp_dir, dvc, mocker):
    reproduce_stage_mock = mocker.patch(
        "dvc.repo.reproduce._reproduce_stage", returns=[]
    )
    tmp_dir.dvc_gen({"pre-foo": "pre-foo"})

    dvc.run(single_stage=True, deps=["pre-foo"], outs=["foo"], cmd="echo foo > foo")
    dvc.run(single_stage=True, deps=["foo"], outs=["bar"], cmd="echo bar > bar")
    dvc.run(single_stage=True, deps=["foo"], outs=["baz"], cmd="echo baz > baz")
    dvc.run(single_stage=True, deps=["bar"], outs=["boop"], cmd="echo boop > boop")

    reproduce_stage_mock.reset_mock()

    dvc.reproduce(all_pipelines=True)

    assert reproduce_stage_mock.call_count == 5


def test_get_stage_files(tmp_dir, dvc):
    tmp_dir.dvc_gen("dvc-dep", "dvc-dep")
    tmp_dir.gen("other-dep", "other-dep")

    stage = dvc.stage.add(
        name="stage",
        cmd="foo",
        deps=["dvc-dep", "other-dep"],
        outs=["dvc-out"],
        outs_no_cache=["other-out"],
    )
    result = set(_get_stage_files(stage))
    assert result == {
        stage.dvcfile.relpath,
        str(tmp_dir / "other-dep"),
        str(tmp_dir / "other-out"),
    }


def test_get_stage_files_wdir(tmp_dir, dvc):
    tmp_dir.gen({"dir": {"dvc-dep": "dvc-dep", "other-dep": "other-dep"}})
    dvc.add(os.path.join("dir", "dvc-dep"))

    stage = dvc.stage.add(
        name="stage",
        cmd="foo",
        wdir="dir",
        deps=["dvc-dep", "other-dep"],
        outs=["dvc-out"],
        outs_no_cache=["other-out"],
    )
    result = set(_get_stage_files(stage))
    assert result == {
        stage.dvcfile.relpath,
        str(tmp_dir / "dir" / "other-dep"),
        str(tmp_dir / "dir" / "other-out"),
    }




tests/unit/repo/test_run.py
import pytest

from dvc.exceptions import InvalidArgumentError


def test_file(tmp_dir, dvc):
    msg = (
        "`--file` is currently incompatible with `-n|--name` "
        "and requires `--single-stage`"
    )
    with pytest.raises(InvalidArgumentError, match=msg):
        dvc.run(fname="path/dvc.yaml", name="my", cmd="mycmd")




tests/unit/repo/test_scm_context.py
import re

import pytest

from dvc.repo.scm_context import SCMContext
from dvc.scm import Git, NoSCM


def pytest_generate_tests(metafunc):
    if "scm_context" in metafunc.fixturenames:
        metafunc.parametrize("scm_context", ["scm", "no_scm"], indirect=True)


@pytest.fixture
def scm_context(request, mocker):
    spec = Git if getattr(request, "param", "scm") == "scm" else NoSCM
    # we'll test `ignore` and `ignore_remove` in a functional test.
    return SCMContext(
        scm=mocker.MagicMock(
            spec=spec,
            **{
                "ignore_remove.return_value": ".gitignore",
                "ignore.return_value": ".gitignore",
            },
        )
    )


def test_scm_track_file(scm_context):
    scm_context.track_file("foo")
    assert scm_context.files_to_track == {"foo"}
    scm_context.track_file("bar")
    assert scm_context.files_to_track == {"foo", "bar"}


def test_scm_track_changed_files(scm_context):
    scm_context.track_changed_files()
    scm_context.scm.add.assert_not_called()

    scm_context.track_file("foo")
    scm_context.track_changed_files()
    scm_context.scm.add.assert_called_once_with({"foo"})


def test_ignore(scm_context):
    scm_context.ignore("foo")

    scm_context.scm.ignore.assert_called_once_with("foo")
    assert scm_context.files_to_track == {".gitignore"}
    assert scm_context.ignored_paths == ["foo"]


def test_ignore_remove(scm_context):
    scm_context.ignore_remove("foo")
    scm_context.scm.ignore_remove.assert_called_once_with("foo")
    assert scm_context.files_to_track == {".gitignore"}


def test_scm_context_reset_on_exit(scm_context):
    with scm_context:
        scm_context.ignore("foo")
        scm_context.track_file("bar")
    assert not scm_context.files_to_track
    assert not scm_context.ignored_paths


def test_scm_context_autostage_changed_files(scm_context):
    scm_context.autostage = True

    with scm_context:
        scm_context.track_file("foo")
        assert scm_context.files_to_track == {"foo"}

    assert not scm_context.files_to_track
    assert not scm_context.ignored_paths
    scm_context.scm.add.assert_called_once_with({"foo"})


def test_scm_context_clears_ignores_on_error(scm_context):
    class CustomException(Exception):
        pass

    with pytest.raises(CustomException), scm_context():  # noqa: PT012
        scm_context.ignore("foo")
        assert scm_context.ignored_paths == ["foo"]
        raise CustomException

    scm_context.scm.ignore_remove.assert_called_once_with("foo")
    assert scm_context.files_to_track == {".gitignore"}
    assert not scm_context.ignored_paths


@pytest.mark.parametrize("autostage", [True, False])
@pytest.mark.parametrize("quiet", [True, False])
def test_scm_context_on_no_files_to_track(caplog, scm_context, autostage, quiet):
    with scm_context(autostage=autostage, quiet=quiet):
        pass

    scm_context.scm.assert_not_called()
    assert not caplog.text


def test_scm_context_remind_to_track(caplog, scm_context):
    with scm_context() as context:
        context.track_file("foo")
        context.track_file("lorem ipsum")
        assert context.files_to_track == {"foo", "lorem ipsum"}

    if isinstance(scm_context.scm, NoSCM):
        assert not caplog.text
    else:
        assert "To track the changes with git, run:" in caplog.text
        match = re.search(r"git add(?: (('.*?')|(\S+)))*", caplog.text)
        assert match
        assert set(match.groups()) == {"'lorem ipsum'", "foo"}


def test_scm_context_remind_disable(caplog, scm_context):
    with scm_context(quiet=True) as context:
        context.track_file("foo")
        assert context.files_to_track == {"foo"}
    assert not caplog.text

    assert scm_context.quiet is False
    scm_context.quiet = True
    with scm_context() as context:
        context.track_file("foo")
        assert context.files_to_track == {"foo"}
    assert not caplog.text


def test_scm_context_decorator(scm_context, mocker):
    from dvc.repo.scm_context import scm_context as decorator

    repo = mocker.MagicMock(scm_context=scm_context)

    def test_method(repo, *args, **kwargs):
        scm_context.track_file("foo")

    method = mocker.MagicMock(wraps=test_method)
    decorator(method, autostage=True)(repo, "arg", kw=1)
    method.assert_called_once_with(repo, "arg", kw=1)
    scm_context.scm.add.assert_called_once_with({"foo"})




tests/unit/repo/experiments/__init__.py




tests/unit/repo/experiments/conftest.py
from functools import partial

import pytest

from dvc_task.app import FSApp

DEFAULT_ITERATIONS = 2


@pytest.fixture
def exp_stage(tmp_dir, scm, dvc, copy_script):
    tmp_dir.gen("params.yaml", "foo: 1")
    stage = dvc.run(
        cmd="python copy.py params.yaml metrics.yaml",
        metrics_no_cache=["metrics.yaml"],
        params=["foo"],
        name="copy-file",
        deps=["copy.py"],
    )
    scm.add(
        [
            "dvc.yaml",
            "dvc.lock",
            "copy.py",
            "params.yaml",
            "metrics.yaml",
            ".gitignore",
        ]
    )
    scm.commit("init")
    return stage


@pytest.fixture
def failed_exp_stage(tmp_dir, scm, dvc, copy_script):
    tmp_dir.gen("params.yaml", "foo: 1")
    stage = dvc.stage.add(
        cmd="python -c 'import sys; sys.exit(1)'",
        metrics_no_cache=["failed-metrics.yaml"],
        params=["foo"],
        name="failed-copy-file",
        deps=["copy.py"],
    )
    scm.add(
        [
            "dvc.yaml",
            "dvc.lock",
            "copy.py",
            "params.yaml",
            "failed-metrics.yaml",
            ".gitignore",
        ]
    )
    scm.commit("init")
    return stage


def _thread_worker(app, **kwargs):
    # Based on pytest-celery's celery_worker fixture but using thread pool
    # instead of solo pool so that broadcast/control API is available
    from celery.contrib.testing import worker

    app.loader.import_task_module("celery.contrib.testing.tasks")
    return worker.start_worker(app, pool="threads", **kwargs)


@pytest.fixture(scope="session")
def session_app(tmp_path_factory) -> FSApp:
    """Session scoped experiments queue celery app."""
    from kombu.transport.filesystem import Channel

    # related to https://github.com/iterative/dvc-task/issues/61
    Channel.QoS.restore_at_shutdown = False

    from dvc_task.app import FSApp

    wdir = tmp_path_factory.mktemp("dvc-test-celery")
    app = FSApp(
        "dvc-exp-local",
        wdir=wdir,
        mkdir=True,
        include=[
            "dvc.repo.experiments.queue.tasks",
            "dvc_task.proc.tasks",
        ],
    )
    app.conf.update({"task_acks_late": True, "result_expires": None})
    return app


@pytest.fixture(scope="session")
def session_worker(session_app):
    """Session scoped celery worker that runs in separate thread(s)."""
    with _thread_worker(
        session_app,
        concurrency=1,
        ping_task_timeout=20,
        loglevel="DEBUG",
    ) as worker:
        yield worker


@pytest.fixture
def session_queue(tmp_dir, dvc, scm, mocker, session_app, session_worker):
    """Patches experiments celery queue for pytest testing.

    Uses session-scoped celery worker.
    """
    queue = dvc.experiments.celery_queue
    queue.celery = session_app
    queue.worker = session_worker
    mocker.patch.object(queue, "_spawn_worker")
    return queue


@pytest.fixture
def test_queue(tmp_dir, dvc, scm, mocker):
    """Patches experiments celery queue for pytest testing.

    Uses function-scoped celery worker which runs in separate thread(s).
    """
    import celery

    queue = dvc.experiments.celery_queue
    mocker.patch.object(queue, "_spawn_worker")

    f = partial(
        _thread_worker,
        queue.celery,
        concurrency=1,
        ping_task_timeout=20,
    )
    exc = None
    for _ in range(3):
        try:
            with f() as worker:
                mocker.patch.object(queue, "worker", return_value=worker)
                yield queue
                return
        except celery.exceptions.TimeoutError as e:
            exc = e
            continue
    assert exc
    raise exc




tests/unit/repo/experiments/test_executor_status.py
import os

import pytest

from dvc.exceptions import ReproductionError
from dvc.repo.experiments.executor.base import ExecutorInfo, TaskStatus
from dvc.repo.experiments.queue.tasks import cleanup_exp, collect_exp, setup_exp


def test_celery_queue_success_status(dvc, scm, test_queue, exp_stage):
    queue_entry = test_queue._stash_exp(
        params={"params.yaml": ["foo=1"]},
        targets=exp_stage.addressing,
        name="success",
    )
    infofile = test_queue.get_infofile_path(queue_entry.stash_rev)
    executor = setup_exp.s(queue_entry.asdict())()
    executor_info = ExecutorInfo.load_json(infofile)
    assert executor_info.status == TaskStatus.PREPARING

    cmd = ["dvc", "exp", "exec-run", "--infofile", infofile]
    proc_dict = test_queue.proc.run_signature(cmd, name=queue_entry.stash_rev)()

    executor_info = ExecutorInfo.load_json(infofile)
    assert executor_info.status == TaskStatus.SUCCESS

    collect_exp.s(proc_dict, queue_entry.asdict())()
    cleanup_exp.s(executor, infofile)()
    executor_info = ExecutorInfo.load_json(infofile)
    assert executor_info.status == TaskStatus.FINISHED


def test_celery_queue_failure_status(dvc, scm, test_queue, failed_exp_stage):
    queue_entry = test_queue._stash_exp(
        params={"params.yaml": ["foo=1"]},
        targets=failed_exp_stage.addressing,
        name="failed",
    )
    infofile = test_queue.get_infofile_path(queue_entry.stash_rev)
    setup_exp.s(queue_entry.asdict())()
    cmd = ["dvc", "exp", "exec-run", "--infofile", infofile]
    test_queue.proc.run_signature(cmd, name=queue_entry.stash_rev)()
    executor_info = ExecutorInfo.load_json(infofile)
    assert executor_info.status == TaskStatus.FAILED


@pytest.mark.parametrize("queue_type", ["workspace_queue", "tempdir_queue"])
def test_workspace_executor_success_status(dvc, scm, exp_stage, queue_type):
    workspace_queue = getattr(dvc.experiments, queue_type)
    queue_entry = workspace_queue.put(
        params={"params.yaml": ["foo=1"]},
        targets=exp_stage.addressing,
        name="success",
    )
    name = workspace_queue._EXEC_NAME or queue_entry.stash_rev
    infofile = workspace_queue.get_infofile_path(name)
    entry, executor = workspace_queue.get()
    rev = entry.stash_rev
    exec_result = executor.reproduce(
        info=executor.info,
        rev=rev,
        infofile=infofile,
    )
    executor_info = ExecutorInfo.load_json(infofile)
    assert executor_info.status == TaskStatus.SUCCESS
    if exec_result.ref_info:
        workspace_queue.collect_executor(dvc.experiments, executor, exec_result)
    executor.cleanup(infofile)

    if queue_type == "tempdir_queue":
        executor_info = ExecutorInfo.load_json(infofile)
        assert executor_info.status == TaskStatus.FINISHED
    else:
        assert not os.path.exists(infofile)


@pytest.mark.parametrize(
    "queue_type",
    ["workspace_queue", "tempdir_queue"],
)
def test_workspace_executor_failed_status(dvc, scm, failed_exp_stage, queue_type):
    queue = getattr(dvc.experiments, queue_type)
    queue.put(
        params={"params.yaml": ["foo=1"]},
        targets=failed_exp_stage.addressing,
        name="failed",
    )
    entry, executor = queue.get()
    name = queue._EXEC_NAME or entry.stash_rev
    infofile = queue.get_infofile_path(name)
    rev = entry.stash_rev

    with pytest.raises(ReproductionError):
        executor.reproduce(
            info=executor.info,
            rev=rev,
            infofile=infofile,
        )
    executor_info = ExecutorInfo.load_json(infofile)
    assert executor_info.status == TaskStatus.FAILED

    cleanup_exp.s(executor, infofile)()
    if queue_type == "workspace_queue":
        assert not os.path.exists(infofile)
    else:
        executor_info = ExecutorInfo.load_json(infofile)
        assert executor_info.status == TaskStatus.FAILED


def test_executor_status_compatiblity():
    data = {
        "git_url": "file:///Users/home",
        "baseline_rev": "123",
        "location": "dvc-task",
        "root_dir": "/Users/home/8088/.dvc/tmp/exps/tmpx85892cx",
        "dvc_dir": ".dvc",
        "collected": True,
    }
    result = ExecutorInfo.from_dict(data)
    assert result.status == TaskStatus.FINISHED




tests/unit/repo/experiments/test_remove.py
from dvc.repo.experiments.queue.base import QueueDoneResult


def test_remove_done_tasks(dvc, test_queue, scm, mocker):
    from funcy import concat

    failed_test = ["failed1", "failed2"]
    success_test = ["success1", "success2"]

    # create mock ref info
    ref_info_dict = {}
    for name in success_test:
        ref_info_dict[name] = mocker.Mock()
        ref_info_dict[name].name = name
    for name in failed_test:
        ref_info_dict[name] = None

    # create mock queue entry
    entry_dict = {}
    for name in concat(failed_test, success_test):
        entry_dict[name] = mocker.Mock(stash_rev=name)
        entry_dict[name].name = name

    done_iter = [
        QueueDoneResult(entry_dict[name], None)
        for name in concat(failed_test, success_test)
    ]

    mocker.patch.object(test_queue, "iter_done", return_value=done_iter)

    mocker.patch(
        "dvc.repo.experiments.utils.resolve_name",
        autospec=True,
        return_value=ref_info_dict,
    )

    remove_exp_refs = mocker.patch(
        "dvc.repo.experiments.utils.remove_exp_refs",
    )
    remove_tasks_mocker = mocker.patch(
        "dvc.repo.experiments.queue.remove.remove_tasks",
    )

    assert (
        dvc.experiments.remove(failed_test + success_test) == failed_test + success_test
    )

    remove_tasks_mocker.assert_called_once_with(
        test_queue,
        [entry_dict[name] for name in failed_test + success_test],
    )

    remove_exp_refs.assert_called_once_with(
        dvc.scm, [ref_info_dict[name] for name in success_test]
    )




tests/unit/repo/experiments/test_utils.py
import pytest

from dvc.exceptions import InvalidArgumentError
from dvc.repo.experiments.refs import EXPS_NAMESPACE, ExpRefInfo
from dvc.repo.experiments.utils import check_ref_format, resolve_name, to_studio_params


def commit_exp_ref(tmp_dir, scm, file="foo", contents="foo", name="foo"):
    tmp_dir.scm_gen(file, contents, commit="init")
    rev = scm.get_rev()
    ref = "/".join([EXPS_NAMESPACE, "ab", "c123", name])
    scm.gitpython.set_ref(ref, rev)
    return ref, rev


@pytest.mark.parametrize("use_url", [True, False])
@pytest.mark.parametrize("name_only", [True, False])
def test_resolve_exp_ref(tmp_dir, scm, git_upstream, name_only, use_url):
    ref, _ = commit_exp_ref(tmp_dir, scm)
    name = "foo" if name_only else ref
    result = resolve_name(scm, [name, "notexist"])
    assert isinstance(result[name], ExpRefInfo)
    assert str(result[name]) == ref
    assert result["notexist"] is None

    scm.push_refspecs(git_upstream.url, f"{ref}:{ref}")
    remote = git_upstream.url if use_url else git_upstream.remote
    name = "foo" if name_only else ref
    remote_ref_info = resolve_name(scm, [name], remote)[name]
    assert isinstance(remote_ref_info, ExpRefInfo)
    assert str(remote_ref_info) == ref


@pytest.mark.parametrize(
    "name,result",
    [
        ("name", True),
        ("group/name", False),
        ("na me", False),
        ("invalid/.name", False),
        ("@", pytest.param(False, marks=pytest.mark.xfail)),
        (":", False),
        ("^", False),
        ("*", False),
        ("~", False),
        ("?", False),
    ],
)
def test_run_check_ref_format(scm, name, result):
    ref = ExpRefInfo("abc123", name)
    if result:
        check_ref_format(scm, ref)
    else:
        with pytest.raises(InvalidArgumentError):
            check_ref_format(scm, ref)


@pytest.mark.parametrize(
    "params,expected",
    [
        (
            {"workspace": {"data": {"params.yaml": {"data": {"foo": 1}}}}},
            {"params.yaml": {"foo": 1}},
        ),
        (
            {"workspace": {"data": {"params.yaml": {"error": "FileNotFound"}}}},
            {"params.yaml": {}},
        ),
        (
            {"workspace": {"error": "something went wrong"}},
            {},
        ),
    ],
)
def test_to_studio_params(params, expected):
    assert to_studio_params(params) == expected




tests/unit/repo/experiments/queue/__init__.py




tests/unit/repo/experiments/queue/test_celery.py
import time

import pytest
from celery import shared_task
from celery.result import AsyncResult

from dvc.exceptions import DvcException
from dvc.repo.experiments.exceptions import UnresolvedExpNamesError
from dvc.repo.experiments.queue.base import QueueDoneResult
from dvc.repo.experiments.queue.exceptions import CannotKillTasksError


def test_shutdown_no_tasks(test_queue, mocker):
    shutdown_spy = mocker.spy(test_queue.celery.control, "shutdown")
    test_queue.shutdown()
    shutdown_spy.assert_called_once()


@shared_task
def _foo(arg=None):
    return "foo"


def test_shutdown(test_queue, mocker):
    shutdown_spy = mocker.patch("celery.app.control.Control.shutdown")
    test_queue.shutdown()
    shutdown_spy.assert_called_once()


def test_shutdown_with_kill(test_queue, mocker):
    mock_entry_foo = mocker.Mock(stash_rev="af12de")
    mock_entry_foo.name = "foo"
    mock_entry_bar = mocker.Mock(stash_rev="bar")
    mock_entry_bar.name = None

    shutdown_spy = mocker.patch("celery.app.control.Control.shutdown")
    mocker.patch.object(
        test_queue,
        "iter_active",
        return_value=[mock_entry_foo, mock_entry_bar],
    )
    kill_spy = mocker.patch.object(test_queue, "_kill_entries")

    test_queue.shutdown(kill=True)

    shutdown_spy.assert_called_once()
    kill_spy.assert_called_once_with(
        {mock_entry_foo: "foo", mock_entry_bar: "bar"}, True
    )


def test_post_run_after_kill(test_queue):
    from celery import chain

    sig_bar = test_queue.proc.run_signature(
        ["python3", "-c", "import time; time.sleep(10)"], name="bar"
    )
    sig_bar.freeze()
    sig_foo = _foo.s()
    result_foo = sig_foo.freeze()
    run_chain = chain(sig_bar, sig_foo)

    run_chain.delay()
    timeout = time.time() + 10

    while True:
        try:
            test_queue.proc.kill("bar")
            assert result_foo.status == "PENDING"
            break
        except ProcessLookupError:
            time.sleep(0.1)
        if time.time() > timeout:
            raise TimeoutError()

    assert result_foo.get(timeout=10) == "foo"


@pytest.mark.parametrize("force", [True, False])
def test_celery_queue_kill(test_queue, mocker, force):
    mock_entry_foo = mocker.Mock(stash_rev="foo")
    mock_entry_bar = mocker.Mock(stash_rev="bar")
    mock_entry_foobar = mocker.Mock(stash_rev="foobar")

    mocker.patch.object(
        test_queue,
        "iter_active",
        return_value={mock_entry_foo, mock_entry_bar, mock_entry_foobar},
    )
    mocker.patch.object(
        test_queue,
        "match_queue_entry_by_name",
        return_value={
            "bar": mock_entry_bar,
            "foo": mock_entry_foo,
            "foobar": mock_entry_foobar,
        },
    )
    mocker.patch.object(
        test_queue,
        "_get_running_task_ids",
        return_value={"foo", "foobar"},
    )
    mocker.patch.object(
        test_queue,
        "_iter_processed",
        return_value=[
            (mocker.Mock(headers={"id": "foo"}), mock_entry_foo),
            (mocker.Mock(headers={"id": "bar"}), mock_entry_bar),
            (mocker.Mock(headers={"id": "foobar"}), mock_entry_foobar),
        ],
    )
    mocker.patch.object(
        AsyncResult,
        "ready",
        return_value=False,
    )
    mark_mocker = mocker.patch.object(
        test_queue.celery.backend,
        "mark_as_failure",
    )

    def kill_function(rev):
        if rev == "foo":
            return True
        raise ProcessLookupError

    kill_mock = mocker.patch.object(
        test_queue.proc,
        "kill" if force else "interrupt",
        side_effect=mocker.MagicMock(side_effect=kill_function),
    )
    with pytest.raises(CannotKillTasksError, match="Task 'foobar' is initializing,"):
        test_queue.kill(["bar", "foo", "foobar"], force=force)
    assert kill_mock.called_once_with(mock_entry_foo.stash_rev)
    assert kill_mock.called_once_with(mock_entry_bar.stash_rev)
    assert kill_mock.called_once_with(mock_entry_foobar.stash_rev)
    mark_mocker.assert_called_once_with("bar", None)


@pytest.mark.parametrize("force", [True, False])
def test_celery_queue_kill_invalid(test_queue, mocker, force):
    mock_entry_foo = mocker.Mock(stash_rev="foo")
    mock_entry_bar = mocker.Mock(stash_rev="bar")

    mocker.patch.object(
        test_queue,
        "match_queue_entry_by_name",
        return_value={
            "bar": mock_entry_bar,
            "foo": mock_entry_foo,
            "foobar": None,
        },
    )

    kill_mock = mocker.patch.object(test_queue, "_kill_entries")

    with pytest.raises(UnresolvedExpNamesError):
        test_queue.kill(["bar", "foo", "foobar"], force=force)
    assert kill_mock.called_once_with(
        {mock_entry_foo: "foo", mock_entry_bar: "bar"}, force
    )


@pytest.mark.parametrize("status", ["FAILURE", "SUCCESS"])
def test_queue_iter_done_task(test_queue, mocker, status):
    mock_entry = mocker.Mock(stash_rev=_foo.name)

    result = mocker.Mock(status=status)

    mocker.patch.object(
        test_queue,
        "_iter_done_tasks",
        return_value=[(result, mock_entry)],
    )

    if status == "FAILURE":
        assert list(test_queue.iter_failed()) == [QueueDoneResult(mock_entry, None)]

    elif status == "SUCCESS":
        with pytest.raises(DvcException, match="Invalid experiment"):
            assert list(test_queue.iter_success())


def test_queue_status(test_queue, scm, mocker):
    from datetime import datetime

    active_entry = mocker.Mock(stash_rev="active")
    active_entry.name = "foo"
    queued_entry = mocker.Mock(stash_rev="queued")
    queued_entry.name = None
    failed_entry = mocker.Mock(stash_rev="failed")
    failed_entry.name = "bar"
    success_entry = mocker.Mock(stash_rev="success")
    success_entry.name = None
    success_result = mocker.Mock(ref_info=mocker.Mock())
    success_result.ref_info.name = "foobar"

    def resolve_commit(rev):
        if rev == "active":
            commit_time = datetime(2022, 8, 7).timestamp()
        elif rev == "queued":
            commit_time = datetime(2022, 8, 6).timestamp()
        elif rev == "failed":
            commit_time = datetime(2022, 8, 5).timestamp()
        elif rev == "success":
            commit_time = datetime(2022, 8, 4).timestamp()
        return mocker.Mock(commit_time=commit_time)

    mocker.patch.object(
        scm,
        "resolve_commit",
        side_effect=mocker.MagicMock(side_effect=resolve_commit),
    )

    mocker.patch.object(
        test_queue,
        "iter_active",
        return_value=[active_entry],
    )
    mocker.patch.object(
        test_queue,
        "iter_queued",
        return_value=[queued_entry],
    )
    mocker.patch.object(
        test_queue,
        "iter_failed",
        return_value=[(failed_entry, None)],
    )
    mocker.patch.object(
        test_queue,
        "iter_success",
        return_value=[(success_entry, success_result)],
    )

    assert test_queue.status() == [
        {
            "name": "foo",
            "rev": "active",
            "status": "Running",
            "timestamp": datetime(2022, 8, 7, 0, 0, 0),
        },
        {
            "name": None,
            "rev": "queued",
            "status": "Queued",
            "timestamp": datetime(2022, 8, 6, 0, 0, 0),
        },
        {
            "name": "bar",
            "rev": "failed",
            "status": "Failed",
            "timestamp": datetime(2022, 8, 5, 0, 0, 0),
        },
        {
            "name": "foobar",
            "rev": "success",
            "status": "Success",
            "timestamp": datetime(2022, 8, 4, 0, 0, 0),
        },
    ]




tests/unit/repo/experiments/queue/test_remove.py
from unittest.mock import call

from dvc.repo.experiments.queue.base import QueueDoneResult


def test_remove_queued(test_queue, mocker):
    queued_test = ["queue1", "queue2", "queue3"]

    stash_dict = {}
    for name in queued_test:
        stash_dict[name] = mocker.Mock()

    msg_dict = {}
    entry_dict = {}
    for name in queued_test:
        msg_dict[name] = mocker.Mock(delivery_tag=f"msg_{name}")
        entry_dict[name] = mocker.Mock(stash_rev=name)
        entry_dict[name].name = name

    msg_iter = [(msg_dict[name], entry_dict[name]) for name in queued_test]
    entry_iter = [entry_dict[name] for name in queued_test]

    stash = mocker.patch.object(test_queue, "stash", return_value=mocker.Mock())
    stash.stash_revs = stash_dict
    mocker.patch.object(test_queue, "_iter_queued", return_value=msg_iter)
    mocker.patch.object(test_queue, "iter_queued", return_value=entry_iter)

    remove_revs_mocker = mocker.patch.object(test_queue.stash, "remove_revs")
    reject_mocker = mocker.patch.object(test_queue.celery, "reject")

    assert test_queue.remove(["queue2"]) == ["queue2"]
    reject_mocker.assert_called_once_with("msg_queue2")
    remove_revs_mocker.assert_called_once_with([stash_dict["queue2"]])
    remove_revs_mocker.reset_mock()
    reject_mocker.reset_mock()

    assert test_queue.clear(queued=True) == queued_test
    remove_revs_mocker.assert_called_once_with(list(stash_dict.values()))
    reject_mocker.assert_has_calls(
        [call("msg_queue1"), call("msg_queue2"), call("msg_queue3")]
    )


def test_remove_done(test_queue, mocker):
    from funcy import concat

    failed_test = ["failed1", "failed2", "failed3"]
    success_test = ["success1", "success2", "success3"]

    stash_dict = {}
    for name in failed_test:
        stash_dict[name] = mocker.Mock()

    msg_dict = {}
    entry_dict = {}
    for name in concat(failed_test, success_test):
        msg_dict[name] = mocker.Mock(delivery_tag=f"msg_{name}", headers={"id": 0})
        entry_dict[name] = mocker.Mock(stash_rev=name)
        entry_dict[name].name = name

    msg_iter = [
        (msg_dict[name], entry_dict[name]) for name in concat(failed_test, success_test)
    ]
    done_iter = [
        QueueDoneResult(entry_dict[name], None)
        for name in concat(failed_test, success_test)
    ]
    failed_iter = [QueueDoneResult(entry_dict[name], None) for name in failed_test]
    success_iter = [QueueDoneResult(entry_dict[name], None) for name in success_test]

    stash = mocker.patch.object(test_queue, "failed_stash", return_value=mocker.Mock())
    stash.stash_revs = stash_dict
    mocker.patch.object(test_queue, "_iter_processed", return_value=msg_iter)
    mocker.patch.object(test_queue, "iter_done", return_value=done_iter)
    mocker.patch.object(test_queue, "iter_success", return_value=success_iter)
    mocker.patch.object(test_queue, "iter_failed", return_value=failed_iter)
    mocker.patch("celery.result.AsyncResult", return_value=mocker.Mock())

    remove_revs_mocker = mocker.patch.object(test_queue.failed_stash, "remove_revs")
    purge_mocker = mocker.patch.object(test_queue.celery, "purge")

    assert test_queue.remove(["failed3", "success2"]) == [
        "failed3",
        "success2",
    ]
    remove_revs_mocker.assert_called_once_with([stash_dict["failed3"]])
    purge_mocker.assert_has_calls([call("msg_failed3"), call("msg_success2")])

    remove_revs_mocker.reset_mock()
    purge_mocker.reset_mock()

    assert set(test_queue.clear(success=True, failed=True)) == set(failed_test) | set(
        success_test
    )
    purge_mocker.assert_has_calls(
        [
            call("msg_failed1"),
            call("msg_failed2"),
            call("msg_failed3"),
            call("msg_success1"),
            call("msg_success2"),
            call("msg_success3"),
        ],
        any_order=True,
    )
    remove_revs_mocker.assert_called_once_with(list(stash_dict.values()))




tests/unit/repo/plots/__init__.py




tests/unit/repo/plots/test_diff.py
import pytest

from dvc.repo.plots.diff import _revisions


@pytest.mark.parametrize(
    "arg_revisions,is_dirty,expected_revisions",
    [
        ([], False, ["workspace"]),
        ([], True, ["HEAD", "workspace"]),
        (["v1", "v2", "workspace"], False, ["v1", "v2", "workspace"]),
        (["v1", "v2", "workspace"], True, ["v1", "v2", "workspace"]),
    ],
)
def test_revisions(mocker, arg_revisions, is_dirty, expected_revisions):
    mock_scm = mocker.Mock()
    mock_scm.configure_mock(
        **{"is_dirty.return_value": is_dirty, "get_ref.return_value": None}
    )
    mock_repo = mocker.Mock(scm=mock_scm)
    assert _revisions(mock_repo, arg_revisions, False) == expected_revisions


@pytest.mark.parametrize(
    "arg_revisions,baseline,expected_revisions",
    [
        (["v1"], "v0", ["v1", "v0"]),
        (["v1"], None, ["v1", "workspace"]),
        (["v1", "v2"], "v0", ["v1", "v2"]),
        (["v1", "v2"], None, ["v1", "v2"]),
    ],
)
def test_revisions_experiment(mocker, arg_revisions, baseline, expected_revisions):
    mock_scm = mocker.Mock()
    mock_scm.configure_mock(
        **{"is_dirty.return_value": False, "get_ref.return_value": None}
    )
    mock_experiments = mocker.Mock()
    mock_experiments.configure_mock(**{"get_baseline.return_value": baseline})
    mock_repo = mocker.Mock(scm=mock_scm, experiments=mock_experiments)
    assert _revisions(mock_repo, arg_revisions, True) == expected_revisions




tests/unit/scm/__init__.py




tests/unit/scm/test_scm.py
from datetime import datetime

from scmrepo.exceptions import SCMError

from dvc.repo.experiments import ExpRefInfo
from dvc.scm import iter_revs


def test_iter_revs(
    tmp_dir,
    scm,
    mocker,
):
    """
    new         other
     ‚îÇ            ‚îÇ
    old (tag) ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
    root
    """
    old = scm.active_branch()
    tmp_dir.scm_gen("foo", "init", commit="init")
    rev_root = scm.get_rev()
    tmp_dir.scm_gen("foo", "old", commit="old")
    rev_old = scm.get_rev()
    scm.checkout("new", create_new=True)
    tmp_dir.scm_gen("foo", "new", commit="new")
    rev_new = scm.get_rev()
    scm.checkout(old)
    scm.tag("tag")
    scm.checkout("other", create_new=True)
    tmp_dir.scm_gen("foo", "other", commit="new")
    rev_other = scm.get_rev()

    ref = ExpRefInfo(rev_root, "exp1")
    scm.set_ref(str(ref), rev_new)
    ref = ExpRefInfo(rev_root, "exp2")
    scm.set_ref(str(ref), rev_old)

    gen = iter_revs(scm, [rev_root, "new"], 1)
    assert gen == {rev_root: [rev_root], rev_new: ["new"]}
    gen = iter_revs(scm, ["new"], 2)
    assert gen == {rev_new: ["new"], rev_old: [rev_old]}
    gen = iter_revs(scm, ["other"], -1)
    assert gen == {
        rev_other: ["other"],
        rev_old: [rev_old],
        rev_root: [rev_root],
    }
    gen = iter_revs(scm, ["tag"])
    assert gen == {rev_old: ["tag"]}
    gen = iter_revs(scm, all_branches=True)
    assert gen == {rev_old: [old], rev_new: ["new"], rev_other: ["other"]}
    gen = iter_revs(scm, all_tags=True)
    assert gen == {rev_old: ["tag"]}
    gen = iter_revs(scm, all_commits=True)
    assert gen == {
        rev_old: [rev_old],
        rev_new: [rev_new],
        rev_other: [rev_other],
        rev_root: [rev_root],
    }
    gen = iter_revs(scm, all_experiments=True)
    assert gen == {
        rev_new: [rev_new],
        rev_old: [rev_old],
    }

    def _resolve_commit(rev):
        from scmrepo.git.objects import GitCommit

        if rev == rev_root:
            return GitCommit(
                "dummy",
                commit_time=datetime(2022, 6, 28).timestamp(),
                commit_time_offset=0,
                message="dummy",
                parents=["dummy"],
            )
        if rev == rev_old:
            raise SCMError
        return GitCommit(
            "dummy",
            commit_time=datetime(2022, 6, 30).timestamp(),
            commit_time_offset=0,
            message="dummy",
            parents=["dummy"],
        )

    mocker.patch(
        "scmrepo.git.Git.resolve_commit",
        mocker.MagicMock(side_effect=_resolve_commit),
    )

    gen = iter_revs(scm, commit_date="2022-06-29")
    assert gen == {
        rev_new: [rev_new],
        rev_old: [rev_old],
        rev_other: [rev_other],
    }




tests/unit/stage/__init__.py




tests/unit/stage/test_cache.py
import os

import pytest

import dvc.output as dvc_output


def test_stage_cache(tmp_dir, dvc, mocker):
    tmp_dir.gen("dep", "dep")
    tmp_dir.gen(
        "script.py",
        'open("out", "w+").write("out"); ',
    )
    stage = dvc.run(
        cmd="python script.py",
        deps=["script.py", "dep"],
        outs=["out"],
        single_stage=True,
    )

    with dvc.lock:
        stage.remove(remove_outs=True, force=True)

    assert not (tmp_dir / "out").exists()
    assert not (tmp_dir / "out.dvc").exists()

    cache_dir = os.path.join(
        dvc.stage_cache.cache_dir,
        "4b",
        "4b495dc2b14e1ca5bd5f2765a99ddd8785523a8342efe6bcadac012c2db01165",
    )
    cache_file = os.path.join(
        cache_dir,
        "78c427104a8e20216031c36d9c7620733066fff33ada254ad3fca551c1f8152b",
    )

    assert os.path.isdir(cache_dir)
    assert os.listdir(cache_dir) == [os.path.basename(cache_file)]
    assert os.path.isfile(cache_file)

    run_spy = mocker.patch("dvc.stage.run.cmd_run")
    checkout_spy = mocker.spy(dvc_output, "checkout")
    with dvc.lock:
        stage.run()

    assert not run_spy.called
    assert checkout_spy.call_count == 2

    assert (tmp_dir / "out").exists()
    assert (tmp_dir / "out").read_text() == "out"


def test_stage_cache_params(tmp_dir, dvc, mocker):
    tmp_dir.gen("params.yaml", "foo: 1\nbar: 2")
    tmp_dir.gen("myparams.yaml", "baz: 3\nqux: 4")
    tmp_dir.gen(
        "script.py",
        'open("out", "w+").write("out"); ',
    )
    stage = dvc.run(
        cmd="python script.py",
        params=["foo,bar", "myparams.yaml:baz,qux"],
        outs=["out"],
        single_stage=True,
    )

    with dvc.lock:
        stage.remove(remove_outs=True, force=True)

    assert not (tmp_dir / "out").exists()
    assert not (tmp_dir / "out.dvc").exists()

    cache_dir = os.path.join(
        dvc.stage_cache.cache_dir,
        "8f",
        "8fdb377d1b4c0a303b788771b122dfba9bbbbc43f14ce41d35715cf4fea08459",
    )
    cache_file = os.path.join(
        cache_dir,
        "202ea269108bf98bea3e15f978e7929864728956c9df8d927a5c7d74fc4fedc8",
    )

    assert os.path.isdir(cache_dir)
    assert os.listdir(cache_dir) == [os.path.basename(cache_file)]
    assert os.path.isfile(cache_file)

    run_spy = mocker.patch("dvc.stage.run.cmd_run")
    checkout_spy = mocker.spy(dvc_output, "checkout")
    with dvc.lock:
        stage.run()

    assert not run_spy.called
    assert checkout_spy.call_count == 2

    assert (tmp_dir / "out").exists()
    assert (tmp_dir / "out").read_text() == "out"


def test_stage_cache_wdir(tmp_dir, dvc, mocker):
    tmp_dir.gen("dep", "dep")
    tmp_dir.gen(
        "script.py",
        'open("out", "w+").write("out"); ',
    )
    tmp_dir.gen({"wdir": {}})
    stage = dvc.run(
        cmd="python ../script.py",
        deps=["../script.py", "../dep"],
        outs=["out"],
        single_stage=True,
        wdir="wdir",
    )

    with dvc.lock:
        stage.remove(remove_outs=True, force=True)

    assert not (tmp_dir / "wdir" / "out").exists()
    assert not (tmp_dir / "wdir" / "out.dvc").exists()

    cache_dir = os.path.join(
        dvc.stage_cache.cache_dir,
        "b5",
        "b5d5548c43725139aa3419eb50717e062fe9b81f866f401fd6fd778d2a97822d",
    )
    cache_file = os.path.join(
        cache_dir,
        "e0a59f6a5bd193032a4d1500abd89c9b08a2e9b26c724015e0bc6374295a3b9f",
    )

    assert os.path.isdir(cache_dir)
    assert os.listdir(cache_dir) == [os.path.basename(cache_file)]
    assert os.path.isfile(cache_file)

    run_spy = mocker.patch("dvc.stage.run.cmd_run")
    checkout_spy = mocker.spy(dvc_output, "checkout")
    with dvc.lock:
        stage.run()

    assert not run_spy.called
    assert checkout_spy.call_count == 2

    assert (tmp_dir / "wdir" / "out").exists()
    assert (tmp_dir / "wdir" / "out").read_text() == "out"


def test_shared_stage_cache(tmp_dir, dvc, run_copy):
    import stat

    from dvc.cachemgr import CacheManager

    tmp_dir.gen("foo", "foo")

    with dvc.config.edit() as config:
        config["cache"]["shared"] = "group"

    dvc.cache = CacheManager(dvc)

    assert not os.path.exists(dvc.cache.local.path)

    run_copy("foo", "bar", name="copy-foo-bar")

    parent_cache_dir = os.path.join(dvc.stage_cache.cache_dir, "fd")
    cache_dir = os.path.join(
        parent_cache_dir,
        "fdbe7847136ebe88f59a29ee5a568f893cab031f74f7d3ab050828b53fd0033a",
    )
    cache_file = os.path.join(
        cache_dir,
        "8716052b2074f5acf1a379529d47d6ef1c1f50c2281a7489b8d7ce251f234f86",
    )

    # sanity check
    assert os.path.isdir(cache_dir)
    assert os.listdir(cache_dir) == [os.path.basename(cache_file)]
    assert os.path.isfile(cache_file)

    def _mode(path):
        return stat.S_IMODE(os.stat(path).st_mode)

    if os.name == "nt":
        dir_mode = 0o777
        file_mode = 0o666
    else:
        dir_mode = 0o2775
        file_mode = 0o664

    assert _mode(dvc.cache.local.path) == dir_mode
    assert _mode(dvc.stage_cache.cache_dir) == dir_mode
    assert _mode(parent_cache_dir) == dir_mode
    assert _mode(cache_dir) == dir_mode
    assert _mode(cache_file) == file_mode


@pytest.mark.parametrize(
    "kwargs",
    [
        {},
        {"cmd": "cmd"},
        {"cmd": "cmd", "deps": ["path"]},
        {"cmd": "cmd", "outs": ["path"]},
        {"always_changed": True},
    ],
)
def test_unhashable(tmp_dir, dvc, mocker, kwargs):
    from dvc.stage import Stage, create_stage
    from dvc.stage.cache import RunCacheNotFoundError, StageCache

    cache = StageCache(dvc)
    stage = create_stage(Stage, path="stage.dvc", repo=dvc, **kwargs)
    get_stage_hash = mocker.patch("dvc.stage.cache._get_stage_hash")
    assert cache.save(stage) is None
    assert get_stage_hash.not_called
    with pytest.raises(RunCacheNotFoundError):
        cache.restore(stage)
    assert get_stage_hash.not_called




tests/unit/stage/test_loader_pipeline_file.py
import os
from copy import deepcopy
from itertools import chain

import pytest

from dvc.dvcfile import PROJECT_FILE, load_file
from dvc.stage import PipelineStage, create_stage
from dvc.stage.loader import StageLoader
from dvc.stage.serialize import split_params_deps
from dvc_data.hashfile.hash_info import HashInfo


@pytest.fixture
def stage_data():
    return {"cmd": "command", "deps": ["foo"], "outs": ["bar"]}


@pytest.fixture
def lock_data():
    return {
        "cmd": "command",
        "deps": [{"path": "foo", "md5": "foo_checksum"}],
        "outs": [{"path": "bar", "md5": "bar_checksum"}],
    }


def test_fill_from_lock_deps_outs(dvc, lock_data):
    stage = create_stage(PipelineStage, dvc, PROJECT_FILE, deps=["foo"], outs=["bar"])

    for item in chain(stage.deps, stage.outs):
        assert not item.hash_info

    StageLoader.fill_from_lock(stage, lock_data)

    assert stage.deps[0].hash_info == HashInfo("md5", "foo_checksum")
    assert stage.outs[0].hash_info == HashInfo("md5", "bar_checksum")


def test_fill_from_lock_outs_isexec(dvc):
    stage = create_stage(PipelineStage, dvc, PROJECT_FILE, outs=["foo"])

    assert not stage.outs[0].meta.isexec

    StageLoader.fill_from_lock(
        stage,
        {
            "cmd": "command",
            "outs": [{"path": "foo", "md5": "foo_checksum", "isexec": True}],
        },
    )

    assert stage.outs[0].def_path == "foo"
    assert stage.outs[0].hash_info == HashInfo("md5", "foo_checksum")
    assert stage.outs[0].meta.isexec


def test_fill_from_lock_params(dvc, lock_data):
    stage = create_stage(
        PipelineStage,
        dvc,
        PROJECT_FILE,
        deps=["foo"],
        outs=["bar"],
        params=[
            "lorem",
            "lorem.ipsum",
            {"myparams.yaml": ["ipsum", "foobar"]},
        ],
    )
    lock_data["params"] = {
        "params.yaml": {
            "lorem": "lorem",
            "lorem.ipsum": ["i", "p", "s", "u", "m"],
        },
        "myparams.yaml": {
            # missing value in lock for `foobar` params
            "ipsum": "ipsum"
        },
    }
    params_deps = split_params_deps(stage)[0]
    assert set(params_deps[0].params) == {"lorem", "lorem.ipsum"}
    assert set(params_deps[1].params) == {"ipsum", "foobar"}
    assert not params_deps[0].hash_info
    assert not params_deps[1].hash_info

    StageLoader.fill_from_lock(stage, lock_data)
    assert params_deps[0].hash_info.value == lock_data["params"]["params.yaml"]
    assert params_deps[1].hash_info.value == lock_data["params"]["myparams.yaml"]


def test_fill_from_lock_missing_params_section(dvc, lock_data):
    stage = create_stage(
        PipelineStage,
        dvc,
        PROJECT_FILE,
        deps=["foo"],
        outs=["bar"],
        params=["lorem", "lorem.ipsum", {"myparams.yaml": ["ipsum"]}],
    )
    params_deps = split_params_deps(stage)[0]
    StageLoader.fill_from_lock(stage, lock_data)
    assert not params_deps[0].hash_info
    assert not params_deps[1].hash_info


def test_fill_from_lock_missing_checksums(dvc, lock_data):
    stage = create_stage(
        PipelineStage,
        dvc,
        PROJECT_FILE,
        deps=["foo", "foo1"],
        outs=["bar", "bar1"],
    )

    StageLoader.fill_from_lock(stage, lock_data)

    assert stage.deps[0].hash_info == HashInfo("md5", "foo_checksum")
    assert stage.outs[0].hash_info == HashInfo("md5", "bar_checksum")
    assert not stage.deps[1].hash_info
    assert not stage.outs[1].hash_info


def test_fill_from_lock_use_appropriate_checksum(dvc, lock_data):
    stage = create_stage(
        PipelineStage,
        dvc,
        PROJECT_FILE,
        deps=["s3://dvc-temp/foo"],
        outs=["bar"],
    )
    lock_data["deps"] = [{"path": "s3://dvc-temp/foo", "etag": "e-tag"}]
    StageLoader.fill_from_lock(stage, lock_data)
    assert stage.deps[0].hash_info == HashInfo("etag", "e-tag")
    assert stage.outs[0].hash_info == HashInfo("md5", "bar_checksum")


def test_fill_from_lock_with_missing_sections(dvc, lock_data):
    stage = create_stage(PipelineStage, dvc, PROJECT_FILE, deps=["foo"], outs=["bar"])
    lock = deepcopy(lock_data)
    del lock["deps"]
    StageLoader.fill_from_lock(stage, lock)
    assert not stage.deps[0].hash_info
    assert stage.outs[0].hash_info == HashInfo("md5", "bar_checksum")

    lock = deepcopy(lock_data)
    del lock["outs"]
    StageLoader.fill_from_lock(stage, lock)
    assert stage.deps[0].hash_info == HashInfo("md5", "foo_checksum")
    assert not stage.outs[0].hash_info


def test_fill_from_lock_empty_data(dvc):
    stage = create_stage(PipelineStage, dvc, PROJECT_FILE, deps=["foo"], outs=["bar"])
    StageLoader.fill_from_lock(stage, None)
    assert not stage.deps[0].hash_info
    assert not stage.outs[0].hash_info
    StageLoader.fill_from_lock(stage, {})
    assert not stage.deps[0].hash_info
    assert not stage.outs[0].hash_info


def test_load_stage(dvc, stage_data, lock_data):
    dvcfile = load_file(dvc, PROJECT_FILE)
    stage = StageLoader.load_stage(dvcfile, "stage-1", stage_data, lock_data)

    assert stage.wdir == os.path.abspath(os.curdir)
    assert stage.name == "stage-1"
    assert stage.cmd == "command"
    assert stage.path == os.path.abspath(PROJECT_FILE)
    assert stage.deps[0].def_path == "foo"
    assert stage.deps[0].hash_info == HashInfo("md5", "foo_checksum")
    assert stage.outs[0].def_path == "bar"
    assert stage.outs[0].hash_info == HashInfo("md5", "bar_checksum")


def test_load_stage_cmd_with_list(dvc, stage_data, lock_data):
    stage_data["cmd"] = ["cmd-0", "cmd-1"]
    dvcfile = load_file(dvc, PROJECT_FILE)
    stage = StageLoader.load_stage(dvcfile, "stage-1", stage_data, lock_data)
    assert stage.cmd == ["cmd-0", "cmd-1"]


def test_load_stage_outs_with_flags(dvc, stage_data, lock_data):
    stage_data["outs"] = [{"foo": {"cache": False}}]
    dvcfile = load_file(dvc, PROJECT_FILE)
    stage = StageLoader.load_stage(dvcfile, "stage-1", stage_data, lock_data)
    assert stage.outs[0].use_cache is False


def test_load_stage_no_lock(dvc, stage_data):
    dvcfile = load_file(dvc, PROJECT_FILE)
    stage = StageLoader.load_stage(dvcfile, "stage-1", stage_data)
    assert stage.deps[0].def_path == "foo"
    assert stage.outs[0].def_path == "bar"
    assert not stage.deps[0].hash_info
    assert not stage.outs[0].hash_info


def test_load_stage_with_params(dvc, stage_data, lock_data):
    lock_data["params"] = {"params.yaml": {"lorem": "ipsum"}}
    stage_data["params"] = ["lorem"]
    dvcfile = load_file(dvc, PROJECT_FILE)
    stage = StageLoader.load_stage(dvcfile, "stage-1", stage_data, lock_data)

    params, deps = split_params_deps(stage)
    assert deps[0].def_path == "foo"
    assert stage.outs[0].def_path == "bar"
    assert params[0].def_path == "params.yaml"
    assert params[0].hash_info == HashInfo("params", {"lorem": "ipsum"})
    assert deps[0].hash_info == HashInfo("md5", "foo_checksum")
    assert stage.outs[0].hash_info == HashInfo("md5", "bar_checksum")


@pytest.mark.parametrize("typ", ["metrics", "plots"])
def test_load_stage_with_metrics_and_plots(dvc, stage_data, lock_data, typ):
    stage_data[typ] = stage_data.pop("outs")
    dvcfile = load_file(dvc, PROJECT_FILE)
    stage = StageLoader.load_stage(dvcfile, "stage-1", stage_data, lock_data)

    assert stage.outs[0].def_path == "bar"
    assert stage.outs[0].hash_info == HashInfo("md5", "bar_checksum")


def test_load_changed_command(dvc, stage_data, lock_data):
    dvcfile = load_file(dvc, PROJECT_FILE)
    stage = StageLoader.load_stage(dvcfile, "stage-1", stage_data)
    assert not stage.cmd_changed
    assert stage.cmd == "command"

    lock_data["cmd"] = "different-command"
    stage = StageLoader.load_stage(dvcfile, "stage-1", stage_data, lock_data)
    assert stage.cmd_changed
    assert stage.cmd == "command"


def test_load_stage_wdir_and_path_correctly(dvc, stage_data, lock_data):
    stage_data["wdir"] = "dir"
    dvcfile = load_file(dvc, PROJECT_FILE)
    stage = StageLoader.load_stage(dvcfile, "stage-1", stage_data, lock_data)

    assert stage.wdir == os.path.abspath("dir")
    assert stage.path == os.path.abspath(PROJECT_FILE)


def test_load_stage_mapping(dvc, stage_data, lock_data):
    dvcfile = load_file(dvc, PROJECT_FILE)
    dvcfile.contents = {"stages": {"stage": stage_data}}
    dvcfile.lockfile_contents = {"stage": lock_data}

    assert len(dvcfile.stages) == 1
    assert "stage" in dvcfile.stages
    assert "stage1" not in dvcfile.stages
    assert dvcfile.stages.keys() == {"stage"}
    assert isinstance(dvcfile.stages["stage"], PipelineStage)




tests/unit/stage/test_run.py
import logging

import pytest

from dvc.stage import Stage
from dvc.stage.run import run_stage


@pytest.mark.parametrize(
    "cmd, expected",
    [
        ("mycmd arg1 arg2", ["> mycmd arg1 arg2"]),
        (["mycmd1 arg1", "mycmd2 arg2"], ["> mycmd1 arg1", "> mycmd2 arg2"]),
    ],
)
def test_run_stage_dry(caplog, dvc, cmd, expected):
    with caplog.at_level(level=logging.INFO, logger="dvc"):
        stage = Stage(dvc, "stage.dvc", cmd=cmd)
        run_stage(stage, dry=True)

    expected.insert(0, "Running stage 'stage.dvc':")
    assert caplog.messages == expected




tests/unit/stage/test_serialize_pipeline_file.py
import os

import pytest
from voluptuous import Schema as _Schema

from dvc import output
from dvc.dvcfile import PROJECT_FILE
from dvc.schema import SINGLE_PIPELINE_STAGE_SCHEMA
from dvc.stage import PipelineStage, create_stage
from dvc.stage.serialize import to_pipeline_file as _to_pipeline_file

kwargs = {"name": "something", "cmd": "command", "path": PROJECT_FILE}
Schema = _Schema(SINGLE_PIPELINE_STAGE_SCHEMA)


def to_pipeline_file(stage):
    """Validate schema on each serialization."""
    e = _to_pipeline_file(stage)
    assert len(Schema(e)) == 1
    return e


def test_cmd(dvc):
    stage = create_stage(PipelineStage, dvc, **kwargs)
    entry = to_pipeline_file(stage)
    assert entry == {"something": {"cmd": "command"}}


def test_wdir(dvc):
    stage = create_stage(PipelineStage, dvc, **kwargs)
    assert stage.PARAM_WDIR not in to_pipeline_file(stage)["something"]

    stage.wdir = os.curdir
    assert stage.PARAM_WDIR not in to_pipeline_file(stage)["something"]

    stage.wdir = "some-dir"
    assert to_pipeline_file(stage)["something"][stage.PARAM_WDIR] == "some-dir"


def test_deps_sorted(dvc):
    stage = create_stage(
        PipelineStage, dvc, deps=["a", "quick", "lazy", "fox"], **kwargs
    )
    assert to_pipeline_file(stage)["something"][stage.PARAM_DEPS] == [
        "a",
        "fox",
        "lazy",
        "quick",
    ]


def test_outs_sorted(dvc):
    stage = create_stage(
        PipelineStage,
        dvc,
        outs=["too", "many", "outs"],
        deps=["foo"],
        **kwargs,
    )
    assert to_pipeline_file(stage)["something"][stage.PARAM_OUTS] == [
        "many",
        "outs",
        "too",
    ]


def test_params_sorted(dvc):
    params = [
        "lorem",
        "ipsum",
        {"custom.yaml": ["wxyz", "pqrs", "baz"]},
        {"params.yaml": ["barr"]},
    ]
    stage = create_stage(
        PipelineStage, dvc, outs=["bar"], deps=["foo"], params=params, **kwargs
    )
    assert to_pipeline_file(stage)["something"][stage.PARAM_PARAMS] == [
        "barr",
        "ipsum",
        "lorem",
        {"custom.yaml": ["baz", "pqrs", "wxyz"]},
    ]


def test_params_file_sorted(dvc):
    params = [
        "lorem",
        "ipsum",
        {"custom.yaml": ["wxyz", "pqrs", "baz"]},
        {"a-file-of-params.yaml": ["barr"]},
    ]
    stage = create_stage(
        PipelineStage, dvc, outs=["bar"], deps=["foo"], params=params, **kwargs
    )
    assert to_pipeline_file(stage)["something"][stage.PARAM_PARAMS] == [
        "ipsum",
        "lorem",
        {"a-file-of-params.yaml": ["barr"]},
        {"custom.yaml": ["baz", "pqrs", "wxyz"]},
    ]


def test_params_file_without_targets(dvc):
    params = [
        "foo",
        "bar",
        {"params.yaml": None},
        {"custom.yaml": ["wxyz", "pqrs", "baz"]},
        {"a-file-of-params.yaml": None},
        {"a-file-of-params.yaml": ["barr"]},
    ]
    stage = create_stage(
        PipelineStage, dvc, outs=["bar"], deps=["foo"], params=params, **kwargs
    )
    assert to_pipeline_file(stage)["something"][stage.PARAM_PARAMS] == [
        {"a-file-of-params.yaml": None},
        {"custom.yaml": ["baz", "pqrs", "wxyz"]},
        {"params.yaml": None},
    ]


@pytest.mark.parametrize(
    "typ, extra",
    [("plots", {"plot": True}), ("metrics", {"metric": True}), ("outs", {})],
)
def test_outs_and_outs_flags_are_sorted(dvc, typ, extra):
    stage = create_stage(PipelineStage, dvc, deps=["input"], **kwargs)
    stage.outs += output.loads_from(stage, ["barr"], use_cache=False, **extra)
    stage.outs += output.loads_from(
        stage, ["foobar"], use_cache=False, persist=True, **extra
    )
    stage.outs += output.loads_from(stage, ["foo"], persist=True, **extra)
    stage.outs += output.loads_from(stage, ["bar"], **extra)

    serialized_outs = to_pipeline_file(stage)["something"][typ]
    assert serialized_outs == [
        "bar",
        {"barr": {"cache": False}},
        {"foo": {"persist": True}},
        {"foobar": {"cache": False, "persist": True}},
    ]
    assert list(serialized_outs[3]["foobar"].keys()) == ["cache", "persist"]


def test_plot_props(dvc):
    props = {"x": "1"}
    stage = create_stage(PipelineStage, dvc, plots=["plot_file"], **kwargs)
    stage.outs[0].plot = props

    assert to_pipeline_file(stage)["something"][stage.PARAM_PLOTS] == [
        {"plot_file": props}
    ]


def test_frozen(dvc):
    stage = create_stage(PipelineStage, dvc, outs=["output"], deps=["input"], **kwargs)
    assert stage.PARAM_FROZEN not in to_pipeline_file(stage)["something"]

    stage = create_stage(PipelineStage, dvc, **kwargs, frozen=True)
    assert to_pipeline_file(stage)["something"][stage.PARAM_FROZEN] is True


def test_always_changed(dvc):
    stage = create_stage(PipelineStage, dvc, outs=["output"], deps=["input"], **kwargs)
    assert stage.PARAM_ALWAYS_CHANGED not in to_pipeline_file(stage)["something"]

    stage = create_stage(PipelineStage, dvc, **kwargs, always_changed=True)
    assert to_pipeline_file(stage)["something"][stage.PARAM_ALWAYS_CHANGED] is True


def test_order(dvc):
    stage = create_stage(
        PipelineStage,
        dvc,
        outs=["output"],
        deps=["input"],
        **kwargs,
        always_changed=True,
        frozen=True,
    )
    # `create_stage` checks for existence of `wdir`
    stage.wdir = "some-dir"
    assert list(to_pipeline_file(stage)["something"].keys()) == [
        "cmd",
        "wdir",
        "deps",
        "outs",
        "frozen",
        "always_changed",
    ]


@pytest.mark.parametrize("typ", ["outs", "metrics", "plots", "params", "deps", None])
def test_order_deps_outs(dvc, typ):
    all_types = ["deps", "params", "outs", "metrics", "plots"]
    all_types = [item for item in all_types if item != typ]
    extra = {key: [f"foo-{i}"] for i, key in enumerate(all_types)}

    stage = create_stage(PipelineStage, dvc, **kwargs, **extra)
    assert typ not in to_pipeline_file(stage)["something"]
    assert list(to_pipeline_file(stage)["something"].keys()) == [
        "cmd",
        *all_types,
    ]




tests/unit/stage/test_serialize_pipeline_lock.py
from collections import OrderedDict

import pytest
from voluptuous import Schema as _Schema

from dvc.dvcfile import PROJECT_FILE
from dvc.schema import LOCK_FILE_STAGE_SCHEMA, LOCKFILE_STAGES_SCHEMA
from dvc.stage import PipelineStage, create_stage
from dvc.stage.serialize import DEFAULT_PARAMS_FILE, to_lockfile
from dvc.stage.serialize import to_single_stage_lockfile as _to_single_stage_lockfile
from dvc.stage.utils import split_params_deps
from dvc_data.hashfile.hash_info import HashInfo

kwargs = {"name": "something", "cmd": "command", "path": PROJECT_FILE}
Schema = _Schema(LOCK_FILE_STAGE_SCHEMA)


def to_single_stage_lockfile(stage):
    """Validate schema on each serialization."""
    e = _to_single_stage_lockfile(stage)
    assert Schema(e)
    return e


def test_lock(dvc):
    stage = create_stage(PipelineStage, dvc, **kwargs)
    assert to_single_stage_lockfile(stage) == {"cmd": "command"}


def test_lock_deps(dvc):
    stage = create_stage(PipelineStage, dvc, deps=["input"], **kwargs)
    stage.deps[0].hash_info = HashInfo("md5", "md-five")
    assert to_single_stage_lockfile(stage) == OrderedDict(
        [
            ("cmd", "command"),
            ("deps", [OrderedDict([("path", "input"), ("md5", "md-five")])]),
        ]
    )


def test_lock_deps_order(dvc):
    stage = create_stage(PipelineStage, dvc, deps=["input1", "input0"], **kwargs)
    stage.deps[0].hash_info = HashInfo("md5", "md-one1")
    stage.deps[1].hash_info = HashInfo("md5", "md-zer0")
    assert to_single_stage_lockfile(stage) == OrderedDict(
        [
            ("cmd", "command"),
            (
                "deps",
                [
                    OrderedDict([("path", "input0"), ("md5", "md-zer0")]),
                    OrderedDict([("path", "input1"), ("md5", "md-one1")]),
                ],
            ),
        ]
    )


def test_lock_params(dvc):
    stage = create_stage(PipelineStage, dvc, params=["lorem.ipsum", "abc"], **kwargs)
    stage.deps[0].hash_info = HashInfo(
        "params", {"lorem.ipsum": {"lorem1": 1, "lorem2": 2}, "abc": 3}
    )
    assert to_single_stage_lockfile(stage)["params"][
        DEFAULT_PARAMS_FILE
    ] == OrderedDict([("abc", 3), ("lorem.ipsum", {"lorem1": 1, "lorem2": 2})])


def test_lock_params_file_sorted(dvc):
    stage = create_stage(
        PipelineStage,
        dvc,
        params=[
            "lorem.ipsum",
            "abc",
            {"myparams.yaml": ["foo", "foobar"]},
            {"a-params-file.yaml": ["bar", "barr"]},
        ],
        **kwargs,
    )
    stage.deps[0].hash_info = HashInfo(
        "params", {"lorem.ipsum": {"lorem1": 1, "lorem2": 2}, "abc": 3}
    )
    stage.deps[1].hash_info = HashInfo(
        "params", {"foo": ["f", "o", "o"], "foobar": "foobar"}
    )
    stage.deps[2].hash_info = HashInfo(
        "params", {"bar": ["b", "a", "r"], "barr": "barr"}
    )
    assert to_single_stage_lockfile(stage)["params"] == OrderedDict(
        [
            (
                DEFAULT_PARAMS_FILE,
                OrderedDict([("abc", 3), ("lorem.ipsum", {"lorem1": 1, "lorem2": 2})]),
            ),
            (
                "a-params-file.yaml",
                OrderedDict([("bar", ["b", "a", "r"]), ("barr", "barr")]),
            ),
            (
                "myparams.yaml",
                OrderedDict([("foo", ["f", "o", "o"]), ("foobar", "foobar")]),
            ),
        ]
    )


def test_lock_params_no_values_filled(dvc):
    stage = create_stage(PipelineStage, dvc, params=["lorem.ipsum", "abc"], **kwargs)
    assert to_single_stage_lockfile(stage) == {"cmd": "command"}


@pytest.mark.parametrize(
    "info, expected",
    [
        (None, {}),
        ({}, {}),
        ({"foo": "foo", "bar": "bar"}, {"bar": "bar", "foo": "foo"}),
    ],
)
def test_lock_params_without_targets(dvc, info, expected):
    stage = create_stage(PipelineStage, dvc, params=[{"params.yaml": None}], **kwargs)
    stage.deps[0].fill_values(info)
    assert to_single_stage_lockfile(stage) == {
        "cmd": "command",
        "params": {"params.yaml": OrderedDict(expected)},
    }


@pytest.mark.parametrize("typ", ["plots", "metrics", "outs"])
def test_lock_outs(dvc, typ):
    stage = create_stage(PipelineStage, dvc, **{typ: ["input"]}, **kwargs)
    stage.outs[0].hash_info = HashInfo("md5", "md-five")
    assert to_single_stage_lockfile(stage) == OrderedDict(
        [
            ("cmd", "command"),
            ("outs", [OrderedDict([("path", "input"), ("md5", "md-five")])]),
        ]
    )


@pytest.mark.parametrize("typ", ["plots", "metrics", "outs"])
def test_lock_outs_isexec(dvc, typ):
    stage = create_stage(PipelineStage, dvc, **{typ: ["input"]}, **kwargs)
    stage.outs[0].hash_info = HashInfo("md5", "md-five")
    stage.outs[0].meta.isexec = True
    assert to_single_stage_lockfile(stage) == OrderedDict(
        [
            ("cmd", "command"),
            (
                "outs",
                [
                    OrderedDict(
                        [
                            ("path", "input"),
                            ("md5", "md-five"),
                            ("isexec", True),
                        ]
                    )
                ],
            ),
        ]
    )


@pytest.mark.parametrize("typ", ["plots", "metrics", "outs"])
def test_lock_outs_order(dvc, typ):
    stage = create_stage(PipelineStage, dvc, **{typ: ["input1", "input0"]}, **kwargs)
    stage.outs[0].hash_info = HashInfo("md5", "md-one1")
    stage.outs[1].hash_info = HashInfo("md5", "md-zer0")
    assert to_single_stage_lockfile(stage) == OrderedDict(
        [
            ("cmd", "command"),
            (
                "outs",
                [
                    OrderedDict([("path", "input0"), ("md5", "md-zer0")]),
                    OrderedDict([("path", "input1"), ("md5", "md-one1")]),
                ],
            ),
        ]
    )


def test_dump_nondefault_hash(dvc):
    stage = create_stage(PipelineStage, dvc, deps=["s3://dvc-temp/file"], **kwargs)
    stage.deps[0].hash_info = HashInfo("md5", "value")
    assert to_single_stage_lockfile(stage) == OrderedDict(
        [
            ("cmd", "command"),
            (
                "deps",
                [OrderedDict([("path", "s3://dvc-temp/file"), ("md5", "value")])],
            ),
        ]
    )


def test_order(dvc):
    stage = create_stage(
        PipelineStage,
        dvc,
        deps=["input"],
        outs=["output"],
        params=["foo-param"],
        **kwargs,
    )
    params, deps = split_params_deps(stage)

    deps[0].hash_info = HashInfo("md5", "md-five")
    params[0].hash_info = HashInfo("params", {"foo-param": "value"})
    stage.outs[0].hash_info = HashInfo("md5", "md5-output")

    assert to_single_stage_lockfile(stage) == OrderedDict(
        [
            ("cmd", "command"),
            ("deps", [{"path": "input", "md5": "md-five"}]),
            ("params", {"params.yaml": {"foo-param": "value"}}),
            ("outs", [{"path": "output", "md5": "md5-output"}]),
        ]
    )


def test_to_lockfile(dvc):
    stage = create_stage(PipelineStage, dvc, deps=["input"], **kwargs)
    stage.deps[0].hash_info = HashInfo("md5", "md-five")
    entry = to_lockfile(stage)
    assert len(entry) == 1
    _Schema(LOCKFILE_STAGES_SCHEMA)(entry)
    assert entry == {
        "something": OrderedDict(
            [
                ("cmd", "command"),
                ("deps", [{"path": "input", "md5": "md-five"}]),
            ]
        )
    }


def test_to_single_stage_lockfile_cloud_versioning_dir(dvc):
    stage = create_stage(PipelineStage, dvc, outs=["dir"], **kwargs)
    stage.outs[0].hash_info = HashInfo("md5", "md-five.dir")
    files = [
        {
            "size": 3,
            "version_id": "WYRG4BglP7pD.gEoJP6a4AqOhl.FRA.h",
            "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
            "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
            "relpath": "bar",
        },
        {
            "size": 3,
            "version_id": "0vL53tFVY5vVAoJ4HG2jCS1mEcohDPE0",
            "etag": "acbd18db4cc2f85cedef654fccc4a4d8",
            "md5": "acbd18db4cc2f85cedef654fccc4a4d8",
            "relpath": "foo",
        },
    ]
    stage.outs[0].files = files
    e = _to_single_stage_lockfile(stage, with_files=True)
    assert Schema(e)
    assert e["outs"][0] == {"path": "dir", "files": files}




tests/unit/stage/test_stage.py
import os
import signal
import subprocess
import threading

import pytest

from dvc.dependency.repo import RepoDependency
from dvc.stage import Stage
from dvc.stage.exceptions import StageUpdateError

TEST_STAGE_DICT = {
    "md5": "123456",
    "cmd": "mycmd",
    "outs": [{"path": "a", "md5": "123456789"}],
    "deps": [{"path": "b", "md5": "987654321"}],
}


def test_stage_checksum(mocker):
    stage = Stage(None, "path", cmd="mycmd")

    mocker.patch.object(stage, "dumpd", return_value=TEST_STAGE_DICT)
    assert stage.compute_md5() == "e9521a22111493406ea64a88cda63e0b"


def test_wdir_default_ignored(mocker):
    stage = Stage(None, "path", cmd="mycmd")
    d = dict(TEST_STAGE_DICT, wdir=".")

    mocker.patch.object(stage, "dumpd", return_value=d)
    assert stage.compute_md5() == "e9521a22111493406ea64a88cda63e0b"


def test_wdir_non_default_is_not_ignored(mocker):
    stage = Stage(None, "path", cmd="mycmd")
    d = dict(TEST_STAGE_DICT, wdir="..")

    mocker.patch.object(stage, "dumpd", return_value=d)
    assert stage.compute_md5() == "2ceba15e87f6848aa756502c1e6d24e9"


def test_meta_ignored(mocker):
    stage = Stage(None, "path", cmd="mycmd")
    d = dict(TEST_STAGE_DICT, meta={"author": "Suor"})

    mocker.patch.object(stage, "dumpd", return_value=d)
    assert stage.compute_md5() == "e9521a22111493406ea64a88cda63e0b"


def test_path_conversion(dvc):
    stage = Stage(dvc, "path")

    stage.wdir = os.path.join("..", "..")
    assert stage.dumpd()["wdir"] == "../.."


def test_stage_update(dvc, mocker):
    stage = Stage(dvc, "path", "cmd")
    dep = RepoDependency({"url": "example.com"}, stage, "dep_path")
    mocker.patch.object(dep, "update", return_value=None)

    stage = Stage(dvc, "path", deps=[dep])
    reproduce = mocker.patch.object(stage, "reproduce")
    is_repo_import = mocker.patch(
        __name__ + ".Stage.is_repo_import", new_callable=mocker.PropertyMock
    )

    is_repo_import.return_value = True
    with dvc.lock:
        stage.update()
    assert reproduce.called_once_with()

    is_repo_import.return_value = False
    with pytest.raises(StageUpdateError):
        stage.update()


@pytest.mark.skipif(
    not isinstance(
        threading.current_thread(),
        threading._MainThread,
    ),
    reason="Not running in the main thread.",
)
def test_stage_run_ignore_sigint(dvc, mocker):
    proc = mocker.Mock()
    communicate = mocker.Mock()
    proc.configure_mock(returncode=0, communicate=communicate)
    popen = mocker.patch.object(subprocess, "Popen", return_value=proc)
    signal_mock = mocker.patch("signal.signal")

    dvc.run(cmd="path", single_stage=True)

    assert popen.called_once()
    assert communicate.called_once_with()
    signal_mock.assert_any_call(signal.SIGINT, signal.SIG_IGN)
    assert signal.getsignal(signal.SIGINT) == signal.default_int_handler


def test_always_changed(dvc):
    stage = Stage(dvc, "path", always_changed=True)
    stage.save()
    with dvc.lock:
        assert stage.changed()
        assert stage.status()["path"] == ["always changed"]


def test_external_outs(tmp_path_factory, dvc):
    from dvc.stage import create_stage
    from dvc.stage.exceptions import StageExternalOutputsError

    tmp_path = tmp_path_factory.mktemp("external-outs")
    foo = tmp_path / "foo"
    foo.write_text("foo")

    with pytest.raises(StageExternalOutputsError):
        create_stage(Stage, dvc, "path.dvc", outs=[os.fspath(foo)])

    with dvc.config.edit() as conf:
        conf["remote"]["myremote"] = {"url": os.fspath(tmp_path)}

    create_stage(Stage, dvc, "path.dvc", outs=["remote://myremote/foo"])
    create_stage(Stage, dvc, "path.dvc", outs=[os.fspath(foo)], external=True)




tests/unit/stage/test_utils.py
import os

from dvc.fs import localfs
from dvc.stage.utils import resolve_paths


def test_resolve_paths():
    p = os.path.join("dir", "subdir")
    file_path = os.path.join(p, "dvc.yaml")

    path, wdir = resolve_paths(fs=localfs, path=file_path, wdir="dir")
    assert path == os.path.abspath(file_path)
    assert wdir == os.path.abspath(os.path.join(p, "dir"))

    path, wdir = resolve_paths(fs=localfs, path=file_path)
    assert path == os.path.abspath(file_path)
    assert wdir == os.path.abspath(p)

    path, wdir = resolve_paths(fs=localfs, path=file_path, wdir="../../some-dir")
    assert path == os.path.abspath(file_path)
    assert wdir == os.path.abspath("some-dir")




tests/unit/ui/__init__.py




tests/unit/ui/test_console.py
import datetime
import textwrap

import pytest

from dvc.ui import Console


def test_write(capsys):
    """Test that ui.write works."""
    console = Console(enable=True)
    message = "hello world"
    console.write(message)
    console.error_write(message)

    captured = capsys.readouterr()
    assert captured.out == f"{message}\n"
    assert captured.err == f"{message}\n"


@pytest.mark.parametrize(
    "isatty, expected_output",
    [
        (
            True,
            textwrap.dedent(
                """\
        {
          "hello": "world",
          "date": "1970-01-01 00:00:00"
        }
    """
            ),
        ),
        (
            False,
            textwrap.dedent(
                """\
        {"hello": "world", "date": "1970-01-01 00:00:00"}
        """
            ),
        ),
    ],
)
def test_write_json(capsys, mocker, isatty, expected_output):
    """Test that ui.write_json works."""

    console = Console(enable=True)
    mocker.patch.object(console, "isatty", return_value=isatty)
    message = {"hello": "world", "date": datetime.datetime(1970, 1, 1)}
    console.write_json(message, default=str)
    captured = capsys.readouterr()
    assert captured.out == expected_output


def test_capsys_works(capsys):
    """Sanity check that capsys can capture outputs from a global ui."""
    from dvc.ui import ui

    message = "hello world"
    ui.write(message)
    ui.error_write(message)

    captured = capsys.readouterr()
    assert captured.out == f"{message}\n"
    assert captured.err == f"{message}\n"




tests/unit/ui/test_pager.py
import pytest

from dvc.env import DVC_PAGER
from dvc.ui.pager import DEFAULT_PAGER, LESS, PAGER_ENV, find_pager, make_pager, pager


@pytest.fixture(autouse=True)
def clear_envs(monkeypatch):
    monkeypatch.delenv(DVC_PAGER, raising=False)
    monkeypatch.delenv(PAGER_ENV, raising=False)
    monkeypatch.delenv(LESS, raising=False)


def test_find_pager_when_not_isatty(mocker):
    mocker.patch("sys.stdout.isatty", return_value=False)
    assert find_pager() is None


def test_find_pager_uses_custom_pager_when_dvc_pager_env_var_is_defined(
    mocker, monkeypatch
):
    monkeypatch.setenv(DVC_PAGER, "my-pager")
    mocker.patch("sys.stdout.isatty", return_value=True)

    assert find_pager() == "my-pager"


def test_find_pager_uses_custom_pager_when_pager_env_is_defined(mocker, monkeypatch):
    monkeypatch.setenv(PAGER_ENV, "my-pager")
    mocker.patch("sys.stdout.isatty", return_value=True)

    assert find_pager() == "my-pager"


def test_find_pager_uses_default_pager_when_found(mocker):
    mocker.patch("sys.stdout.isatty", return_value=True)
    mocker.patch("os.system", return_value=0)

    assert DEFAULT_PAGER in find_pager()


def test_find_pager_fails_to_find_any_pager(mocker):
    mocker.patch("os.system", return_value=1)
    mocker.patch("sys.stdout.isatty", return_value=True)

    assert find_pager() is None


@pytest.mark.parametrize("env", [DVC_PAGER, PAGER_ENV, None])
def test_dvc_sets_default_options_on_less_without_less_env(mocker, monkeypatch, env):
    if env:
        monkeypatch.setenv(env, "less")
    mocker.patch("sys.stdout.isatty", return_value=True)
    mocker.patch("os.system", return_value=0)

    assert (
        find_pager()
        == "less --quit-if-one-screen --RAW-CONTROL-CHARS --chop-long-lines --no-init"
    )


@pytest.mark.parametrize("env", [DVC_PAGER, PAGER_ENV, None])
def test_dvc_sets_some_options_on_less_if_less_env_defined(mocker, monkeypatch, env):
    if env:
        monkeypatch.setenv(env, "less")
    mocker.patch("sys.stdout.isatty", return_value=True)
    mocker.patch("os.system", return_value=0)
    monkeypatch.setenv(LESS, "-R")

    assert find_pager() == "less --RAW-CONTROL-CHARS --chop-long-lines"


def test_make_pager_when_no_pager_found(mocker, monkeypatch):
    assert make_pager(None).__name__ == "plainpager"


def test_pager(mocker, monkeypatch):
    monkeypatch.setenv(DVC_PAGER, "my-pager")
    mocker.patch("sys.stdout.isatty", return_value=True)

    m_make_pager = mocker.patch("dvc.ui.pager.make_pager")
    _pager = m_make_pager.return_value = mocker.MagicMock()

    pager("hello world")
    m_make_pager.assert_called_once_with("my-pager")
    _pager.assert_called_once_with("hello world")




tests/unit/ui/test_table.py
import textwrap

import pytest
from rich.style import Style

from dvc.ui import ui


def test_plain(capsys):
    ui.table(
        [("foo", "bar"), ("foo1", "bar1"), ("foo2", "bar2")],
        headers=["first", "second"],
    )
    captured = capsys.readouterr()
    assert captured.out == textwrap.dedent(
        """\
        first    second
        foo      bar
        foo1     bar1
        foo2     bar2
    """
    )


def test_plain_md(capsys):
    ui.table(
        [("foo", "bar"), ("foo1", "bar1"), ("foo2", "bar2")],
        headers=["first", "second"],
        markdown=True,
    )
    captured = capsys.readouterr()
    assert captured.out == textwrap.dedent(
        """\
        | first   | second   |
        |---------|----------|
        | foo     | bar      |
        | foo1    | bar1     |
        | foo2    | bar2     |\n
    """
    )


def test_plain_pager(mocker):
    pager_mock = mocker.patch("dvc.ui.pager.pager")
    ui.table(
        [("foo", "bar"), ("foo1", "bar1"), ("foo2", "bar2")],
        headers=["first", "second"],
        pager=True,
    )

    pager_mock.assert_called_once_with(
        textwrap.dedent(
            """\
            first    second
            foo      bar
            foo1     bar1
            foo2     bar2
            """
        )
    )


def test_plain_headerless(capsys):
    ui.table([("foo", "bar"), ("foo1", "bar1"), ("foo2", "bar2")])
    captured = capsys.readouterr()
    assert captured.out == textwrap.dedent(
        """\
        foo   bar
        foo1  bar1
        foo2  bar2
    """
    )


def test_rich_simple(capsys):
    ui.table(
        [("foo", "bar"), ("foo1", "bar1"), ("foo2", "bar2")],
        headers=["first", "second"],
        rich_table=True,
    )
    # not able to test the actual style for now
    captured = capsys.readouterr()
    assert [row.strip() for row in captured.out.splitlines() if row.strip()] == [
        "first  second",
        "foo    bar",
        "foo1   bar1",
        "foo2   bar2",
    ]


def test_rich_headerless(capsys):
    ui.table([("foo", "bar"), ("foo1", "bar1"), ("foo2", "bar2")], rich_table=True)
    captured = capsys.readouterr()
    assert [row.strip() for row in captured.out.splitlines() if row.strip()] == [
        "foo   bar",
        "foo1  bar1",
        "foo2  bar2",
    ]


def test_rich_border(capsys):
    ui.table(
        [("foo", "bar"), ("foo1", "bar1"), ("foo2", "bar2")],
        headers=["first", "second"],
        rich_table=True,
        borders="simple",
    )
    captured = capsys.readouterr()
    assert [row.strip() for row in captured.out.splitlines() if row.strip()] == [
        "first   second",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ",
        "foo     bar",
        "foo1    bar1",
        "foo2    bar2",
    ]


@pytest.mark.parametrize(
    "extra_opts",
    [
        {"header_styles": [{"style": Style(bold=True)}]},
        {"header_styles": {"first": {"style": Style(bold=True)}}},
        {"row_styles": [{"style": Style(bold=True)}]},
    ],
)
def test_rich_styles(capsys, extra_opts):
    ui.table(
        [("foo", "bar"), ("foo1", "bar1"), ("foo2", "bar2")],
        headers=["first", "second"],
        rich_table=True,
        **extra_opts,
    )
    # not able to test the actual style for now
    captured = capsys.readouterr()
    assert [row.strip() for row in captured.out.splitlines() if row.strip()] == [
        "first  second",
        "foo    bar",
        "foo1   bar1",
        "foo2   bar2",
    ]


def test_rich_pager(mocker):
    pager_mock = mocker.patch("dvc.ui.pager.pager")

    ui.table(
        [("foo", "bar"), ("foo1", "bar1"), ("foo2", "bar2")],
        headers=["first", "second"],
        rich_table=True,
        pager=True,
    )
    received_text = pager_mock.call_args[0][0]
    assert [row.strip() for row in received_text.splitlines() if row.strip()] == [
        "first  second",
        "foo    bar",
        "foo1   bar1",
        "foo2   bar2",
    ]


@pytest.mark.parametrize("rich_table", [True, False])
def test_empty(capsys, rich_table):
    ui.table([], rich_table=rich_table)
    out, err = capsys.readouterr()
    assert (out, err) == ("", "")


def test_empty_markdown(capsys):
    ui.table([], headers=["Col1", "Col2"], markdown=True)
    out, err = capsys.readouterr()
    assert (out, err) == ("| Col1   | Col2   |\n|--------|--------|\n\n", "")




tests/unit/utils/__init__.py




tests/unit/utils/test_cli_parse.py
import pytest

from dvc.utils.cli_parse import parse_params, to_path_overrides


def test_parse_params():
    assert parse_params(
        [
            "param1",
            "file1:param1,param2",
            "file2:param2",
            "file1:param3,param4,",
            "param2,param10",
            "param3,",
            "file3:",
        ]
    ) == [
        {"params.yaml": ["param1", "param2", "param10", "param3"]},
        {"file1": ["param1", "param2", "param3", "param4"]},
        {"file2": ["param2"]},
        {"file3": []},
    ]


@pytest.mark.parametrize(
    "params,expected",
    [
        (["foo=1"], {"params.yaml": ["foo=1"]}),
        (["foo={bar: 1}"], {"params.yaml": ["foo={bar: 1}"]}),
        (["foo.0=bar"], {"params.yaml": ["foo.0=bar"]}),
        (["params.json:foo={bar: 1}"], {"params.json": ["foo={bar: 1}"]}),
        (
            ["params.json:foo={bar: 1}", "baz=2", "goo=3"],
            {
                "params.json": ["foo={bar: 1}"],
                "params.yaml": ["baz=2", "goo=3"],
            },
        ),
    ],
)
def test_to_path_overrides(params, expected):
    assert to_path_overrides(params) == expected




tests/unit/utils/test_collections.py
import json

import pytest

from dvc.utils.collections import (
    apply_diff,
    merge_dicts,
    remove_missing_keys,
    to_omegaconf,
)
from dvc.utils.serialize import dumps_yaml


class MyDict(dict):
    pass


class MyInt(int):
    pass


def test_apply_diff_is_inplace():
    dest = MyDict()
    dest.attr = 42
    apply_diff({}, dest)

    assert type(dest) is MyDict, "Preserves class"
    assert dest.attr == 42, "Preserves custom attrs"


def test_apply_diff_mapping():
    src = {"a": 1}
    dest = {"b": 2}
    apply_diff(src, dest)
    assert dest == src, "Adds and removes keys"

    src = {"a": 1}
    dest = {"a": MyInt(1)}
    apply_diff(src, dest)
    assert type(dest["a"]) is MyInt, "Does not replace equals"

    src = {"d": {"a": 1}}
    inner = {}
    dest = {"d": inner}
    apply_diff(src, dest)
    assert dest["d"] is inner, "Updates inner dicts"


def test_apply_diff_seq():
    src = [1]
    dest = [MyInt(1)]
    apply_diff(src, dest)
    assert type(dest[0]) is MyInt, "Does not replace equals"

    src = {"l": [1]}
    inner = []
    dest = {"l": inner}
    apply_diff(src, dest)
    assert dest["l"] is inner, "Updates inner lists"


def is_serializable(d):
    json.dumps(d)
    dumps_yaml(d)
    return True


def test_to_omegaconf():
    class CustomDict(dict):
        pass

    class CustomList(list):
        pass

    data = {
        "foo": CustomDict(bar=1, bag=CustomList([1, 2])),
        "goo": CustomList([CustomDict(goobar=1)]),
    }
    new_data = to_omegaconf(data)
    assert not isinstance(new_data["foo"], CustomDict)
    assert not isinstance(new_data["foo"]["bag"], CustomList)
    assert not isinstance(new_data["goo"], CustomList)
    assert not isinstance(new_data["goo"][0], CustomDict)


@pytest.mark.parametrize(
    "changes, expected",
    [
        [{"foo": "baz"}, {"foo": "baz", "goo": {"bag": 3}, "lorem": False}],
        [
            {"foo": "baz", "goo": "bar"},
            {"foo": "baz", "goo": "bar", "lorem": False},
        ],
        [
            {"goo": {"bag": 4}},
            {"foo": {"bar": 1, "baz": 2}, "goo": {"bag": 4}, "lorem": False},
        ],
        [
            {"foo": {"bar": 1, "baz": 2, 0: "bar"}},
            {
                "foo": {"bar": 1, "baz": 2, 0: "bar"},
                "goo": {"bag": 3},
                "lorem": False,
            },
        ],
        [
            {"lorem": {"ipsum": 3}},
            {
                "foo": {"bar": 1, "baz": 2},
                "goo": {"bag": 3},
                "lorem": {"ipsum": 3},
            },
        ],
        [{}, {"foo": {"bar": 1, "baz": 2}, "goo": {"bag": 3}, "lorem": False}],
    ],
)
def test_merge_dicts(changes, expected):
    params = {"foo": {"bar": 1, "baz": 2}, "goo": {"bag": 3}, "lorem": False}
    merged = merge_dicts(params, changes)
    assert merged == expected == params
    assert params is merged  # references should be preserved
    assert is_serializable(params)


@pytest.mark.parametrize(
    "changes, expected",
    [
        [{"foo": "baz"}, {"foo": {"baz": 2}}],
        [
            {"foo": "baz", "goo": "bag"},
            {"foo": {"baz": 2}, "goo": {"bag": 3}},
        ],
        [{}, {}],
    ],
)
def test_remove_missing_keys(changes, expected):
    params = {"foo": {"bar": 1, "baz": 2}, "goo": {"bag": 3}, "lorem": False}
    removed = remove_missing_keys(params, changes)
    assert removed == expected == params
    assert params is removed  # references should be preserved
    assert is_serializable(params)




tests/unit/utils/test_executors.py
import operator
import time

import pytest
from funcy import raiser

from dvc.utils.threadpool import ThreadPoolExecutor


@pytest.mark.parametrize("wait", [True, False])
@pytest.mark.parametrize("cancel_futures", [True, False])
def test_cancel_futures(wait, cancel_futures):
    """Modified from
    https://github.com/python/cpython/blob/4d2403f/Lib/test/test_concurrent_futures.py#L354
    """
    executor = ThreadPoolExecutor(max_workers=2)
    fs = [executor.submit(time.sleep, 0.1) for _ in range(50)]
    executor.shutdown(wait=wait, cancel_futures=cancel_futures)

    if not wait:
        for t in executor._threads:
            t.join()

    cancelled = [fut for fut in fs if fut.cancelled()]
    # Use "not fut.cancelled()" instead of "fut.done()" to include futures
    # that may have been left in a pending state.
    others = [fut for fut in fs if not fut.cancelled()]

    # Ensure the other futures were able to finish.
    for fut in others:
        assert fut.done()
        assert fut.exception() is None

    if not cancel_futures:
        # there should be no cancelled futures
        assert len(cancelled) == 0
        assert len(others) == len(fs)
    else:
        # We can't guarantee the exact number of cancellations, but we can
        # guarantee that *some* were cancelled. With few workers, many of
        # the submitted futures should have been cancelled.
        assert len(cancelled) > 20
        # Similar to the number of cancelled futures, we can't guarantee the
        # exact number that completed. But, we can guarantee that at least
        # one finished.
        assert len(others) > 0


def test_cancel_on_error_context_manager(mocker):
    executor = ThreadPoolExecutor(max_workers=2, cancel_on_error=True)
    spy = mocker.spy(executor, "shutdown")
    with pytest.raises(RuntimeError), executor:  # noqa: PT012
        future1 = executor.submit(operator.mul, 2, 21)
        future2 = executor.submit(time.sleep, 0.1)
        future3 = executor.submit(raiser(RuntimeError), "This is an error")
        fs = [executor.submit(time.sleep, 0.1) for _ in range(50)]

        assert future1.result() == 42
        assert future2.result() is None
        _ = future3.result()

    spy.assert_called_once_with(wait=True, cancel_futures=True)

    cancelled = [fut for fut in fs if fut.cancelled()]
    others = [fut for fut in fs if not fut.cancelled()]

    for fut in others:
        assert fut.done()
        assert fut.exception() is None

    assert len(cancelled) > 20
    assert len(others) > 0




tests/unit/utils/test_fs.py
import os

import pytest

import dvc
from dvc.fs import system
from dvc.utils import relpath
from dvc.utils.fs import (
    BasePathNotInCheckedPathException,
    contains_symlink_up_to,
    path_isin,
    remove,
)


def test_should_raise_exception_on_base_path_not_in_path():
    with pytest.raises(BasePathNotInCheckedPathException):
        contains_symlink_up_to(os.path.join("foo", "path"), "bar")


def test_should_return_true_on_symlink_in_path(mocker):
    mocker.patch.object(system, "is_symlink", return_value=True)
    base_path = "foo"
    path = os.path.join(base_path, "bar")
    assert contains_symlink_up_to(path, base_path)


def test_should_return_false_on_path_eq_to_base_path(mocker):
    mocker.patch.object(system, "is_symlink", return_value=False)
    path = "path"
    assert not contains_symlink_up_to(path, path)


def test_should_return_false_on_no_more_dirs_below_path(mocker):
    mocker.patch.object(system, "is_symlink", return_value=False)
    dirname_patch = mocker.patch.object(os.path, "dirname", side_effect=lambda arg: arg)
    assert not contains_symlink_up_to(os.path.join("foo", "path"), "foo")
    dirname_patch.assert_called_once()


def test_should_return_false_when_base_path_is_symlink(mocker):
    base_path = "foo"
    target_path = os.path.join(base_path, "bar")

    def base_path_is_symlink(path):
        if path == base_path:
            return True
        return False

    mocker.patch.object(
        system,
        "is_symlink",
        return_value=True,
        side_effect=base_path_is_symlink,
    )
    assert not contains_symlink_up_to(target_path, base_path)


def test_path_object_and_str_are_valid_arg_types():
    base_path = "foo"
    target_path = os.path.join(base_path, "bar")
    assert not contains_symlink_up_to(target_path, base_path)
    assert not contains_symlink_up_to(target_path, base_path)


def test_should_call_recursive_on_no_condition_matched(mocker):
    mocker.patch.object(system, "is_symlink", return_value=False)

    contains_symlink_spy = mocker.spy(dvc.utils.fs, "contains_symlink_up_to")

    # call from full path to match contains_symlink_spy patch path
    assert not dvc.utils.fs.contains_symlink_up_to(os.path.join("foo", "path"), "foo")
    assert contains_symlink_spy.mock.call_count == 2


@pytest.mark.skipif(os.name != "nt", reason="Windows specific")
def test_relpath_windows_different_drives():
    path1 = os.path.join("A:", os.sep, "some", "path")
    path2 = os.path.join("B:", os.sep, "other", "path")
    assert relpath(path1, path2) == path1

    rel = relpath(path1, path2)
    assert isinstance(rel, str)
    assert rel == path1


def test_remove(tmp_dir):
    tmp_dir.gen({"foo": "foo content"})
    path = "foo"

    remove(path)
    assert not os.path.isfile(path)


def test_path_isin_positive():
    child = os.path.join("path", "to", "folder")

    assert path_isin(child, os.path.join("path", "to", ""))
    assert path_isin(child, os.path.join("path", "to"))
    assert path_isin(child, os.path.join("path", ""))
    assert path_isin(child, os.path.join("path"))


def test_path_isin_on_same_path():
    path = os.path.join("path", "to", "folder")
    path_with_sep = os.path.join(path, "")

    assert not path_isin(path, path)
    assert not path_isin(path, path_with_sep)
    assert not path_isin(path_with_sep, path)
    assert not path_isin(path_with_sep, path_with_sep)


def test_path_isin_on_common_substring_path():
    path1 = os.path.join("path", "to", "folder1")
    path2 = os.path.join("path", "to", "folder")

    assert not path_isin(path1, path2)


def test_path_isin_with_absolute_path():
    parent = os.path.abspath("path")
    child = os.path.join(parent, "to", "folder")

    assert path_isin(child, parent)


def test_path_isin_case_sensitive():
    child = os.path.join("path", "to", "folder")
    parent = os.path.join("PATH", "TO")

    assert path_isin(child, parent) == (os.name == "nt")


@pytest.mark.skipif(os.name != "nt", reason="Windows specific")
def test_contains_symlink_case_sensitive_win():
    child = os.path.join("path", "to", "folder")
    parent = os.path.join("PATH", "TO")
    assert contains_symlink_up_to(child, parent) is False


@pytest.mark.skipif(os.name == "nt", reason="Posix specific")
def test_contains_symlink_case_sensitive_posix():
    child = os.path.join("path", "to", "folder")
    parent = os.path.join("PATH", "TO")
    with pytest.raises(BasePathNotInCheckedPathException):
        contains_symlink_up_to(child, parent)


def test_makedirs(tmp_dir):
    path = os.path.join(tmp_dir, "directory")

    os.makedirs(path)
    assert os.path.isdir(path)




tests/unit/utils/test_humanize.py
from collections import OrderedDict

import pytest

from dvc.utils.humanize import get_summary, truncate_text


def test_get_summary():
    # dict, so that we could delete from it easily
    stats = OrderedDict(
        [
            ("fetched", 3),
            ("added", ["file1", "file2", "file3"]),
            ("deleted", ["file4", "file5"]),
            ("modified", ["file6", "file7"]),
        ]
    )

    assert (
        get_summary(stats.items())
        == "3 files fetched, 3 files added, 2 files deleted and 2 files modified"
    )

    del stats["fetched"]
    del stats["deleted"][1]
    assert (
        get_summary(stats.items())
        == "3 files added, 1 file deleted and 2 files modified"
    )

    del stats["deleted"][0]
    assert get_summary(stats.items()) == "3 files added and 2 files modified"

    del stats["modified"]
    assert get_summary(stats.items()) == "3 files added"

    assert not get_summary([])
    assert not get_summary([("x", 0), ("y", [])])
    assert get_summary([("x", 1), ("y", [])]) == "1 file x"


def test_truncate_text():
    text = "lorem ipsum"
    length = 5

    truncated = truncate_text(text, length)
    # length should not cross the max length
    assert len(truncated) == length
    assert truncated[:-1] == text[: length - 1]
    # last character should be ellipsis
    assert truncated[-1] == "‚Ä¶"

    truncated = truncate_text(text, length, with_ellipsis=False)
    # length should not cross the max length
    assert len(truncated) == length
    assert truncated == text[:length]


@pytest.mark.parametrize("with_ellipsis", [True, False])
def test_truncate_text_smaller_than_max_length(with_ellipsis):
    text = "lorem ipsum"

    # exact match as length
    truncated = truncate_text(text, len(text), with_ellipsis=with_ellipsis)
    assert len(truncated) == len(text)
    assert truncated == text

    # max_length > len(text)
    truncated = truncate_text(text, len(text) + 1, with_ellipsis=with_ellipsis)
    assert len(truncated) == len(text)
    assert truncated == text




tests/unit/utils/test_studio.py
from urllib.parse import urljoin

import pytest
from requests import Response

from dvc.env import (
    DVC_STUDIO_OFFLINE,
    DVC_STUDIO_REPO_URL,
    DVC_STUDIO_TOKEN,
    DVC_STUDIO_URL,
)
from dvc.utils.studio import STUDIO_URL, config_to_env, env_to_config, notify_refs

CONFIG = {"offline": True, "repo_url": "repo_url", "token": "token", "url": "url"}


ENV = {
    DVC_STUDIO_OFFLINE: True,
    DVC_STUDIO_REPO_URL: "repo_url",
    DVC_STUDIO_TOKEN: "token",
    DVC_STUDIO_URL: "url",
}


@pytest.mark.parametrize(
    "status_code, side_effect",
    [
        (200, {}),  # success
        (401, {"detail": "unauthorized"}),  # should not fail on client errors
        (500, ValueError),  # should not fail even on server errors
    ],
)
def test_notify_refs(mocker, status_code, side_effect):
    response = Response()
    response.status_code = status_code
    mocker.patch.object(response, "json", side_effect=[side_effect])

    mock_post = mocker.patch("requests.Session.post", return_value=response)

    notify_refs(
        "git@github.com:iterative/dvc.git",
        "TOKEN",
        pushed=["p1", "p2"],
        removed=["r1", "r2"],
    )

    assert mock_post.called
    assert mock_post.call_args == mocker.call(
        urljoin(STUDIO_URL, "/webhook/dvc"),
        json={
            "repo_url": "git@github.com:iterative/dvc.git",
            "client": "dvc",
            "refs": {
                "pushed": ["p1", "p2"],
                "removed": ["r1", "r2"],
            },
        },
        headers={"Authorization": "token TOKEN"},
        timeout=5,
        allow_redirects=False,
    )


def test_config_to_env():
    assert config_to_env(CONFIG) == ENV


def test_env_to_config():
    assert env_to_config(ENV) == CONFIG




tests/unit/utils/test_utils.py
import os

import pytest

from dvc.utils import dict_sha256, fix_env, parse_target, relpath, resolve_output


@pytest.mark.skipif(os.name == "nt", reason="pyenv-win is not supported")
@pytest.mark.parametrize(
    "path, orig",
    [
        (
            "/pyenv/bin:/pyenv/libexec:/pyenv/plugins/plugin:/orig/path1:/orig/path2",
            "/orig/path1:/orig/path2",
        ),
        (
            "/pyenv/bin:/pyenv/libexec:/orig/path1:/orig/path2",
            "/orig/path1:/orig/path2",
        ),
        (
            "/pyenv/bin:/some/libexec:/pyenv/plugins/plugin:/orig/path1:/orig/path2",
            "/orig/path1:/orig/path2",
        ),
        ("/orig/path1:/orig/path2", "/orig/path1:/orig/path2"),
        (
            "/orig/path1:/orig/path2:/pyenv/bin:/pyenv/libexec",
            "/orig/path1:/orig/path2:/pyenv/bin:/pyenv/libexec",
        ),
    ],
)
def test_fix_env_pyenv(path, orig):
    env = {
        "PATH": path,
        "PYENV_ROOT": "/pyenv",
        "PYENV_VERSION": "3.7.2",
        "PYENV_DIR": "/some/dir",
        "PYENV_HOOK_PATH": "/some/hook/path",
    }
    assert fix_env(env)["PATH"] == orig


@pytest.mark.skipif(os.name != "nt", reason="Windows specific")
def test_relpath_windows(monkeypatch):
    """test that relpath correctly generated when run on a
    windows network share. The drive mapped path is mapped
    to a UNC path by os.path.realpath"""

    def dummy_realpath(path):
        return path.replace("x:", "\\\\server\\share")

    monkeypatch.setattr(os.path, "realpath", dummy_realpath)
    assert (
        relpath("x:\\dir1\\dir2\\file.txt", "\\\\server\\share\\dir1")
        == "dir2\\file.txt"
    )

    assert (
        relpath("y:\\dir1\\dir2\\file.txt", "\\\\server\\share\\dir1")
        == "y:\\dir1\\dir2\\file.txt"
    )


@pytest.mark.parametrize(
    "inp,out,is_dir,expected",
    [
        ["target", None, False, "target"],
        ["target", "dir", True, os.path.join("dir", "target")],
        ["target", "file_target", False, "file_target"],
        [
            "target",
            os.path.join("dir", "subdir"),
            True,
            os.path.join("dir", "subdir", "target"),
        ],
        ["dir/", None, False, "dir"],
        ["dir", None, False, "dir"],
        ["dir", "other_dir", False, "other_dir"],
        ["dir", "other_dir", True, os.path.join("other_dir", "dir")],
    ],
)
def test_resolve_output(inp, out, is_dir, expected, mocker):
    mocker.patch("os.path.isdir", return_value=is_dir)
    result = resolve_output(inp, out)
    assert result == expected


@pytest.mark.parametrize(
    "inp,out, default",
    [
        ["dvc.yaml", ("dvc.yaml", None), None],
        ["dvc.yaml:name", ("dvc.yaml", "name"), None],
        [":name", ("dvc.yaml", "name"), None],
        ["stage.dvc", ("stage.dvc", None), None],
        ["dvc.yaml:name", ("dvc.yaml", "name"), None],
        ["../models/stage.dvc", ("../models/stage.dvc", None), "def"],
        [":name", ("default", "name"), "default"],
        [":name", ("default", "name"), "default"],
        ["something.dvc:name", ("something.dvc", "name"), None],
        ["../something.dvc:name", ("../something.dvc", "name"), None],
        ["file", (None, "file"), None],
        ["build@15", (None, "build@15"), None],
        ["build@{'level': 35}", (None, "build@{'level': 35}"), None],
        [":build@15", ("dvc.yaml", "build@15"), None],
        [":build@{'level': 35}", ("dvc.yaml", "build@{'level': 35}"), None],
        ["dvc.yaml:build@15", ("dvc.yaml", "build@15"), None],
        [
            "dvc.yaml:build@{'level': 35}",
            ("dvc.yaml", "build@{'level': 35}"),
            None,
        ],
        [
            "build2@{'level': [1, 2, 3]}",
            (None, "build2@{'level': [1, 2, 3]}"),
            None,
        ],
        [
            ":build2@{'level': [1, 2, 3]}",
            ("dvc.yaml", "build2@{'level': [1, 2, 3]}"),
            None,
        ],
        [
            "dvc.yaml:build2@{'level': [1, 2, 3]}",
            ("dvc.yaml", "build2@{'level': [1, 2, 3]}"),
            None,
        ],
    ],
)
def test_parse_target(inp, out, default):
    assert parse_target(inp, default) == out


def test_hint_on_lockfile():
    with pytest.raises(Exception, match="Did you mean: `dvc.yaml:name`?") as e:
        assert parse_target("dvc.lock:name")
    assert "dvc.yaml:name" in str(e.value)


@pytest.mark.parametrize(
    "d,sha",
    [
        (
            {
                "cmd": "echo content > out",
                "deps": {"dep": "2254342becceafbd04538e0a38696791"},
                "outs": {"out": "f75b8179e4bbe7e2b4a074dcef62de95"},
            },
            "f472eda60f09660a4750e8b3208cf90b3a3b24e5f42e0371d829710e9464d74a",
        ),
        (
            {
                "cmd": "echo content > out",
                "deps": {"dep": "2254342becceafbd04538e0a38696791"},
                "outs": ["out"],
            },
            "a239b67073bd58affcdb81fff3305d1726c6e7f9c86f3d4fca0e92e8147dc7b0",
        ),
    ],
)
def test_dict_sha256(d, sha):
    assert dict_sha256(d) == sha




tests/unit/utils/serialize/__init__.py




tests/unit/utils/serialize/test_python.py
import pytest

from dvc.utils.serialize import parse_py


@pytest.mark.parametrize(
    "text,result",
    [
        ("BOOL = True", {"BOOL": True}),
        ("INT = 5", {"INT": 5}),
        ("FLOAT = 0.001", {"FLOAT": 0.001}),
        ("STR = 'abc'", {"STR": "abc"}),
        ("DICT = {'a': 1, 'b': 2}", {"DICT": {"a": 1, "b": 2}}),
        ("LIST = [1, 2, 3]", {"LIST": [1, 2, 3]}),
        ("SET = {1, 2, 3}", {"SET": {1, 2, 3}}),
        ("TUPLE = (10, 100)", {"TUPLE": (10, 100)}),
        ("NONE = None", {"NONE": None}),
        ("UNARY_OP = -1", {"UNARY_OP": -1}),
        (
            """class TrainConfig:

            EPOCHS = 70

            def __init__(self):
                self.layers = 5
                self.layers = 9  # TrainConfig.layers param will be 9
                bar = 3  # Will NOT be found since it's locally scoped
            """,
            {"TrainConfig": {"EPOCHS": 70, "layers": 9}},
        ),
    ],
)
def test_parse_valid_types(text, result):
    assert parse_py(text, "foo") == result


@pytest.mark.parametrize(
    "text",
    [
        "CONSTRUCTOR = dict(a=1, b=2)",
        "SUM = 1 + 2",
    ],
)
def test_parse_invalid_types(text):
    assert parse_py(text, "foo") == {}




tests/unit/utils/serialize/test_toml.py
def test_preserve_comments(tmp_dir):
    from dvc.utils.serialize._toml import modify_toml

    contents_fmt = """\
#A Title
[foo]
bar = {} # meaning of life
baz = [1, 2]
"""
    tmp_dir.gen("params.toml", contents_fmt.format("42"))

    with modify_toml("params.toml") as d:
        d["foo"]["bar"] //= 2
    assert (tmp_dir / "params.toml").read_text() == contents_fmt.format("21")


def test_parse_toml_type():
    from tomlkit.toml_document import TOMLDocument

    from dvc.utils.serialize._toml import parse_toml

    contents = "# A Title [foo]\nbar = 42# meaning of life\nbaz = [1, 2]\n"

    parsed = parse_toml(contents, ".")
    assert not isinstance(parsed, TOMLDocument)
    assert isinstance(parsed, dict)


def test_parse_toml_for_update():
    from tomlkit.toml_document import TOMLDocument

    from dvc.utils.serialize._toml import parse_toml_for_update

    contents = "# A Title [foo]\nbar = 42# meaning of life\nbaz = [1, 2]\n"

    parsed = parse_toml_for_update(contents, ".")
    assert isinstance(parsed, TOMLDocument)
    assert isinstance(parsed, dict)




tests/unit/utils/serialize/test_yaml.py
import pytest

from dvc.utils.serialize import (
    EncodingError,
    YAMLFileCorruptedError,
    load_yaml,
    parse_yaml,
)


def test_parse_yaml_duplicate_key_error():
    text = """\
    mykey:
    - foo
    mykey:
    - bar
    """
    with pytest.raises(YAMLFileCorruptedError):
        parse_yaml(text, "mypath")


def test_parse_yaml_invalid_unicode(tmp_dir):
    filename = "invalid_utf8.yaml"
    tmp_dir.gen(filename, b"\x80some: stuff")

    with pytest.raises(EncodingError) as excinfo:
        load_yaml(tmp_dir / filename)

    assert filename in excinfo.value.path
    assert excinfo.value.encoding == "utf-8"




tests/utils/__init__.py
import csv
import os
from contextlib import contextmanager

import pytest
from funcy import first

from dvc.scm import Git

# rewrite assertions in assert, pytest does not rewrite for other modules
# than tests itself.
pytest.register_assert_rewrite("tests.utils.asserts")


def get_gitignore_content():
    with open(Git.GITIGNORE, encoding="utf-8") as gitignore:
        return gitignore.read().splitlines()


@contextmanager
def cd(newdir):
    prevdir = os.getcwd()
    os.chdir(os.path.expanduser(newdir))
    try:
        yield
    finally:
        os.chdir(prevdir)


def to_posixpath(path):
    return path.replace("\\", "/")


def dump_sv(stream, metrics, delimiter=",", header=True):
    if header:
        writer = csv.DictWriter(
            stream, fieldnames=list(first(metrics).keys()), delimiter=delimiter
        )
        writer.writeheader()
        writer.writerows(metrics)
    else:
        writer = csv.writer(stream)
        for d in metrics:
            writer.writerow(list(d.values()))


@contextmanager
def console_width(console, width):
    console_options = console.options
    original = console_options.max_width
    con_width = console._width

    try:
        console_options.max_width = width
        console._width = width
        yield
    finally:
        console_options.max_width = original
        console._width = con_width


class ANY:
    def __init__(self, expected_type):
        self.expected_type = expected_type

    def __repr__(self):
        return "Any" + self.expected_type.__name__.capitalize()

    def __eq__(self, other):
        return isinstance(other, self.expected_type)




tests/utils/asserts.py
from typing import TYPE_CHECKING, Any, Dict
from unittest.mock import ANY

if TYPE_CHECKING:
    from unittest.mock import Mock


def issubset(subset: Dict, superset: Dict) -> bool:
    assert {**superset, **subset} == superset
    return True


def called_once_with_subset(m: "Mock", *args: Any, **kwargs: Any) -> bool:
    m.assert_called_once()
    m_args, m_kwargs = m.call_args

    expected_args = m_args + (ANY,) * (len(m_args) - len(args) - 1)
    expected_kwargs = {k: kwargs.get(k, ANY) for k in m_kwargs}
    m.assert_called_with(*expected_args, **expected_kwargs)

    return True




tests/utils/plots.py
import dpath

dpath.options.ALLOW_EMPTY_STRING_KEYS = True


def get_plot(plots_data, revision, typ="sources", file=None, endkey="data"):
    if file is not None:
        return dpath.get(plots_data, [revision, typ, "data", file, endkey])
    return dpath.get(plots_data, [revision, typ, endkey])
